{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NYC__TaxiFare__XGBoost__STP__dEI.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9wHutsqZUcn"
      },
      "source": [
        "XGBoost Regression - 'real-world' example: NYC Taxi-Fare Predictor\n",
        "\n",
        "GP dEI versus STP nu = 3 dEI (winner)\n",
        "\n",
        "https://www.kaggle.com/c/new-york-city-taxi-fare-prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7PwmXsgZO8D",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "2e3df752-9f99-4f47-a566-eb83e843bc42"
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-9f629550-cd52-46c3-9be1-de11d6b7cb3e\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-9f629550-cd52-46c3-9be1-de11d6b7cb3e\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"conorc2006\",\"key\":\"c5c5a6382a7d50c022aab991694fc17f\"}'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMwbJ6hjZltI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb94293d-500d-46ba-f8c1-d88f9173838b"
      },
      "source": [
        "## Ensure the kaggle.json file is present:\n",
        "!ls -lha kaggle.json"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-rw-r--r-- 1 root root 66 Mar 17 08:52 kaggle.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8Pu-UlWZovH"
      },
      "source": [
        "## Next, install the Kaggle API client:\n",
        "!pip install -q kaggle"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUOQ4SE7Zuj3"
      },
      "source": [
        "## The Kaggle API Client expects this file to be ~/.kaggle\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJcEztjCZxOn"
      },
      "source": [
        "## Permissions' change\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-u4Tmj7ZUD3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd3eab56-994a-4180-b8b3-f0bdd74cc4bb"
      },
      "source": [
        "!kaggle competitions download -c new-york-city-taxi-fare-prediction"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.12 / client 1.5.4)\n",
            "Downloading GCP-Coupons-Instructions.rtf to /content\n",
            "  0% 0.00/486 [00:00<?, ?B/s]\n",
            "100% 486/486 [00:00<00:00, 824kB/s]\n",
            "Downloading sample_submission.csv to /content\n",
            "  0% 0.00/335k [00:00<?, ?B/s]\n",
            "100% 335k/335k [00:00<00:00, 46.7MB/s]\n",
            "Downloading train.csv.zip to /content\n",
            " 99% 1.55G/1.56G [00:20<00:00, 37.2MB/s]\n",
            "100% 1.56G/1.56G [00:20<00:00, 80.2MB/s]\n",
            "Downloading test.csv to /content\n",
            "  0% 0.00/960k [00:00<?, ?B/s]\n",
            "100% 960k/960k [00:00<00:00, 135MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-0Pe1i4Z2R_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e45ba2b-8ede-42f1-b4b4-4760d49c0ab2"
      },
      "source": [
        "!pip install pyGPGO"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyGPGO\n",
            "  Downloading https://files.pythonhosted.org/packages/8d/87/a113c91ba014708114f7635d5c0f6a5e5c773480c5f0a537b257a02d180d/pyGPGO-0.4.0.dev1.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pyGPGO) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from pyGPGO) (1.4.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from pyGPGO) (1.0.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from pyGPGO) (0.22.2.post1)\n",
            "Requirement already satisfied: theano in /usr/local/lib/python3.7/dist-packages (from pyGPGO) (1.0.5)\n",
            "Requirement already satisfied: pyMC3 in /usr/local/lib/python3.7/dist-packages (from pyGPGO) (3.7)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from theano->pyGPGO) (1.15.0)\n",
            "Requirement already satisfied: h5py>=2.7.0 in /usr/local/lib/python3.7/dist-packages (from pyMC3->pyGPGO) (2.10.0)\n",
            "Requirement already satisfied: pandas>=0.18.0 in /usr/local/lib/python3.7/dist-packages (from pyMC3->pyGPGO) (1.1.5)\n",
            "Requirement already satisfied: patsy>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from pyMC3->pyGPGO) (0.5.1)\n",
            "Requirement already satisfied: tqdm>=4.8.4 in /usr/local/lib/python3.7/dist-packages (from pyMC3->pyGPGO) (4.41.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.18.0->pyMC3->pyGPGO) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.18.0->pyMC3->pyGPGO) (2018.9)\n",
            "Building wheels for collected packages: pyGPGO\n",
            "  Building wheel for pyGPGO (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyGPGO: filename=pyGPGO-0.4.0.dev1-cp37-none-any.whl size=19866 sha256=a8266237f22aa3785a1474725b09a6a5c6524bb6dccd07808dd5f49b02654aa0\n",
            "  Stored in directory: /root/.cache/pip/wheels/d1/27/04/c4fa3bfe194d36e3cd51542132f43415a6813114a5e8301acb\n",
            "Successfully built pyGPGO\n",
            "Installing collected packages: pyGPGO\n",
            "Successfully installed pyGPGO-0.4.0.dev1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7zDTf1naBsH"
      },
      "source": [
        "# Load some default Python modules:\n",
        "\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "import pandas as pd\n",
        "import pymc3 as pm\n",
        "import matplotlib.pyplot as plt\n",
        "import xgboost as xgb\n",
        "import time\n",
        "\n",
        "from matplotlib.pyplot import rc\n",
        "rc('font',**{'family':'sans-serif','sans-serif':['Helvetica']})\n",
        "rc('text', usetex=False)\n",
        "### % matplotlib inline\n",
        "import seaborn as sns\n",
        "plt.style.use('seaborn-whitegrid')\n",
        "\n",
        "from collections import OrderedDict\n",
        "from joblib import Parallel, delayed\n",
        "from numpy.linalg import slogdet, inv, cholesky, solve\n",
        "from scipy.optimize import minimize\n",
        "from scipy.spatial.distance import cdist\n",
        "from scipy.special import gamma\n",
        "from scipy.stats import norm, t\n",
        "from joblib import Parallel, delayed\n",
        "\n",
        "from pyGPGO.logger import EventLogger\n",
        "from pyGPGO.GPGO import GPGO\n",
        "from pyGPGO.surrogates.GaussianProcess import GaussianProcess\n",
        "from pyGPGO.surrogates.tStudentProcess import tStudentProcess\n",
        "from pyGPGO.surrogates.tStudentProcess import logpdf\n",
        "from pyGPGO.acquisition import Acquisition\n",
        "from pyGPGO.covfunc import squaredExponential\n",
        "from sklearn.model_selection import cross_val_score, train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from xgboost import XGBRegressor\n",
        "from pandas_datareader import data\n",
        "\n",
        "import warnings\n",
        "import random\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXicekJhaE0P"
      },
      "source": [
        "# Read data in pandas dataframe:\n",
        "\n",
        "df_train =  pd.read_csv('/content/train.csv.zip', nrows = 1_000_000, parse_dates=[\"pickup_datetime\"])"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQ0mDzt_cBmw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "399d9ba5-f4e4-4cbb-c06f-b829d74f92ec"
      },
      "source": [
        "# List first rows:\n",
        "\n",
        "df_train.head()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>key</th>\n",
              "      <th>fare_amount</th>\n",
              "      <th>pickup_datetime</th>\n",
              "      <th>pickup_longitude</th>\n",
              "      <th>pickup_latitude</th>\n",
              "      <th>dropoff_longitude</th>\n",
              "      <th>dropoff_latitude</th>\n",
              "      <th>passenger_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2009-06-15 17:26:21.0000001</td>\n",
              "      <td>4.5</td>\n",
              "      <td>2009-06-15 17:26:21+00:00</td>\n",
              "      <td>-73.844311</td>\n",
              "      <td>40.721319</td>\n",
              "      <td>-73.841610</td>\n",
              "      <td>40.712278</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2010-01-05 16:52:16.0000002</td>\n",
              "      <td>16.9</td>\n",
              "      <td>2010-01-05 16:52:16+00:00</td>\n",
              "      <td>-74.016048</td>\n",
              "      <td>40.711303</td>\n",
              "      <td>-73.979268</td>\n",
              "      <td>40.782004</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2011-08-18 00:35:00.00000049</td>\n",
              "      <td>5.7</td>\n",
              "      <td>2011-08-18 00:35:00+00:00</td>\n",
              "      <td>-73.982738</td>\n",
              "      <td>40.761270</td>\n",
              "      <td>-73.991242</td>\n",
              "      <td>40.750562</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2012-04-21 04:30:42.0000001</td>\n",
              "      <td>7.7</td>\n",
              "      <td>2012-04-21 04:30:42+00:00</td>\n",
              "      <td>-73.987130</td>\n",
              "      <td>40.733143</td>\n",
              "      <td>-73.991567</td>\n",
              "      <td>40.758092</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2010-03-09 07:51:00.000000135</td>\n",
              "      <td>5.3</td>\n",
              "      <td>2010-03-09 07:51:00+00:00</td>\n",
              "      <td>-73.968095</td>\n",
              "      <td>40.768008</td>\n",
              "      <td>-73.956655</td>\n",
              "      <td>40.783762</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                             key  ...  passenger_count\n",
              "0    2009-06-15 17:26:21.0000001  ...                1\n",
              "1    2010-01-05 16:52:16.0000002  ...                1\n",
              "2   2011-08-18 00:35:00.00000049  ...                2\n",
              "3    2012-04-21 04:30:42.0000001  ...                1\n",
              "4  2010-03-09 07:51:00.000000135  ...                1\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9fZujMycFMo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "442d115f-896e-421f-d657-c4a65a5e808d"
      },
      "source": [
        "# Format 'pickup_datetime' variable:\n",
        "\n",
        "df_train['pickup_datetime'] =  pd.to_datetime(df_train['pickup_datetime'], utc=True, format='%Y-%m-%d %H:%M')\n",
        "df_train['pickup_datetime'].head()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0   2009-06-15 17:26:21+00:00\n",
              "1   2010-01-05 16:52:16+00:00\n",
              "2   2011-08-18 00:35:00+00:00\n",
              "3   2012-04-21 04:30:42+00:00\n",
              "4   2010-03-09 07:51:00+00:00\n",
              "Name: pickup_datetime, dtype: datetime64[ns, UTC]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nReKu62HcVFI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "e389b7f0-dbf2-4ab0-8f2a-ea3fabf463e6"
      },
      "source": [
        "df_train.sort_values(by = 'pickup_datetime').tail() ### June 2015 the final month"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>key</th>\n",
              "      <th>fare_amount</th>\n",
              "      <th>pickup_datetime</th>\n",
              "      <th>pickup_longitude</th>\n",
              "      <th>pickup_latitude</th>\n",
              "      <th>dropoff_longitude</th>\n",
              "      <th>dropoff_latitude</th>\n",
              "      <th>passenger_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>286276</th>\n",
              "      <td>2015-06-30 23:38:21.0000003</td>\n",
              "      <td>26.5</td>\n",
              "      <td>2015-06-30 23:38:21+00:00</td>\n",
              "      <td>-74.008385</td>\n",
              "      <td>40.711571</td>\n",
              "      <td>-73.884071</td>\n",
              "      <td>40.737385</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>955575</th>\n",
              "      <td>2015-06-30 23:45:57.0000003</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2015-06-30 23:45:57+00:00</td>\n",
              "      <td>-74.002342</td>\n",
              "      <td>40.739819</td>\n",
              "      <td>-74.005829</td>\n",
              "      <td>40.745239</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>915826</th>\n",
              "      <td>2015-06-30 23:48:35.0000005</td>\n",
              "      <td>30.5</td>\n",
              "      <td>2015-06-30 23:48:35+00:00</td>\n",
              "      <td>-73.983826</td>\n",
              "      <td>40.729546</td>\n",
              "      <td>-73.927917</td>\n",
              "      <td>40.661186</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>751350</th>\n",
              "      <td>2015-06-30 23:53:23.0000002</td>\n",
              "      <td>3.5</td>\n",
              "      <td>2015-06-30 23:53:23+00:00</td>\n",
              "      <td>-73.978020</td>\n",
              "      <td>40.757439</td>\n",
              "      <td>-73.980705</td>\n",
              "      <td>40.753544</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>785182</th>\n",
              "      <td>2015-06-30 23:53:49.0000003</td>\n",
              "      <td>7.5</td>\n",
              "      <td>2015-06-30 23:53:49+00:00</td>\n",
              "      <td>-73.959969</td>\n",
              "      <td>40.762405</td>\n",
              "      <td>-73.953064</td>\n",
              "      <td>40.782688</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                key  ...  passenger_count\n",
              "286276  2015-06-30 23:38:21.0000003  ...                5\n",
              "955575  2015-06-30 23:45:57.0000003  ...                1\n",
              "915826  2015-06-30 23:48:35.0000005  ...                2\n",
              "751350  2015-06-30 23:53:23.0000002  ...                1\n",
              "785182  2015-06-30 23:53:49.0000003  ...                1\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W9j9LnIfcXcX"
      },
      "source": [
        "# Add time variables:\n",
        "\n",
        "df_train['hour'] = df_train['pickup_datetime'].dt.hour\n",
        "df_train['weekday'] = df_train['pickup_datetime'].dt.weekday\n",
        "df_train['month'] = df_train['pickup_datetime'].dt.month\n",
        "df_train['year'] = df_train['pickup_datetime'].dt.year"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVyFZIVIcaj3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "43a2820b-4d62-4549-a010-df6416c6a8e4"
      },
      "source": [
        "df_train = df_train.drop(['pickup_datetime','key'], axis = 1)\n",
        "df_train.head()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>fare_amount</th>\n",
              "      <th>pickup_longitude</th>\n",
              "      <th>pickup_latitude</th>\n",
              "      <th>dropoff_longitude</th>\n",
              "      <th>dropoff_latitude</th>\n",
              "      <th>passenger_count</th>\n",
              "      <th>hour</th>\n",
              "      <th>weekday</th>\n",
              "      <th>month</th>\n",
              "      <th>year</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4.5</td>\n",
              "      <td>-73.844311</td>\n",
              "      <td>40.721319</td>\n",
              "      <td>-73.841610</td>\n",
              "      <td>40.712278</td>\n",
              "      <td>1</td>\n",
              "      <td>17</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>2009</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>16.9</td>\n",
              "      <td>-74.016048</td>\n",
              "      <td>40.711303</td>\n",
              "      <td>-73.979268</td>\n",
              "      <td>40.782004</td>\n",
              "      <td>1</td>\n",
              "      <td>16</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2010</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5.7</td>\n",
              "      <td>-73.982738</td>\n",
              "      <td>40.761270</td>\n",
              "      <td>-73.991242</td>\n",
              "      <td>40.750562</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>2011</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>7.7</td>\n",
              "      <td>-73.987130</td>\n",
              "      <td>40.733143</td>\n",
              "      <td>-73.991567</td>\n",
              "      <td>40.758092</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>2012</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5.3</td>\n",
              "      <td>-73.968095</td>\n",
              "      <td>40.768008</td>\n",
              "      <td>-73.956655</td>\n",
              "      <td>40.783762</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2010</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   fare_amount  pickup_longitude  pickup_latitude  ...  weekday  month  year\n",
              "0          4.5        -73.844311        40.721319  ...        0      6  2009\n",
              "1         16.9        -74.016048        40.711303  ...        1      1  2010\n",
              "2          5.7        -73.982738        40.761270  ...        3      8  2011\n",
              "3          7.7        -73.987130        40.733143  ...        5      4  2012\n",
              "4          5.3        -73.968095        40.768008  ...        1      3  2010\n",
              "\n",
              "[5 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVfm-KSqcdVY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d89dff0-1667-4149-e168-6c2f64338176"
      },
      "source": [
        "# Remove negative fares and postive outliers:\n",
        "\n",
        "df_train = df_train[df_train.fare_amount>=0]\n",
        "df_train = df_train[df_train.fare_amount<=60]\n",
        "print('New size: %d' % len(df_train))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "New size: 997297\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTVDAD2KchTv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6fab724-fbf2-4632-d1f7-52fc4ade4804"
      },
      "source": [
        "# Remove missing data:\n",
        "\n",
        "df_train = df_train.dropna(how = 'any', axis = 'rows')\n",
        "print('New size: %d' % len(df_train))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "New size: 997288\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUYksJ2cclVQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbeba487-641d-4fca-d526-07925ec64a66"
      },
      "source": [
        "# June 2015 NYC taxi data (Wu et al, 2017):\n",
        "\n",
        "df_train = df_train[df_train.month==6]\n",
        "df_train = df_train[df_train.year==2015]\n",
        "print('New size: %d' % len(df_train))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "New size: 11269\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXgSHPyYcnuv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "outputId": "e57bd610-b511-475a-a7d8-2b7fd139134f"
      },
      "source": [
        "# Histogram fare plot:\n",
        "\n",
        "df_train[df_train.fare_amount<15].fare_amount.hist(bins=100, figsize=(16,5), color = \"red\")\n",
        "plt.xlabel('$ US Dollars', weight = 'bold', family = 'Arial')\n",
        "plt.title('June 2015 Fares', weight = 'bold', family = 'Arial')\n",
        "plt.grid(b=None)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "findfont: Font family ['Arial'] not found. Falling back to DejaVu Sans.\n",
            "findfont: Font family ['Arial'] not found. Falling back to DejaVu Sans.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1152x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9TMSdAAjcr4o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "992ba784-1a5d-41a5-d3f1-64266f6afa14"
      },
      "source": [
        "y = df_train.fare_amount.values + 1e-10\n",
        "y ### for supervised learning: output vector y"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([22.54,  8.  , 34.  , ...,  4.5 ,  6.5 ,  7.  ])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FOeHvi3cu1n",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "edc6898a-f21f-49ea-c278-47f622134c31"
      },
      "source": [
        "# List first rows (post-cleaning):\n",
        "\n",
        "df_train.head()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>fare_amount</th>\n",
              "      <th>pickup_longitude</th>\n",
              "      <th>pickup_latitude</th>\n",
              "      <th>dropoff_longitude</th>\n",
              "      <th>dropoff_latitude</th>\n",
              "      <th>passenger_count</th>\n",
              "      <th>hour</th>\n",
              "      <th>weekday</th>\n",
              "      <th>month</th>\n",
              "      <th>year</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>22.54</td>\n",
              "      <td>-74.010483</td>\n",
              "      <td>40.717667</td>\n",
              "      <td>-73.985771</td>\n",
              "      <td>40.660366</td>\n",
              "      <td>1</td>\n",
              "      <td>21</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>310</th>\n",
              "      <td>8.00</td>\n",
              "      <td>-74.010727</td>\n",
              "      <td>40.710091</td>\n",
              "      <td>-73.998100</td>\n",
              "      <td>40.722900</td>\n",
              "      <td>1</td>\n",
              "      <td>9</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>314</th>\n",
              "      <td>34.00</td>\n",
              "      <td>-73.974899</td>\n",
              "      <td>40.751095</td>\n",
              "      <td>-73.908546</td>\n",
              "      <td>40.881878</td>\n",
              "      <td>0</td>\n",
              "      <td>23</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>321</th>\n",
              "      <td>8.00</td>\n",
              "      <td>-73.961784</td>\n",
              "      <td>40.759579</td>\n",
              "      <td>-73.978943</td>\n",
              "      <td>40.772606</td>\n",
              "      <td>4</td>\n",
              "      <td>21</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>486</th>\n",
              "      <td>11.50</td>\n",
              "      <td>-73.957443</td>\n",
              "      <td>40.761703</td>\n",
              "      <td>-73.973236</td>\n",
              "      <td>40.787079</td>\n",
              "      <td>1</td>\n",
              "      <td>19</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     fare_amount  pickup_longitude  pickup_latitude  ...  weekday  month  year\n",
              "31         22.54        -74.010483        40.717667  ...        6      6  2015\n",
              "310         8.00        -74.010727        40.710091  ...        5      6  2015\n",
              "314        34.00        -73.974899        40.751095  ...        1      6  2015\n",
              "321         8.00        -73.961784        40.759579  ...        0      6  2015\n",
              "486        11.50        -73.957443        40.761703  ...        0      6  2015\n",
              "\n",
              "[5 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-lT9BBicw4P",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "08fb6ddf-cb87-4ced-b77f-ca954b9eb5f0"
      },
      "source": [
        "X = df_train.drop(['fare_amount', 'month', 'year'], axis = 1)\n",
        "X.head() ### for supervised learning: input matrix X"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pickup_longitude</th>\n",
              "      <th>pickup_latitude</th>\n",
              "      <th>dropoff_longitude</th>\n",
              "      <th>dropoff_latitude</th>\n",
              "      <th>passenger_count</th>\n",
              "      <th>hour</th>\n",
              "      <th>weekday</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>-74.010483</td>\n",
              "      <td>40.717667</td>\n",
              "      <td>-73.985771</td>\n",
              "      <td>40.660366</td>\n",
              "      <td>1</td>\n",
              "      <td>21</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>310</th>\n",
              "      <td>-74.010727</td>\n",
              "      <td>40.710091</td>\n",
              "      <td>-73.998100</td>\n",
              "      <td>40.722900</td>\n",
              "      <td>1</td>\n",
              "      <td>9</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>314</th>\n",
              "      <td>-73.974899</td>\n",
              "      <td>40.751095</td>\n",
              "      <td>-73.908546</td>\n",
              "      <td>40.881878</td>\n",
              "      <td>0</td>\n",
              "      <td>23</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>321</th>\n",
              "      <td>-73.961784</td>\n",
              "      <td>40.759579</td>\n",
              "      <td>-73.978943</td>\n",
              "      <td>40.772606</td>\n",
              "      <td>4</td>\n",
              "      <td>21</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>486</th>\n",
              "      <td>-73.957443</td>\n",
              "      <td>40.761703</td>\n",
              "      <td>-73.973236</td>\n",
              "      <td>40.787079</td>\n",
              "      <td>1</td>\n",
              "      <td>19</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     pickup_longitude  pickup_latitude  ...  hour  weekday\n",
              "31         -74.010483        40.717667  ...    21        6\n",
              "310        -74.010727        40.710091  ...     9        5\n",
              "314        -73.974899        40.751095  ...    23        1\n",
              "321        -73.961784        40.759579  ...    21        0\n",
              "486        -73.957443        40.761703  ...    19        0\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2eC8SDPzczNY"
      },
      "source": [
        "### Optimum rmse: regression model objective function is Root Mean Square Error (RMSE); \n",
        "### Should be minimized (as close to zero as possible):\n",
        "\n",
        "y_global_orig = 0"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GoTmWEhSc1qQ"
      },
      "source": [
        "### Bayesian Optimization - inputs:\n",
        "\n",
        "obj_func = 'XGBoost'\n",
        "n_start_AcqFunc = 100\n",
        "n_test = n_start_AcqFunc # test points\n",
        "df = 3 # nu\n",
        "\n",
        "util_loser = 'dEI_GP'\n",
        "util_winner = 'dEI_STP'\n",
        "n_init = 5 # random initialisations\n",
        "\n",
        "test_perc = 0.15\n",
        "train_perc = 1 - test_perc\n",
        "\n",
        "n_test = int(len(df_train) * test_perc)\n",
        "n_train = int(len(df_train) - n_test)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2ngnRxbc7cg"
      },
      "source": [
        "### Objective function:\n",
        "\n",
        "if obj_func == 'XGBoost': # 6-D\n",
        "            \n",
        "    # Constraints:\n",
        "    param_lb_alpha = 0\n",
        "    param_ub_alpha = 10\n",
        "    \n",
        "    param_lb_gamma = 0\n",
        "    param_ub_gamma = 10\n",
        "    \n",
        "    param_lb_max_depth = 5\n",
        "    param_ub_max_depth = 15\n",
        "    \n",
        "    param_lb_min_child_weight = 1\n",
        "    param_ub_min_child_weight = 20\n",
        "    \n",
        "    param_lb_subsample = .5\n",
        "    param_ub_subsample = 1\n",
        "    \n",
        "    param_lb_colsample = .1\n",
        "    param_ub_colsample = 1\n",
        "    \n",
        "    # 6-D inputs' parameter bounds:\n",
        "    param = { 'alpha':  ('cont', (param_lb_alpha, param_ub_alpha)),\n",
        "         'gamma':  ('cont', (param_lb_gamma, param_ub_gamma)),     \n",
        "         'max_depth':  ('int', (param_lb_max_depth, param_ub_max_depth)),\n",
        "         'subsample':  ('cont', (param_lb_subsample, param_ub_subsample)),\n",
        "          'min_child_weight':  ('int', (param_lb_min_child_weight, param_ub_min_child_weight)),\n",
        "            'colsample': ('cont', (param_lb_colsample, param_ub_colsample))\n",
        "        }\n",
        "       \n",
        "    # True y bounds:\n",
        "    dim = 6\n",
        "    \n",
        "    max_iter = 20  # iterations of Bayesian optimization\n",
        "    \n",
        "    operator = 1 \n",
        "    \n",
        "    n_est = 3"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmJsNX29c_xA"
      },
      "source": [
        "### Derivatives - Squared-exponential covariance function:\n",
        "\n",
        "def l2norm_(X, Xstar):\n",
        "    \n",
        "    return cdist(X, Xstar)\n",
        "\n",
        "def kronDelta(X, Xstar):\n",
        "\n",
        "    return cdist(X, Xstar) < np.finfo(np.float32).eps\n",
        "\n",
        "class squaredExponentialDeriv(squaredExponential):\n",
        "    \n",
        "    def K(self, X, Xstar):\n",
        "        \n",
        "        r = (l2norm_(X, Xstar)/self.l)\n",
        "        K = self.sigmaf * np.exp(-1/2*r **2) + self.sigman * kronDelta(X, Xstar)\n",
        "        return K\n",
        "    \n",
        "    def dK(self, X, Xstar):\n",
        "        \n",
        "        r = (l2norm_(X, Xstar)/self.l)\n",
        "        dK = self.sigmaf/self.l**2 * np.exp(-1/2 * r **2) * l2norm_(X, Xstar)\n",
        "        return dK\n",
        "    \n",
        "        \n",
        "    def d2K(self, X, Xstar):\n",
        "        \n",
        "        r = (l2norm_(X, Xstar)/self.l)\n",
        "        d2K = self.sigmaf/self.l**2 * np.exp(-1/2 * r **2) * (1 - r **2)\n",
        "        return d2K\n",
        "    \n",
        "cov_func = squaredExponentialDeriv()\n",
        "d_cov_func = squaredExponentialDeriv()\n",
        "    "
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9ZuEB2VdE0W"
      },
      "source": [
        "### Set-seeds:\n",
        "\n",
        "run_num_1 = 111\n",
        "run_num_2 = 113\n",
        "run_num_3 = 3333\n",
        "run_num_4 = 4444\n",
        "run_num_5 = 5555\n",
        "run_num_6 = 6\n",
        "run_num_7 = 7777\n",
        "run_num_8 = 8878\n",
        "run_num_9 = 999\n",
        "run_num_10 = 1000\n",
        "run_num_11 = 1113\n",
        "run_num_12 = 1234\n",
        "run_num_13 = 234\n",
        "run_num_14 = 888\n",
        "run_num_15 = 1557\n",
        "run_num_16 = 1666\n",
        "run_num_17 = 71\n",
        "run_num_18 = 8\n",
        "run_num_19 = 1999\n",
        "run_num_20 = 2000"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XgHMFEyPdCk4"
      },
      "source": [
        "### Cumulative Regret Calculator:\n",
        "\n",
        "def min_max_array(x):\n",
        "    new_list = []\n",
        "    for i, num in enumerate(x):\n",
        "            new_list.append(np.min(x[0:i+1]))\n",
        "    return new_list"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJMhL70fdHz_"
      },
      "source": [
        "class Acquisition_new(Acquisition):    \n",
        "    def __init__(self, mode, eps=1e-08, **params):\n",
        "        \n",
        "        self.params = params\n",
        "        self.eps = eps\n",
        "\n",
        "        mode_dict = {\n",
        "            'dEI_GP': self.dEI_GP,\n",
        "            'dEI_STP': self.dEI_STP\n",
        "        }\n",
        "\n",
        "        self.f = mode_dict[mode]\n",
        "    \n",
        "    def dEI_GP(self, tau, mean, std, ds, dm, dvdv, d2v, d2m):\n",
        "        z = -1 * (tau - mean - self.eps) / (std + self.eps)\n",
        "        \n",
        "        dsdx = ds / 2 * (std + self.eps)\n",
        "        d2sdx = -dsdx**2 / ((std + self.eps)) - dvdv / (std + self.eps) - d2v / (std + self.eps)\n",
        "        dmdx = (dm - z * dsdx) / (std + self.eps)\n",
        "        d2mdx = (d2m - (z * d2sdx + 2 * dmdx * dsdx)) / (std + self.eps)\n",
        "        \n",
        "        f = (std + self.eps) * (z * norm.cdf(z) + norm.pdf(z)[0])\n",
        "        df = (f / (std + self.eps) * dsdx + (std + self.eps) * norm.cdf(z) * dmdx)\n",
        "        d2f = (f / (std + self.eps) * d2sdx + dsdx * dmdx * norm.cdf(z) \\\n",
        "            + d2mdx * (std + self.eps) * norm.cdf(z) + dsdx * norm.cdf(z) * dmdx \\\n",
        "            + norm.pdf(z)[0] * (std + self.eps) * dmdx)\n",
        "            \n",
        "        return f, df, d2f\n",
        "\n",
        "    def dEI_STP(self, tau, mean, std, ds, dm, dvdv, d2v, d2m, nu=3.0):\n",
        "    \n",
        "        gamma = -1 * (tau - mean - self.eps) / (std + self.eps)\n",
        "        dsdx = ds / 2 * (std + self.eps)\n",
        "        dmdx = (dm - gamma * dsdx) / (std + self.eps)\n",
        "        f = (std + self.eps) * (gamma * t.cdf(gamma, df=nu) + (nu + gamma ** 2)/(nu - 1) * t.pdf(gamma, df=nu))\n",
        "        df = (gamma * t.cdf(gamma, df=nu) + (nu + gamma ** 2)/(nu - 1) * t.pdf(gamma,df=nu)[0]) * dsdx \\\n",
        "             + (std + self.eps) * (t.cdf(gamma,df=nu) * dmdx + gamma * t.pdf(gamma, df=nu) * \\\n",
        "             (1 - (nu + gamma ** 2)/(nu - 1) + 2/(nu - 1) * dmdx))\n",
        "        return f, df\n",
        "    \n",
        "    def _eval(self, tau, mean, std):\n",
        "    \n",
        "        return self.f(tau, mean, std, **self.params)\n",
        "    \n",
        "    def d_eval(self, tau, mean, std, ds, dm, dvdv, d2v, d2m):\n",
        "    \n",
        "        return self.f(tau, mean, std, ds, dm, dvdv, d2v, d2m, **self.params)\n",
        "\n",
        "    def d_eval_stp(self, tau, mean, std, ds, dm, dvdv, d2v, d2m, nu=3.0):\n",
        "    \n",
        "        return self.f(tau, mean, std, ds, dm, dvdv, d2v, d2m, nu=3.0, **self.params)\n"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "comHkJv9dH_O"
      },
      "source": [
        "### Surrogate derivatives: \n",
        "\n",
        "from scipy.linalg import cholesky, solve\n",
        "\n",
        "class dGaussianProcess(GaussianProcess):\n",
        "    l = 1\n",
        "    sigmaf = 1\n",
        "    sigman = 1e-6\n",
        "\n",
        "    def AcqGrad(self, Xstar, return_std=False):\n",
        "        r_X = l2norm_(self.X, self.X)/self.l\n",
        "        K = self.sigmaf * np.exp(-1/2*r_X **2) + self.sigman * kronDelta(self.X, self.X)\n",
        "        L = cholesky(K).T\n",
        "        alpha = solve(L.T, solve(L, self.y))\n",
        "        Xstar = np.atleast_2d(Xstar)\n",
        "        Kstar = squaredExponentialDeriv.K(self, self.X, Xstar).T\n",
        "        dKstar = squaredExponentialDeriv.dK(self, self.X, Xstar).T\n",
        "        d2Kstar = squaredExponentialDeriv.d2K(self, self.X, Xstar).T\n",
        "        v = solve(self.L, Kstar.T)\n",
        "        dv = solve(self.L, dKstar.T)\n",
        "        d2v = solve(self.L, d2Kstar.T)\n",
        "        \n",
        "        ds = -2 * np.dot(dv.T, v)\n",
        "        dvdv = np.dot(dv.T, dv)\n",
        "        d2s = -2 * (dvdv + d2v)\n",
        "        \n",
        "        dm = np.dot(dKstar, alpha)\n",
        "        d2m = np.dot(d2Kstar, alpha)\n",
        "        return ds, dm, dvdv, d2v, d2m\n",
        "\n",
        "class dtStudentProcess(tStudentProcess):\n",
        "    l = 1\n",
        "    sigmaf = 1\n",
        "    sigman = 1e-6\n",
        "    \n",
        "    def AcqGrad(self, Xstar, return_std=False):\n",
        "        r_X = l2norm_(self.X, self.X)/self.l\n",
        "        K = self.sigmaf * np.exp(-1/2*r_X **2) + self.sigman * kronDelta(self.X, self.X)\n",
        "        L = cholesky(K).T\n",
        "        alpha = solve(L.T, solve(L, self.y))\n",
        "        Xstar = np.atleast_2d(Xstar)\n",
        "        Kstar = squaredExponentialDeriv.K(self, self.X, Xstar).T\n",
        "        dKstar = squaredExponentialDeriv.dK(self, self.X, Xstar).T\n",
        "        d2Kstar = squaredExponentialDeriv.d2K(self, self.X, Xstar).T\n",
        "        v = solve(L, Kstar.T)\n",
        "        dv = solve(L, dKstar.T)\n",
        "        d2v = solve(L, d2Kstar.T)\n",
        "\n",
        "        smd_adj = (self.nu + self.beta1 - 2) / (self.nu + self.n1 - 2)\n",
        "        \n",
        "        ds = -2 * smd_adj * np.dot(dv.T, v)\n",
        "        dvdv = np.dot(dv.T, dv)\n",
        "        d2s = -2 * smd_adj * (dvdv + d2v)\n",
        "        \n",
        "        dm = np.dot(dKstar, alpha)\n",
        "        d2m = np.dot(d2Kstar, alpha)\n",
        "        return ds, dm, dvdv, d2v, d2m"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S422jNLsdIMm"
      },
      "source": [
        "class dGPGO(GPGO):  \n",
        "    n_start = n_start_AcqFunc\n",
        "    eps = 1e-08\n",
        "        \n",
        "    def func(self, xnew):\n",
        "        new_mean, new_var = self.GP.predict(xnew, return_std=True)\n",
        "        new_std = np.sqrt(new_var + 1e-6)\n",
        "        ds, dm, dvdv, d2v, d2m = self.GP.AcqGrad(xnew, return_std=True)\n",
        "        f  = np.empty((self.n_start,))\n",
        "        df = np.empty((self.n_start,))\n",
        "        f  = -self.A.d_eval(self.tau, new_mean, new_std, ds=ds, dm=dm, dvdv=dvdv, d2v=d2v, d2m=d2m)[0]\n",
        "        df = -self.A.d_eval(self.tau, new_mean, new_std, ds=ds, dm=dm, dvdv=dvdv, d2v=d2v, d2m=d2m)[1]\n",
        "        df_array = np.full((len(xnew),),df)\n",
        "        return f, df_array\n",
        "        \n",
        "    def d_optimizeAcq(self, method='L-BFGS-B', n_start=n_start):\n",
        "        start_points_dict = [self._sampleParam() for i in range(n_start)]\n",
        "        start_points_arr = np.array([list(s.values())\n",
        "                                     for s in start_points_dict])\n",
        "        x_best = np.empty((n_start, len(self.parameter_key)))\n",
        "        f_best = np.empty((n_start,))\n",
        "        opt = Parallel(n_jobs=self.n_jobs)(delayed(minimize)(self.func,\n",
        "                                                                 x0=start_point,\n",
        "                                                                 method=method,\n",
        "                                                                 jac = True,\n",
        "                                                                 bounds=self.parameter_range) for start_point in\n",
        "                                               start_points_arr)\n",
        "        x_best = np.array([res.x for res in opt])\n",
        "        f_best = np.array([np.atleast_1d(res.fun)[0] for res in opt])\n",
        "\n",
        "        self.x_best = x_best\n",
        "        self.f_best = f_best\n",
        "        self.best = x_best[np.argmin(f_best)]\n",
        "    \n",
        "    def run(self, max_iter=10, init_evals=3, resume=False):\n",
        "        \n",
        "        if not resume:\n",
        "            self.init_evals = init_evals\n",
        "            self._firstRun(self.init_evals)\n",
        "            self.logger._printInit(self)\n",
        "        for iteration in range(max_iter):\n",
        "            self.d_optimizeAcq()\n",
        "            self.updateGP()\n",
        "            self.logger._printCurrent(self)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwEwZD0qdIPO"
      },
      "source": [
        "## dGPGO_stp - BayesOpt derivatives' class: Student's-t\n",
        "\n",
        "class dGPGO_stp(GPGO):  \n",
        "    n_start = 100\n",
        "        \n",
        "    def func_stp(self, xnew):\n",
        "        new_mean, new_var = self.GP.predict(xnew, return_std=True)\n",
        "        new_std = np.sqrt(new_var + 1e-6)\n",
        "        ds, dm, dvdv, d2v, d2m = self.GP.AcqGrad(xnew, return_std=True)\n",
        "        f  = np.empty((self.n_start,))\n",
        "        df = np.empty((self.n_start,))\n",
        "        f  = -self.A.d_eval_stp(self.tau, new_mean, new_std, ds=ds, dm=dm, dvdv=dvdv, d2v=d2v, d2m=d2m, nu=df)[0]\n",
        "        df = -self.A.d_eval_stp(self.tau, new_mean, new_std, ds=ds, dm=dm, dvdv=dvdv, d2v=d2v, d2m=d2m, nu=df)[1]\n",
        "        df_array = np.full((len(xnew),),df)\n",
        "        return f, df_array\n",
        "        \n",
        "    def d_optimizeAcq_stp(self, method='L-BFGS-B', n_start=n_start):\n",
        "        start_points_dict = [self._sampleParam() for i in range(n_start)]\n",
        "        start_points_arr = np.array([list(s.values())\n",
        "                                     for s in start_points_dict])\n",
        "        x_best = np.empty((n_start, len(self.parameter_key)))\n",
        "        f_best = np.empty((n_start,))\n",
        "        opt = Parallel(n_jobs=self.n_jobs)(delayed(minimize)(self.func_stp,\n",
        "                                                                 x0=start_point,\n",
        "                                                                 method=method,\n",
        "                                                                 jac = True,\n",
        "                                                                 bounds=self.parameter_range) for start_point in\n",
        "                                               start_points_arr)\n",
        "        x_best = np.array([res.x for res in opt])\n",
        "        f_best = np.array([np.atleast_1d(res.fun)[0] for res in opt])\n",
        "\n",
        "        self.x_best = x_best\n",
        "        self.f_best = f_best\n",
        "        self.best = x_best[np.argmin(f_best)]\n",
        "    \n",
        "    def run(self, max_iter=10, init_evals=3, resume=False):\n",
        "        \n",
        "        if not resume:\n",
        "            self.init_evals = init_evals\n",
        "            self._firstRun(self.init_evals)\n",
        "            self.logger._printInit(self)\n",
        "        for iteration in range(max_iter):\n",
        "            self.d_optimizeAcq_stp()\n",
        "            self.updateGP()\n",
        "            self.logger._printCurrent(self)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HlilveEgdIR_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1032e80a-476a-42a1-e452-c82478c349c0"
      },
      "source": [
        "start_lose = time.time()\n",
        "start_lose"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1615971335.121318"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wlzDSHbUG-c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a637700-d91d-4234-b6de-5f81df54bc10"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 1\n",
        "\n",
        "np.random.seed(run_num_1)\n",
        "surrogate_loser_1 = dGaussianProcess(cov_func)\n",
        "\n",
        "X_train1, X_test1, y_train1, y_test1 = train_test_split(X, y, test_size=test_perc, random_state=run_num_1)\n",
        "\n",
        "def f_syn_polarity1(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_1, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train1, y=y_train1).mean())\n",
        "    return operator * score\n",
        "\n",
        "loser_1 = dGPGO(surrogate_loser_1, Acquisition_new(util_loser), f_syn_polarity1, param, n_jobs = -1) # define BayesOpt\n",
        "loser_1.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_loser_1 = loser_1.getResult()[0]\n",
        "params_loser_1['max_depth'] = int(params_loser_1['max_depth'])\n",
        "params_loser_1['min_child_weight'] = int(params_loser_1['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_loser_train1 = xgb.DMatrix(X_train1, y_train1)\n",
        "dX_loser_test1 = xgb.DMatrix(X_test1, y_test1)\n",
        "model_loser_1 = xgb.train(params_loser_1, dX_loser_train1)\n",
        "pred_loser_1 = model_loser_1.predict(dX_loser_test1)\n",
        "\n",
        "rmse_loser_1 = np.sqrt(mean_squared_error(pred_loser_1, y_test1))\n",
        "rmse_loser_1"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 6.12170176  1.69069754 11.          0.92695323  3.          0.36579277]. \t  -0.5875926438135084 \t -0.45245711231879204\n",
            "init   \t [ 1.49162957  0.22478325  6.          0.85613654 15.          0.19363322]. \t  -0.6811472770543097 \t -0.45245711231879204\n",
            "init   \t [ 5.27957805  2.81819356 11.          0.81062146 14.          0.74826845]. \t  -0.46117277972616416 \t -0.45245711231879204\n",
            "init   \t [3.81060005 6.24236569 7.         0.95038709 5.         0.9861745 ]. \t  -0.45245711231879204 \t -0.45245711231879204\n",
            "init   \t [4.77531134 7.87990376 6.         0.78863691 6.         0.47918609]. \t  -0.5612893767574513 \t -0.45245711231879204\n",
            "1      \t [0.  0.  5.  0.5 1.  0.1]. \t  -0.6826322236836013 \t -0.45245711231879204\n",
            "2      \t [ 9.60774341  8.94642175  5.          0.60295867 15.          0.76784922]. \t  -0.5001571385184761 \t -0.45245711231879204\n",
            "3      \t [ 0.4810761   9.31563532  8.          0.92845546 14.          0.70014982]. \t  -0.5056468669267833 \t -0.45245711231879204\n",
            "4      \t [ 9.85849625  0.83439907  7.          0.7705582  19.          0.15487995]. \t  -0.6815452883308839 \t -0.45245711231879204\n",
            "5      \t [ 9.1348132   9.54667443 13.          0.95557113 18.          0.11016711]. \t  -0.6791252728059957 \t -0.45245711231879204\n",
            "6      \t [ 9.97122318  8.75901621 13.          0.9904996   9.          0.60161775]. \t  -0.4895864123043516 \t -0.45245711231879204\n",
            "7      \t [7.19079936 0.87565629 5.         0.54470564 8.         0.4449021 ]. \t  -0.5736147518216788 \t -0.45245711231879204\n",
            "8      \t [ 5.24989998  8.90398408 13.          0.54165135  1.          0.74771929]. \t  -0.46141851807096074 \t -0.45245711231879204\n",
            "9      \t [ 1.7157918   5.89141512 14.          0.5296881  10.          0.1803042 ]. \t  -0.6825568503758982 \t -0.45245711231879204\n",
            "10     \t [ 1.33491429  4.9488224  14.          0.57825982 18.          0.42075517]. \t  -0.5921600742966667 \t -0.45245711231879204\n",
            "11     \t [8.99095022 4.91206514 6.         0.87212346 1.         0.53198502]. \t  -0.5638457176592562 \t -0.45245711231879204\n",
            "12     \t [0.51753753 1.68511981 9.         0.80360039 9.         0.45200858]. \t  -0.5426731188677099 \t -0.45245711231879204\n",
            "13     \t [ 0.47819106  0.69361363 14.          0.7271902   3.          0.48615024]. \t  -0.5388370338312534 \t -0.45245711231879204\n",
            "14     \t [ 3.39786838  7.15897283  5.          0.7138544  19.          0.22047835]. \t  -0.6821252760743777 \t -0.45245711231879204\n",
            "15     \t [0.4939583  9.37390762 8.         0.7641092  1.         0.4917045 ]. \t  -0.5481894484993145 \t -0.45245711231879204\n",
            "16     \t [ 8.70209222  0.21363429 14.          0.98391211 18.          0.50653374]. \t  -0.5341942124085062 \t -0.45245711231879204\n",
            "17     \t [ 5.30693639  9.58195731  9.          0.53796245 11.          0.46099251]. \t  -0.5549909911648074 \t -0.45245711231879204\n",
            "18     \t [ 3.83524595  9.61883757 12.          0.81445794 17.          0.17700713]. \t  -0.6797728963073484 \t -0.45245711231879204\n",
            "19     \t [ 0.25234021  8.87254669 14.          0.7101496   5.          0.12345742]. \t  -0.684133223230854 \t -0.45245711231879204\n",
            "20     \t [9.81917647 9.19103216 5.         0.82619792 9.         0.10869763]. \t  -0.6822789922076578 \t -0.45245711231879204\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.87018129255363"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ClJ9rN2KUJzy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bcbed1c-d754-4588-a0b3-59ee7c66f2b4"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 2\n",
        "\n",
        "np.random.seed(run_num_2)\n",
        "surrogate_loser_2 = dGaussianProcess(cov_func)\n",
        "\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X, y, test_size=test_perc, random_state=run_num_2)\n",
        "\n",
        "def f_syn_polarity2(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_2, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train2, y=y_train2).mean())\n",
        "    return operator * score\n",
        "\n",
        "loser_2 = dGPGO(surrogate_loser_2, Acquisition_new(util_loser), f_syn_polarity2, param, n_jobs = -1) # define BayesOpt\n",
        "loser_2.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_loser_2 = loser_2.getResult()[0]\n",
        "params_loser_2['max_depth'] = int(params_loser_2['max_depth'])\n",
        "params_loser_2['min_child_weight'] = int(params_loser_2['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_loser_train2 = xgb.DMatrix(X_train2, y_train2)\n",
        "dX_loser_test2 = xgb.DMatrix(X_test2, y_test2)\n",
        "model_loser_2 = xgb.train(params_loser_2, dX_loser_train2)\n",
        "pred_loser_2 = model_loser_2.predict(dX_loser_test2)\n",
        "\n",
        "rmse_loser_2 = np.sqrt(mean_squared_error(pred_loser_2, y_test2))\n",
        "rmse_loser_2"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 8.51985493  0.73903599  7.          0.71824677 17.          0.61827209]. \t  -0.5233872966482411 \t -0.42848969661986275\n",
            "init   \t [ 8.4047092   4.35120551 12.          0.84231905 16.          0.80172484]. \t  -0.42848969661986275 \t -0.42848969661986275\n",
            "init   \t [ 6.42749368  9.61026168 10.          0.73363448 15.          0.84932207]. \t  -0.43419399571353007 \t -0.42848969661986275\n",
            "init   \t [ 2.66008363  8.36685389  6.          0.92082163 12.          0.69999264]. \t  -0.5353675801921922 \t -0.42848969661986275\n",
            "init   \t [ 1.74779478  2.17691512 12.          0.71794562  2.          0.48990646]. \t  -0.5346620615324694 \t -0.42848969661986275\n",
            "1      \t [0.65899411 0.08911785 5.         0.57513718 8.         0.6649358 ]. \t  -0.5435492302872315 \t -0.42848969661986275\n",
            "2      \t [7.50154303 9.49874736 7.         0.83842364 4.         0.5714185 ]. \t  -0.5422231033003287 \t -0.42848969661986275\n",
            "3      \t [9.88263606 1.71181353 7.         0.97275193 1.         0.74825108]. \t  -0.441747506473179 \t -0.42848969661986275\n",
            "4      \t [ 0.50045611  0.9041258  14.          0.89701685 13.          0.41057236]. \t  -0.5651338432083423 \t -0.42848969661986275\n",
            "5      \t [ 8.92213252  4.56062357 13.          0.93869871  8.          0.53052713]. \t  -0.5289605254310144 \t -0.42848969661986275\n",
            "6      \t [ 3.60181415  8.33097203 14.          0.9882562   5.          0.89449689]. \t  \u001b[92m-0.4137972346428497\u001b[0m \t -0.4137972346428497\n",
            "7      \t [ 0.11546318  7.06183776 14.          0.68323086 19.          0.99255453]. \t  -0.42239160139562504 \t -0.4137972346428497\n",
            "8      \t [0.  0.  5.  0.5 1.  0.1]. \t  -0.6860882831213788 \t -0.4137972346428497\n",
            "9      \t [ 0.10082071  2.20765876  6.          0.79154439 17.          0.83321591]. \t  -0.45448270216379194 \t -0.4137972346428497\n",
            "10     \t [0.32359376 9.19341132 6.         0.74153554 1.         0.25113354]. \t  -0.6853813278526151 \t -0.4137972346428497\n",
            "11     \t [ 9.07851132  6.67468648 14.          0.65555017  1.          0.96610958]. \t  -0.424975755228691 \t -0.4137972346428497\n",
            "12     \t [ 9.80974078  5.67295504  6.          0.91127307 13.          0.44957804]. \t  -0.5591662642332185 \t -0.4137972346428497\n",
            "13     \t [ 2.77198081  0.67245678 14.          0.56883806 19.          0.9553332 ]. \t  -0.4333861030323646 \t -0.4137972346428497\n",
            "14     \t [6.86928577 3.72419603 7.         0.55484489 8.         0.11419186]. \t  -0.6867308079831548 \t -0.4137972346428497\n",
            "15     \t [ 0.83926825  9.31286624 13.          0.83595717 11.          0.92691071]. \t  -0.41668725948491847 \t -0.4137972346428497\n",
            "16     \t [ 2.2599948   8.5777314   7.          0.92101066 18.          0.81278145]. \t  -0.44458847008023056 \t -0.4137972346428497\n",
            "17     \t [ 7.48756016  0.13555104 14.          0.75269236 12.          0.41177876]. \t  -0.5678014017572689 \t -0.4137972346428497\n",
            "18     \t [2.94548912 4.25555055 5.         0.68007425 4.         0.7533029 ]. \t  -0.45906401928062657 \t -0.4137972346428497\n",
            "19     \t [ 8.06744935  9.40296538  6.          0.60147647 19.          0.17678037]. \t  -0.6864884735391246 \t -0.4137972346428497\n",
            "20     \t [ 3.87077387  0.04333662 11.          0.79311966  8.          0.7797114 ]. \t  -0.423652004810958 \t -0.4137972346428497\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.343487868410211"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-45l3NU4UNiI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f7c7aa2-4fc8-493e-d9b2-42d8ddcd869f"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 3\n",
        "\n",
        "np.random.seed(run_num_3)\n",
        "surrogate_loser_3 = dGaussianProcess(cov_func)\n",
        "\n",
        "X_train3, X_test3, y_train3, y_test3 = train_test_split(X, y, test_size=test_perc, random_state=run_num_3)\n",
        "\n",
        "def f_syn_polarity3(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_3, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train3, y=y_train3).mean())\n",
        "    return operator * score\n",
        "\n",
        "loser_3 = dGPGO(surrogate_loser_3, Acquisition_new(util_loser), f_syn_polarity3, param, n_jobs = -1) # define BayesOpt\n",
        "loser_3.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_loser_3 = loser_3.getResult()[0]\n",
        "params_loser_3['max_depth'] = int(params_loser_3['max_depth'])\n",
        "params_loser_3['min_child_weight'] = int(params_loser_3['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_loser_train3 = xgb.DMatrix(X_train3, y_train3)\n",
        "dX_loser_test3 = xgb.DMatrix(X_test3, y_test3)\n",
        "model_loser_3 = xgb.train(params_loser_3, dX_loser_train3)\n",
        "pred_loser_3 = model_loser_3.predict(dX_loser_test3)\n",
        "\n",
        "rmse_loser_3 = np.sqrt(mean_squared_error(pred_loser_3, y_test3))\n",
        "rmse_loser_3"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 7.51575607  1.09251275 13.          0.80377147 16.          0.31229624]. \t  -0.6858957949546655 \t -0.5081673732303724\n",
            "init   \t [6.12794891 1.19652403 5.         0.74822174 8.         0.33695838]. \t  -0.6849895205987827 \t -0.5081673732303724\n",
            "init   \t [ 6.21631223  4.4909976  10.          0.66764529  9.          0.77207323]. \t  -0.5081673732303724 \t -0.5081673732303724\n",
            "init   \t [ 0.58127233  4.79918477 11.          0.64526552 15.          0.62431773]. \t  -0.5106501747238601 \t -0.5081673732303724\n",
            "init   \t [ 5.76228258  0.67219724  7.          0.88228691 13.          0.29867219]. \t  -0.6839256266327581 \t -0.5081673732303724\n",
            "1      \t [9.79994489 9.57810833 8.         0.93763368 1.         0.1772549 ]. \t  -0.7316297465421655 \t -0.5081673732303724\n",
            "2      \t [ 3.27075931  5.07207678 14.          0.99587284  2.          0.51564135]. \t  -0.6620507374994331 \t -0.5081673732303724\n",
            "3      \t [0.         0.         5.         0.5        1.19792586 0.1       ]. \t  -0.732320887566852 \t -0.5081673732303724\n",
            "4      \t [ 8.88346709  9.03427619  8.          0.525814   19.          0.22866902]. \t  -0.732406939137863 \t -0.5081673732303724\n",
            "5      \t [1.21637479 8.34226514 5.         0.57939054 5.         0.55479603]. \t  -0.6531491980452168 \t -0.5081673732303724\n",
            "6      \t [ 2.93278685  9.15897164 14.          0.67343746 10.          0.85376406]. \t  \u001b[92m-0.504811636744922\u001b[0m \t -0.504811636744922\n",
            "7      \t [ 1.1995358   9.87930749  5.          0.84448568 18.          0.26022822]. \t  -0.7297224033867351 \t -0.504811636744922\n",
            "8      \t [ 1.96525323  0.16048396 14.          0.65133015  9.          0.3003508 ]. \t  -0.6860931128073492 \t -0.504811636744922\n",
            "9      \t [6.19566593 0.54575662 9.         0.53247577 1.         0.58149275]. \t  -0.5171483404397559 \t -0.504811636744922\n",
            "10     \t [ 3.97093318  9.58939295  6.          0.5381492  12.          0.74760437]. \t  -0.5217629748454513 \t -0.504811636744922\n",
            "11     \t [ 9.94719165  9.26136843 13.          0.64663072 11.          0.47018827]. \t  -0.6546346492088932 \t -0.504811636744922\n",
            "12     \t [9.55919323 7.66342182 6.         0.63144136 7.         0.82664124]. \t  -0.5195220027397864 \t -0.504811636744922\n",
            "13     \t [ 2.80079796  0.61761702  5.          0.62606225 19.          0.71334647]. \t  -0.5274647167409906 \t -0.504811636744922\n",
            "14     \t [ 9.59599852  3.11576364  5.          0.59256945 17.          0.56621848]. \t  -0.6538676177721936 \t -0.504811636744922\n",
            "15     \t [ 8.91533927  0.16704192 13.          0.66647549  6.          0.25014   ]. \t  -0.7314177593008601 \t -0.504811636744922\n",
            "16     \t [ 4.33253274  9.14516752 14.          0.94398901 19.          0.74581708]. \t  \u001b[92m-0.5007721901039625\u001b[0m \t -0.5007721901039625\n",
            "17     \t [9.75857066 2.84550968 5.         0.7181157  3.         0.67923616]. \t  -0.5274197984747595 \t -0.5007721901039625\n",
            "18     \t [ 0.          0.          5.          0.5        10.44274008  0.1       ]. \t  -0.7323784626096901 \t -0.5007721901039625\n",
            "19     \t [ 9.94497871  9.34276152 14.          0.66308198  5.          0.66061477]. \t  -0.5102482930096841 \t -0.5007721901039625\n",
            "20     \t [ 0.45956381  8.89338741 11.          0.67604128  4.          0.20043998]. \t  -0.7307540586635348 \t -0.5007721901039625\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.6249009343932785"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voPfk1UDUQU0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c4068ee-05c7-4463-acdd-1c73092c7f6f"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 4\n",
        "\n",
        "np.random.seed(run_num_4)\n",
        "surrogate_loser_4 = dGaussianProcess(cov_func)\n",
        "\n",
        "X_train4, X_test4, y_train4, y_test4 = train_test_split(X, y, test_size=test_perc, random_state=run_num_4)\n",
        "\n",
        "def f_syn_polarity4(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_4, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train4, y=y_train4).mean())\n",
        "    return operator * score\n",
        "\n",
        "loser_4 = dGPGO(surrogate_loser_4, Acquisition_new(util_loser), f_syn_polarity4, param, n_jobs = -1) # define BayesOpt\n",
        "loser_4.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_loser_4 = loser_4.getResult()[0]\n",
        "params_loser_4['max_depth'] = int(params_loser_4['max_depth'])\n",
        "params_loser_4['min_child_weight'] = int(params_loser_4['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_loser_train4 = xgb.DMatrix(X_train4, y_train4)\n",
        "dX_loser_test4 = xgb.DMatrix(X_test4, y_test4)\n",
        "model_loser_4 = xgb.train(params_loser_4, dX_loser_train4)\n",
        "pred_loser_4 = model_loser_4.predict(dX_loser_test4)\n",
        "\n",
        "rmse_loser_4 = np.sqrt(mean_squared_error(pred_loser_4, y_test4))\n",
        "rmse_loser_4"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 9.42798506  1.23319667 14.          0.81910755  7.          0.7021549 ]. \t  -0.5581090891563811 \t -0.5581090891563811\n",
            "init   \t [ 5.54158317  7.71233962 10.          0.59117644  6.          0.31476458]. \t  -0.6204181120564479 \t -0.5581090891563811\n",
            "init   \t [2.18282634 2.93794127 7.         0.84739327 5.         0.62716229]. \t  -0.5626656925564042 \t -0.5581090891563811\n",
            "init   \t [ 5.66159581  4.68678814 11.          0.54147808  9.          0.58609221]. \t  -0.5595885941852147 \t -0.5581090891563811\n",
            "init   \t [ 3.02160958  6.07432614  6.          0.92038559 14.          0.12778609]. \t  -0.6361772582153176 \t -0.5581090891563811\n",
            "1      \t [ 1.89460697  0.10324856 13.          0.70356349 16.          0.67776197]. \t  \u001b[92m-0.5568243993303088\u001b[0m \t -0.5568243993303088\n",
            "2      \t [ 9.69969     4.32607677 14.          0.87739158 16.          0.66014279]. \t  -0.5573098211239071 \t -0.5568243993303088\n",
            "3      \t [ 2.2210328   8.31054196 13.          0.70317243 19.          0.56953182]. \t  -0.573504425404327 \t -0.5568243993303088\n",
            "4      \t [ 0.14706294  0.75702217 14.          0.56309891  8.          0.29887971]. \t  -0.6254087880261816 \t -0.5568243993303088\n",
            "5      \t [ 6.94218243  0.32144048  6.          0.59286079 19.          0.70636825]. \t  -0.5706574487524134 \t -0.5568243993303088\n",
            "6      \t [ 9.24159749  9.43575685  7.          0.98380459 19.          0.34937346]. \t  -0.6187589419936593 \t -0.5568243993303088\n",
            "7      \t [7.57184449 0.0324327  9.         0.86318723 1.         0.6413689 ]. \t  -0.5611723012436254 \t -0.5568243993303088\n",
            "8      \t [ 1.62755426  6.7921099  14.          0.91903975  1.          0.61791343]. \t  -0.5680865264437001 \t -0.5568243993303088\n",
            "9      \t [ 9.43074486  0.98044049  5.          0.96290856 10.          0.27397556]. \t  -0.6390681992417737 \t -0.5568243993303088\n",
            "10     \t [ 8.73128599  9.22658128 14.          0.98344346  2.          0.13489277]. \t  -0.6368164613179 \t -0.5568243993303088\n",
            "11     \t [ 0.          0.          7.37879691  0.5        12.37879691  0.1       ]. \t  -0.6353530029272869 \t -0.5568243993303088\n",
            "12     \t [8.91398595 7.00476796 6.         0.72156795 1.         0.41673037]. \t  -0.6210807206819127 \t -0.5568243993303088\n",
            "13     \t [ 9.58851274  9.75423983 14.          0.60815214 12.          0.97572229]. \t  \u001b[92m-0.44045254560081915\u001b[0m \t -0.44045254560081915\n",
            "14     \t [ 9.87497198  6.68701692  6.          0.55980787 13.          0.32605223]. \t  -0.6200123103666009 \t -0.44045254560081915\n",
            "15     \t [ 2.16464782  9.35692886 11.          0.97431128 12.          0.43257419]. \t  -0.5747879420555002 \t -0.44045254560081915\n",
            "16     \t [0.  0.  5.  0.5 1.  0.1]. \t  -0.6351353738423114 \t -0.44045254560081915\n",
            "17     \t [0.94508697 7.16234343 8.48038022 0.5        1.48038022 0.1       ]. \t  -0.6346230590373485 \t -0.44045254560081915\n",
            "18     \t [7.36355448 5.3077281  5.         0.5709887  7.         0.87569884]. \t  -0.467633753337906 \t -0.44045254560081915\n",
            "19     \t [ 7.93069008  0.11830338 11.          0.97110397 13.          0.22887984]. \t  -0.6356672520315936 \t -0.44045254560081915\n",
            "20     \t [ 6.39787933  3.84205635 14.          0.72880814  3.          0.21974221]. \t  -0.6364956155454153 \t -0.44045254560081915\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.469425637772456"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kEnTd7MUdlv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34329275-7e1c-4e12-b811-444fde47364a"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 5\n",
        "\n",
        "np.random.seed(run_num_5)\n",
        "surrogate_loser_5 = dGaussianProcess(cov_func)\n",
        "\n",
        "X_train5, X_test5, y_train5, y_test5 = train_test_split(X, y, test_size=test_perc, random_state=run_num_5)\n",
        "\n",
        "def f_syn_polarity5(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_5, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train5, y=y_train5).mean())\n",
        "    return operator * score\n",
        "\n",
        "loser_5 = dGPGO(surrogate_loser_5, Acquisition_new(util_loser), f_syn_polarity5, param, n_jobs = -1) # define BayesOpt\n",
        "loser_5.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_loser_5 = loser_5.getResult()[0]\n",
        "params_loser_5['max_depth'] = int(params_loser_5['max_depth'])\n",
        "params_loser_5['min_child_weight'] = int(params_loser_5['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_loser_train5 = xgb.DMatrix(X_train5, y_train5)\n",
        "dX_loser_test5 = xgb.DMatrix(X_test5, y_test5)\n",
        "model_loser_5 = xgb.train(params_loser_5, dX_loser_train5)\n",
        "pred_loser_5 = model_loser_5.predict(dX_loser_test5)\n",
        "\n",
        "rmse_loser_5 = np.sqrt(mean_squared_error(pred_loser_5, y_test5))\n",
        "rmse_loser_5"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 5.57353274  5.41760627  8.          0.53710825 19.          0.26034376]. \t  -0.6523797617249512 \t -0.466901759574539\n",
            "init   \t [5.17826437 9.78817462 5.         0.84048437 6.         0.79712615]. \t  -0.4886603987288415 \t -0.466901759574539\n",
            "init   \t [ 9.84476201  9.65484856  5.          0.59464714 19.          0.34528747]. \t  -0.564220001422124 \t -0.466901759574539\n",
            "init   \t [ 9.17476842  4.53262239 13.          0.64368565 10.          0.515308  ]. \t  -0.5049042831858346 \t -0.466901759574539\n",
            "init   \t [ 8.66719465  7.99685372 12.          0.79833571 17.          0.72443466]. \t  -0.466901759574539 \t -0.466901759574539\n",
            "1      \t [ 3.31480734  6.81602635 12.          0.95330947  1.          0.27334927]. \t  -0.6531167190113543 \t -0.466901759574539\n",
            "2      \t [2.53140694 0.29740057 9.         0.7505917  7.         0.82287404]. \t  -0.47475119282304395 \t -0.466901759574539\n",
            "3      \t [ 9.84728748  0.28222034 13.          0.93969018  3.          0.22339517]. \t  -0.6522247903981228 \t -0.466901759574539\n",
            "4      \t [ 0.68116737  3.94603776 14.          0.80380753 14.          0.77317996]. \t  \u001b[92m-0.4613270217842209\u001b[0m \t -0.4613270217842209\n",
            "5      \t [ 9.6928838   0.38736829  6.          0.56167539 11.          0.4775038 ]. \t  -0.5239028011959734 \t -0.4613270217842209\n",
            "6      \t [ 1.62699639  9.0444625   7.          0.60549869 13.          0.13190699]. \t  -0.6526723866694732 \t -0.4613270217842209\n",
            "7      \t [1.904453  0.        5.1118395 0.5       1.        0.1      ]. \t  -0.654517615956795 \t -0.4613270217842209\n",
            "8      \t [ 9.56204849  0.80573312 12.          0.60476955 16.          0.23612747]. \t  -0.6524022297735614 \t -0.4613270217842209\n",
            "9      \t [ 1.05461775  1.17661042  5.          0.5648338  17.          0.90649901]. \t  -0.4702337390488589 \t -0.4613270217842209\n",
            "10     \t [ 0.29281635  9.83144868 11.          0.57517626 19.          0.13846438]. \t  -0.6543975191191339 \t -0.4613270217842209\n",
            "11     \t [9.66604659 1.51730123 5.         0.59597224 4.         0.66643448]. \t  -0.5016316198301449 \t -0.4613270217842209\n",
            "12     \t [0.57140531 6.2823043  5.         0.70475062 2.         0.3328708 ]. \t  -0.5594485185267623 \t -0.4613270217842209\n",
            "13     \t [ 2.5240907   6.84169344 14.          0.61623097  8.          0.50182056]. \t  -0.5037582468108619 \t -0.4613270217842209\n",
            "14     \t [0.0858834  4.38043443 5.         0.57114514 9.         0.11102852]. \t  -0.6531285980012218 \t -0.4613270217842209\n",
            "15     \t [ 9.08851157  7.7443441   6.          0.74314397 12.          0.52508293]. \t  -0.5238407378055661 \t -0.4613270217842209\n",
            "16     \t [ 0.62266475  1.9018111  14.          0.63329124  3.          0.94054341]. \t  \u001b[92m-0.42529491633232686\u001b[0m \t -0.42529491633232686\n",
            "17     \t [ 9.23494676  7.58606552 12.          0.78487568  2.          0.96478853]. \t  -0.42939677763278594 \t -0.42529491633232686\n",
            "18     \t [ 5.11817103  4.38089559  9.          0.53776664 13.          0.7209118 ]. \t  -0.4792507830025482 \t -0.42529491633232686\n",
            "19     \t [8.44899175 7.77761284 6.         0.53217155 1.         0.55958377]. \t  -0.5238817589404079 \t -0.42529491633232686\n",
            "20     \t [ 3.67612744  5.42599858 14.          0.93140269 19.          0.80853077]. \t  -0.4626498801811607 \t -0.42529491633232686\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.31722690080873"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjVSH6caUgyy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7624f32c-9fc1-4038-b1fb-9fc80fb1681e"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 6\n",
        "\n",
        "np.random.seed(run_num_6)\n",
        "surrogate_loser_6 = dGaussianProcess(cov_func)\n",
        "\n",
        "X_train6, X_test6, y_train6, y_test6 = train_test_split(X, y, test_size=test_perc, random_state=run_num_6)\n",
        "\n",
        "def f_syn_polarity6(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_6, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train6, y=y_train6).mean())\n",
        "    return operator * score\n",
        "\n",
        "loser_6 = dGPGO(surrogate_loser_6, Acquisition_new(util_loser), f_syn_polarity6, param, n_jobs = -1) # define BayesOpt\n",
        "loser_6.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_loser_6 = loser_6.getResult()[0]\n",
        "params_loser_6['max_depth'] = int(params_loser_6['max_depth'])\n",
        "params_loser_6['min_child_weight'] = int(params_loser_6['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_loser_train6 = xgb.DMatrix(X_train6, y_train6)\n",
        "dX_loser_test6 = xgb.DMatrix(X_test6, y_test6)\n",
        "model_loser_6 = xgb.train(params_loser_6, dX_loser_train6)\n",
        "pred_loser_6 = model_loser_6.predict(dX_loser_test6)\n",
        "\n",
        "rmse_loser_6 = np.sqrt(mean_squared_error(pred_loser_6, y_test6))\n",
        "rmse_loser_6"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [8.92860151 3.31979805 5.         0.99251441 2.         0.57683563]. \t  -0.5471181110640053 \t -0.5219867521874206\n",
            "init   \t [4.18807429 3.35407849 9.         0.87750649 3.         0.56623277]. \t  -0.629286701398377 \t -0.5219867521874206\n",
            "init   \t [ 5.788586    6.45355096 14.          0.70660047 12.          0.82154882]. \t  -0.5219867521874206 \t -0.5219867521874206\n",
            "init   \t [4.58184578 6.73834679 5.         0.90108528 3.         0.65482895]. \t  -0.5476166257785141 \t -0.5219867521874206\n",
            "init   \t [ 4.42510505  5.75952352 14.          0.97882365 15.          0.29525604]. \t  -0.6655867879161768 \t -0.5219867521874206\n",
            "1      \t [ 7.09628606  0.82458553  6.          0.52978945 17.          0.78827589]. \t  -0.5368551114917748 \t -0.5219867521874206\n",
            "2      \t [ 1.88885215  8.62524025  6.          0.90916563 14.          0.40379086]. \t  -0.6680072912825885 \t -0.5219867521874206\n",
            "3      \t [ 0.5654966   9.52584762 14.          0.82016688  4.          0.10717254]. \t  -0.7389555129670606 \t -0.5219867521874206\n",
            "4      \t [ 9.63033822  0.03793409 13.          0.68076302  7.          0.91391681]. \t  \u001b[92m-0.4301920669254681\u001b[0m \t -0.4301920669254681\n",
            "5      \t [ 9.97469028  9.15579642 12.          0.66745021 19.          0.28959158]. \t  -0.6668248751122167 \t -0.4301920669254681\n",
            "6      \t [ 9.85880187  8.48475629 13.          0.76006171  2.          0.58751173]. \t  -0.5403277210407594 \t -0.4301920669254681\n",
            "7      \t [ 9.51844     9.62786679  5.          0.56989798 15.          0.51068013]. \t  -0.6345288616058514 \t -0.4301920669254681\n",
            "8      \t [ 0.15151883  3.04875397  5.          0.61495161 19.          0.79420245]. \t  -0.5357132607573185 \t -0.4301920669254681\n",
            "9      \t [ 3.08077333  0.89569015  6.          0.84149914 11.          0.94992477]. \t  -0.4472711154368675 \t -0.4301920669254681\n",
            "10     \t [ 0.19816257  1.95434851 14.          0.59292238  7.          0.17971262]. \t  -0.7383261448869979 \t -0.4301920669254681\n",
            "11     \t [ 0.56219975  9.42928268 11.          0.51903033 19.          0.52214278]. \t  -0.6326946461691906 \t -0.4301920669254681\n",
            "12     \t [9.58176304 9.94093363 6.         0.96280567 8.         0.32855082]. \t  -0.6677341620025121 \t -0.4301920669254681\n",
            "13     \t [ 5.43877488  0.17504551 13.          0.58721411 19.          0.60115325]. \t  -0.5401916158723044 \t -0.4301920669254681\n",
            "14     \t [0.  0.  5.  0.5 1.  0.1]. \t  -0.7382709735940614 \t -0.4301920669254681\n",
            "15     \t [9.18163606 3.89469538 8.         0.59772622 8.         0.18901946]. \t  -0.7380580836797348 \t -0.4301920669254681\n",
            "16     \t [0.52725495 9.82259272 6.         0.62080595 7.         0.68770749]. \t  -0.5497290569747493 \t -0.4301920669254681\n",
            "17     \t [ 6.71573436  6.18076517  8.          0.50736725 18.          0.99692636]. \t  -0.4548470613295324 \t -0.4301920669254681\n",
            "18     \t [ 7.23381367  0.21465732 13.          0.75484136 13.          0.44717401]. \t  -0.6306144837329356 \t -0.4301920669254681\n",
            "19     \t [ 0.17525712  4.19183624 13.          0.93093846  1.          0.77496282]. \t  -0.5254340278392874 \t -0.4301920669254681\n",
            "20     \t [ 0.22896089  2.69806987 14.          0.53297649 18.          0.65056758]. \t  -0.5430966302037052 \t -0.4301920669254681\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.1451215046465775"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1WsphKSUj19",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97c034bc-eba0-4318-f605-a67aa417cda4"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 7\n",
        "\n",
        "np.random.seed(run_num_7)\n",
        "surrogate_loser_7 = dGaussianProcess(cov_func)\n",
        "\n",
        "X_train7, X_test7, y_train7, y_test7 = train_test_split(X, y, test_size=test_perc, random_state=run_num_7)\n",
        "\n",
        "def f_syn_polarity7(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_7, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train7, y=y_train7).mean())\n",
        "    return operator * score\n",
        "\n",
        "loser_7 = dGPGO(surrogate_loser_7, Acquisition_new(util_loser), f_syn_polarity7, param, n_jobs = -1) # define BayesOpt\n",
        "loser_7.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_loser_7 = loser_7.getResult()[0]\n",
        "params_loser_7['max_depth'] = int(params_loser_7['max_depth'])\n",
        "params_loser_7['min_child_weight'] = int(params_loser_7['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_loser_train7 = xgb.DMatrix(X_train7, y_train7)\n",
        "dX_loser_test7 = xgb.DMatrix(X_test7, y_test7)\n",
        "model_loser_7 = xgb.train(params_loser_7, dX_loser_train7)\n",
        "pred_loser_7 = model_loser_7.predict(dX_loser_test7)\n",
        "\n",
        "rmse_loser_7 = np.sqrt(mean_squared_error(pred_loser_7, y_test7))\n",
        "rmse_loser_7"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 8.11505126  6.92324143 13.          0.94046175 12.          0.3650384 ]. \t  -0.612852517751432 \t -0.4284992540085738\n",
            "init   \t [ 5.15357029  9.0066636   6.          0.74104227 15.          0.8966337 ]. \t  -0.45500673865987984 \t -0.4284992540085738\n",
            "init   \t [ 2.63548913  9.6610934  14.          0.64712408  1.          0.80820948]. \t  -0.43730487111474525 \t -0.4284992540085738\n",
            "init   \t [ 1.40821426  1.72758589 12.          0.70495069  5.          0.86632715]. \t  -0.4284992540085738 \t -0.4284992540085738\n",
            "init   \t [2.85126987 2.73434054 7.         0.88394733 8.         0.65545939]. \t  -0.48253390492139586 \t -0.4284992540085738\n",
            "1      \t [1.03697999e-02 2.51695028e+00 1.40000000e+01 6.19497248e-01\n",
            " 1.90000000e+01 7.01827417e-01]. \t  -0.47758423778274467 \t -0.4284992540085738\n",
            "2      \t [9.9089961  7.50235155 7.         0.67925503 5.         0.2471192 ]. \t  -0.6736712023667494 \t -0.4284992540085738\n",
            "3      \t [ 7.21196079  0.11384945  8.          0.67755731 17.          0.74323626]. \t  -0.4647919397201027 \t -0.4284992540085738\n",
            "4      \t [0.35283087 9.2950614  5.         0.93054416 3.         0.27625531]. \t  -0.670508947897148 \t -0.4284992540085738\n",
            "5      \t [0.  0.  5.  0.5 1.  0.1]. \t  -0.6745888440506937 \t -0.4284992540085738\n",
            "6      \t [9.76759527 0.48761621 7.         0.96362191 1.         0.137157  ]. \t  -0.6715547907624335 \t -0.4284992540085738\n",
            "7      \t [ 8.53387117  0.35946342 13.          0.65332712  6.          0.68302293]. \t  -0.4712802345283902 \t -0.4284992540085738\n",
            "8      \t [ 0.45155781  2.33209808  5.          0.73660734 17.          0.83890491]. \t  -0.4804814871680856 \t -0.4284992540085738\n",
            "9      \t [ 0.30506512  9.51761383 13.          0.5277377  16.          0.92234588]. \t  -0.4390633386106087 \t -0.4284992540085738\n",
            "10     \t [ 9.21287749  3.65759887  5.          0.85687073 12.          0.88459516]. \t  -0.45867733996321497 \t -0.4284992540085738\n",
            "11     \t [ 1.14642054  8.54710172 12.          0.83923566  8.          0.25317439]. \t  -0.6716592254658202 \t -0.4284992540085738\n",
            "12     \t [ 6.43879816  6.53341304 14.          0.70290258 19.          0.43578856]. \t  -0.5879212739277196 \t -0.4284992540085738\n",
            "13     \t [ 0.28968499  1.98768694 14.          0.97996073 12.          0.15615797]. \t  -0.6725851052423111 \t -0.4284992540085738\n",
            "14     \t [5.4721942  4.70341961 9.         0.675658   1.         0.91263074]. \t  -0.44024324260844283 \t -0.4284992540085738\n",
            "15     \t [ 7.56572451  6.67046024 14.          0.88731871  4.          0.29940932]. \t  -0.6117537685654454 \t -0.4284992540085738\n",
            "16     \t [4.54036829 9.23672573 7.         0.78071654 7.         0.925943  ]. \t  -0.44422074369565967 \t -0.4284992540085738\n",
            "17     \t [ 9.52566208  5.8884151   6.          0.78376025 19.          0.40148749]. \t  -0.6258103990077785 \t -0.4284992540085738\n",
            "18     \t [ 7.79919254  0.6285715  14.          0.87477175 15.          0.15632002]. \t  -0.6708453498973665 \t -0.4284992540085738\n",
            "19     \t [ 0.          0.          8.44191182  0.5        11.44191182  0.1       ]. \t  -0.6744471853974358 \t -0.4284992540085738\n",
            "20     \t [ 3.1237334   5.58211355  9.          0.8525378  19.          0.1292964 ]. \t  -0.6703702828267247 \t -0.4284992540085738\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.492050628419268"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hI8sFP4ZUmOs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fca5bb7-5872-486f-d8d6-0182f9b55170"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 8\n",
        "\n",
        "np.random.seed(run_num_8)\n",
        "surrogate_loser_8 = dGaussianProcess(cov_func)\n",
        "\n",
        "X_train8, X_test8, y_train8, y_test8 = train_test_split(X, y, test_size=test_perc, random_state=run_num_8)\n",
        "\n",
        "def f_syn_polarity8(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_8, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train8, y=y_train8).mean())\n",
        "    return operator * score\n",
        "\n",
        "loser_8 = dGPGO(surrogate_loser_8, Acquisition_new(util_loser), f_syn_polarity8, param, n_jobs = -1) # define BayesOpt\n",
        "loser_8.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_loser_8 = loser_8.getResult()[0]\n",
        "params_loser_8['max_depth'] = int(params_loser_8['max_depth'])\n",
        "params_loser_8['min_child_weight'] = int(params_loser_8['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_loser_train8 = xgb.DMatrix(X_train8, y_train8)\n",
        "dX_loser_test8 = xgb.DMatrix(X_test8, y_test8)\n",
        "model_loser_8 = xgb.train(params_loser_8, dX_loser_train8)\n",
        "pred_loser_8 = model_loser_8.predict(dX_loser_test8)\n",
        "\n",
        "rmse_loser_8 = np.sqrt(mean_squared_error(pred_loser_8, y_test8))\n",
        "rmse_loser_8"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 1.29221489  0.77396898 11.          0.56062468  3.          0.25451879]. \t  -0.6231794271723858 \t -0.4674926150198253\n",
            "init   \t [ 0.10611429  1.37648061  8.          0.6050375  15.          0.790205  ]. \t  -0.4674926150198253 \t -0.4674926150198253\n",
            "init   \t [ 8.35239571  4.16908043  7.          0.67902746 14.          0.16683749]. \t  -0.6176439249709439 \t -0.4674926150198253\n",
            "init   \t [ 6.06301777  7.27127491 12.          0.87295531 11.          0.37694382]. \t  -0.5951520828762356 \t -0.4674926150198253\n",
            "init   \t [8.85209637 7.89269955 6.         0.72444854 3.         0.47766986]. \t  -0.5688519156580504 \t -0.4674926150198253\n",
            "1      \t [ 1.47825581  9.16010046 11.          0.96606546  1.          0.92358292]. \t  \u001b[92m-0.43518063190798095\u001b[0m \t -0.43518063190798095\n",
            "2      \t [ 7.52273592  2.30141271 14.          0.75505047 19.          0.77804046]. \t  -0.45158957529537797 \t -0.43518063190798095\n",
            "3      \t [1.64420908 9.47353882 5.         0.79993338 7.         0.15673599]. \t  -0.6175838614360663 \t -0.43518063190798095\n",
            "4      \t [ 1.02399475  9.65113789  9.          0.53947972 17.          0.8292164 ]. \t  -0.4719352875928771 \t -0.43518063190798095\n",
            "5      \t [8.14212583 0.61314553 6.         0.86488725 6.         0.28574491]. \t  -0.5954481959734638 \t -0.43518063190798095\n",
            "6      \t [ 8.76387285  7.83054608 13.          0.54159783  4.          0.24710422]. \t  -0.6207082177098945 \t -0.43518063190798095\n",
            "7      \t [ 9.0517509   9.45530828 11.          0.85305105 19.          0.79961763]. \t  -0.4589725747508552 \t -0.43518063190798095\n",
            "8      \t [ 7.77762087  0.97608919 14.          0.56979605  8.          0.73436998]. \t  -0.4483480686509709 \t -0.43518063190798095\n",
            "9      \t [2.67676373 3.95611485 5.         0.79739163 2.         0.89987835]. \t  -0.45674643004535687 \t -0.43518063190798095\n",
            "10     \t [ 0.62854908  4.77207773 14.          0.78182185 18.          0.73284936]. \t  -0.4461225555243119 \t -0.43518063190798095\n",
            "11     \t [3.06290827 1.66002331 6.         0.51411222 9.         0.98529356]. \t  -0.461628022517463 \t -0.43518063190798095\n",
            "12     \t [ 0.55545334  6.5757175   9.          0.88529579 12.          0.24203797]. \t  -0.6187020960077592 \t -0.43518063190798095\n",
            "13     \t [ 9.10053278  1.48753729 11.          0.9741735   1.          0.65977303]. \t  -0.5223704878111852 \t -0.43518063190798095\n",
            "14     \t [ 9.48329103  0.2076913   7.          0.76655383 19.          0.88164364]. \t  -0.4616094756962891 \t -0.43518063190798095\n",
            "15     \t [ 0.81390458  8.42248516 12.          0.78729971  7.          0.71328057]. \t  -0.5176932464043367 \t -0.43518063190798095\n",
            "16     \t [ 5.51535579  8.63063044  6.          0.64760098 18.          0.14185561]. \t  -0.6172074564902792 \t -0.43518063190798095\n",
            "17     \t [ 0.81210087  0.8533868  13.          0.66456412 12.          0.37769434]. \t  -0.5971747957799627 \t -0.43518063190798095\n",
            "18     \t [ 8.74769008  9.46041181  5.          0.76888057 12.          0.28611188]. \t  -0.5996998823178632 \t -0.43518063190798095\n",
            "19     \t [ 6.67918913  1.91467598 12.          0.83062216 13.          0.72788563]. \t  -0.45035357506572193 \t -0.43518063190798095\n",
            "20     \t [ 4.53564763  5.14157265 14.          0.52558575  1.          0.72718907]. \t  -0.45119962298136185 \t -0.43518063190798095\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.711175290813375"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vw5IYus6UpAn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be64b4c6-f7ba-49f4-a04b-f97d9086eb2d"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 9\n",
        "\n",
        "np.random.seed(run_num_9)\n",
        "surrogate_loser_9 = dGaussianProcess(cov_func)\n",
        "\n",
        "X_train9, X_test9, y_train9, y_test9 = train_test_split(X, y, test_size=test_perc, random_state=run_num_9)\n",
        "\n",
        "def f_syn_polarity9(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_9, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train9, y=y_train9).mean())\n",
        "    return operator * score\n",
        "\n",
        "loser_9 = dGPGO(surrogate_loser_9, Acquisition_new(util_loser), f_syn_polarity9, param, n_jobs = -1) # define BayesOpt\n",
        "loser_9.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_loser_9 = loser_9.getResult()[0]\n",
        "params_loser_9['max_depth'] = int(params_loser_9['max_depth'])\n",
        "params_loser_9['min_child_weight'] = int(params_loser_9['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_loser_train9 = xgb.DMatrix(X_train9, y_train9)\n",
        "dX_loser_test9 = xgb.DMatrix(X_test9, y_test9)\n",
        "model_loser_9 = xgb.train(params_loser_9, dX_loser_train9)\n",
        "pred_loser_9 = model_loser_9.predict(dX_loser_test9)\n",
        "\n",
        "rmse_loser_9 = np.sqrt(mean_squared_error(pred_loser_9, y_test9))\n",
        "rmse_loser_9"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 8.0342804   5.27522296 13.          0.60741904 17.          0.15336222]. \t  -0.6890593508433319 \t -0.4886355541929722\n",
            "init   \t [7.56063644 1.79876369 7.         0.84869647 5.         0.55956942]. \t  -0.5532305414240943 \t -0.4886355541929722\n",
            "init   \t [2.43675539 0.20225128 6.         0.51669963 7.         0.4646446 ]. \t  -0.5609155518755553 \t -0.4886355541929722\n",
            "init   \t [ 7.6043547   4.73758376  9.          0.87564624 15.          0.65751286]. \t  -0.4886355541929722 \t -0.4886355541929722\n",
            "init   \t [ 4.70349314  6.71029586  6.          0.97680389 11.          0.41971652]. \t  -0.6548660487332898 \t -0.4886355541929722\n",
            "1      \t [ 0.20910387  7.25545521 14.          0.59407892  3.          0.95410586]. \t  \u001b[92m-0.4312356520914486\u001b[0m \t -0.4312356520914486\n",
            "2      \t [ 2.14892068  9.25577985 14.          0.84828472 11.          0.2793841 ]. \t  -0.6883959263608518 \t -0.4312356520914486\n",
            "3      \t [ 0.37305557  1.02756836 13.          0.57348052 13.          0.68410192]. \t  -0.4836872799099695 \t -0.4312356520914486\n",
            "4      \t [ 7.98252507  8.04957695 12.          0.6367811   3.          0.85775382]. \t  -0.4368210491589304 \t -0.4312356520914486\n",
            "5      \t [ 1.2588195   0.04438454  7.          0.70342864 18.          0.58554745]. \t  -0.4951281025154505 \t -0.4312356520914486\n",
            "6      \t [ 9.17710587  9.71573287  5.          0.9584376  19.          0.78072441]. \t  -0.4682275209845145 \t -0.4312356520914486\n",
            "7      \t [6.11416909 9.54959201 5.         0.91943284 1.         0.10968146]. \t  -0.6901756354808697 \t -0.4312356520914486\n",
            "8      \t [ 0.31270412  9.94818438  8.          0.84397658 19.          0.43974503]. \t  -0.5483333357303245 \t -0.4312356520914486\n",
            "9      \t [ 8.76504058  0.46155195 14.          0.61459938 10.          0.19095511]. \t  -0.6902913672662703 \t -0.4312356520914486\n",
            "10     \t [0.  0.  5.  0.5 1.  0.1]. \t  -0.6893514282293631 \t -0.4312356520914486\n",
            "11     \t [ 0.76412088  5.02527282 14.          0.53794557 19.          0.6779987 ]. \t  -0.4862552398836468 \t -0.4312356520914486\n",
            "12     \t [ 8.7007577   8.84182108 11.          0.88074977 10.          0.89589347]. \t  -0.43700927635364534 \t -0.4312356520914486\n",
            "13     \t [1.26882135 8.44257007 8.         0.93932154 4.         0.79202967]. \t  -0.44680835713197 \t -0.4312356520914486\n",
            "14     \t [ 3.38351629  2.89927867 12.          0.76452212  7.          0.61222347]. \t  -0.4809111856529227 \t -0.4312356520914486\n",
            "15     \t [ 7.69267031  0.52709121  5.          0.97712342 11.          0.47111142]. \t  -0.5671609865381374 \t -0.4312356520914486\n",
            "16     \t [ 5.04465897  1.7242392  11.          0.82507363  1.          0.66070113]. \t  -0.48737194712175996 \t -0.4312356520914486\n",
            "17     \t [ 9.35257803  1.01913578 10.          0.61034129 19.          0.44798121]. \t  -0.5514224578726403 \t -0.4312356520914486\n",
            "18     \t [ 3.56621236  4.91536137  7.          0.71255145 19.          0.80604757]. \t  -0.4579607623923458 \t -0.4312356520914486\n",
            "19     \t [ 0.         0.         5.         0.5       12.6024416  0.1      ]. \t  -0.6889398896191611 \t -0.4312356520914486\n",
            "20     \t [9.96752263 8.45749074 7.         0.92612294 6.         0.22771617]. \t  -0.6878594070452866 \t -0.4312356520914486\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.130636454660636"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YD494io_Ur7V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "805ae67f-0757-45b1-ad7d-f6fb40ec8c8d"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 10\n",
        "\n",
        "np.random.seed(run_num_10)\n",
        "surrogate_loser_10 = dGaussianProcess(cov_func)\n",
        "\n",
        "X_train10, X_test10, y_train10, y_test10 = train_test_split(X, y, test_size=test_perc, random_state=run_num_10)\n",
        "\n",
        "def f_syn_polarity10(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_10, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train10, y=y_train10).mean())\n",
        "    return operator * score\n",
        "\n",
        "loser_10 = dGPGO(surrogate_loser_10, Acquisition_new(util_loser), f_syn_polarity10, param, n_jobs = -1) # define BayesOpt\n",
        "loser_10.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_loser_10 = loser_10.getResult()[0]\n",
        "params_loser_10['max_depth'] = int(params_loser_10['max_depth'])\n",
        "params_loser_10['min_child_weight'] = int(params_loser_10['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_loser_train10 = xgb.DMatrix(X_train10, y_train10)\n",
        "dX_loser_test10 = xgb.DMatrix(X_test10, y_test10)\n",
        "model_loser_10 = xgb.train(params_loser_10, dX_loser_train10)\n",
        "pred_loser_10 = model_loser_10.predict(dX_loser_test10)\n",
        "\n",
        "rmse_loser_10 = np.sqrt(mean_squared_error(pred_loser_10, y_test10))\n",
        "rmse_loser_10"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 6.53589585  1.15006943  6.          0.93623727 14.          0.13663866]. \t  -0.6929852048256443 \t -0.4483221221388197\n",
            "init   \t [ 3.97194461  2.33132197 14.          0.9533253  19.          0.26403087]. \t  -0.6932702114662359 \t -0.4483221221388197\n",
            "init   \t [ 7.43539415  0.69582081  9.          0.9763222  11.          0.12608349]. \t  -0.6931242497450955 \t -0.4483221221388197\n",
            "init   \t [ 9.82027485  3.39637684  9.          0.78522537 19.          0.87618843]. \t  -0.4483221221388197 \t -0.4483221221388197\n",
            "init   \t [ 0.57576207  5.82646405 12.          0.77704362 18.          0.11865044]. \t  -0.6946096327074635 \t -0.4483221221388197\n",
            "1      \t [ 3.17878299  6.40666671 10.          0.71928645  2.          0.37920433]. \t  -0.6059745513602156 \t -0.4483221221388197\n",
            "2      \t [ 3.37448201  8.10780464 13.          0.50477079 10.          0.914384  ]. \t  \u001b[92m-0.43629727149692393\u001b[0m \t -0.43629727149692393\n",
            "3      \t [ 9.185601    0.61355962 14.          0.74945204  2.          0.47381106]. \t  -0.5791988425691649 \t -0.43629727149692393\n",
            "4      \t [8.35411693 8.91554079 5.         0.73130633 7.         0.67857606]. \t  -0.5771647264709212 \t -0.43629727149692393\n",
            "5      \t [ 6.07704325  9.68005151  5.          0.52932858 19.          0.88896368]. \t  -0.4736105624167554 \t -0.43629727149692393\n",
            "6      \t [ 8.63718494  8.55246455 14.          0.55824531 18.          0.4228152 ]. \t  -0.6085826477793935 \t -0.43629727149692393\n",
            "7      \t [9.97805381 2.33960922 6.         0.72769906 2.         0.16112568]. \t  -0.693543381593374 \t -0.43629727149692393\n",
            "8      \t [0.  0.  5.  0.5 1.  0.1]. \t  -0.6914754878687801 \t -0.43629727149692393\n",
            "9      \t [ 0.03680135  0.50694581 14.          0.91124751  8.          0.87026982]. \t  \u001b[92m-0.4215933675272626\u001b[0m \t -0.4215933675272626\n",
            "10     \t [ 1.86936633  8.33951478  6.          0.64608958 13.          0.87352832]. \t  -0.45750043698066534 \t -0.4215933675272626\n",
            "11     \t [ 9.77077394  8.65786606 12.          0.7706968   5.          0.42456779]. \t  -0.6030224397086726 \t -0.4215933675272626\n",
            "12     \t [ 0.1202295   0.91999335  5.          0.65757131 10.          0.24500243]. \t  -0.6930477249479814 \t -0.4215933675272626\n",
            "13     \t [0.50440288 9.75457432 5.         0.8137431  6.         0.15250773]. \t  -0.6941630128319229 \t -0.4215933675272626\n",
            "14     \t [ 9.91477928  9.35319949  6.          0.75202808 14.          0.13042843]. \t  -0.6936633884299808 \t -0.4215933675272626\n",
            "15     \t [2.78752086e+00 7.83961972e-03 1.10000000e+01 9.72670765e-01\n",
            " 2.00000000e+00 4.67980644e-01]. \t  -0.5809464009440812 \t -0.4215933675272626\n",
            "16     \t [ 0.60051063  0.93658443 10.          0.71787415 13.          0.3650692 ]. \t  -0.6037929489655063 \t -0.4215933675272626\n",
            "17     \t [7.64536232 8.37707893 5.         0.8688534  1.         0.76688319]. \t  -0.5294614889177678 \t -0.4215933675272626\n",
            "18     \t [ 8.58082048  3.39962732 14.          0.7151638   9.          0.76214186]. \t  -0.5049104315367501 \t -0.4215933675272626\n",
            "19     \t [ 8.52646717  0.12245704 14.          0.80155281 15.          0.7100538 ]. \t  -0.5709119331337676 \t -0.4215933675272626\n",
            "20     \t [5.51569281 2.48923068 5.         0.97468437 6.         0.88188042]. \t  -0.4524796762835521 \t -0.4215933675272626\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.436171669561738"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N03Sq0TvUuhp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6440a9d-6191-4db4-ed31-8d749570e464"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 11\n",
        "\n",
        "np.random.seed(run_num_11)\n",
        "surrogate_loser_11 = dGaussianProcess(cov_func)\n",
        "\n",
        "X_train11, X_test11, y_train11, y_test11 = train_test_split(X, y, test_size=test_perc, random_state=run_num_11)\n",
        "\n",
        "def f_syn_polarity11(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_11, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train11, y=y_train11).mean())\n",
        "    return operator * score\n",
        "\n",
        "loser_11 = dGPGO(surrogate_loser_11, Acquisition_new(util_loser), f_syn_polarity11, param, n_jobs = -1) # define BayesOpt\n",
        "loser_11.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_loser_11 = loser_11.getResult()[0]\n",
        "params_loser_11['max_depth'] = int(params_loser_11['max_depth'])\n",
        "params_loser_11['min_child_weight'] = int(params_loser_11['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_loser_train11 = xgb.DMatrix(X_train11, y_train11)\n",
        "dX_loser_test11 = xgb.DMatrix(X_test11, y_test11)\n",
        "model_loser_11 = xgb.train(params_loser_11, dX_loser_train11)\n",
        "pred_loser_11 = model_loser_11.predict(dX_loser_test11)\n",
        "\n",
        "rmse_loser_11 = np.sqrt(mean_squared_error(pred_loser_11, y_test11))\n",
        "rmse_loser_11"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 9.81035663  8.2124785   9.          0.68487173 15.          0.28207988]. \t  -0.7133787013189323 \t -0.47317045732943896\n",
            "init   \t [ 4.91906771  1.38558332  6.          0.618878   19.          0.10263204]. \t  -0.7112023662943636 \t -0.47317045732943896\n",
            "init   \t [ 5.23426175  1.89971159 13.          0.76313779 17.          0.46597853]. \t  -0.47317045732943896 \t -0.47317045732943896\n",
            "init   \t [ 4.0204056   3.75167577  9.          0.86204818 17.          0.26560056]. \t  -0.7132057194891379 \t -0.47317045732943896\n",
            "init   \t [8.8704459  5.64917815 5.         0.51406171 1.         0.82826731]. \t  -0.48048685248123163 \t -0.47317045732943896\n",
            "1      \t [0.08349406 1.89250335 6.         0.66386636 1.         0.88641723]. \t  \u001b[92m-0.4484239383130805\u001b[0m \t -0.4484239383130805\n",
            "2      \t [ 5.35837408  0.31510164 14.          0.97519815  6.          0.3034681 ]. \t  -0.607905230502132 \t -0.4484239383130805\n",
            "3      \t [ 4.66315557  9.30076713 11.          0.83534598  8.          0.21303816]. \t  -0.7135282416096518 \t -0.4484239383130805\n",
            "4      \t [9.02046973 1.16865322 7.         0.6807515  8.         0.2962635 ]. \t  -0.6068266615066733 \t -0.4484239383130805\n",
            "5      \t [0.79988529 9.85424348 5.         0.5743044  3.         0.67901433]. \t  -0.48209943859810334 \t -0.4484239383130805\n",
            "6      \t [ 0.02057794  7.14089873 13.          0.90855163  1.          0.61823707]. \t  -0.45805892074172583 \t -0.4484239383130805\n",
            "7      \t [ 0.97603689  3.49871064  9.          0.91846837 10.          0.35158251]. \t  -0.6085109103402477 \t -0.4484239383130805\n",
            "8      \t [ 9.27240029  9.57413463 14.          0.58544186  3.          0.73779027]. \t  -0.45267877796490286 \t -0.4484239383130805\n",
            "9      \t [ 0.4027375   9.38447167 12.          0.94595612 15.          0.85310792]. \t  -0.451136347669006 \t -0.4484239383130805\n",
            "10     \t [ 1.3440527   8.97080429  5.          0.80277597 12.          0.63284111]. \t  -0.48349651102568225 \t -0.4484239383130805\n",
            "11     \t [ 7.94225687  3.68352192 13.          0.94902199 11.          0.95471054]. \t  \u001b[92m-0.422771965919715\u001b[0m \t -0.422771965919715\n",
            "12     \t [9.20332934 9.98167579 5.         0.57575366 6.         0.3717594 ]. \t  -0.6046860081027627 \t -0.422771965919715\n",
            "13     \t [5.68758712 2.62033511 9.         0.67614771 3.         0.67632008]. \t  -0.47052528509931824 \t -0.422771965919715\n",
            "14     \t [2.94143112e-03 7.03829535e-01 1.30000000e+01 5.98722168e-01\n",
            " 2.00000000e+00 1.01335355e-01]. \t  -0.7122743790062083 \t -0.422771965919715\n",
            "15     \t [ 0.81561387  9.31451412  5.          0.70594876 19.          0.98284163]. \t  -0.45988817082889427 \t -0.422771965919715\n",
            "16     \t [ 9.8968071   1.89203294 13.          0.7888373   2.          0.82539259]. \t  -0.45294282860990764 \t -0.422771965919715\n",
            "17     \t [ 8.37835592  2.52983026  5.          0.97806207 14.          0.6500905 ]. \t  -0.48098936825483635 \t -0.422771965919715\n",
            "18     \t [ 0.56769151  4.18340522 14.          0.91967306 13.          0.92813967]. \t  \u001b[92m-0.41912566738485735\u001b[0m \t -0.41912566738485735\n",
            "19     \t [ 4.05595265  0.          5.          0.5        10.09765104  0.1       ]. \t  -0.7116217008807669 \t -0.41912566738485735\n",
            "20     \t [ 8.98906297  7.01351387 13.          0.77197065 19.          0.47265223]. \t  -0.475345282554151 \t -0.41912566738485735\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.3224670398414355"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_nP9lQjUztV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4146778-cd53-42b2-9cbc-0c033849188e"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 12\n",
        "\n",
        "np.random.seed(run_num_12)\n",
        "surrogate_loser_12 = dGaussianProcess(cov_func)\n",
        "\n",
        "X_train12, X_test12, y_train12, y_test12 = train_test_split(X, y, test_size=test_perc, random_state=run_num_12)\n",
        "\n",
        "def f_syn_polarity12(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_12, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train12, y=y_train12).mean())\n",
        "    return operator * score\n",
        "\n",
        "loser_12 = dGPGO(surrogate_loser_12, Acquisition_new(util_loser), f_syn_polarity12, param, n_jobs = -1) # define BayesOpt\n",
        "loser_12.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_loser_12 = loser_12.getResult()[0]\n",
        "params_loser_12['max_depth'] = int(params_loser_12['max_depth'])\n",
        "params_loser_12['min_child_weight'] = int(params_loser_12['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_loser_train12 = xgb.DMatrix(X_train12, y_train12)\n",
        "dX_loser_test12 = xgb.DMatrix(X_test12, y_test12)\n",
        "model_loser_12 = xgb.train(params_loser_12, dX_loser_train12)\n",
        "pred_loser_12 = model_loser_12.predict(dX_loser_test12)\n",
        "\n",
        "rmse_loser_12 = np.sqrt(mean_squared_error(pred_loser_12, y_test12))\n",
        "rmse_loser_12"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 1.9151945   6.22108771  9.          0.89267929 16.          0.87460279]. \t  -0.4420077321695894 \t -0.4420077321695894\n",
            "init   \t [ 1.50636966  1.98518759 11.          0.67890863 17.          0.5381501 ]. \t  -0.5998539491151165 \t -0.4420077321695894\n",
            "init   \t [ 3.31015428  8.02639569  7.          0.75154158 13.          0.11992952]. \t  -0.7324503600335561 \t -0.4420077321695894\n",
            "init   \t [ 2.90728553  2.46394443  5.          0.94461307 17.          0.43194161]. \t  -0.5995790338913064 \t -0.4420077321695894\n",
            "init   \t [ 9.33140102  6.51378143  8.          0.76907392 18.          0.38515251]. \t  -0.5982523495604211 \t -0.4420077321695894\n",
            "1      \t [5.68098653 8.6912739  6.         0.52027807 1.         0.51631894]. \t  -0.5999713561677879 \t -0.4420077321695894\n",
            "2      \t [ 4.61089026  7.78540446 14.          0.61560224  6.          0.60910898]. \t  -0.5310432089168232 \t -0.4420077321695894\n",
            "3      \t [ 7.71510638  1.71782806 11.          0.9748513   2.          0.33442931]. \t  -0.5954035506799399 \t -0.4420077321695894\n",
            "4      \t [ 0.42901773  1.02682351 10.          0.62582208  7.          0.40519792]. \t  -0.600033100066767 \t -0.4420077321695894\n",
            "5      \t [ 9.48743388  2.23858916 14.          0.84020064 13.          0.41361313]. \t  -0.5921503481987865 \t -0.4420077321695894\n",
            "6      \t [ 7.09099339  0.08964775  6.          0.68605854 11.          0.16899055]. \t  -0.7330665763811677 \t -0.4420077321695894\n",
            "7      \t [0.  0.  5.  0.5 1.  0.1]. \t  -0.7334076855722197 \t -0.4420077321695894\n",
            "8      \t [ 4.8875203   9.38595347 14.          0.7902783  19.          0.67669595]. \t  -0.5289545232453541 \t -0.4420077321695894\n",
            "9      \t [ 0.50387213  8.28483675 14.          0.60639358 12.          0.19061796]. \t  -0.7343015225078143 \t -0.4420077321695894\n",
            "10     \t [ 1.81216609  5.49695067 10.52048254  0.5         1.          0.1       ]. \t  -0.7336073261479503 \t -0.4420077321695894\n",
            "11     \t [ 9.8847664   8.41680653 10.          0.63099084  9.          0.83145315]. \t  -0.5042925534600994 \t -0.4420077321695894\n",
            "12     \t [ 0.          0.          5.          0.5        11.52941427  0.1       ]. \t  -0.7334775686570922 \t -0.4420077321695894\n",
            "13     \t [9.43877299 3.79912488 5.         0.53841687 4.         0.20857531]. \t  -0.7351835256464294 \t -0.4420077321695894\n",
            "14     \t [0.41721862 8.87551388 5.         0.69088851 6.         0.86973069]. \t  -0.468169334397084 \t -0.4420077321695894\n",
            "15     \t [ 9.68675553  3.68641979 14.          0.63310045  7.          0.87631691]. \t  \u001b[92m-0.44169445577667543\u001b[0m \t -0.44169445577667543\n",
            "16     \t [ 9.64220337  0.08866879  7.          0.55877602 19.          0.80403324]. \t  -0.5167375732665379 \t -0.44169445577667543\n",
            "17     \t [ 8.95792842  8.74686238 12.          0.60340754  1.          0.64076061]. \t  -0.5341583767040248 \t -0.44169445577667543\n",
            "18     \t [ 3.80794615  1.17683    14.          0.6618018  11.          0.78665992]. \t  -0.497041017578099 \t -0.44169445577667543\n",
            "19     \t [ 0.60473619  9.7310361   5.          0.764657   19.          0.44899982]. \t  -0.602200593591361 \t -0.44169445577667543\n",
            "20     \t [ 7.25143535  1.86402411 14.          0.59310957 19.          0.53879904]. \t  -0.5987977809908415 \t -0.44169445577667543\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3.9962203731848227"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDI2Bi9vU05U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83e4c0fe-61a6-4604-e656-ca2411497d8d"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 13\n",
        "\n",
        "np.random.seed(run_num_13)\n",
        "surrogate_loser_13 = dGaussianProcess(cov_func)\n",
        "\n",
        "X_train13, X_test13, y_train13, y_test13 = train_test_split(X, y, test_size=test_perc, random_state=run_num_13)\n",
        "\n",
        "def f_syn_polarity13(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_13, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train13, y=y_train13).mean())\n",
        "    return operator * score\n",
        "\n",
        "loser_13 = dGPGO(surrogate_loser_13, Acquisition_new(util_loser), f_syn_polarity13, param, n_jobs = -1) # define BayesOpt\n",
        "loser_13.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_loser_13 = loser_13.getResult()[0]\n",
        "params_loser_13['max_depth'] = int(params_loser_13['max_depth'])\n",
        "params_loser_13['min_child_weight'] = int(params_loser_13['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_loser_train13 = xgb.DMatrix(X_train13, y_train13)\n",
        "dX_loser_test13 = xgb.DMatrix(X_test13, y_test13)\n",
        "model_loser_13 = xgb.train(params_loser_13, dX_loser_train13)\n",
        "pred_loser_13 = model_loser_13.predict(dX_loser_test13)\n",
        "\n",
        "rmse_loser_13 = np.sqrt(mean_squared_error(pred_loser_13, y_test13))\n",
        "rmse_loser_13"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 0.30967343  2.54282073  6.          0.9850871  17.          0.50848087]. \t  -0.5693403578256062 \t -0.4459810811310791\n",
            "init   \t [ 6.95119021  3.25593423 14.          0.8602375   8.          0.82055713]. \t  -0.5112217679187058 \t -0.4459810811310791\n",
            "init   \t [ 5.09252675  8.19921021 10.          0.78792002 16.          0.382158  ]. \t  -0.581693558072188 \t -0.4459810811310791\n",
            "init   \t [ 5.4465191   3.19841462 14.          0.90654581  5.          0.10359415]. \t  -0.6818818523119694 \t -0.4459810811310791\n",
            "init   \t [ 9.8393864   9.89619708  9.          0.63387763 10.          0.88979174]. \t  -0.4459810811310791 \t -0.4459810811310791\n",
            "1      \t [ 9.27118046  9.4344906  10.          0.71578295  1.          0.5463837 ]. \t  -0.5604994598662556 \t -0.4459810811310791\n",
            "2      \t [0.21910824 0.53051294 5.         0.62754372 4.         0.55299122]. \t  -0.578556985871312 \t -0.4459810811310791\n",
            "3      \t [0.32904954 9.80442032 8.         0.7285653  2.         0.94646442]. \t  \u001b[92m-0.43885135786496293\u001b[0m \t -0.43885135786496293\n",
            "4      \t [ 8.54553338  0.49573541 12.          0.81406546 19.          0.20999807]. \t  -0.6795516887212301 \t -0.43885135786496293\n",
            "5      \t [9.40145549 1.43629395 5.         0.63655396 1.         0.63002732]. \t  -0.5662893508870628 \t -0.43885135786496293\n",
            "6      \t [ 8.7238127   0.85665784  6.          0.75815556 12.          0.87180469]. \t  -0.4644522650882914 \t -0.43885135786496293\n",
            "7      \t [ 0.11400844  9.5774521   6.          0.90472821 11.          0.18513309]. \t  -0.6793968950126906 \t -0.43885135786496293\n",
            "8      \t [ 1.95513735  0.12334702 13.          0.54467307 14.          0.86324917]. \t  \u001b[92m-0.43794830902637205\u001b[0m \t -0.43794830902637205\n",
            "9      \t [ 2.89959158  8.10209185 14.          0.9094105   1.          0.86003598]. \t  \u001b[92m-0.42582758559630707\u001b[0m \t -0.42582758559630707\n",
            "10     \t [ 0.63533588  7.88973169 13.          0.71777205  8.          0.22258282]. \t  -0.6847473450330067 \t -0.42582758559630707\n",
            "11     \t [4.59853484 7.19189873 5.         0.92777191 6.         0.5760847 ]. \t  -0.5604008171822471 \t -0.42582758559630707\n",
            "12     \t [ 9.78022669  6.34740254  5.          0.81941991 15.          0.17866883]. \t  -0.6810440437950237 \t -0.42582758559630707\n",
            "13     \t [ 0.44924058  6.01817024 13.          0.62113095 19.          0.22629481]. \t  -0.6838258323776948 \t -0.42582758559630707\n",
            "14     \t [2.08434216 0.20801851 5.85566131 0.5        9.85566131 0.1       ]. \t  -0.6829533902190683 \t -0.42582758559630707\n",
            "15     \t [ 8.93459678  5.75013258 14.          0.86506851 14.          0.11026757]. \t  -0.6798789215383618 \t -0.42582758559630707\n",
            "16     \t [ 1.2199304   9.32862427  5.          0.98694875 18.          0.69275841]. \t  -0.5593626212561581 \t -0.42582758559630707\n",
            "17     \t [7.30733696 1.06961871 9.         0.6583612  6.         0.41488291]. \t  -0.5857440675365031 \t -0.42582758559630707\n",
            "18     \t [ 0.25964514  2.37009305 11.          0.85577938  4.          0.42985865]. \t  -0.556481072440868 \t -0.42582758559630707\n",
            "19     \t [ 5.54919942  8.87232522  5.          0.84416975 12.          0.31830005]. \t  -0.5998464916577737 \t -0.42582758559630707\n",
            "20     \t [ 9.03439965  9.39442441 14.          0.98025678 19.          0.55921955]. \t  -0.550861248380592 \t -0.42582758559630707\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.159065314760995"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z2F_Q194U3uu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70a41f65-7fa3-4795-ccec-aadbd8e3d8ba"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 14\n",
        "\n",
        "np.random.seed(run_num_14)\n",
        "surrogate_loser_14 = dGaussianProcess(cov_func)\n",
        "\n",
        "X_train14, X_test14, y_train14, y_test14 = train_test_split(X, y, test_size=test_perc, random_state=run_num_14)\n",
        "\n",
        "def f_syn_polarity14(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_14, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train14, y=y_train14).mean())\n",
        "    return operator * score\n",
        "\n",
        "loser_14 = dGPGO(surrogate_loser_14, Acquisition_new(util_loser), f_syn_polarity14, param, n_jobs = -1) # define BayesOpt\n",
        "loser_14.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_loser_14 = loser_14.getResult()[0]\n",
        "params_loser_14['max_depth'] = int(params_loser_14['max_depth'])\n",
        "params_loser_14['min_child_weight'] = int(params_loser_14['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_loser_train14 = xgb.DMatrix(X_train14, y_train14)\n",
        "dX_loser_test14 = xgb.DMatrix(X_test14, y_test14)\n",
        "model_loser_14 = xgb.train(params_loser_14, dX_loser_train14)\n",
        "pred_loser_14 = model_loser_14.predict(dX_loser_test14)\n",
        "\n",
        "rmse_loser_14 = np.sqrt(mean_squared_error(pred_loser_14, y_test14))\n",
        "rmse_loser_14"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 8.59560606  1.64569495 11.          0.89021195 15.          0.4857008 ]. \t  -0.6102441482204597 \t -0.4517949441804544\n",
            "init   \t [ 0.57460093  9.25007434  5.          0.56647642 17.          0.7533211 ]. \t  -0.5614002279921607 \t -0.4517949441804544\n",
            "init   \t [1.90674396 4.32019547 9.         0.93708225 3.         0.91775905]. \t  -0.4517949441804544 \t -0.4517949441804544\n",
            "init   \t [ 0.36770475  8.43368681 14.          0.6826844  15.          0.6291966 ]. \t  -0.5407924410004543 \t -0.4517949441804544\n",
            "init   \t [ 5.33840324  0.449182   14.          0.52045439  2.          0.48604556]. \t  -0.6242722768101235 \t -0.4517949441804544\n",
            "1      \t [ 9.54492219  9.07631824  8.          0.54579372 12.          0.38295996]. \t  -0.6262431675183743 \t -0.4517949441804544\n",
            "2      \t [9.53270461 0.83608685 6.         0.99976104 3.         0.60747265]. \t  -0.5533922160484371 \t -0.4517949441804544\n",
            "3      \t [ 1.15250646  0.51569303  5.          0.69322415 17.          0.16291682]. \t  -0.6917729046487754 \t -0.4517949441804544\n",
            "4      \t [ 6.37447116  9.98862868 14.          0.72259558  4.          0.61899393]. \t  -0.5434760859463281 \t -0.4517949441804544\n",
            "5      \t [ 8.08070558  7.65123431 14.          0.59104644 19.          0.9302005 ]. \t  -0.45873776977159153 \t -0.4517949441804544\n",
            "6      \t [ 6.26901123  2.37239677  5.          0.95063922 11.          0.52876485]. \t  -0.6158458956422692 \t -0.4517949441804544\n",
            "7      \t [1.0907871  9.11150389 5.         0.58744161 9.         0.87439163]. \t  -0.4933476055445382 \t -0.4517949441804544\n",
            "8      \t [ 2.21919917  0.31865167 14.          0.54399684 14.          0.18056849]. \t  -0.6942842535038787 \t -0.4517949441804544\n",
            "9      \t [ 7.34435618  8.39108681  7.          0.81547994 19.          0.62767067]. \t  -0.546522525570801 \t -0.4517949441804544\n",
            "10     \t [ 6.57539862  4.15362663 14.          0.96053826  9.          0.2080482 ]. \t  -0.691194495047777 \t -0.4517949441804544\n",
            "11     \t [6.00529466 8.33461243 6.         0.97581801 4.         0.31582778]. \t  -0.6226835813168086 \t -0.4517949441804544\n",
            "12     \t [0.56405811 0.         5.         0.5        7.860223   0.1       ]. \t  -0.6942634907462908 \t -0.4517949441804544\n",
            "13     \t [0.  0.  5.  0.5 1.  0.1]. \t  -0.6943016112587715 \t -0.4517949441804544\n",
            "14     \t [ 0.84209312  7.86749962 14.          0.8648448   1.          0.1570582 ]. \t  -0.6937301125121068 \t -0.4517949441804544\n",
            "15     \t [ 1.22436035  9.67689861 12.          0.7460835   9.          0.69284382]. \t  -0.5399272231860202 \t -0.4517949441804544\n",
            "16     \t [ 4.4386325   0.37972087 10.          0.61118217  8.          0.15198669]. \t  -0.6912722224579347 \t -0.4517949441804544\n",
            "17     \t [ 7.0496565   9.86456792 14.          0.62416475 11.          0.68843039]. \t  -0.5429606831910581 \t -0.4517949441804544\n",
            "18     \t [ 3.32112913  6.27078367  9.          0.98238085 14.          0.82374888]. \t  -0.5322541144293694 \t -0.4517949441804544\n",
            "19     \t [ 9.66203958  6.20819976 11.          0.71875858  5.          0.43480687]. \t  -0.6129974118264951 \t -0.4517949441804544\n",
            "20     \t [ 0.13684155  3.86104617 14.          0.82673561  9.          0.20283653]. \t  -0.6919355943160154 \t -0.4517949441804544\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.334202643456426"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Po5wImJaU6VC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d182981-430b-46de-c0b2-af4c97f7a955"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 15\n",
        "\n",
        "np.random.seed(run_num_15)\n",
        "surrogate_loser_15 = dGaussianProcess(cov_func)\n",
        "\n",
        "X_train15, X_test15, y_train15, y_test15 = train_test_split(X, y, test_size=test_perc, random_state=run_num_15)\n",
        "\n",
        "def f_syn_polarity15(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_15, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train15, y=y_train15).mean())\n",
        "    return operator * score\n",
        "\n",
        "loser_15 = dGPGO(surrogate_loser_15, Acquisition_new(util_loser), f_syn_polarity15, param, n_jobs = -1) # define BayesOpt\n",
        "loser_15.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_loser_15 = loser_15.getResult()[0]\n",
        "params_loser_15['max_depth'] = int(params_loser_15['max_depth'])\n",
        "params_loser_15['min_child_weight'] = int(params_loser_15['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_loser_train15 = xgb.DMatrix(X_train15, y_train15)\n",
        "dX_loser_test15 = xgb.DMatrix(X_test15, y_test15)\n",
        "model_loser_15 = xgb.train(params_loser_15, dX_loser_train15)\n",
        "pred_loser_15 = model_loser_15.predict(dX_loser_test15)\n",
        "\n",
        "rmse_loser_15 = np.sqrt(mean_squared_error(pred_loser_15, y_test15))\n",
        "rmse_loser_15"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 5.64315424  8.97297685  6.          0.81493635 11.          0.33008564]. \t  -0.5946480910132583 \t -0.4392277798535851\n",
            "init   \t [9.51455383 9.60803032 5.         0.68485728 1.         0.30145081]. \t  -0.5968379804084254 \t -0.4392277798535851\n",
            "init   \t [ 8.11809889  6.32508536  7.          0.61766329 19.          0.51383159]. \t  -0.5092318398204285 \t -0.4392277798535851\n",
            "init   \t [ 7.28571251  5.36071389 10.          0.76240297 12.          0.12035683]. \t  -0.6346120183758519 \t -0.4392277798535851\n",
            "init   \t [ 6.80847768  3.59767194 14.          0.51365585 16.          0.89730673]. \t  -0.4392277798535851 \t -0.4392277798535851\n",
            "1      \t [ 2.28792065  8.58421878 12.          0.67551126  1.          0.37971264]. \t  -0.596161946051267 \t -0.4392277798535851\n",
            "2      \t [ 9.35473751  1.48895841 12.          0.55362474  4.          0.11106606]. \t  -0.6327865627642436 \t -0.4392277798535851\n",
            "3      \t [3.91572323 2.23174129 6.         0.50937407 1.         0.51336364]. \t  -0.5160141940687206 \t -0.4392277798535851\n",
            "4      \t [ 0.74184801  3.17830864 14.          0.90502664  8.          0.31107615]. \t  -0.5905073342805591 \t -0.4392277798535851\n",
            "5      \t [ 1.33182259  0.8125478   9.          0.94558338 18.          0.71777254]. \t  -0.4467705016632985 \t -0.4392277798535851\n",
            "6      \t [ 0.48538268  1.39730148  6.          0.8238236  11.          0.45741795]. \t  -0.5144696373796631 \t -0.4392277798535851\n",
            "7      \t [ 0.57885492  9.35143843  6.          0.69337529 17.          0.15510814]. \t  -0.6331051904120661 \t -0.4392277798535851\n",
            "8      \t [0.24960251 9.45540541 5.         0.62890215 3.         0.73551484]. \t  -0.47327438015853385 \t -0.4392277798535851\n",
            "9      \t [ 8.23428085  9.77240559 13.          0.85977854  8.          0.93876208]. \t  \u001b[92m-0.4281095031928478\u001b[0m \t -0.4281095031928478\n",
            "10     \t [ 1.35750397  9.83931966 14.          0.92698791 15.          0.2272101 ]. \t  -0.6341880539033447 \t -0.4281095031928478\n",
            "11     \t [ 9.85965179  9.04589828 13.          0.95564786 19.          0.1426314 ]. \t  -0.632071954329094 \t -0.4281095031928478\n",
            "12     \t [9.06175259 4.11518452 5.         0.87621161 6.         0.52300377]. \t  -0.5192107686497183 \t -0.4281095031928478\n",
            "13     \t [ 0.15053668  0.43786928 13.          0.62231098  1.          0.88145445]. \t  -0.43523183945633176 \t -0.4281095031928478\n",
            "14     \t [ 7.18775342  0.2062489   5.          0.98239776 17.          0.28043211]. \t  -0.634545784625454 \t -0.4281095031928478\n",
            "15     \t [0.10178544 7.49949902 9.         0.60008085 8.         0.35520416]. \t  -0.5902884239300654 \t -0.4281095031928478\n",
            "16     \t [0.22091899 0.18516021 8.21358087 0.5        5.21358087 0.1       ]. \t  -0.6326798838280104 \t -0.4281095031928478\n",
            "17     \t [ 8.4324691   8.27251391 11.          0.71354715  2.          0.54685464]. \t  -0.498601956775111 \t -0.4281095031928478\n",
            "18     \t [ 2.12400437  9.06769045 14.          0.70585245  9.          0.90846764]. \t  -0.4286407197787245 \t -0.4281095031928478\n",
            "19     \t [9.58390631 0.99765082 6.         0.72021045 1.         0.39555686]. \t  -0.5960083204162412 \t -0.4281095031928478\n",
            "20     \t [ 9.96011127  1.55235537 14.          0.63004724 11.          0.14483121]. \t  -0.6331081546428745 \t -0.4281095031928478\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.189953800077167"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HrAQN-pU9Qo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71c3e927-3002-4b4f-f79b-00edfe3be4fe"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 16\n",
        "\n",
        "np.random.seed(run_num_16)\n",
        "surrogate_loser_16 = dGaussianProcess(cov_func)\n",
        "\n",
        "X_train16, X_test16, y_train16, y_test16 = train_test_split(X, y, test_size=test_perc, random_state=run_num_16)\n",
        "\n",
        "def f_syn_polarity16(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_16, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train16, y=y_train16).mean())\n",
        "    return operator * score\n",
        "\n",
        "loser_16 = dGPGO(surrogate_loser_16, Acquisition_new(util_loser), f_syn_polarity16, param, n_jobs = -1) # define BayesOpt\n",
        "loser_16.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_loser_16 = loser_16.getResult()[0]\n",
        "params_loser_16['max_depth'] = int(params_loser_16['max_depth'])\n",
        "params_loser_16['min_child_weight'] = int(params_loser_16['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_loser_train16 = xgb.DMatrix(X_train16, y_train16)\n",
        "dX_loser_test16 = xgb.DMatrix(X_test16, y_test16)\n",
        "model_loser_16 = xgb.train(params_loser_16, dX_loser_train16)\n",
        "pred_loser_16 = model_loser_16.predict(dX_loser_test16)\n",
        "\n",
        "rmse_loser_16 = np.sqrt(mean_squared_error(pred_loser_16, y_test16))\n",
        "rmse_loser_16"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [8.63948332 3.16366002 7.         0.75395583 1.         0.56999262]. \t  -0.5713220948397237 \t -0.4331621293825035\n",
            "init   \t [ 3.3426621   9.77029134 14.          0.93751218 17.          0.80549151]. \t  -0.4331621293825035 \t -0.4331621293825035\n",
            "init   \t [ 3.36871073  4.8308288  13.          0.91984982 14.          0.48829226]. \t  -0.5561611494046279 \t -0.4331621293825035\n",
            "init   \t [ 8.84540736  1.59826081  8.          0.75772888 14.          0.29029227]. \t  -0.6835926126526815 \t -0.4331621293825035\n",
            "init   \t [ 7.74981136  8.34377636 13.          0.83182409 15.          0.19378622]. \t  -0.7677243885128009 \t -0.4331621293825035\n",
            "1      \t [ 0.60074712  1.1402995   5.          0.75926551 10.          0.30612858]. \t  -0.6844266759595645 \t -0.4331621293825035\n",
            "2      \t [ 3.6191694   8.20490707  5.          0.81280746 16.          0.99306688]. \t  -0.4581574120389309 \t -0.4331621293825035\n",
            "3      \t [ 0.55111138  3.69634376 11.          0.53864409  3.          0.92267876]. \t  -0.43468988385343615 \t -0.4331621293825035\n",
            "4      \t [9.52987381 8.92087345 5.         0.95441282 7.         0.10412548]. \t  -0.7679863416880384 \t -0.4331621293825035\n",
            "5      \t [ 8.31345174  8.15216774 14.          0.81052242  4.          0.5053732 ]. \t  -0.5586554806770512 \t -0.4331621293825035\n",
            "6      \t [ 7.65476591  1.74203379 12.          0.78837413  7.          0.71460419]. \t  -0.4382687724745528 \t -0.4331621293825035\n",
            "7      \t [1.68021993 9.25252958 9.         0.78643474 9.         0.55639556]. \t  -0.5630452959468858 \t -0.4331621293825035\n",
            "8      \t [0.  0.  5.  0.5 1.  0.1]. \t  -0.7670392049994392 \t -0.4331621293825035\n",
            "9      \t [ 2.6903245   0.3560009   7.          0.98255651 19.          0.9851534 ]. \t  -0.4412750880856537 \t -0.4331621293825035\n",
            "10     \t [ 0.4751313   0.18137204 13.          0.70186285 10.          0.37625565]. \t  -0.68582313055112 \t -0.4331621293825035\n",
            "11     \t [ 8.54895835  0.69961229 14.          0.57654595 19.          0.20454627]. \t  -0.7677953049378372 \t -0.4331621293825035\n",
            "12     \t [ 8.76905299  8.15213676  8.          0.5845007  19.          0.27314045]. \t  -0.7678874315811665 \t -0.4331621293825035\n",
            "13     \t [ 1.48533436  9.57311484 14.          0.8193199   3.          0.99329178]. \t  \u001b[92m-0.4185372839160708\u001b[0m \t -0.4185372839160708\n",
            "14     \t [4.49490325 9.22009337 8.         0.71314799 3.         0.58255672]. \t  -0.48560746523878195 \t -0.4185372839160708\n",
            "15     \t [ 0.76287087  0.49621071 13.          0.81150873 18.          0.76762748]. \t  -0.4363142295126112 \t -0.4185372839160708\n",
            "16     \t [0.56061777 6.48476685 5.         0.81826035 5.         0.59167243]. \t  -0.5183862929687044 \t -0.4185372839160708\n",
            "17     \t [8.8152352  0.5898958  5.         0.831707   9.         0.92137997]. \t  -0.45519808186832184 \t -0.4185372839160708\n",
            "18     \t [ 8.76208148  1.79224635  5.          0.52306305 19.          0.78632749]. \t  -0.4871095639687443 \t -0.4185372839160708\n",
            "19     \t [ 9.63332031  8.534049   10.          0.55556169 10.          0.35894848]. \t  -0.6847217417593005 \t -0.4185372839160708\n",
            "20     \t [ 4.52692506  0.27496659 10.          0.6345842   1.          0.46474283]. \t  -0.5631095705887621 \t -0.4185372839160708\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.361483761377183"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXelbcAVVCqO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24d99d85-a8a6-42c0-df97-021289fdba0d"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 17\n",
        "\n",
        "np.random.seed(run_num_17)\n",
        "surrogate_loser_17 = dGaussianProcess(cov_func)\n",
        "\n",
        "X_train17, X_test17, y_train17, y_test17 = train_test_split(X, y, test_size=test_perc, random_state=run_num_17)\n",
        "\n",
        "def f_syn_polarity17(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_17, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train17, y=y_train17).mean())\n",
        "    return operator * score\n",
        "\n",
        "loser_17 = dGPGO(surrogate_loser_17, Acquisition_new(util_loser), f_syn_polarity17, param, n_jobs = -1) # define BayesOpt\n",
        "loser_17.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_loser_17 = loser_17.getResult()[0]\n",
        "params_loser_17['max_depth'] = int(params_loser_17['max_depth'])\n",
        "params_loser_17['min_child_weight'] = int(params_loser_17['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_loser_train17 = xgb.DMatrix(X_train17, y_train17)\n",
        "dX_loser_test17 = xgb.DMatrix(X_test17, y_test17)\n",
        "model_loser_17 = xgb.train(params_loser_17, dX_loser_train17)\n",
        "pred_loser_17 = model_loser_17.predict(dX_loser_test17)\n",
        "\n",
        "rmse_loser_17 = np.sqrt(mean_squared_error(pred_loser_17, y_test17))\n",
        "rmse_loser_17"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 1.85575275  3.86599303 13.          0.85645882  2.          0.78699813]. \t  -0.4597056260580034 \t -0.4597056260580034\n",
            "init   \t [1.97780831 2.92527676 8.         0.92268607 9.         0.39017299]. \t  -0.5687207024308397 \t -0.4597056260580034\n",
            "init   \t [ 7.16656007  1.028324    5.          0.96975483 15.          0.90932849]. \t  -0.4688025715067707 \t -0.4597056260580034\n",
            "init   \t [ 2.46007304  9.53596657  7.          0.71704714 11.          0.17659118]. \t  -0.6983036523024195 \t -0.4597056260580034\n",
            "init   \t [3.44161135 9.92427612 8.         0.96331165 2.         0.29160186]. \t  -0.5675253504862205 \t -0.4597056260580034\n",
            "1      \t [ 9.98828606  3.49693094 13.          0.57156882  8.          0.14604355]. \t  -0.6992595730796743 \t -0.4597056260580034\n",
            "2      \t [ 6.95619362  6.95953721 12.          0.92317057 19.          0.34981799]. \t  -0.5628121282495753 \t -0.4597056260580034\n",
            "3      \t [8.58397417 0.63190934 7.         0.80049626 2.         0.98851802]. \t  \u001b[92m-0.453932525933606\u001b[0m \t -0.453932525933606\n",
            "4      \t [ 1.79054908  0.97767001 14.          0.72051649 17.          0.75055942]. \t  -0.4586950539387324 \t -0.453932525933606\n",
            "5      \t [0.  0.  5.  0.5 1.  0.1]. \t  -0.6990710246949499 \t -0.453932525933606\n",
            "6      \t [ 0.19331023  6.0899655   6.          0.97237498 19.          0.59441475]. \t  -0.54171231691412 \t -0.453932525933606\n",
            "7      \t [ 8.95440724  5.31304705 13.          0.73043408  1.          0.56398539]. \t  -0.5373247826064305 \t -0.453932525933606\n",
            "8      \t [ 8.26790477  9.9524702   5.          0.59716297 16.          0.88254342]. \t  -0.47691281251538375 \t -0.453932525933606\n",
            "9      \t [ 0.90443942  7.22834774 14.          0.92580232 12.          0.13465218]. \t  -0.6965996897404226 \t -0.453932525933606\n",
            "10     \t [ 8.52285971  0.31262108 12.          0.8797653  16.          0.34954268]. \t  -0.5627660085466504 \t -0.453932525933606\n",
            "11     \t [ 7.87455923  9.60434668 13.          0.6097695  11.          0.18672661]. \t  -0.6988278637935389 \t -0.453932525933606\n",
            "12     \t [7.77469206 6.81026607 6.         0.84173013 6.         0.4321758 ]. \t  -0.5485988100854867 \t -0.453932525933606\n",
            "13     \t [ 1.78559475  9.94726594 14.          0.84124706  5.          0.41412983]. \t  -0.5643771257102934 \t -0.453932525933606\n",
            "14     \t [ 3.55475249  2.77437165 14.          0.64501595  8.          0.71111614]. \t  -0.5328675498676292 \t -0.453932525933606\n",
            "15     \t [9.7645138  6.96943344 7.         0.64264761 1.         0.5601612 ]. \t  -0.5443333775093777 \t -0.453932525933606\n",
            "16     \t [ 1.13635919  9.59512209 12.          0.5730343  19.          0.36892674]. \t  -0.5697966935770927 \t -0.453932525933606\n",
            "17     \t [6.7007636  0.         5.         0.5        9.02769561 0.1       ]. \t  -0.6997654771340948 \t -0.453932525933606\n",
            "18     \t [2.67758022 4.58792547 5.         0.80193375 3.         0.36129163]. \t  -0.5801383609237367 \t -0.453932525933606\n",
            "19     \t [ 0.          0.11898767  8.66620882  0.5        15.66620882  0.1       ]. \t  -0.6994511860723951 \t -0.453932525933606\n",
            "20     \t [ 4.7028715   4.84232296 11.          0.86477956 14.          0.62900428]. \t  -0.534274537118095 \t -0.453932525933606\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.347883948412589"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJG2fAtAVFDZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7825f7c7-54fa-4f94-cc52-c415daf92abd"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 18\n",
        "\n",
        "np.random.seed(run_num_18)\n",
        "surrogate_loser_18 = dGaussianProcess(cov_func)\n",
        "\n",
        "X_train18, X_test18, y_train18, y_test18 = train_test_split(X, y, test_size=test_perc, random_state=run_num_18)\n",
        "\n",
        "def f_syn_polarity18(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_11, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train18, y=y_train18).mean())\n",
        "    return operator * score\n",
        "\n",
        "loser_18 = dGPGO(surrogate_loser_18, Acquisition_new(util_loser), f_syn_polarity18, param, n_jobs = -1) # define BayesOpt\n",
        "loser_18.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_loser_18 = loser_18.getResult()[0]\n",
        "params_loser_18['max_depth'] = int(params_loser_18['max_depth'])\n",
        "params_loser_18['min_child_weight'] = int(params_loser_18['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_loser_train18 = xgb.DMatrix(X_train18, y_train18)\n",
        "dX_loser_test18 = xgb.DMatrix(X_test18, y_test18)\n",
        "model_loser_18 = xgb.train(params_loser_18, dX_loser_train18)\n",
        "pred_loser_18 = model_loser_18.predict(dX_loser_test18)\n",
        "\n",
        "rmse_loser_18 = np.sqrt(mean_squared_error(pred_loser_18, y_test18))\n",
        "rmse_loser_18"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 8.73429403  9.68540663 10.          0.68875849  9.          0.48011572]. \t  -0.48312028405718427 \t -0.4337207096533448\n",
            "init   \t [ 6.12033333  7.66062926  8.          0.76133734 13.          0.93379456]. \t  -0.44701743563076113 \t -0.4337207096533448\n",
            "init   \t [ 1.46524679  7.01527914  7.          0.90913299 10.          0.36016753]. \t  -0.6042122061654378 \t -0.4337207096533448\n",
            "init   \t [ 9.73855241  3.33774046 14.          0.53290419  7.          0.7088681 ]. \t  -0.47205818581725156 \t -0.4337207096533448\n",
            "init   \t [ 3.00618018  1.82702795 11.          0.75681389 14.          0.98627449]. \t  -0.4337207096533448 \t -0.4337207096533448\n",
            "1      \t [ 1.46586883  1.13810325 14.          0.90052794  5.          0.16182112]. \t  -0.7143993241707929 \t -0.4337207096533448\n",
            "2      \t [ 9.17850108  5.22481201 14.          0.60105853 18.          0.63442006]. \t  -0.47652759630987357 \t -0.4337207096533448\n",
            "3      \t [6.20612396 4.37552923 6.         0.71941082 3.         0.90943147]. \t  -0.4522652783180721 \t -0.4337207096533448\n",
            "4      \t [ 0.42107725  9.92938812 12.          0.92462675 18.          0.88352347]. \t  \u001b[92m-0.42696094495506925\u001b[0m \t -0.42696094495506925\n",
            "5      \t [ 1.45418057  8.89399    11.          0.93175096  1.          0.52391385]. \t  -0.4748883821619788 \t -0.42696094495506925\n",
            "6      \t [0.         0.         5.21827599 0.5        6.21827599 0.1       ]. \t  -0.7142978598833383 \t -0.42696094495506925\n",
            "7      \t [ 8.67626106  0.1397848   6.          0.55771681 18.          0.89120888]. \t  -0.4678231542313444 \t -0.42696094495506925\n",
            "8      \t [ 7.20807715  7.41236348  5.          0.52425285 19.          0.24651851]. \t  -0.7151531754828971 \t -0.42696094495506925\n",
            "9      \t [7.94412856 1.24068931 7.         0.96649935 9.         0.47928692]. \t  -0.48764145519377255 \t -0.42696094495506925\n",
            "10     \t [ 2.66410938  9.83743919 13.          0.58899688  8.          0.29675269]. \t  -0.6087724199154481 \t -0.42696094495506925\n",
            "11     \t [ 8.62753755  4.44899956 12.          0.98231134  1.          0.88389967]. \t  \u001b[92m-0.42538744453232347\u001b[0m \t -0.42538744453232347\n",
            "12     \t [ 1.2277143   1.05411485  5.          0.50198686 13.          0.89258454]. \t  -0.4690426518871574 \t -0.42538744453232347\n",
            "13     \t [ 3.86727486  0.04582369 10.          0.54629234  1.          0.59406244]. \t  -0.47507759871416455 \t -0.42538744453232347\n",
            "14     \t [9.89654007 7.91459554 5.         0.64703423 6.         0.95875063]. \t  -0.4610204259667869 \t -0.42538744453232347\n",
            "15     \t [ 9.49381141  1.31696272 11.          0.51283555 13.          0.25364349]. \t  -0.7155727045133423 \t -0.42538744453232347\n",
            "16     \t [0.         3.85327138 5.71339306 0.5        1.         0.1       ]. \t  -0.7130688739294866 \t -0.42538744453232347\n",
            "17     \t [ 1.04974938  7.3117691   5.          0.60120794 17.          0.83587336]. \t  -0.49012043280719225 \t -0.42538744453232347\n",
            "18     \t [5.83203328 9.4812958  5.         0.60479193 1.         0.24995758]. \t  -0.7139401129279975 \t -0.42538744453232347\n",
            "19     \t [ 3.86883296  7.31704712 14.          0.75047101 14.          0.80828494]. \t  -0.4565506921595134 \t -0.42538744453232347\n",
            "20     \t [ 4.0233419   3.91847735 10.          0.87935081  7.          0.57035954]. \t  -0.47552190244029113 \t -0.42538744453232347\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3.937438334950126"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHidSEGcVHvG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8977216-31e9-4ae9-8c2e-d9e045b0d001"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 19\n",
        "\n",
        "np.random.seed(run_num_19)\n",
        "surrogate_loser_19 = dGaussianProcess(cov_func)\n",
        "\n",
        "X_train19, X_test19, y_train19, y_test19 = train_test_split(X, y, test_size=test_perc, random_state=run_num_19)\n",
        "\n",
        "def f_syn_polarity19(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_19, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train19, y=y_train19).mean())\n",
        "    return operator * score\n",
        "\n",
        "loser_19 = dGPGO(surrogate_loser_19, Acquisition_new(util_loser), f_syn_polarity19, param, n_jobs = -1) # define BayesOpt\n",
        "loser_19.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_loser_19 = loser_19.getResult()[0]\n",
        "params_loser_19['max_depth'] = int(params_loser_19['max_depth'])\n",
        "params_loser_19['min_child_weight'] = int(params_loser_19['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_loser_train19 = xgb.DMatrix(X_train19, y_train19)\n",
        "dX_loser_test19 = xgb.DMatrix(X_test19, y_test19)\n",
        "model_loser_19 = xgb.train(params_loser_19, dX_loser_train19)\n",
        "pred_loser_19 = model_loser_19.predict(dX_loser_test19)\n",
        "\n",
        "rmse_loser_19 = np.sqrt(mean_squared_error(pred_loser_19, y_test19))\n",
        "rmse_loser_19"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 8.24520174  9.84626759 12.          0.81846905 12.          0.27594309]. \t  -0.6624515538441494 \t -0.4321567975765851\n",
            "init   \t [ 8.6021676   8.64300514 14.          0.95067208 18.          0.13363564]. \t  -0.663790961968593 \t -0.4321567975765851\n",
            "init   \t [ 3.02486657  6.06229509 10.          0.50494482  9.          0.10218675]. \t  -0.668875700942164 \t -0.4321567975765851\n",
            "init   \t [ 6.57586195  8.01231848  7.          0.63945407 18.          0.49206406]. \t  -0.5640092362987262 \t -0.4321567975765851\n",
            "init   \t [ 6.05008856  8.15558548 14.          0.64718943  8.          0.90611106]. \t  -0.4321567975765851 \t -0.4321567975765851\n",
            "1      \t [ 0.85655458  0.69294498  9.          0.90992265 19.          0.25605959]. \t  -0.6632879756016564 \t -0.4321567975765851\n",
            "2      \t [ 1.16239725  9.89254371 11.          0.98469188  1.          0.20271996]. \t  -0.6659068700895453 \t -0.4321567975765851\n",
            "3      \t [9.26425961 9.717621   5.         0.8856331  5.         0.65589573]. \t  -0.5256636565447839 \t -0.4321567975765851\n",
            "4      \t [8.38158931 1.39953008 5.         0.6836482  9.         0.46934447]. \t  -0.5657479189361881 \t -0.4321567975765851\n",
            "5      \t [ 9.89729152  2.88709408 11.          0.55121781  3.          0.58381748]. \t  -0.5217676759230677 \t -0.4321567975765851\n",
            "6      \t [0.  0.  5.  0.5 1.  0.1]. \t  -0.6682393310586452 \t -0.4321567975765851\n",
            "7      \t [ 0.52039992  0.56380532 14.          0.70324781  2.          0.77427978]. \t  -0.44119318049659856 \t -0.4321567975765851\n",
            "8      \t [ 9.45607821  0.96438893 14.          0.56821928 16.          0.48995458]. \t  -0.560546633196372 \t -0.4321567975765851\n",
            "9      \t [ 0.56432836  7.79494932 13.          0.55871992 19.          0.10266045]. \t  -0.6660467600439527 \t -0.4321567975765851\n",
            "10     \t [ 3.16676683  1.78770811 13.          0.5650927  13.          0.7417697 ]. \t  -0.4472490143246649 \t -0.4321567975765851\n",
            "11     \t [5.56939336 3.95670769 5.         0.95243816 3.         0.31115527]. \t  -0.6118696715045784 \t -0.4321567975765851\n",
            "12     \t [0.         0.         6.79132524 0.5        9.79132524 0.1       ]. \t  -0.6680142576361826 \t -0.4321567975765851\n",
            "13     \t [ 7.46605559  8.55652968 13.          0.99452706  1.          0.43764631]. \t  -0.5573240867828692 \t -0.4321567975765851\n",
            "14     \t [ 9.00192632  0.12142981  6.          0.72039779 17.          0.26695117]. \t  -0.6635472276398457 \t -0.4321567975765851\n",
            "15     \t [ 1.20113738  5.88016015  6.          0.57839075 15.          0.60752162]. \t  -0.5267223663659426 \t -0.4321567975765851\n",
            "16     \t [1.7958895  9.19371155 5.         0.54448974 5.         0.93868386]. \t  -0.4669880318809955 \t -0.4321567975765851\n",
            "17     \t [ 9.62487731  6.86253449  5.          0.66465947 12.          0.21107909]. \t  -0.6659019290689656 \t -0.4321567975765851\n",
            "18     \t [ 7.01475491  4.53933393 10.          0.82075044 14.          0.83058729]. \t  -0.4517750017757683 \t -0.4321567975765851\n",
            "19     \t [ 5.75277105  1.25995511 14.          0.97274397  8.          0.39753437]. \t  -0.6135363407765908 \t -0.4321567975765851\n",
            "20     \t [ 2.00664037  5.88682269 14.          0.94696775  4.          0.80981462]. \t  -0.4349577325456077 \t -0.4321567975765851\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.2307134915487135"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWGPYRJhVKsO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54e0d5cd-10e8-45de-f8fa-e750b56483d2"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 20\n",
        "\n",
        "np.random.seed(run_num_20)\n",
        "surrogate_loser_20 = dGaussianProcess(cov_func)\n",
        "\n",
        "X_train20, X_test20, y_train20, y_test20 = train_test_split(X, y, test_size=test_perc, random_state=run_num_20)\n",
        "\n",
        "def f_syn_polarity20(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_20, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train20, y=y_train20).mean())\n",
        "    return operator * score\n",
        "\n",
        "loser_20 = dGPGO(surrogate_loser_20, Acquisition_new(util_loser), f_syn_polarity20, param, n_jobs = -1) # define BayesOpt\n",
        "loser_20.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_loser_20 = loser_20.getResult()[0]\n",
        "params_loser_20['max_depth'] = int(params_loser_20['max_depth'])\n",
        "params_loser_20['min_child_weight'] = int(params_loser_20['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_loser_train20 = xgb.DMatrix(X_train20, y_train20)\n",
        "dX_loser_test20 = xgb.DMatrix(X_test20, y_test20)\n",
        "model_loser_20 = xgb.train(params_loser_20, dX_loser_train20)\n",
        "pred_loser_20 = model_loser_20.predict(dX_loser_test20)\n",
        "\n",
        "rmse_loser_20 = np.sqrt(mean_squared_error(pred_loser_20, y_test20))\n",
        "rmse_loser_20"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 5.70517285  5.64528755 11.          0.51213999 19.          0.57883228]. \t  -0.5510634045248362 \t -0.4667910180825121\n",
            "init   \t [0.68106288 5.8452906  6.         0.58037829 8.         0.87974899]. \t  -0.4667910180825121 \t -0.4667910180825121\n",
            "init   \t [ 4.04121426  7.17971417 11.          0.57603584 10.          0.19954225]. \t  -0.6541702754539586 \t -0.4667910180825121\n",
            "init   \t [ 5.27556761  8.67655329 13.          0.54252273  4.          0.48570796]. \t  -0.5813476654605825 \t -0.4667910180825121\n",
            "init   \t [ 5.30003889  5.73946822 12.          0.52666698 16.          0.77467019]. \t  -0.5078849923055367 \t -0.4667910180825121\n",
            "1      \t [ 9.25190708  0.65043072 10.          0.56676233  5.          0.41289847]. \t  -0.6164858434894066 \t -0.4667910180825121\n",
            "2      \t [8.65689736 8.11684598 5.         0.53907072 1.         0.87574215]. \t  -0.47576906935129115 \t -0.4667910180825121\n",
            "3      \t [6.49728107e-01 7.72862043e-04 7.00000000e+00 5.71905117e-01\n",
            " 2.00000000e+00 1.02014940e-01]. \t  -0.6533990377801203 \t -0.4667910180825121\n",
            "4      \t [ 9.51806562  0.44900923  6.          0.62997227 16.          0.40028974]. \t  -0.6257141325107729 \t -0.4667910180825121\n",
            "5      \t [ 0.48808143  1.97030839  7.          0.79892377 16.          0.98477167]. \t  \u001b[92m-0.4602371335347518\u001b[0m \t -0.4602371335347518\n",
            "6      \t [ 9.98029391  8.23632437  6.          0.63606408 13.          0.22908429]. \t  -0.6532745897130707 \t -0.4602371335347518\n",
            "7      \t [ 3.9245879   0.87058505 14.          0.68269183  1.          0.34048922]. \t  -0.623283664424315 \t -0.4602371335347518\n",
            "8      \t [ 0.45468431  0.13267263 10.          0.667214    9.          0.47071141]. \t  -0.5807933165122651 \t -0.4602371335347518\n",
            "9      \t [ 4.64792903  9.65609988  6.          0.78696753 17.          0.90406502]. \t  -0.47227152594951016 \t -0.4602371335347518\n",
            "10     \t [ 9.73713042  9.77875019 14.          0.8552366  13.          0.6588042 ]. \t  -0.5450528828222597 \t -0.4602371335347518\n",
            "11     \t [ 9.74564452  1.02954501 14.          0.98964301 12.          0.47743205]. \t  -0.5773481317621234 \t -0.4602371335347518\n",
            "12     \t [ 7.76906477  2.6060444   5.          0.53491145 10.          0.41560504]. \t  -0.627993339841767 \t -0.4602371335347518\n",
            "13     \t [6.49958993 1.98622034 6.         0.83635209 1.         0.2931143 ]. \t  -0.6276302072456058 \t -0.4602371335347518\n",
            "14     \t [1.36051555 9.95957797 7.         0.9242116  2.         0.87097462]. \t  \u001b[92m-0.449167771194887\u001b[0m \t -0.449167771194887\n",
            "15     \t [ 0.20978943  8.45447688 14.          0.77351134 17.          0.2656195 ]. \t  -0.6586945947582249 \t -0.449167771194887\n",
            "16     \t [7.55121074 9.86496533 6.         0.88828535 7.         0.3097795 ]. \t  -0.6258969685789838 \t -0.449167771194887\n",
            "17     \t [ 1.88681738  3.46035586 14.          0.68070687  6.          0.9944693 ]. \t  \u001b[92m-0.42905917541157645\u001b[0m \t -0.42905917541157645\n",
            "18     \t [ 1.00455473  4.86452269 10.          0.65242434  1.          0.22603818]. \t  -0.6579989052523156 \t -0.42905917541157645\n",
            "19     \t [ 0.87144588  0.14777143 14.          0.88891044 13.          0.11582186]. \t  -0.6586044677275812 \t -0.42905917541157645\n",
            "20     \t [ 8.17732484  5.22697425 11.          0.61303379  1.          0.32672413]. \t  -0.6191817873624627 \t -0.42905917541157645\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.2900830531756915"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1d_1LyydIfe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee7b2dc9-7cd3-4943-85f5-f5d1ea48c24e"
      },
      "source": [
        "end_lose = time.time()\n",
        "end_lose\n",
        "\n",
        "time_lose = end_lose - start_lose\n",
        "time_lose\n",
        "\n",
        "start_win = time.time()\n",
        "start_win"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1615972904.3117816"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZAyOw7XYVwAf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20e2efc6-c2c8-4aee-d380-b261594994a5"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 1 \n",
        "\n",
        "np.random.seed(run_num_1)\n",
        "surrogate_winner_1 = dtStudentProcess(d_cov_func, nu = df)\n",
        "\n",
        "X_train1, X_test1, y_train1, y_test1 = train_test_split(X, y, test_size=test_perc, random_state=run_num_1)\n",
        "\n",
        "def f_syn_polarity1(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_1, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train1, y=y_train1).mean())\n",
        "    return operator * score\n",
        "\n",
        "winner_1 = dGPGO_stp(surrogate_winner_1, Acquisition_new(util_winner), f_syn_polarity1, param, n_jobs = -1) # Define BayesOpt\n",
        "winner_1.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_winner_1 = winner_1.getResult()[0]\n",
        "params_winner_1['max_depth'] = int(params_winner_1['max_depth'])\n",
        "params_winner_1['min_child_weight'] = int(params_winner_1['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_winner_train1 = xgb.DMatrix(X_train1, y_train1)\n",
        "dX_winner_test1 = xgb.DMatrix(X_test1, y_test1)\n",
        "model_winner_1 = xgb.train(params_winner_1, dX_winner_train1)\n",
        "pred_winner_1 = model_winner_1.predict(dX_winner_test1)\n",
        "\n",
        "rmse_winner_1 = np.sqrt(mean_squared_error(pred_winner_1, y_test1))\n",
        "rmse_winner_1"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 6.12170176  1.69069754 11.          0.92695323  3.          0.36579277]. \t  -0.5875926438135084 \t -0.45245711231879204\n",
            "init   \t [ 1.49162957  0.22478325  6.          0.85613654 15.          0.19363322]. \t  -0.6811472770543097 \t -0.45245711231879204\n",
            "init   \t [ 5.27957805  2.81819356 11.          0.81062146 14.          0.74826845]. \t  -0.46117277972616416 \t -0.45245711231879204\n",
            "init   \t [3.81060005 6.24236569 7.         0.95038709 5.         0.9861745 ]. \t  -0.45245711231879204 \t -0.45245711231879204\n",
            "init   \t [4.77531134 7.87990376 6.         0.78863691 6.         0.47918609]. \t  -0.5612893767574513 \t -0.45245711231879204\n",
            "1      \t [ 8.28495996  8.75496601  5.          0.95020974 18.          0.2862647 ]. \t  -0.5992477418063687 \t -0.45245711231879204\n",
            "2      \t [ 1.54653084  8.85277533 10.          0.53898233 18.          0.74883978]. \t  -0.4727718253663954 \t -0.45245711231879204\n",
            "3      \t [ 0.39700633  8.08942284 14.          0.7083877  10.          0.62267441]. \t  -0.493623478965383 \t -0.45245711231879204\n",
            "4      \t [0.  0.  5.  0.5 1.  0.1]. \t  -0.6826322236836013 \t -0.45245711231879204\n",
            "5      \t [ 8.46942679  9.63483083 14.          0.86421899  7.          0.60511455]. \t  -0.4912126856170227 \t -0.45245711231879204\n",
            "6      \t [ 9.82507595  1.56602987  6.          0.51893222 18.          0.48410065]. \t  -0.5715304540031785 \t -0.45245711231879204\n",
            "7      \t [ 9.47779614  9.12501886 14.          0.71871616 18.          0.46278861]. \t  -0.5412895915282432 \t -0.45245711231879204\n",
            "8      \t [ 0.1853115   8.98526847  5.          0.8540841  12.          0.79920234]. \t  -0.49199328039433976 \t -0.45245711231879204\n",
            "9      \t [9.70661037 0.22169724 6.         0.9214776  8.         0.80575274]. \t  -0.482425598895672 \t -0.45245711231879204\n",
            "10     \t [ 6.55881934  9.61009639  9.          0.71637903 12.          0.38458856]. \t  -0.5912939015315611 \t -0.45245711231879204\n",
            "11     \t [ 1.72102482  9.78447433 14.          0.77917525  3.          0.49113021]. \t  -0.5383844221387294 \t -0.45245711231879204\n",
            "12     \t [0.51753753 1.68511981 9.         0.80360039 9.         0.45200858]. \t  -0.5426731188677099 \t -0.45245711231879204\n",
            "13     \t [ 0.47819106  0.69361363 14.          0.7271902   3.          0.48615024]. \t  -0.5388370338312534 \t -0.45245711231879204\n",
            "14     \t [ 9.2397612   6.19696923 13.          0.86546449  1.          0.66132276]. \t  -0.4970602025326462 \t -0.45245711231879204\n",
            "15     \t [ 2.02410169  1.22367174 12.          0.7653715  19.          0.88484083]. \t  \u001b[92m-0.4468851480454651\u001b[0m \t -0.4468851480454651\n",
            "16     \t [ 8.70209222  0.21363429 14.          0.98391211 18.          0.50653374]. \t  -0.5341942124085062 \t -0.4468851480454651\n",
            "17     \t [9.47136026 9.00560534 5.         0.98230268 2.         0.49514519]. \t  -0.5664293849910045 \t -0.4468851480454651\n",
            "18     \t [8.42183872 2.7124637  5.         0.83754722 2.         0.34952499]. \t  -0.599759594599271 \t -0.4468851480454651\n",
            "19     \t [ 9.82386305  4.58516929 12.          0.79683878 10.          0.23266768]. \t  -0.6796591236881087 \t -0.4468851480454651\n",
            "20     \t [ 6.99523272  4.18356505  5.          0.58159574 13.          0.36637616]. \t  -0.6019542260743794 \t -0.4468851480454651\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.485430541260721"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hrDQbChpZ48F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de4a18ea-09d2-473d-900e-a5d96209fa57"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 2 \n",
        "\n",
        "np.random.seed(run_num_2)\n",
        "surrogate_winner_2 = dtStudentProcess(d_cov_func, nu = df)\n",
        "\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X, y, test_size=test_perc, random_state=run_num_2)\n",
        "\n",
        "def f_syn_polarity2(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_2, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train2, y=y_train2).mean())\n",
        "    return operator * score\n",
        "\n",
        "winner_2 = dGPGO_stp(surrogate_winner_2, Acquisition_new(util_winner), f_syn_polarity2, param, n_jobs = -1) # Define BayesOpt\n",
        "winner_2.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_winner_2 = winner_2.getResult()[0]\n",
        "params_winner_2['max_depth'] = int(params_winner_2['max_depth'])\n",
        "params_winner_2['min_child_weight'] = int(params_winner_2['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_winner_train2 = xgb.DMatrix(X_train2, y_train2)\n",
        "dX_winner_test2 = xgb.DMatrix(X_test2, y_test2)\n",
        "model_winner_2 = xgb.train(params_winner_2, dX_winner_train2)\n",
        "pred_winner_2 = model_winner_2.predict(dX_winner_test2)\n",
        "\n",
        "rmse_winner_2 = np.sqrt(mean_squared_error(pred_winner_2, y_test2))\n",
        "rmse_winner_2"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 8.51985493  0.73903599  7.          0.71824677 17.          0.61827209]. \t  -0.5233872966482411 \t -0.42848969661986275\n",
            "init   \t [ 8.4047092   4.35120551 12.          0.84231905 16.          0.80172484]. \t  -0.42848969661986275 \t -0.42848969661986275\n",
            "init   \t [ 6.42749368  9.61026168 10.          0.73363448 15.          0.84932207]. \t  -0.43419399571353007 \t -0.42848969661986275\n",
            "init   \t [ 2.66008363  8.36685389  6.          0.92082163 12.          0.69999264]. \t  -0.5353675801921922 \t -0.42848969661986275\n",
            "init   \t [ 1.74779478  2.17691512 12.          0.71794562  2.          0.48990646]. \t  -0.5346620615324694 \t -0.42848969661986275\n",
            "1      \t [0.65899411 0.08911785 5.         0.57513718 8.         0.6649358 ]. \t  -0.5435492302872315 \t -0.42848969661986275\n",
            "2      \t [7.50154303 9.49874736 7.         0.83842364 4.         0.5714185 ]. \t  -0.5422231033003287 \t -0.42848969661986275\n",
            "3      \t [9.88263606 1.71181353 7.         0.97275193 1.         0.74825108]. \t  -0.441747506473179 \t -0.42848969661986275\n",
            "4      \t [ 0.38425009  3.26044865 12.          0.89101602 19.          0.732816  ]. \t  \u001b[92m-0.4225097924337229\u001b[0m \t -0.4225097924337229\n",
            "5      \t [ 0.17098677  4.24340552 14.          0.58045842 10.          0.64319766]. \t  -0.5100599626019375 \t -0.4225097924337229\n",
            "6      \t [ 8.24807017  4.40295761 13.          0.99966982  5.          0.29909156]. \t  -0.5674503952477116 \t -0.4225097924337229\n",
            "7      \t [8.17228996 0.99614058 7.         0.60509314 8.         0.9040458 ]. \t  -0.44411068565064227 \t -0.4225097924337229\n",
            "8      \t [2.07302894 5.13942199 5.         0.5        1.         0.1       ]. \t  -0.6866146837383418 \t -0.4225097924337229\n",
            "9      \t [ 2.26995028  7.26113474  5.          0.89244153 19.          0.72822627]. \t  -0.46107668663747675 \t -0.4225097924337229\n",
            "10     \t [ 2.44707807  9.48955384 12.          0.5766543   5.          0.45830797]. \t  -0.5358953782855285 \t -0.4225097924337229\n",
            "11     \t [ 0.21362464  0.85295603  6.3210262   0.5        15.05028377  0.11275512]. \t  -0.6857026367955917 \t -0.4225097924337229\n",
            "12     \t [ 9.80974078  5.67295504  6.          0.91127307 13.          0.44957804]. \t  -0.5591662642332185 \t -0.4225097924337229\n",
            "13     \t [ 8.36253694  0.57899647 14.          0.6792832  11.          0.81681485]. \t  -0.42836400911298467 \t -0.4225097924337229\n",
            "14     \t [ 0.99039506  9.70962894 14.          0.53225067 15.          0.94095862]. \t  -0.43140168430232356 \t -0.4225097924337229\n",
            "15     \t [ 4.36825079  3.49854888  9.          0.52840141 12.          0.70471796]. \t  -0.5188136652126865 \t -0.4225097924337229\n",
            "16     \t [ 5.38187691  6.92621159 14.          0.62086458  1.          0.35180496]. \t  -0.5742881458739184 \t -0.4225097924337229\n",
            "17     \t [0.         0.         5.93930688 0.5        1.         0.1       ]. \t  -0.6860882831213788 \t -0.4225097924337229\n",
            "18     \t [ 8.11913722  7.7931672  14.          0.50393458 11.          0.23511725]. \t  -0.6881787024693147 \t -0.4225097924337229\n",
            "19     \t [ 3.91805945  0.19887111 14.          0.50128619 16.          0.19602343]. \t  -0.6878506151361972 \t -0.4225097924337229\n",
            "20     \t [ 3.87077387  0.04333662 11.          0.79311966  8.          0.7797114 ]. \t  -0.423652004810958 \t -0.4225097924337229\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.316024710268154"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpUPyXRfZ95Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b53bdbac-5844-4b6e-df53-f5c7850d2baf"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 3 \n",
        "\n",
        "np.random.seed(run_num_3)\n",
        "surrogate_winner_3 = dtStudentProcess(d_cov_func, nu = df)\n",
        "\n",
        "X_train3, X_test3, y_train3, y_test3 = train_test_split(X, y, test_size=test_perc, random_state=run_num_3)\n",
        "\n",
        "def f_syn_polarity3(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_3, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train3, y=y_train3).mean())\n",
        "    return operator * score\n",
        "\n",
        "winner_3 = dGPGO_stp(surrogate_winner_3, Acquisition_new(util_winner), f_syn_polarity3, param, n_jobs = -1) # Define BayesOpt\n",
        "winner_3.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_winner_3 = winner_3.getResult()[0]\n",
        "params_winner_3['max_depth'] = int(params_winner_3['max_depth'])\n",
        "params_winner_3['min_child_weight'] = int(params_winner_3['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_winner_train3 = xgb.DMatrix(X_train3, y_train3)\n",
        "dX_winner_test3 = xgb.DMatrix(X_test3, y_test3)\n",
        "model_winner_3 = xgb.train(params_winner_3, dX_winner_train3)\n",
        "pred_winner_3 = model_winner_3.predict(dX_winner_test3)\n",
        "\n",
        "rmse_winner_3 = np.sqrt(mean_squared_error(pred_winner_3, y_test3))\n",
        "rmse_winner_3"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 7.51575607  1.09251275 13.          0.80377147 16.          0.31229624]. \t  -0.6858957949546655 \t -0.5081673732303724\n",
            "init   \t [6.12794891 1.19652403 5.         0.74822174 8.         0.33695838]. \t  -0.6849895205987827 \t -0.5081673732303724\n",
            "init   \t [ 6.21631223  4.4909976  10.          0.66764529  9.          0.77207323]. \t  -0.5081673732303724 \t -0.5081673732303724\n",
            "init   \t [ 0.58127233  4.79918477 11.          0.64526552 15.          0.62431773]. \t  -0.5106501747238601 \t -0.5081673732303724\n",
            "init   \t [ 5.76228258  0.67219724  7.          0.88228691 13.          0.29867219]. \t  -0.6839256266327581 \t -0.5081673732303724\n",
            "1      \t [9.79994489 9.57810833 8.         0.93763368 1.         0.1772549 ]. \t  -0.7316297465421655 \t -0.5081673732303724\n",
            "2      \t [ 3.27075931  5.07207678 14.          0.99587284  2.          0.51564135]. \t  -0.6620507374994331 \t -0.5081673732303724\n",
            "3      \t [1.12049804 7.19863393 6.         0.79566554 3.         0.23444312]. \t  -0.730351254834763 \t -0.5081673732303724\n",
            "4      \t [ 8.88346709  9.03427619  8.          0.525814   19.          0.22866902]. \t  -0.732406939137863 \t -0.5081673732303724\n",
            "5      \t [0.         0.         7.20706762 0.5        1.         0.1       ]. \t  -0.7325630500220159 \t -0.5081673732303724\n",
            "6      \t [ 2.93278685  9.15897164 14.          0.67343746 10.          0.85376406]. \t  \u001b[92m-0.504811636744922\u001b[0m \t -0.504811636744922\n",
            "7      \t [ 1.1995358   9.87930749  5.          0.84448568 18.          0.26022822]. \t  -0.7297224033867351 \t -0.504811636744922\n",
            "8      \t [ 1.96525323  0.16048396 14.          0.65133015  9.          0.3003508 ]. \t  -0.6860931128073492 \t -0.504811636744922\n",
            "9      \t [9.42850095 2.86423008 7.         0.53320886 1.         0.21528864]. \t  -0.731472889644752 \t -0.504811636744922\n",
            "10     \t [ 3.97093318  9.58939295  6.          0.5381492  12.          0.74760437]. \t  -0.5217629748454513 \t -0.504811636744922\n",
            "11     \t [ 9.94719165  9.26136843 13.          0.64663072 11.          0.47018827]. \t  -0.6546346492088932 \t -0.504811636744922\n",
            "12     \t [9.55919323 7.66342182 6.         0.63144136 7.         0.82664124]. \t  -0.5195220027397864 \t -0.504811636744922\n",
            "13     \t [ 2.80079796  0.61761702  5.          0.62606225 19.          0.71334647]. \t  -0.5274647167409906 \t -0.504811636744922\n",
            "14     \t [ 9.59599852  3.11576364  5.          0.59256945 17.          0.56621848]. \t  -0.6538676177721936 \t -0.504811636744922\n",
            "15     \t [ 8.91533927  0.16704192 13.          0.66647549  6.          0.25014   ]. \t  -0.7314177593008601 \t -0.504811636744922\n",
            "16     \t [ 4.33253274  9.14516752 14.          0.94398901 19.          0.74581708]. \t  \u001b[92m-0.5007721901039625\u001b[0m \t -0.5007721901039625\n",
            "17     \t [ 0.09682186  0.07772773  6.          0.97564015 14.          0.32743526]. \t  -0.6832859483672176 \t -0.5007721901039625\n",
            "18     \t [1.10064571 4.6546626  7.         0.87274071 8.         0.84230849]. \t  -0.5100013025983998 \t -0.5007721901039625\n",
            "19     \t [ 9.94497871  9.34276152 14.          0.66308198  5.          0.66061477]. \t  -0.5102482930096841 \t -0.5007721901039625\n",
            "20     \t [ 4.56626432  0.17636113 10.          0.6263154   3.          0.70243687]. \t  -0.5121689561366458 \t -0.5007721901039625\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.6249009343932785"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKX_nfEaaAwm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a691562-a46d-4181-e276-54dd52896a7a"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 4 \n",
        "\n",
        "np.random.seed(run_num_4)\n",
        "surrogate_winner_4 = dtStudentProcess(d_cov_func, nu = df)\n",
        "\n",
        "X_train4, X_test4, y_train4, y_test4 = train_test_split(X, y, test_size=test_perc, random_state=run_num_4)\n",
        "\n",
        "def f_syn_polarity4(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_4, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train4, y=y_train4).mean())\n",
        "    return operator * score\n",
        "\n",
        "winner_4 = dGPGO_stp(surrogate_winner_4, Acquisition_new(util_winner), f_syn_polarity4, param, n_jobs = -1) # Define BayesOpt\n",
        "winner_4.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_winner_4 = winner_4.getResult()[0]\n",
        "params_winner_4['max_depth'] = int(params_winner_4['max_depth'])\n",
        "params_winner_4['min_child_weight'] = int(params_winner_4['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_winner_train4 = xgb.DMatrix(X_train4, y_train4)\n",
        "dX_winner_test4 = xgb.DMatrix(X_test4, y_test4)\n",
        "model_winner_4 = xgb.train(params_winner_4, dX_winner_train4)\n",
        "pred_winner_4 = model_winner_4.predict(dX_winner_test4)\n",
        "\n",
        "rmse_winner_4 = np.sqrt(mean_squared_error(pred_winner_4, y_test4))\n",
        "rmse_winner_4"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 9.42798506  1.23319667 14.          0.81910755  7.          0.7021549 ]. \t  -0.5581090891563811 \t -0.5581090891563811\n",
            "init   \t [ 5.54158317  7.71233962 10.          0.59117644  6.          0.31476458]. \t  -0.6204181120564479 \t -0.5581090891563811\n",
            "init   \t [2.18282634 2.93794127 7.         0.84739327 5.         0.62716229]. \t  -0.5626656925564042 \t -0.5581090891563811\n",
            "init   \t [ 5.66159581  4.68678814 11.          0.54147808  9.          0.58609221]. \t  -0.5595885941852147 \t -0.5581090891563811\n",
            "init   \t [ 3.02160958  6.07432614  6.          0.92038559 14.          0.12778609]. \t  -0.6361772582153176 \t -0.5581090891563811\n",
            "1      \t [ 1.89460697  0.10324856 13.          0.70356349 16.          0.67776197]. \t  \u001b[92m-0.5568243993303088\u001b[0m \t -0.5568243993303088\n",
            "2      \t [ 9.69969     4.32607677 14.          0.87739158 16.          0.66014279]. \t  -0.5573098211239071 \t -0.5568243993303088\n",
            "3      \t [ 2.2210328   8.31054196 13.          0.70317243 19.          0.56953182]. \t  -0.573504425404327 \t -0.5568243993303088\n",
            "4      \t [ 0.14706294  0.75702217 14.          0.56309891  8.          0.29887971]. \t  -0.6254087880261816 \t -0.5568243993303088\n",
            "5      \t [ 6.94218243  0.32144048  6.          0.59286079 19.          0.70636825]. \t  -0.5706574487524134 \t -0.5568243993303088\n",
            "6      \t [ 9.24159749  9.43575685  7.          0.98380459 19.          0.34937346]. \t  -0.6187589419936593 \t -0.5568243993303088\n",
            "7      \t [7.57184449 0.0324327  9.         0.86318723 1.         0.6413689 ]. \t  -0.5611723012436254 \t -0.5568243993303088\n",
            "8      \t [ 1.62755426  6.7921099  14.          0.91903975  1.          0.61791343]. \t  -0.5680865264437001 \t -0.5568243993303088\n",
            "9      \t [ 9.43074486  0.98044049  5.          0.96290856 10.          0.27397556]. \t  -0.6390681992417737 \t -0.5568243993303088\n",
            "10     \t [ 8.73128599  9.22658128 14.          0.98344346  2.          0.13489277]. \t  -0.6368164613179 \t -0.5568243993303088\n",
            "11     \t [1.55248233 9.95757959 7.         0.54989079 2.         0.51763002]. \t  -0.582693852300668 \t -0.5568243993303088\n",
            "12     \t [8.91398595 7.00476796 6.         0.72156795 1.         0.41673037]. \t  -0.6210807206819127 \t -0.5568243993303088\n",
            "13     \t [ 0.          0.          5.          0.5        14.45283389  0.1       ]. \t  -0.6362983560686634 \t -0.5568243993303088\n",
            "14     \t [ 9.87497198  6.68701692  6.          0.55980787 13.          0.32605223]. \t  -0.6200123103666009 \t -0.5568243993303088\n",
            "15     \t [ 2.16464782  9.35692886 11.          0.97431128 12.          0.43257419]. \t  -0.5747879420555002 \t -0.5568243993303088\n",
            "16     \t [0.  0.  5.  0.5 1.  0.1]. \t  -0.6351353738423114 \t -0.5568243993303088\n",
            "17     \t [ 7.91655335  8.74006544 14.          0.78747975 13.          0.15170626]. \t  -0.637399081621971 \t -0.5568243993303088\n",
            "18     \t [7.36355448 5.3077281  5.         0.5709887  7.         0.87569884]. \t  \u001b[92m-0.467633753337906\u001b[0m \t -0.467633753337906\n",
            "19     \t [ 7.93069008  0.11830338 11.          0.97110397 13.          0.22887984]. \t  -0.6356672520315936 \t -0.467633753337906\n",
            "20     \t [ 0.          2.31764107 11.03688005  0.5         1.          0.1       ]. \t  -0.639782182705242 \t -0.467633753337906\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5.219368258415548"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJmI9saAaEG1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41f3095c-36f1-41cb-cc9f-7dc75dd5813a"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 5 \n",
        "\n",
        "np.random.seed(run_num_5)\n",
        "surrogate_winner_5 = dtStudentProcess(d_cov_func, nu = df)\n",
        "\n",
        "X_train5, X_test5, y_train5, y_test5 = train_test_split(X, y, test_size=test_perc, random_state=run_num_5)\n",
        "\n",
        "def f_syn_polarity5(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_5, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train5, y=y_train5).mean())\n",
        "    return operator * score\n",
        "\n",
        "winner_5 = dGPGO_stp(surrogate_winner_5, Acquisition_new(util_winner), f_syn_polarity5, param, n_jobs = -1) # Define BayesOpt\n",
        "winner_5.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_winner_5 = winner_5.getResult()[0]\n",
        "params_winner_5['max_depth'] = int(params_winner_5['max_depth'])\n",
        "params_winner_5['min_child_weight'] = int(params_winner_5['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_winner_train5 = xgb.DMatrix(X_train5, y_train5)\n",
        "dX_winner_test5 = xgb.DMatrix(X_test5, y_test5)\n",
        "model_winner_5 = xgb.train(params_winner_5, dX_winner_train5)\n",
        "pred_winner_5 = model_winner_5.predict(dX_winner_test5)\n",
        "\n",
        "rmse_winner_5 = np.sqrt(mean_squared_error(pred_winner_5, y_test5))\n",
        "rmse_winner_5"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 5.57353274  5.41760627  8.          0.53710825 19.          0.26034376]. \t  -0.6523797617249512 \t -0.466901759574539\n",
            "init   \t [5.17826437 9.78817462 5.         0.84048437 6.         0.79712615]. \t  -0.4886603987288415 \t -0.466901759574539\n",
            "init   \t [ 9.84476201  9.65484856  5.          0.59464714 19.          0.34528747]. \t  -0.564220001422124 \t -0.466901759574539\n",
            "init   \t [ 9.17476842  4.53262239 13.          0.64368565 10.          0.515308  ]. \t  -0.5049042831858346 \t -0.466901759574539\n",
            "init   \t [ 8.66719465  7.99685372 12.          0.79833571 17.          0.72443466]. \t  -0.466901759574539 \t -0.466901759574539\n",
            "1      \t [ 3.31480734  6.81602635 12.          0.95330947  1.          0.27334927]. \t  -0.6531167190113543 \t -0.466901759574539\n",
            "2      \t [2.53140694 0.29740057 9.         0.7505917  7.         0.82287404]. \t  -0.47475119282304395 \t -0.466901759574539\n",
            "3      \t [ 9.84728748  0.28222034 13.          0.93969018  3.          0.22339517]. \t  -0.6522247903981228 \t -0.466901759574539\n",
            "4      \t [ 0.68116737  3.94603776 14.          0.80380753 14.          0.77317996]. \t  \u001b[92m-0.4613270217842209\u001b[0m \t -0.4613270217842209\n",
            "5      \t [ 0.          1.8734047   5.          0.5        13.80652685  0.1       ]. \t  -0.6542775776665287 \t -0.4613270217842209\n",
            "6      \t [ 7.37808152  3.09398454  5.          0.52382233 11.          0.85769413]. \t  -0.4720942743727381 \t -0.4613270217842209\n",
            "7      \t [ 0.96088206  9.84240251  9.          0.79592881 15.          0.83460449]. \t  -0.47215351911272185 \t -0.4613270217842209\n",
            "8      \t [0.80889248 3.12552036 5.         0.5        1.11518096 0.1       ]. \t  -0.6543035676347659 \t -0.4613270217842209\n",
            "9      \t [ 8.03008374  0.3089668  14.          0.83940159 17.          0.95993796]. \t  \u001b[92m-0.422374749501848\u001b[0m \t -0.422374749501848\n",
            "10     \t [ 1.28998978  8.90442798 10.          0.78625314  8.          0.13448748]. \t  -0.6545570207229267 \t -0.422374749501848\n",
            "11     \t [9.66604659 1.51730123 5.         0.59597224 4.         0.66643448]. \t  -0.5016316198301449 \t -0.422374749501848\n",
            "12     \t [ 5.90831681  0.         10.58284552  0.5        12.58284552  0.1       ]. \t  -0.6545770166148003 \t -0.422374749501848\n",
            "13     \t [ 2.76670882  9.36479927 14.          0.77451901 19.          0.10649887]. \t  -0.6539377811460048 \t -0.422374749501848\n",
            "14     \t [0.02835933 9.74346527 7.         0.54472382 1.         0.83619676]. \t  -0.48645443650554226 \t -0.422374749501848\n",
            "15     \t [ 0.41141124  2.98741279 14.          0.89325556  5.          0.96782593]. \t  \u001b[92m-0.41974824323039056\u001b[0m \t -0.41974824323039056\n",
            "16     \t [ 9.18140039  6.18574918 14.          0.54379     2.          0.20360757]. \t  -0.653783149665795 \t -0.41974824323039056\n",
            "17     \t [5.01765399 1.33682784 9.45378697 0.65187973 1.         0.29682301]. \t  -0.5584142632794714 \t -0.41974824323039056\n",
            "18     \t [8.85736332 8.79266201 9.         0.62829779 4.         0.19188442]. \t  -0.6533286558878253 \t -0.41974824323039056\n",
            "19     \t [0.97958336 5.75967986 5.         0.68099068 9.         0.35821337]. \t  -0.5620242357732329 \t -0.41974824323039056\n",
            "20     \t [ 8.32286546  0.75756777  6.          0.69885376 18.          0.98511179]. \t  -0.46068165397024996 \t -0.41974824323039056\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.229607302259219"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulhEolsxaG4k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96804c64-bafc-4b6f-86b9-6cc98d5c7b18"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 6 \n",
        "\n",
        "np.random.seed(run_num_6)\n",
        "surrogate_winner_6 = dtStudentProcess(d_cov_func, nu = df)\n",
        "\n",
        "X_train6, X_test6, y_train6, y_test6 = train_test_split(X, y, test_size=test_perc, random_state=run_num_6)\n",
        "\n",
        "def f_syn_polarity6(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=int(min_child_weight),\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_6, objective = 'reg:squarederror', eval_metric = 'rmse')\n",
        "    score = np.array(cross_val_score(reg, X=X_train6, y=y_train6).mean())\n",
        "    return operator * score\n",
        "\n",
        "winner_6 = dGPGO_stp(surrogate_winner_6, Acquisition_new(util_winner), f_syn_polarity6, param, n_jobs = -1) # Define BayesOpt\n",
        "winner_6.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_winner_6 = winner_6.getResult()[0]\n",
        "params_winner_6['max_depth'] = int(params_winner_6['max_depth'])\n",
        "params_winner_6['min_child_weight'] = int(params_winner_6['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_winner_train6 = xgb.DMatrix(X_train6, y_train6)\n",
        "dX_winner_test6 = xgb.DMatrix(X_test6, y_test6)\n",
        "model_winner_6 = xgb.train(params_winner_6, dX_winner_train6)\n",
        "pred_winner_6 = model_winner_6.predict(dX_winner_test6)\n",
        "\n",
        "rmse_winner_6 = np.sqrt(mean_squared_error(pred_winner_6, y_test6))\n",
        "rmse_winner_6"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [8.92860151 3.31979805 5.         0.99251441 2.         0.57683563]. \t  -0.5471181110640053 \t -0.5219867521874206\n",
            "init   \t [4.18807429 3.35407849 9.         0.87750649 3.         0.56623277]. \t  -0.629286701398377 \t -0.5219867521874206\n",
            "init   \t [ 5.788586    6.45355096 14.          0.70660047 12.          0.82154882]. \t  -0.5219867521874206 \t -0.5219867521874206\n",
            "init   \t [4.58184578 6.73834679 5.         0.90108528 3.         0.65482895]. \t  -0.5476166257785141 \t -0.5219867521874206\n",
            "init   \t [ 4.42510505  5.75952352 14.          0.97882365 15.          0.29525604]. \t  -0.6655867879161768 \t -0.5219867521874206\n",
            "1      \t [ 7.09628606  0.82458553  6.          0.52978945 17.          0.78827589]. \t  -0.5368551114917748 \t -0.5219867521874206\n",
            "2      \t [ 1.88885215  8.62524025  6.          0.90916563 14.          0.40379086]. \t  -0.6680072912825885 \t -0.5219867521874206\n",
            "3      \t [ 0.5654966   9.52584762 14.          0.82016688  4.          0.10717254]. \t  -0.7389555129670606 \t -0.5219867521874206\n",
            "4      \t [ 9.63033822  0.03793409 13.          0.68076302  7.          0.91391681]. \t  \u001b[92m-0.4301920669254681\u001b[0m \t -0.4301920669254681\n",
            "5      \t [ 9.97469028  9.15579642 12.          0.66745021 19.          0.28959158]. \t  -0.6668248751122167 \t -0.4301920669254681\n",
            "6      \t [ 9.85880187  8.48475629 13.          0.76006171  2.          0.58751173]. \t  -0.5403277210407594 \t -0.4301920669254681\n",
            "7      \t [ 0.          1.31564189  9.76293486  0.5        12.76293486  0.1       ]. \t  -0.7381824897935839 \t -0.4301920669254681\n",
            "8      \t [ 0.15151883  3.04875397  5.          0.61495161 19.          0.79420245]. \t  -0.5357132607573185 \t -0.4301920669254681\n",
            "9      \t [8.54696331 5.974818   8.         0.90388102 8.         0.88828384]. \t  -0.43507640693861804 \t -0.4301920669254681\n",
            "10     \t [ 0.19816257  1.95434851 14.          0.59292238  7.          0.17971262]. \t  -0.7383261448869979 \t -0.4301920669254681\n",
            "11     \t [ 0.56219975  9.42928268 11.          0.51903033 19.          0.52214278]. \t  -0.6326946461691906 \t -0.4301920669254681\n",
            "12     \t [0.33151659 0.         5.         0.5        1.         0.1       ]. \t  -0.7383748210903848 \t -0.4301920669254681\n",
            "13     \t [ 8.50163021  8.12423367  5.          0.94828328 19.          0.51887983]. \t  -0.632037430520529 \t -0.4301920669254681\n",
            "14     \t [4.03675408 0.13973262 7.         0.65230376 8.         0.78006658]. \t  -0.5306006118754948 \t -0.4301920669254681\n",
            "15     \t [0.02368926 6.73338636 8.         0.75491444 9.         0.43643271]. \t  -0.6313059012485673 \t -0.4301920669254681\n",
            "16     \t [ 3.40976948  0.54790004 14.          0.94339825 19.          0.44582339]. \t  -0.629420612118917 \t -0.4301920669254681\n",
            "17     \t [ 9.34255838  8.95764422  5.          0.62012856 13.          0.94248412]. \t  -0.4616854732002668 \t -0.4301920669254681\n",
            "18     \t [ 7.23381367  0.21465732 13.          0.75484136 13.          0.44717401]. \t  -0.6306144837329356 \t -0.4301920669254681\n",
            "19     \t [ 0.17525712  4.19183624 13.          0.93093846  1.          0.77496282]. \t  -0.5254340278392874 \t -0.4301920669254681\n",
            "20     \t [ 0.40153964  8.1115111  14.          0.63943096 11.          0.91959984]. \t  \u001b[92m-0.4282877219858168\u001b[0m \t -0.4282877219858168\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.151672275228334"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYebx3RVaJ1w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62a9ab30-1425-46a6-d27d-a57498926474"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 7 \n",
        "\n",
        "np.random.seed(run_num_7)\n",
        "surrogate_winner_7 = dtStudentProcess(d_cov_func, nu = df)\n",
        "\n",
        "X_train7, X_test7, y_train7, y_test7 = train_test_split(X, y, test_size=test_perc, random_state=run_num_7)\n",
        "\n",
        "def f_syn_polarity7(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_7, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train7, y=y_train7).mean())\n",
        "    return operator * score\n",
        "\n",
        "winner_7 = dGPGO_stp(surrogate_winner_7, Acquisition_new(util_winner), f_syn_polarity7, param, n_jobs = -1) # Define BayesOpt\n",
        "winner_7.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_winner_7 = winner_7.getResult()[0]\n",
        "params_winner_7['max_depth'] = int(params_winner_7['max_depth'])\n",
        "params_winner_7['min_child_weight'] = int(params_winner_7['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_winner_train7 = xgb.DMatrix(X_train7, y_train7)\n",
        "dX_winner_test7 = xgb.DMatrix(X_test7, y_test7)\n",
        "model_winner_7 = xgb.train(params_winner_7, dX_winner_train7)\n",
        "pred_winner_7 = model_winner_7.predict(dX_winner_test7)\n",
        "\n",
        "rmse_winner_7 = np.sqrt(mean_squared_error(pred_winner_7, y_test7))\n",
        "rmse_winner_7"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 8.11505126  6.92324143 13.          0.94046175 12.          0.3650384 ]. \t  -0.612852517751432 \t -0.4284992540085738\n",
            "init   \t [ 5.15357029  9.0066636   6.          0.74104227 15.          0.8966337 ]. \t  -0.45500673865987984 \t -0.4284992540085738\n",
            "init   \t [ 2.63548913  9.6610934  14.          0.64712408  1.          0.80820948]. \t  -0.43730487111474525 \t -0.4284992540085738\n",
            "init   \t [ 1.40821426  1.72758589 12.          0.70495069  5.          0.86632715]. \t  -0.4284992540085738 \t -0.4284992540085738\n",
            "init   \t [2.85126987 2.73434054 7.         0.88394733 8.         0.65545939]. \t  -0.48253390492139586 \t -0.4284992540085738\n",
            "1      \t [1.03697999e-02 2.51695028e+00 1.40000000e+01 6.19497248e-01\n",
            " 1.90000000e+01 7.01827417e-01]. \t  -0.47758423778274467 \t -0.4284992540085738\n",
            "2      \t [9.9089961  7.50235155 7.         0.67925503 5.         0.2471192 ]. \t  -0.6736712023667494 \t -0.4284992540085738\n",
            "3      \t [ 7.21196079  0.11384945  8.          0.67755731 17.          0.74323626]. \t  -0.4647919397201027 \t -0.4284992540085738\n",
            "4      \t [0.35283087 9.2950614  5.         0.93054416 3.         0.27625531]. \t  -0.670508947897148 \t -0.4284992540085738\n",
            "5      \t [0.  0.  5.  0.5 1.  0.1]. \t  -0.6745888440506937 \t -0.4284992540085738\n",
            "6      \t [9.76759527 0.48761621 7.         0.96362191 1.         0.137157  ]. \t  -0.6715547907624335 \t -0.4284992540085738\n",
            "7      \t [ 8.53387117  0.35946342 13.          0.65332712  6.          0.68302293]. \t  -0.4712802345283902 \t -0.4284992540085738\n",
            "8      \t [ 0.45155781  2.33209808  5.          0.73660734 17.          0.83890491]. \t  -0.4804814871680856 \t -0.4284992540085738\n",
            "9      \t [ 0.30506512  9.51761383 13.          0.5277377  16.          0.92234588]. \t  -0.4390633386106087 \t -0.4284992540085738\n",
            "10     \t [ 9.21287749  3.65759887  5.          0.85687073 12.          0.88459516]. \t  -0.45867733996321497 \t -0.4284992540085738\n",
            "11     \t [ 1.14642054  8.54710172 12.          0.83923566  8.          0.25317439]. \t  -0.6716592254658202 \t -0.4284992540085738\n",
            "12     \t [ 6.43879816  6.53341304 14.          0.70290258 19.          0.43578856]. \t  -0.5879212739277196 \t -0.4284992540085738\n",
            "13     \t [ 0.28968499  1.98768694 14.          0.97996073 12.          0.15615797]. \t  -0.6725851052423111 \t -0.4284992540085738\n",
            "14     \t [5.4721942  4.70341961 9.         0.675658   1.         0.91263074]. \t  -0.44024324260844283 \t -0.4284992540085738\n",
            "15     \t [ 7.56572451  6.67046024 14.          0.88731871  4.          0.29940932]. \t  -0.6117537685654454 \t -0.4284992540085738\n",
            "16     \t [4.54036829 9.23672573 7.         0.78071654 7.         0.925943  ]. \t  -0.44422074369565967 \t -0.4284992540085738\n",
            "17     \t [ 9.52566208  5.8884151   6.          0.78376025 19.          0.40148749]. \t  -0.6258103990077785 \t -0.4284992540085738\n",
            "18     \t [ 7.79919254  0.6285715  14.          0.87477175 15.          0.15632002]. \t  -0.6708453498973665 \t -0.4284992540085738\n",
            "19     \t [9.3259212  0.13501662 8.         0.77280133 8.         0.95737716]. \t  -0.44539283788135425 \t -0.4284992540085738\n",
            "20     \t [ 3.1237334   5.58211355  9.          0.8525378  19.          0.1292964 ]. \t  -0.6703702828267247 \t -0.4284992540085738\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.492050628419268"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xk0IPTSTbIl3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b646ff91-0a5a-4690-f697-e3f491100f94"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 8 \n",
        "\n",
        "np.random.seed(run_num_8)\n",
        "surrogate_winner_8 = dtStudentProcess(d_cov_func, nu = df)\n",
        "\n",
        "X_train8, X_test8, y_train8, y_test8 = train_test_split(X, y, test_size=test_perc, random_state=run_num_8)\n",
        "\n",
        "def f_syn_polarity8(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_8, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train8, y=y_train8).mean())\n",
        "    return operator * score\n",
        "\n",
        "winner_8 = dGPGO_stp(surrogate_winner_8, Acquisition_new(util_winner), f_syn_polarity8, param, n_jobs = -1) # Define BayesOpt\n",
        "winner_8.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_winner_8 = winner_8.getResult()[0]\n",
        "params_winner_8['max_depth'] = int(params_winner_8['max_depth'])\n",
        "params_winner_8['min_child_weight'] = int(params_winner_8['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_winner_train8 = xgb.DMatrix(X_train8, y_train8)\n",
        "dX_winner_test8 = xgb.DMatrix(X_test8, y_test8)\n",
        "model_winner_8 = xgb.train(params_winner_8, dX_winner_train8)\n",
        "pred_winner_8 = model_winner_8.predict(dX_winner_test8)\n",
        "\n",
        "rmse_winner_8 = np.sqrt(mean_squared_error(pred_winner_8, y_test8))\n",
        "rmse_winner_8"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 1.29221489  0.77396898 11.          0.56062468  3.          0.25451879]. \t  -0.6231794271723858 \t -0.4674926150198253\n",
            "init   \t [ 0.10611429  1.37648061  8.          0.6050375  15.          0.790205  ]. \t  -0.4674926150198253 \t -0.4674926150198253\n",
            "init   \t [ 8.35239571  4.16908043  7.          0.67902746 14.          0.16683749]. \t  -0.6176439249709439 \t -0.4674926150198253\n",
            "init   \t [ 6.06301777  7.27127491 12.          0.87295531 11.          0.37694382]. \t  -0.5951520828762356 \t -0.4674926150198253\n",
            "init   \t [8.85209637 7.89269955 6.         0.72444854 3.         0.47766986]. \t  -0.5688519156580504 \t -0.4674926150198253\n",
            "1      \t [ 1.00661818  8.58126121 14.          0.92271297 19.          0.23705776]. \t  -0.6184117893584562 \t -0.4674926150198253\n",
            "2      \t [ 7.52273592  2.30141271 14.          0.75505047 19.          0.77804046]. \t  \u001b[92m-0.45158957529537797\u001b[0m \t -0.45158957529537797\n",
            "3      \t [0.75742669 9.34651137 9.         0.9951685  2.         0.93969225]. \t  \u001b[92m-0.44074269624584594\u001b[0m \t -0.44074269624584594\n",
            "4      \t [ 0.96506636  6.75599738  5.          0.54675104 10.          0.99072888]. \t  -0.466069196104766 \t -0.44074269624584594\n",
            "5      \t [8.14212583 0.61314553 6.         0.86488725 6.         0.28574491]. \t  -0.5954481959734638 \t -0.44074269624584594\n",
            "6      \t [ 6.86776237  9.22091093  6.          0.98224932 19.          0.53828017]. \t  -0.5683715175779595 \t -0.44074269624584594\n",
            "7      \t [ 8.89360526  2.1140719  14.          0.72842074  5.          0.8531299 ]. \t  -0.4410569328948747 \t -0.44074269624584594\n",
            "8      \t [ 3.67598904  0.33397948 13.          0.67802127 12.          0.26996506]. \t  -0.6182367076254358 \t -0.44074269624584594\n",
            "9      \t [2.67676373 3.95611485 5.         0.79739163 2.         0.89987835]. \t  -0.45674643004535687 \t -0.44074269624584594\n",
            "10     \t [0.         0.         5.         0.5        8.30385892 0.1       ]. \t  -0.622230232812085 \t -0.44074269624584594\n",
            "11     \t [ 9.2410685   7.18065382 13.          0.79376037  1.          0.89955829]. \t  \u001b[92m-0.43589894991964684\u001b[0m \t -0.43589894991964684\n",
            "12     \t [ 0.92639476  9.27106288  8.          0.97915423 16.          0.71668829]. \t  -0.4592202320775658 \t -0.43589894991964684\n",
            "13     \t [ 5.04566499  2.23244325  6.          0.98899894 19.          0.3759382 ]. \t  -0.5938655513158236 \t -0.43589894991964684\n",
            "14     \t [7.56583207 0.62009326 9.         0.69680398 1.         0.34605251]. \t  -0.5973503745492188 \t -0.43589894991964684\n",
            "15     \t [4.34786095 5.88605209 9.17460215 0.60867691 5.88144218 0.63384741]. \t  -0.5252148341812515 \t -0.43589894991964684\n",
            "16     \t [ 9.54383706  1.9311562  14.          0.97343577 12.          0.77846223]. \t  -0.4406155942586145 \t -0.43589894991964684\n",
            "17     \t [ 0.13623061  9.23716881 12.          0.87470799  9.          0.1799897 ]. \t  -0.6200548535978804 \t -0.43589894991964684\n",
            "18     \t [ 0.29190819  5.12541317 14.          0.90947093 14.          0.27250801]. \t  -0.6207899263119586 \t -0.43589894991964684\n",
            "19     \t [ 9.34250361  6.36824077 12.          0.75804098 17.          0.14124451]. \t  -0.6168211855664071 \t -0.43589894991964684\n",
            "20     \t [ 1.47920626  6.67134895 14.          0.93014559  2.          0.37436082]. \t  -0.5999577681265806 \t -0.43589894991964684\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.5638295857856095"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9UroEj_RbLSb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5eadc46d-1707-4c8d-cd18-ff56c139dce5"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 9 \n",
        "\n",
        "np.random.seed(run_num_9)\n",
        "surrogate_winner_9 = dtStudentProcess(d_cov_func, nu = df)\n",
        "\n",
        "X_train9, X_test9, y_train9, y_test9 = train_test_split(X, y, test_size=test_perc, random_state=run_num_9)\n",
        "\n",
        "def f_syn_polarity9(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_9, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train9, y=y_train9).mean())\n",
        "    return operator * score\n",
        "\n",
        "winner_9 = dGPGO_stp(surrogate_winner_9, Acquisition_new(util_winner), f_syn_polarity9, param, n_jobs = -1) # Define BayesOpt\n",
        "winner_9.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_winner_9 = winner_9.getResult()[0]\n",
        "params_winner_9['max_depth'] = int(params_winner_9['max_depth'])\n",
        "params_winner_9['min_child_weight'] = int(params_winner_9['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_winner_train9 = xgb.DMatrix(X_train9, y_train9)\n",
        "dX_winner_test9 = xgb.DMatrix(X_test9, y_test9)\n",
        "model_winner_9 = xgb.train(params_winner_9, dX_winner_train9)\n",
        "pred_winner_9 = model_winner_9.predict(dX_winner_test9)\n",
        "\n",
        "rmse_winner_9 = np.sqrt(mean_squared_error(pred_winner_9, y_test9))\n",
        "rmse_winner_9"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 8.0342804   5.27522296 13.          0.60741904 17.          0.15336222]. \t  -0.6890593508433319 \t -0.4886355541929722\n",
            "init   \t [7.56063644 1.79876369 7.         0.84869647 5.         0.55956942]. \t  -0.5532305414240943 \t -0.4886355541929722\n",
            "init   \t [2.43675539 0.20225128 6.         0.51669963 7.         0.4646446 ]. \t  -0.5609155518755553 \t -0.4886355541929722\n",
            "init   \t [ 7.6043547   4.73758376  9.          0.87564624 15.          0.65751286]. \t  -0.4886355541929722 \t -0.4886355541929722\n",
            "init   \t [ 4.70349314  6.71029586  6.          0.97680389 11.          0.41971652]. \t  -0.6548660487332898 \t -0.4886355541929722\n",
            "1      \t [ 0.20910387  7.25545521 14.          0.59407892  3.          0.95410586]. \t  \u001b[92m-0.4312356520914486\u001b[0m \t -0.4312356520914486\n",
            "2      \t [ 2.14892068  9.25577985 14.          0.84828472 11.          0.2793841 ]. \t  -0.6883959263608518 \t -0.4312356520914486\n",
            "3      \t [ 0.37305557  1.02756836 13.          0.57348052 13.          0.68410192]. \t  -0.4836872799099695 \t -0.4312356520914486\n",
            "4      \t [ 7.98252507  8.04957695 12.          0.6367811   3.          0.85775382]. \t  -0.4368210491589304 \t -0.4312356520914486\n",
            "5      \t [ 1.2588195   0.04438454  7.          0.70342864 18.          0.58554745]. \t  -0.4951281025154505 \t -0.4312356520914486\n",
            "6      \t [ 9.17710587  9.71573287  5.          0.9584376  19.          0.78072441]. \t  -0.4682275209845145 \t -0.4312356520914486\n",
            "7      \t [6.11416909 9.54959201 5.         0.91943284 1.         0.10968146]. \t  -0.6901756354808697 \t -0.4312356520914486\n",
            "8      \t [ 0.31270412  9.94818438  8.          0.84397658 19.          0.43974503]. \t  -0.5483333357303245 \t -0.4312356520914486\n",
            "9      \t [ 8.76504058  0.46155195 14.          0.61459938 10.          0.19095511]. \t  -0.6902913672662703 \t -0.4312356520914486\n",
            "10     \t [0.  0.  5.  0.5 1.  0.1]. \t  -0.6893514282293631 \t -0.4312356520914486\n",
            "11     \t [ 0.76412088  5.02527282 14.          0.53794557 19.          0.6779987 ]. \t  -0.4862552398836468 \t -0.4312356520914486\n",
            "12     \t [ 8.7007577   8.84182108 11.          0.88074977 10.          0.89589347]. \t  -0.43700927635364534 \t -0.4312356520914486\n",
            "13     \t [1.26882135 8.44257007 8.         0.93932154 4.         0.79202967]. \t  -0.44680835713197 \t -0.4312356520914486\n",
            "14     \t [ 3.38351629  2.89927867 12.          0.76452212  7.          0.61222347]. \t  -0.4809111856529227 \t -0.4312356520914486\n",
            "15     \t [ 7.69267031  0.52709121  5.          0.97712342 11.          0.47111142]. \t  -0.5671609865381374 \t -0.4312356520914486\n",
            "16     \t [ 5.04465897  1.7242392  11.          0.82507363  1.          0.66070113]. \t  -0.48737194712175996 \t -0.4312356520914486\n",
            "17     \t [ 0.          7.15796319  8.67982059  0.74325714 12.95617177  0.52535237]. \t  -0.5491899181477864 \t -0.4312356520914486\n",
            "18     \t [ 3.56621236  4.91536137  7.          0.71255145 19.          0.80604757]. \t  -0.4579607623923458 \t -0.4312356520914486\n",
            "19     \t [ 0.48791793  1.09047989 14.          0.80757691  3.          0.37971249]. \t  -0.6640500439913284 \t -0.4312356520914486\n",
            "20     \t [9.96752263 8.45749074 7.         0.92612294 6.         0.22771617]. \t  -0.6878594070452866 \t -0.4312356520914486\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.130636454660636"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7VgaJOoJbOIE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10049833-6b11-40d5-e2e3-08b52ef1da2b"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 10 \n",
        "\n",
        "np.random.seed(run_num_10)\n",
        "surrogate_winner_10 = dtStudentProcess(d_cov_func, nu = df)\n",
        "\n",
        "X_train10, X_test10, y_train10, y_test10 = train_test_split(X, y, test_size=test_perc, random_state=run_num_10)\n",
        "\n",
        "def f_syn_polarity10(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_10, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train10, y=y_train10).mean())\n",
        "    return operator * score\n",
        "\n",
        "winner_10 = dGPGO_stp(surrogate_winner_10, Acquisition_new(util_winner), f_syn_polarity10, param, n_jobs = -1) # Define BayesOpt\n",
        "winner_10.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_winner_10 = winner_10.getResult()[0]\n",
        "params_winner_10['max_depth'] = int(params_winner_10['max_depth'])\n",
        "params_winner_10['min_child_weight'] = int(params_winner_10['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_winner_train10 = xgb.DMatrix(X_train10, y_train10)\n",
        "dX_winner_test10 = xgb.DMatrix(X_test10, y_test10)\n",
        "model_winner_10 = xgb.train(params_winner_10, dX_winner_train10)\n",
        "pred_winner_10 = model_winner_10.predict(dX_winner_test10)\n",
        "\n",
        "rmse_winner_10 = np.sqrt(mean_squared_error(pred_winner_10, y_test10))\n",
        "rmse_winner_10"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 6.53589585  1.15006943  6.          0.93623727 14.          0.13663866]. \t  -0.6929852048256443 \t -0.4483221221388197\n",
            "init   \t [ 3.97194461  2.33132197 14.          0.9533253  19.          0.26403087]. \t  -0.6932702114662359 \t -0.4483221221388197\n",
            "init   \t [ 7.43539415  0.69582081  9.          0.9763222  11.          0.12608349]. \t  -0.6931242497450955 \t -0.4483221221388197\n",
            "init   \t [ 9.82027485  3.39637684  9.          0.78522537 19.          0.87618843]. \t  -0.4483221221388197 \t -0.4483221221388197\n",
            "init   \t [ 0.57576207  5.82646405 12.          0.77704362 18.          0.11865044]. \t  -0.6946096327074635 \t -0.4483221221388197\n",
            "1      \t [ 6.01651788  9.70919163  7.          0.6223146  15.          0.10507665]. \t  -0.6924325879252461 \t -0.4483221221388197\n",
            "2      \t [1.70339296 0.4436921  5.         0.5844373  2.         0.68058688]. \t  -0.5784474315368291 \t -0.4483221221388197\n",
            "3      \t [ 0.26043995  4.89348176 14.          0.51660447  9.          0.97374739]. \t  \u001b[92m-0.43451777076695297\u001b[0m \t -0.43451777076695297\n",
            "4      \t [ 9.51459367  2.97626838 10.          0.78292126  1.          0.96515375]. \t  -0.43758762459692707 \t -0.43451777076695297\n",
            "5      \t [4.33454338 9.11291731 6.         0.7883404  2.         0.49230158]. \t  -0.5779880165341125 \t -0.43451777076695297\n",
            "6      \t [ 7.90697916  9.14312267 14.          0.75613081  9.          0.87208929]. \t  \u001b[92m-0.4282600941244823\u001b[0m \t -0.4282600941244823\n",
            "7      \t [8.72554417 8.43289928 5.         0.78366846 8.         0.24269244]. \t  -0.6948360255852956 \t -0.4282600941244823\n",
            "8      \t [0.02631599 1.58750594 6.42659751 0.52869713 9.42659751 0.1       ]. \t  -0.6928568259350094 \t -0.4282600941244823\n",
            "9      \t [ 0.76839003  7.96408221 14.          0.73589275  1.          0.91984177]. \t  \u001b[92m-0.4267572960312064\u001b[0m \t -0.4267572960312064\n",
            "10     \t [ 4.30135807  3.42002359 11.          0.95666213  5.          0.52518087]. \t  -0.5776622908327351 \t -0.4267572960312064\n",
            "11     \t [ 6.04365801  9.11999117 13.          0.57944234 16.          0.80579832]. \t  -0.5155123485607884 \t -0.4267572960312064\n",
            "12     \t [ 0.          0.          8.72720223  0.5        15.72720223  0.1       ]. \t  -0.6923514499580236 \t -0.4267572960312064\n",
            "13     \t [ 7.07869654  9.30947966 13.          0.76949873  1.          0.94513615]. \t  -0.4306670978270799 \t -0.4267572960312064\n",
            "14     \t [ 0.17764678  8.11790563  8.          0.81524592 10.          0.46089495]. \t  -0.5743964115721678 \t -0.4267572960312064\n",
            "15     \t [ 2.63712333  6.8736896   5.          0.85625031 19.          0.13187865]. \t  -0.693208004176694 \t -0.4267572960312064\n",
            "16     \t [ 9.726636    6.23722965 10.          0.587959   12.          0.61748265]. \t  -0.5746029126308202 \t -0.4267572960312064\n",
            "17     \t [ 0.56439215  1.41224062 11.          0.73854341  1.          0.83091883]. \t  -0.5125441859167745 \t -0.4267572960312064\n",
            "18     \t [ 8.58082048  3.39962732 14.          0.7151638   9.          0.76214186]. \t  -0.5049104315367501 \t -0.4267572960312064\n",
            "19     \t [ 8.52646717  0.12245704 14.          0.80155281 15.          0.7100538 ]. \t  -0.5709119331337676 \t -0.4267572960312064\n",
            "20     \t [5.46991996 4.00896759 5.         0.5        5.16554659 0.1       ]. \t  -0.6925263367987318 \t -0.4267572960312064\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.501711564413504"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51z87uHWbRGr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1da3c9db-864e-4951-d10b-25b9bc4bb8d7"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 11 \n",
        "\n",
        "np.random.seed(run_num_11)\n",
        "surrogate_winner_11 = dtStudentProcess(d_cov_func, nu = df)\n",
        "\n",
        "X_train11, X_test11, y_train11, y_test11 = train_test_split(X, y, test_size=test_perc, random_state=run_num_11)\n",
        "\n",
        "def f_syn_polarity11(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_11, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train11, y=y_train11).mean())\n",
        "    return operator * score\n",
        "\n",
        "winner_11 = dGPGO_stp(surrogate_winner_11, Acquisition_new(util_winner), f_syn_polarity11, param, n_jobs = -1) # Define BayesOpt\n",
        "winner_11.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_winner_11 = winner_11.getResult()[0]\n",
        "params_winner_11['max_depth'] = int(params_winner_11['max_depth'])\n",
        "params_winner_11['min_child_weight'] = int(params_winner_11['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_winner_train11 = xgb.DMatrix(X_train11, y_train11)\n",
        "dX_winner_test11 = xgb.DMatrix(X_test11, y_test11)\n",
        "model_winner_11 = xgb.train(params_winner_11, dX_winner_train11)\n",
        "pred_winner_11 = model_winner_11.predict(dX_winner_test11)\n",
        "\n",
        "rmse_winner_11 = np.sqrt(mean_squared_error(pred_winner_11, y_test11))\n",
        "rmse_winner_11"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 9.81035663  8.2124785   9.          0.68487173 15.          0.28207988]. \t  -0.7133787013189323 \t -0.47317045732943896\n",
            "init   \t [ 4.91906771  1.38558332  6.          0.618878   19.          0.10263204]. \t  -0.7112023662943636 \t -0.47317045732943896\n",
            "init   \t [ 5.23426175  1.89971159 13.          0.76313779 17.          0.46597853]. \t  -0.47317045732943896 \t -0.47317045732943896\n",
            "init   \t [ 4.0204056   3.75167577  9.          0.86204818 17.          0.26560056]. \t  -0.7132057194891379 \t -0.47317045732943896\n",
            "init   \t [8.8704459  5.64917815 5.         0.51406171 1.         0.82826731]. \t  -0.48048685248123163 \t -0.47317045732943896\n",
            "1      \t [0.08349406 1.89250335 6.         0.66386636 1.         0.88641723]. \t  \u001b[92m-0.4484239383130805\u001b[0m \t -0.4484239383130805\n",
            "2      \t [ 5.35837408  0.31510164 14.          0.97519815  6.          0.3034681 ]. \t  -0.607905230502132 \t -0.4484239383130805\n",
            "3      \t [ 4.66315557  9.30076713 11.          0.83534598  8.          0.21303816]. \t  -0.7135282416096518 \t -0.4484239383130805\n",
            "4      \t [2.1998714  0.5425518  5.         0.52532605 9.50809882 0.41122361]. \t  -0.6062244549617668 \t -0.4484239383130805\n",
            "5      \t [0.79988529 9.85424348 5.         0.5743044  3.         0.67901433]. \t  -0.48209943859810334 \t -0.4484239383130805\n",
            "6      \t [ 0.02057794  7.14089873 13.          0.90855163  1.          0.61823707]. \t  -0.45805892074172583 \t -0.4484239383130805\n",
            "7      \t [ 0.57400331  5.06757069 13.          0.88309699 12.          0.75334373]. \t  \u001b[92m-0.4470137131126143\u001b[0m \t -0.4470137131126143\n",
            "8      \t [9.50045066 0.32914851 7.         0.62323976 7.         0.78640875]. \t  -0.467371705444857 \t -0.4470137131126143\n",
            "9      \t [ 9.1827122   7.12486171 14.          0.88554665  2.          0.96357613]. \t  \u001b[92m-0.4255664008914337\u001b[0m \t -0.4255664008914337\n",
            "10     \t [ 1.3440527   8.97080429  5.          0.80277597 12.          0.63284111]. \t  -0.48349651102568225 \t -0.4255664008914337\n",
            "11     \t [ 7.94225687  3.68352192 13.          0.94902199 11.          0.95471054]. \t  \u001b[92m-0.422771965919715\u001b[0m \t -0.422771965919715\n",
            "12     \t [9.20332934 9.98167579 5.         0.57575366 6.         0.3717594 ]. \t  -0.6046860081027627 \t -0.422771965919715\n",
            "13     \t [ 0.91095394  8.24018651 14.          0.62745689 19.          0.81243904]. \t  -0.45485744857180277 \t -0.422771965919715\n",
            "14     \t [2.94143112e-03 7.03829535e-01 1.30000000e+01 5.98722168e-01\n",
            " 2.00000000e+00 1.01335355e-01]. \t  -0.7122743790062083 \t -0.422771965919715\n",
            "15     \t [ 0.81561387  9.31451412  5.          0.70594876 19.          0.98284163]. \t  -0.45988817082889427 \t -0.422771965919715\n",
            "16     \t [ 7.20715411  4.9364355   5.          0.68831715 11.          0.71282985]. \t  -0.48766820375870756 \t -0.422771965919715\n",
            "17     \t [4.31690766 5.991756   9.         0.55404285 3.         0.72503038]. \t  -0.4625179189123557 \t -0.422771965919715\n",
            "18     \t [1.31119224 4.99400566 8.         0.90830885 8.         0.58679779]. \t  -0.46840923276454605 \t -0.422771965919715\n",
            "19     \t [ 9.62410435  0.52034344 11.          0.7315532   1.          0.6090895 ]. \t  -0.46464266063515874 \t -0.422771965919715\n",
            "20     \t [ 8.98906297  7.01351387 13.          0.77197065 19.          0.47265223]. \t  -0.475345282554151 \t -0.422771965919715\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.29564873555569"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8jZUeoWbTvn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1f11b59-330a-4eca-acf7-45a50ffe1a15"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 12\n",
        "\n",
        "np.random.seed(run_num_12)\n",
        "surrogate_winner_12 = dtStudentProcess(d_cov_func, nu = df)\n",
        "\n",
        "X_train12, X_test12, y_train12, y_test12 = train_test_split(X, y, test_size=test_perc, random_state=run_num_12)\n",
        "\n",
        "def f_syn_polarity12(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_12, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train12, y=y_train12).mean())\n",
        "    return operator * score\n",
        "\n",
        "winner_12 = dGPGO_stp(surrogate_winner_12, Acquisition_new(util_winner), f_syn_polarity12, param, n_jobs = -1) # Define BayesOpt\n",
        "winner_12.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_winner_12 = winner_12.getResult()[0]\n",
        "params_winner_12['max_depth'] = int(params_winner_12['max_depth'])\n",
        "params_winner_12['min_child_weight'] = int(params_winner_12['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_winner_train12 = xgb.DMatrix(X_train12, y_train12)\n",
        "dX_winner_test12 = xgb.DMatrix(X_test12, y_test12)\n",
        "model_winner_12 = xgb.train(params_winner_12, dX_winner_train12)\n",
        "pred_winner_12 = model_winner_12.predict(dX_winner_test12)\n",
        "\n",
        "rmse_winner_12 = np.sqrt(mean_squared_error(pred_winner_12, y_test12))\n",
        "rmse_winner_12"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 1.9151945   6.22108771  9.          0.89267929 16.          0.87460279]. \t  -0.4420077321695894 \t -0.4420077321695894\n",
            "init   \t [ 1.50636966  1.98518759 11.          0.67890863 17.          0.5381501 ]. \t  -0.5998539491151165 \t -0.4420077321695894\n",
            "init   \t [ 3.31015428  8.02639569  7.          0.75154158 13.          0.11992952]. \t  -0.7324503600335561 \t -0.4420077321695894\n",
            "init   \t [ 2.90728553  2.46394443  5.          0.94461307 17.          0.43194161]. \t  -0.5995790338913064 \t -0.4420077321695894\n",
            "init   \t [ 9.33140102  6.51378143  8.          0.76907392 18.          0.38515251]. \t  -0.5982523495604211 \t -0.4420077321695894\n",
            "1      \t [5.68098653 8.6912739  6.         0.52027807 1.         0.51631894]. \t  -0.5999713561677879 \t -0.4420077321695894\n",
            "2      \t [ 4.61089026  7.78540446 14.          0.61560224  6.          0.60910898]. \t  -0.5310432089168232 \t -0.4420077321695894\n",
            "3      \t [9.12195032 0.87787126 9.         0.87292886 5.         0.2622847 ]. \t  -0.7326988083055951 \t -0.4420077321695894\n",
            "4      \t [ 0.42901773  1.02682351 10.          0.62582208  7.          0.40519792]. \t  -0.600033100066767 \t -0.4420077321695894\n",
            "5      \t [ 9.48743388  2.23858916 14.          0.84020064 13.          0.41361313]. \t  -0.5921503481987865 \t -0.4420077321695894\n",
            "6      \t [0.  0.  5.  0.5 1.  0.1]. \t  -0.7334076855722197 \t -0.4420077321695894\n",
            "7      \t [ 5.1543633   9.69004682 14.          0.74319768 17.          0.13447035]. \t  -0.7322160475319931 \t -0.4420077321695894\n",
            "8      \t [ 0.38300941  7.25414676 13.          0.73658699  1.          0.1727111 ]. \t  -0.7323873646147504 \t -0.4420077321695894\n",
            "9      \t [ 0.50387213  8.28483675 14.          0.60639358 12.          0.19061796]. \t  -0.7343015225078143 \t -0.4420077321695894\n",
            "10     \t [ 9.28455241  7.99305791  9.          0.7151018  11.          0.47357317]. \t  -0.5981360899216244 \t -0.4420077321695894\n",
            "11     \t [ 6.57352389  3.08967805 14.          0.73179937  1.          0.49930476]. \t  -0.6024722906486825 \t -0.4420077321695894\n",
            "12     \t [ 8.3919298   0.97109698  6.          0.54238489 12.          0.11702351]. \t  -0.7351520797981824 \t -0.4420077321695894\n",
            "13     \t [4.95988497 5.1259818  5.         0.5        7.         0.1       ]. \t  -0.7333451305318542 \t -0.4420077321695894\n",
            "14     \t [ 7.73716664  0.90544804 10.          0.79325024 18.          0.20202107]. \t  -0.7325716953666935 \t -0.4420077321695894\n",
            "15     \t [ 0.          0.          5.          0.5        10.35790846  0.1       ]. \t  -0.7335141622603135 \t -0.4420077321695894\n",
            "16     \t [1.96226234 9.53617056 8.         0.8115646  7.         0.24350539]. \t  -0.7321644815711601 \t -0.4420077321695894\n",
            "17     \t [ 8.95792842  8.74686238 12.          0.60340754  1.          0.64076061]. \t  -0.5341583767040248 \t -0.4420077321695894\n",
            "18     \t [ 5.72640258  1.49259688 14.          0.92448749  7.          0.73129851]. \t  -0.4929336813081906 \t -0.4420077321695894\n",
            "19     \t [ 0.60473619  9.7310361   5.          0.764657   19.          0.44899982]. \t  -0.602200593591361 \t -0.4420077321695894\n",
            "20     \t [ 1.38274234  1.16822851 11.65978325  0.5         1.          0.1       ]. \t  -0.7337625374326757 \t -0.4420077321695894\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.301967512402236"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snTrqE2RbWbe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94477503-a324-42c9-f1cd-86a895f18a2e"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 13 \n",
        "\n",
        "np.random.seed(run_num_13)\n",
        "surrogate_winner_13 = dtStudentProcess(d_cov_func, nu = df)\n",
        "\n",
        "X_train13, X_test13, y_train13, y_test13 = train_test_split(X, y, test_size=test_perc, random_state=run_num_13)\n",
        "\n",
        "def f_syn_polarity13(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_13, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train13, y=y_train13).mean())\n",
        "    return operator * score\n",
        "\n",
        "winner_13 = dGPGO_stp(surrogate_winner_13, Acquisition_new(util_winner), f_syn_polarity13, param, n_jobs = -1) # Define BayesOpt\n",
        "winner_13.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_winner_13 = winner_13.getResult()[0]\n",
        "params_winner_13['max_depth'] = int(params_winner_13['max_depth'])\n",
        "params_winner_13['min_child_weight'] = int(params_winner_13['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_winner_train13 = xgb.DMatrix(X_train13, y_train13)\n",
        "dX_winner_test13 = xgb.DMatrix(X_test13, y_test13)\n",
        "model_winner_13 = xgb.train(params_winner_13, dX_winner_train13)\n",
        "pred_winner_13 = model_winner_13.predict(dX_winner_test13)\n",
        "\n",
        "rmse_winner_13 = np.sqrt(mean_squared_error(pred_winner_13, y_test13))\n",
        "rmse_winner_13"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 0.30967343  2.54282073  6.          0.9850871  17.          0.50848087]. \t  -0.5693403578256062 \t -0.4459810811310791\n",
            "init   \t [ 6.95119021  3.25593423 14.          0.8602375   8.          0.82055713]. \t  -0.5112217679187058 \t -0.4459810811310791\n",
            "init   \t [ 5.09252675  8.19921021 10.          0.78792002 16.          0.382158  ]. \t  -0.581693558072188 \t -0.4459810811310791\n",
            "init   \t [ 5.4465191   3.19841462 14.          0.90654581  5.          0.10359415]. \t  -0.6818818523119694 \t -0.4459810811310791\n",
            "init   \t [ 9.8393864   9.89619708  9.          0.63387763 10.          0.88979174]. \t  -0.4459810811310791 \t -0.4459810811310791\n",
            "1      \t [8.53546208 2.43342626 6.         0.87628893 6.         0.52166423]. \t  -0.5707796370702284 \t -0.4459810811310791\n",
            "2      \t [0.21910824 0.53051294 5.         0.62754372 4.         0.55299122]. \t  -0.578556985871312 \t -0.4459810811310791\n",
            "3      \t [5.19754758 9.80305744 5.         0.76448178 1.         0.12247487]. \t  -0.6809935571585586 \t -0.4459810811310791\n",
            "4      \t [ 8.54553338  0.49573541 12.          0.81406546 19.          0.20999807]. \t  -0.6795516887212301 \t -0.4459810811310791\n",
            "5      \t [ 0.06271075  5.52241404  7.          0.72065611 10.          0.32985926]. \t  -0.5870453453795295 \t -0.4459810811310791\n",
            "6      \t [ 0.24368767  9.44599276 13.          0.54443262  5.          0.20315453]. \t  -0.6891808118620875 \t -0.4459810811310791\n",
            "7      \t [ 9.45160315  2.43691163  6.          0.75490059 14.          0.39257474]. \t  -0.5966501799066168 \t -0.4459810811310791\n",
            "8      \t [ 1.95513735  0.12334702 13.          0.54467307 14.          0.86324917]. \t  \u001b[92m-0.43794830902637205\u001b[0m \t -0.43794830902637205\n",
            "9      \t [ 0.16412127  4.84071983 13.          0.69230699 18.          0.95362702]. \t  \u001b[92m-0.43138576497664527\u001b[0m \t -0.43138576497664527\n",
            "10     \t [ 9.93618483  7.92775724 10.          0.62787028  2.          0.33020774]. \t  -0.5857027943547768 \t -0.43138576497664527\n",
            "11     \t [0.         6.32638118 8.9089294  0.5        1.         0.1       ]. \t  -0.684468423507783 \t -0.43138576497664527\n",
            "12     \t [ 2.43642325  0.          5.          0.5        10.19664869  0.1       ]. \t  -0.6828324451195957 \t -0.43138576497664527\n",
            "13     \t [ 3.55031036  6.51223184 14.          0.57397655 12.          0.15010002]. \t  -0.6840790526669337 \t -0.43138576497664527\n",
            "14     \t [ 2.95837637  0.         11.53140901  0.5         1.          0.1       ]. \t  -0.6866389737671648 \t -0.43138576497664527\n",
            "15     \t [ 5.19074175  7.82580786  5.          0.92530381 12.          0.16932439]. \t  -0.6805972166669573 \t -0.43138576497664527\n",
            "16     \t [ 1.2199304   9.32862427  5.          0.98694875 18.          0.69275841]. \t  -0.5593626212561581 \t -0.43138576497664527\n",
            "17     \t [4.69840459 9.9377771  9.         0.55456115 7.         0.54186363]. \t  -0.5597171341594869 \t -0.43138576497664527\n",
            "18     \t [ 9.66002918  7.85306614 14.          0.55586068 13.          0.72905533]. \t  -0.5219948011319895 \t -0.43138576497664527\n",
            "19     \t [ 0.09924473  0.30212491 10.5908288   0.60856719  8.23245961  0.65819225]. \t  -0.5410934192989464 \t -0.43138576497664527\n",
            "20     \t [ 9.03439965  9.39442441 14.          0.98025678 19.          0.55921955]. \t  -0.550861248380592 \t -0.43138576497664527\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.279938406874659"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nAuEsXYbtOnC",
        "outputId": "ec1048a6-45d8-442f-afb2-33736a6f770b"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 14 \r\n",
        "\r\n",
        "np.random.seed(run_num_14)\r\n",
        "surrogate_winner_14 = dtStudentProcess(d_cov_func, nu = df)\r\n",
        "\r\n",
        "X_train14, X_test14, y_train14, y_test14 = train_test_split(X, y, test_size=test_perc, random_state=run_num_14)\r\n",
        "\r\n",
        "def f_syn_polarity14(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\r\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\r\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_14, objective = 'reg:squarederror')\r\n",
        "    score = np.array(cross_val_score(reg, X=X_train14, y=y_train14).mean())\r\n",
        "    return operator * score\r\n",
        "\r\n",
        "winner_14 = dGPGO_stp(surrogate_winner_14, Acquisition_new(util_winner), f_syn_polarity14, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_14.run(max_iter = max_iter, init_evals = n_init) # run\r\n",
        "\r\n",
        "### Return optimal parameters' set:\r\n",
        "params_winner_14 = winner_14.getResult()[0]\r\n",
        "params_winner_14['max_depth'] = int(params_winner_14['max_depth'])\r\n",
        "params_winner_14['min_child_weight'] = int(params_winner_14['min_child_weight'])\r\n",
        "\r\n",
        "### Re-train with optimal parameters, run predictons:\r\n",
        "dX_winner_train14 = xgb.DMatrix(X_train14, y_train14)\r\n",
        "dX_winner_test14 = xgb.DMatrix(X_test14, y_test14)\r\n",
        "model_winner_14 = xgb.train(params_winner_14, dX_winner_train14)\r\n",
        "pred_winner_14 = model_winner_14.predict(dX_winner_test14)\r\n",
        "\r\n",
        "rmse_winner_14 = np.sqrt(mean_squared_error(pred_winner_14, y_test14))\r\n",
        "rmse_winner_14"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 8.59560606  1.64569495 11.          0.89021195 15.          0.4857008 ]. \t  -0.6102441482204597 \t -0.4517949441804544\n",
            "init   \t [ 0.57460093  9.25007434  5.          0.56647642 17.          0.7533211 ]. \t  -0.5614002279921607 \t -0.4517949441804544\n",
            "init   \t [1.90674396 4.32019547 9.         0.93708225 3.         0.91775905]. \t  -0.4517949441804544 \t -0.4517949441804544\n",
            "init   \t [ 0.36770475  8.43368681 14.          0.6826844  15.          0.6291966 ]. \t  -0.5407924410004543 \t -0.4517949441804544\n",
            "init   \t [ 5.33840324  0.449182   14.          0.52045439  2.          0.48604556]. \t  -0.6242722768101235 \t -0.4517949441804544\n",
            "1      \t [ 9.54492219  9.07631824  8.          0.54579372 12.          0.38295996]. \t  -0.6262431675183743 \t -0.4517949441804544\n",
            "2      \t [9.53270461 0.83608685 6.         0.99976104 3.         0.60747265]. \t  -0.5533922160484371 \t -0.4517949441804544\n",
            "3      \t [ 1.15250646  0.51569303  5.          0.69322415 17.          0.16291682]. \t  -0.6917729046487754 \t -0.4517949441804544\n",
            "4      \t [ 8.50805687  8.26285095 14.          0.73716155  1.          0.12935726]. \t  -0.69096859473492 \t -0.4517949441804544\n",
            "5      \t [ 8.08070558  7.65123431 14.          0.59104644 19.          0.9302005 ]. \t  -0.45873776977159153 \t -0.4517949441804544\n",
            "6      \t [ 6.26901123  2.37239677  5.          0.95063922 11.          0.52876485]. \t  -0.6158458956422692 \t -0.4517949441804544\n",
            "7      \t [1.0907871  9.11150389 5.         0.58744161 9.         0.87439163]. \t  -0.4933476055445382 \t -0.4517949441804544\n",
            "8      \t [0.         0.         5.         0.5        7.38175557 0.1       ]. \t  -0.6942305173629594 \t -0.4517949441804544\n",
            "9      \t [ 1.40966237  2.37114664 11.          0.6758664  12.          0.5341086 ]. \t  -0.6145054895998724 \t -0.4517949441804544\n",
            "10     \t [ 4.67438649  9.12283668 14.          0.86393974  9.          0.28445597]. \t  -0.6909478110661416 \t -0.4517949441804544\n",
            "11     \t [ 9.12262801  0.63832928 14.          0.67114219  9.          0.47477925]. \t  -0.6127574020888537 \t -0.4517949441804544\n",
            "12     \t [9.71394099 7.88076583 5.         0.92469304 3.         0.17093777]. \t  -0.6913576852026504 \t -0.4517949441804544\n",
            "13     \t [ 8.89707903  3.19640798  5.          0.97710578 18.          0.42067076]. \t  -0.6248477910885939 \t -0.4517949441804544\n",
            "14     \t [ 0.84209312  7.86749962 14.          0.8648448   1.          0.1570582 ]. \t  -0.6937301125121068 \t -0.4517949441804544\n",
            "15     \t [ 3.38906858  1.42319802 13.          0.65161286 18.          0.80112527]. \t  -0.5362080851699149 \t -0.4517949441804544\n",
            "16     \t [0.  0.  5.  0.5 1.  0.1]. \t  -0.6943016112587715 \t -0.4517949441804544\n",
            "17     \t [4.68629148 9.76798288 9.         0.83498325 1.         0.13284137]. \t  -0.6892620582991471 \t -0.4517949441804544\n",
            "18     \t [ 1.62393567  3.63845704 14.          0.98063234  6.          0.73158162]. \t  -0.5372514711827497 \t -0.4517949441804544\n",
            "19     \t [ 6.94928146  5.48234431 10.          0.83546414  7.          0.79072037]. \t  -0.533952967310828 \t -0.4517949441804544\n",
            "20     \t [ 3.31140136  6.65127404 11.          0.73702698 19.          0.26986884]. \t  -0.6908563233480541 \t -0.4517949441804544\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.334202643456426"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgxvE7Irbbj_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7c4b6db-0092-42ae-9046-6269e07f08be"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 15 \n",
        "\n",
        "np.random.seed(run_num_15)\n",
        "surrogate_winner_15 = dtStudentProcess(d_cov_func, nu = df)\n",
        "\n",
        "X_train15, X_test15, y_train15, y_test15 = train_test_split(X, y, test_size=test_perc, random_state=run_num_15)\n",
        "\n",
        "def f_syn_polarity15(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_15, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train15, y=y_train15).mean())\n",
        "    return operator * score\n",
        "\n",
        "winner_15 = dGPGO_stp(surrogate_winner_15, Acquisition_new(util_winner), f_syn_polarity15, param, n_jobs = -1) # Define BayesOpt\n",
        "winner_15.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_winner_15 = winner_15.getResult()[0]\n",
        "params_winner_15['max_depth'] = int(params_winner_15['max_depth'])\n",
        "params_winner_15['min_child_weight'] = int(params_winner_15['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_winner_train15 = xgb.DMatrix(X_train15, y_train15)\n",
        "dX_winner_test15 = xgb.DMatrix(X_test15, y_test15)\n",
        "model_winner_15 = xgb.train(params_winner_15, dX_winner_train15)\n",
        "pred_winner_15 = model_winner_15.predict(dX_winner_test15)\n",
        "\n",
        "rmse_winner_15 = np.sqrt(mean_squared_error(pred_winner_15, y_test15))\n",
        "rmse_winner_15"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 5.64315424  8.97297685  6.          0.81493635 11.          0.33008564]. \t  -0.5946480910132583 \t -0.4392277798535851\n",
            "init   \t [9.51455383 9.60803032 5.         0.68485728 1.         0.30145081]. \t  -0.5968379804084254 \t -0.4392277798535851\n",
            "init   \t [ 8.11809889  6.32508536  7.          0.61766329 19.          0.51383159]. \t  -0.5092318398204285 \t -0.4392277798535851\n",
            "init   \t [ 7.28571251  5.36071389 10.          0.76240297 12.          0.12035683]. \t  -0.6346120183758519 \t -0.4392277798535851\n",
            "init   \t [ 6.80847768  3.59767194 14.          0.51365585 16.          0.89730673]. \t  -0.4392277798535851 \t -0.4392277798535851\n",
            "1      \t [ 2.28792065  8.58421878 12.          0.67551126  1.          0.37971264]. \t  -0.596161946051267 \t -0.4392277798535851\n",
            "2      \t [2.33954502 2.55434344 6.         0.65277598 1.         0.92016032]. \t  -0.45861645403999785 \t -0.4392277798535851\n",
            "3      \t [ 0.39252895  0.94193855 13.          0.73096739  8.          0.72579132]. \t  \u001b[92m-0.4332783925453615\u001b[0m \t -0.4332783925453615\n",
            "4      \t [ 9.75747436  0.69186497 13.          0.97067331  3.          0.35875607]. \t  -0.5852586813927345 \t -0.4332783925453615\n",
            "5      \t [ 1.33182259  0.8125478   9.          0.94558338 18.          0.71777254]. \t  -0.4467705016632985 \t -0.4332783925453615\n",
            "6      \t [ 0.          0.          5.          0.5        11.79522007  0.1       ]. \t  -0.6325794774811047 \t -0.4332783925453615\n",
            "7      \t [ 0.57885492  9.35143843  6.          0.69337529 17.          0.15510814]. \t  -0.6331051904120661 \t -0.4332783925453615\n",
            "8      \t [ 1.48529264  8.62972367 14.          0.64508709  9.          0.77692511]. \t  -0.43545941553430473 \t -0.4332783925453615\n",
            "9      \t [9.03387535 1.77042469 7.         0.94980677 7.         0.18021095]. \t  -0.6322723702767006 \t -0.4332783925453615\n",
            "10     \t [ 8.23912626  9.07320974 11.          0.79010203  5.          0.98913953]. \t  -0.434155818047409 \t -0.4332783925453615\n",
            "11     \t [ 9.85965179  9.04589828 13.          0.95564786 19.          0.1426314 ]. \t  -0.632071954329094 \t -0.4332783925453615\n",
            "12     \t [0.26358678 4.3545773  6.         0.65476214 7.         0.38674645]. \t  -0.5934679208941246 \t -0.4332783925453615\n",
            "13     \t [ 0.15053668  0.43786928 13.          0.62231098  1.          0.88145445]. \t  -0.43523183945633176 \t -0.4332783925453615\n",
            "14     \t [ 7.18775342  0.2062489   5.          0.98239776 17.          0.28043211]. \t  -0.634545784625454 \t -0.4332783925453615\n",
            "15     \t [ 0.60949292  3.48477622 14.          0.85777356 17.          0.79003356]. \t  -0.43350042013012935 \t -0.4332783925453615\n",
            "16     \t [ 2.22607821  9.62798432 12.          0.81069273 19.          0.90626708]. \t  -0.4349388112942007 \t -0.4332783925453615\n",
            "17     \t [2.84784629 9.31855517 7.         0.91621049 4.         0.73948979]. \t  -0.45174656520685874 \t -0.4332783925453615\n",
            "18     \t [ 4.01610198  0.         10.2031627   0.5        11.2031627   0.1       ]. \t  -0.6322701356810448 \t -0.4332783925453615\n",
            "19     \t [9.58390631 0.99765082 6.         0.72021045 1.         0.39555686]. \t  -0.5960083204162412 \t -0.4332783925453615\n",
            "20     \t [ 9.96011127  1.55235537 14.          0.63004724 11.          0.14483121]. \t  -0.6331081546428745 \t -0.4332783925453615\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.173952938066518"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5TaP6RoGuiNT",
        "outputId": "e31e173b-f1a4-4955-8e76-bd62e34f528c"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 16 \r\n",
        "\r\n",
        "np.random.seed(run_num_16)\r\n",
        "surrogate_winner_16 = dtStudentProcess(d_cov_func, nu = df)\r\n",
        "\r\n",
        "X_train16, X_test16, y_train16, y_test16 = train_test_split(X, y, test_size=test_perc, random_state=run_num_16)\r\n",
        "\r\n",
        "def f_syn_polarity16(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\r\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\r\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_16, objective = 'reg:squarederror')\r\n",
        "    score = np.array(cross_val_score(reg, X=X_train16, y=y_train16).mean())\r\n",
        "    return operator * score\r\n",
        "\r\n",
        "winner_16 = dGPGO_stp(surrogate_winner_16, Acquisition_new(util_winner), f_syn_polarity16, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_16.run(max_iter = max_iter, init_evals = n_init) # run\r\n",
        "\r\n",
        "### Return optimal parameters' set:\r\n",
        "params_winner_16 = winner_16.getResult()[0]\r\n",
        "params_winner_16['max_depth'] = int(params_winner_16['max_depth'])\r\n",
        "params_winner_16['min_child_weight'] = int(params_winner_16['min_child_weight'])\r\n",
        "\r\n",
        "### Re-train with optimal parameters, run predictons:\r\n",
        "dX_winner_train16 = xgb.DMatrix(X_train16, y_train16)\r\n",
        "dX_winner_test16 = xgb.DMatrix(X_test16, y_test16)\r\n",
        "model_winner_16 = xgb.train(params_winner_16, dX_winner_train16)\r\n",
        "pred_winner_16 = model_winner_16.predict(dX_winner_test16)\r\n",
        "\r\n",
        "rmse_winner_16 = np.sqrt(mean_squared_error(pred_winner_16, y_test16))\r\n",
        "rmse_winner_16"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [8.63948332 3.16366002 7.         0.75395583 1.         0.56999262]. \t  -0.5713220948397237 \t -0.4331621293825035\n",
            "init   \t [ 3.3426621   9.77029134 14.          0.93751218 17.          0.80549151]. \t  -0.4331621293825035 \t -0.4331621293825035\n",
            "init   \t [ 3.36871073  4.8308288  13.          0.91984982 14.          0.48829226]. \t  -0.5561611494046279 \t -0.4331621293825035\n",
            "init   \t [ 8.84540736  1.59826081  8.          0.75772888 14.          0.29029227]. \t  -0.6835926126526815 \t -0.4331621293825035\n",
            "init   \t [ 7.74981136  8.34377636 13.          0.83182409 15.          0.19378622]. \t  -0.7677243885128009 \t -0.4331621293825035\n",
            "1      \t [0.90186209 9.9119065  9.         0.96247707 9.         0.82473059]. \t  -0.44000666714577197 \t -0.4331621293825035\n",
            "2      \t [ 3.6191694   8.20490707  5.          0.81280746 16.          0.99306688]. \t  -0.4581574120389309 \t -0.4331621293825035\n",
            "3      \t [2.15072525 1.72325427 6.         0.7282133  7.         0.66792915]. \t  -0.510524353390639 \t -0.4331621293825035\n",
            "4      \t [ 4.49483609  7.05102179 14.          0.67776057  3.          0.50727602]. \t  -0.5621481895640063 \t -0.4331621293825035\n",
            "5      \t [ 4.63470447  0.29792913 13.          0.70911522  7.          0.30290504]. \t  -0.6845759859311331 \t -0.4331621293825035\n",
            "6      \t [9.17745543 7.46026312 5.         0.6284814  9.         0.72652494]. \t  -0.47418011145069017 \t -0.4331621293825035\n",
            "7      \t [2.7669192  6.70200041 7.         0.83888371 1.         0.66194747]. \t  -0.49144662381331905 \t -0.4331621293825035\n",
            "8      \t [ 1.86481894  0.99601669  6.          0.66208635 14.          0.74317901]. \t  -0.4660455507597825 \t -0.4331621293825035\n",
            "9      \t [0.  0.  5.  0.5 1.  0.1]. \t  -0.7670392049994392 \t -0.4331621293825035\n",
            "10     \t [ 9.07329721  6.04353062  6.          0.62940057 19.          0.1206801 ]. \t  -0.7674777671186626 \t -0.4331621293825035\n",
            "11     \t [ 8.54895835  0.69961229 14.          0.57654595 19.          0.20454627]. \t  -0.7677953049378372 \t -0.4331621293825035\n",
            "12     \t [8.53167413 9.15372778 5.         0.57519522 2.         0.50414261]. \t  -0.5950876842055789 \t -0.4331621293825035\n",
            "13     \t [ 9.26465723  9.86116065 13.          0.52207502  6.          0.17735657]. \t  -0.7670798777253309 \t -0.4331621293825035\n",
            "14     \t [ 0.3330471   0.01629553 13.          0.91520431 11.          0.22019979]. \t  -0.7679362913445089 \t -0.4331621293825035\n",
            "15     \t [ 0.76287087  0.49621071 13.          0.81150873 18.          0.76762748]. \t  -0.4363142295126112 \t -0.4331621293825035\n",
            "16     \t [9.9506446  0.41258268 8.         0.70130417 7.         0.77986813]. \t  -0.4500722951786756 \t -0.4331621293825035\n",
            "17     \t [ 8.84742705  5.08354098 12.          0.54989851 10.          0.27761013]. \t  -0.7666143119340576 \t -0.4331621293825035\n",
            "18     \t [ 1.18209412  2.69839249 10.72483922  0.5         1.          0.1       ]. \t  -0.7671009855738531 \t -0.4331621293825035\n",
            "19     \t [ 9.46331313  2.81887466 13.          0.76076385  3.          0.42087971]. \t  -0.6854869575909893 \t -0.4331621293825035\n",
            "20     \t [ 4.13646905  2.05215839  7.          0.94049601 19.          0.92563562]. \t  -0.4455593144467603 \t -0.4331621293825035\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.501874502356394"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NiOaMUmgulbx",
        "outputId": "e48c9b8a-9c32-4fee-a107-84df38a3fde0"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 17 \r\n",
        "\r\n",
        "np.random.seed(run_num_17)\r\n",
        "surrogate_winner_17 = dtStudentProcess(d_cov_func, nu = df)\r\n",
        "\r\n",
        "X_train17, X_test17, y_train17, y_test17 = train_test_split(X, y, test_size=test_perc, random_state=run_num_17)\r\n",
        "\r\n",
        "def f_syn_polarity17(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\r\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\r\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_17, objective = 'reg:squarederror')\r\n",
        "    score = np.array(cross_val_score(reg, X=X_train17, y=y_train17).mean())\r\n",
        "    return operator * score\r\n",
        "\r\n",
        "winner_17 = dGPGO_stp(surrogate_winner_17, Acquisition_new(util_winner), f_syn_polarity17, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_17.run(max_iter = max_iter, init_evals = n_init) # run\r\n",
        "\r\n",
        "### Return optimal parameters' set:\r\n",
        "params_winner_17 = winner_17.getResult()[0]\r\n",
        "params_winner_17['max_depth'] = int(params_winner_17['max_depth'])\r\n",
        "params_winner_17['min_child_weight'] = int(params_winner_17['min_child_weight'])\r\n",
        "\r\n",
        "### Re-train with optimal parameters, run predictons:\r\n",
        "dX_winner_train17 = xgb.DMatrix(X_train17, y_train17)\r\n",
        "dX_winner_test17 = xgb.DMatrix(X_test17, y_test17)\r\n",
        "model_winner_17 = xgb.train(params_winner_17, dX_winner_train17)\r\n",
        "pred_winner_17 = model_winner_17.predict(dX_winner_test17)\r\n",
        "\r\n",
        "rmse_winner_17 = np.sqrt(mean_squared_error(pred_winner_17, y_test17))\r\n",
        "rmse_winner_17"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 1.85575275  3.86599303 13.          0.85645882  2.          0.78699813]. \t  -0.4597056260580034 \t -0.4597056260580034\n",
            "init   \t [1.97780831 2.92527676 8.         0.92268607 9.         0.39017299]. \t  -0.5687207024308397 \t -0.4597056260580034\n",
            "init   \t [ 7.16656007  1.028324    5.          0.96975483 15.          0.90932849]. \t  -0.4688025715067707 \t -0.4597056260580034\n",
            "init   \t [ 2.46007304  9.53596657  7.          0.71704714 11.          0.17659118]. \t  -0.6983036523024195 \t -0.4597056260580034\n",
            "init   \t [3.44161135 9.92427612 8.         0.96331165 2.         0.29160186]. \t  -0.5675253504862205 \t -0.4597056260580034\n",
            "1      \t [ 7.66483352  5.40306801 14.          0.87670581  9.          0.9260652 ]. \t  \u001b[92m-0.43020599740044335\u001b[0m \t -0.43020599740044335\n",
            "2      \t [ 6.95619362  6.95953721 12.          0.92317057 19.          0.34981799]. \t  -0.5628121282495753 \t -0.43020599740044335\n",
            "3      \t [0.  0.  5.  0.5 1.  0.1]. \t  -0.6990710246949499 \t -0.43020599740044335\n",
            "4      \t [ 1.79054908  0.97767001 14.          0.72051649 17.          0.75055942]. \t  -0.4586950539387324 \t -0.43020599740044335\n",
            "5      \t [7.85364213 1.7463908  7.         0.80931735 2.         0.57315987]. \t  -0.5442127017186587 \t -0.43020599740044335\n",
            "6      \t [ 1.77308987  6.92082513  5.          0.52059222 19.          0.18788647]. \t  -0.69931956875667 \t -0.43020599740044335\n",
            "7      \t [ 8.95440724  5.31304705 13.          0.73043408  1.          0.56398539]. \t  -0.5373247826064305 \t -0.43020599740044335\n",
            "8      \t [ 8.26790477  9.9524702   5.          0.59716297 16.          0.88254342]. \t  -0.47691281251538375 \t -0.43020599740044335\n",
            "9      \t [ 0.90443942  7.22834774 14.          0.92580232 12.          0.13465218]. \t  -0.6965996897404226 \t -0.43020599740044335\n",
            "10     \t [ 8.52285971  0.31262108 12.          0.8797653  16.          0.34954268]. \t  -0.5627660085466504 \t -0.43020599740044335\n",
            "11     \t [7.50153168 8.37519947 6.         0.6544959  7.         0.90121291]. \t  -0.46115116499634456 \t -0.43020599740044335\n",
            "12     \t [ 1.83063475  0.7056051   5.          0.62427541 19.          0.60134826]. \t  -0.5515025691718822 \t -0.43020599740044335\n",
            "13     \t [ 1.78559475  9.94726594 14.          0.84124706  5.          0.41412983]. \t  -0.5643771257102934 \t -0.43020599740044335\n",
            "14     \t [ 0.8513654   7.30808802 13.          0.68493838 18.          0.30819132]. \t  -0.5655524008056683 \t -0.43020599740044335\n",
            "15     \t [9.52955352 8.15137211 8.         0.76863454 2.         0.57854032]. \t  -0.5395532171313935 \t -0.43020599740044335\n",
            "16     \t [ 0.         0.         5.         0.5       13.1277356  0.1      ]. \t  -0.699276175504395 \t -0.43020599740044335\n",
            "17     \t [ 9.6713951   2.53929081  8.          0.55813478 10.          0.24505208]. \t  -0.7003519360876083 \t -0.43020599740044335\n",
            "18     \t [ 7.02372752  0.82996782 13.          0.91924474  5.          0.61544084]. \t  -0.530630307048183 \t -0.43020599740044335\n",
            "19     \t [ 5.57429295  9.9484983  10.          0.99835426 15.          0.79220597]. \t  -0.4602719911516614 \t -0.43020599740044335\n",
            "20     \t [ 3.87116535  3.46167946  9.          0.68739534 15.          0.44731298]. \t  -0.5417289076052462 \t -0.43020599740044335\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3.734749153689431"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5H4MWSXFcZjO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c28aed0-68f9-4313-e8e0-c7dd5d70c372"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 18 \n",
        "\n",
        "np.random.seed(run_num_18)\n",
        "surrogate_winner_18 = dtStudentProcess(d_cov_func, nu = df)\n",
        "\n",
        "X_train18, X_test18, y_train18, y_test18 = train_test_split(X, y, test_size=test_perc, random_state=run_num_18)\n",
        "\n",
        "def f_syn_polarity18(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_18, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train18, y=y_train18).mean())\n",
        "    return operator * score\n",
        "\n",
        "winner_18 = dGPGO_stp(surrogate_winner_18, Acquisition_new(util_winner), f_syn_polarity18, param, n_jobs = -1) # Define BayesOpt\n",
        "winner_18.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_winner_18 = winner_18.getResult()[0]\n",
        "params_winner_18['max_depth'] = int(params_winner_18['max_depth'])\n",
        "params_winner_18['min_child_weight'] = int(params_winner_18['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_winner_train18 = xgb.DMatrix(X_train18, y_train18)\n",
        "dX_winner_test18 = xgb.DMatrix(X_test18, y_test18)\n",
        "model_winner_18 = xgb.train(params_winner_18, dX_winner_train18)\n",
        "pred_winner_18 = model_winner_18.predict(dX_winner_test18)\n",
        "\n",
        "rmse_winner_18 = np.sqrt(mean_squared_error(pred_winner_18, y_test18))\n",
        "rmse_winner_18"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 8.73429403  9.68540663 10.          0.68875849  9.          0.48011572]. \t  -0.536396330012702 \t -0.4337279026393176\n",
            "init   \t [ 6.12033333  7.66062926  8.          0.76133734 13.          0.93379456]. \t  -0.4433380408289477 \t -0.4337279026393176\n",
            "init   \t [ 1.46524679  7.01527914  7.          0.90913299 10.          0.36016753]. \t  -0.5541638729833303 \t -0.4337279026393176\n",
            "init   \t [ 9.73855241  3.33774046 14.          0.53290419  7.          0.7088681 ]. \t  -0.5062350384654568 \t -0.4337279026393176\n",
            "init   \t [ 3.00618018  1.82702795 11.          0.75681389 14.          0.98627449]. \t  -0.4337279026393176 \t -0.4337279026393176\n",
            "1      \t [ 1.46586883  1.13810325 14.          0.90052794  5.          0.16182112]. \t  -0.6179729230954069 \t -0.4337279026393176\n",
            "2      \t [ 9.17850108  5.22481201 14.          0.60105853 18.          0.63442006]. \t  -0.5052903785308961 \t -0.4337279026393176\n",
            "3      \t [6.20612396 4.37552923 6.         0.71941082 3.         0.90943147]. \t  -0.45137007433293086 \t -0.4337279026393176\n",
            "4      \t [ 0.42107725  9.92938812 12.          0.92462675 18.          0.88352347]. \t  \u001b[92m-0.4255403331751115\u001b[0m \t -0.4255403331751115\n",
            "5      \t [ 1.45418057  8.89399    11.          0.93175096  1.          0.52391385]. \t  -0.5358666580451181 \t -0.4255403331751115\n",
            "6      \t [0.  0.  5.  0.5 1.  0.1]. \t  -0.6230798973009806 \t -0.4255403331751115\n",
            "7      \t [ 8.67626106  0.1397848   6.          0.55771681 18.          0.89120888]. \t  -0.4650266593401632 \t -0.4255403331751115\n",
            "8      \t [0.         0.         5.         0.5        8.78001975 0.1       ]. \t  -0.6225800740368811 \t -0.4255403331751115\n",
            "9      \t [7.94412856 1.24068931 7.         0.96649935 9.         0.47928692]. \t  -0.5420539933608051 \t -0.4255403331751115\n",
            "10     \t [ 2.66410938  9.83743919 13.          0.58899688  8.          0.29675269]. \t  -0.5550068432497505 \t -0.4255403331751115\n",
            "11     \t [ 8.62753755  4.44899956 12.          0.98231134  1.          0.88389967]. \t  -0.4267991891390327 \t -0.4255403331751115\n",
            "12     \t [ 3.63739627  8.35435528  5.          0.58995076 19.          0.32125273]. \t  -0.5635646610193202 \t -0.4255403331751115\n",
            "13     \t [ 3.86727486  0.04582369 10.          0.54629234  1.          0.59406244]. \t  -0.5128905589861811 \t -0.4255403331751115\n",
            "14     \t [ 0.24957112  3.26102415  5.          0.59065773 17.          0.91920206]. \t  -0.4670777155018547 \t -0.4255403331751115\n",
            "15     \t [ 9.49381141  1.31696272 11.          0.51283555 13.          0.25364349]. \t  -0.6228089112997249 \t -0.4255403331751115\n",
            "16     \t [ 5.63166852  5.17712738 14.          0.50846445 12.          0.90154746]. \t  -0.4392645322335234 \t -0.4255403331751115\n",
            "17     \t [8.67001783 9.76128981 5.         0.58441214 6.         0.57844832]. \t  -0.5341916273199911 \t -0.4255403331751115\n",
            "18     \t [0.80615891 4.04370581 8.         0.97632304 4.         0.55920826]. \t  -0.5382235837397811 \t -0.4255403331751115\n",
            "19     \t [5.81345214 9.69898534 7.         0.79242119 1.         0.72012886]. \t  -0.47859716684644626 \t -0.4255403331751115\n",
            "20     \t [ 5.76004281  5.5635349  10.          0.93580322  7.          0.92297471]. \t  -0.42955832845530767 \t -0.4255403331751115\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3.9005482179018887"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B-zaPbk2uuzH",
        "outputId": "b772705e-a43f-4b07-e95e-2f6342e13595"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 19 \r\n",
        "\r\n",
        "np.random.seed(run_num_19)\r\n",
        "surrogate_winner_19 = dtStudentProcess(d_cov_func, nu = df)\r\n",
        "\r\n",
        "X_train19, X_test19, y_train19, y_test19 = train_test_split(X, y, test_size=test_perc, random_state=run_num_19)\r\n",
        "\r\n",
        "def f_syn_polarity19(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\r\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\r\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_19, objective = 'reg:squarederror')\r\n",
        "    score = np.array(cross_val_score(reg, X=X_train19, y=y_train19).mean())\r\n",
        "    return operator * score\r\n",
        "\r\n",
        "winner_19 = dGPGO_stp(surrogate_winner_19, Acquisition_new(util_winner), f_syn_polarity19, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_19.run(max_iter = max_iter, init_evals = n_init) # run\r\n",
        "\r\n",
        "### Return optimal parameters' set:\r\n",
        "params_winner_19 = winner_19.getResult()[0]\r\n",
        "params_winner_19['max_depth'] = int(params_winner_19['max_depth'])\r\n",
        "params_winner_19['min_child_weight'] = int(params_winner_19['min_child_weight'])\r\n",
        "\r\n",
        "### Re-train with optimal parameters, run predictons:\r\n",
        "dX_winner_train19 = xgb.DMatrix(X_train19, y_train19)\r\n",
        "dX_winner_test19 = xgb.DMatrix(X_test19, y_test19)\r\n",
        "model_winner_19 = xgb.train(params_winner_19, dX_winner_train19)\r\n",
        "pred_winner_19 = model_winner_19.predict(dX_winner_test19)\r\n",
        "\r\n",
        "rmse_winner_19 = np.sqrt(mean_squared_error(pred_winner_19, y_test19))\r\n",
        "rmse_winner_19"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 8.24520174  9.84626759 12.          0.81846905 12.          0.27594309]. \t  -0.6624515538441494 \t -0.4321567975765851\n",
            "init   \t [ 8.6021676   8.64300514 14.          0.95067208 18.          0.13363564]. \t  -0.663790961968593 \t -0.4321567975765851\n",
            "init   \t [ 3.02486657  6.06229509 10.          0.50494482  9.          0.10218675]. \t  -0.668875700942164 \t -0.4321567975765851\n",
            "init   \t [ 6.57586195  8.01231848  7.          0.63945407 18.          0.49206406]. \t  -0.5640092362987262 \t -0.4321567975765851\n",
            "init   \t [ 6.05008856  8.15558548 14.          0.64718943  8.          0.90611106]. \t  -0.4321567975765851 \t -0.4321567975765851\n",
            "1      \t [ 0.85655458  0.69294498  9.          0.90992265 19.          0.25605959]. \t  -0.6632879756016564 \t -0.4321567975765851\n",
            "2      \t [ 1.16239725  9.89254371 11.          0.98469188  1.          0.20271996]. \t  -0.6659068700895453 \t -0.4321567975765851\n",
            "3      \t [9.26425961 9.717621   5.         0.8856331  5.         0.65589573]. \t  -0.5256636565447839 \t -0.4321567975765851\n",
            "4      \t [0.  0.  5.  0.5 1.  0.1]. \t  -0.6682393310586452 \t -0.4321567975765851\n",
            "5      \t [ 9.89729152  2.88709408 11.          0.55121781  3.          0.58381748]. \t  -0.5217676759230677 \t -0.4321567975765851\n",
            "6      \t [ 8.82143334  0.86468577  8.          0.78593165 17.          0.7837218 ]. \t  -0.46211902797215776 \t -0.4321567975765851\n",
            "7      \t [ 0.52039992  0.56380532 14.          0.70324781  2.          0.77427978]. \t  -0.44119318049659856 \t -0.4321567975765851\n",
            "8      \t [ 0.          0.          5.          0.5        10.05610915  0.1       ]. \t  -0.6685138157491576 \t -0.4321567975765851\n",
            "9      \t [ 0.56432836  7.79494932 13.          0.55871992 19.          0.10266045]. \t  -0.6660467600439527 \t -0.4321567975765851\n",
            "10     \t [ 9.31929275  3.48955898  5.          0.98099744 10.          0.38840704]. \t  -0.6128872779834833 \t -0.4321567975765851\n",
            "11     \t [5.56939336 3.95670769 5.         0.95243816 3.         0.31115527]. \t  -0.6118696715045784 \t -0.4321567975765851\n",
            "12     \t [ 2.65686167  2.51625847 14.          0.88290639 15.          0.59478451]. \t  -0.5107861739216322 \t -0.4321567975765851\n",
            "13     \t [ 7.46605559  8.55652968 13.          0.99452706  1.          0.43764631]. \t  -0.5573240867828692 \t -0.4321567975765851\n",
            "14     \t [ 9.53840144  1.46066132 14.          0.95596758 14.          0.90498168]. \t  \u001b[92m-0.42952247663467025\u001b[0m \t -0.42952247663467025\n",
            "15     \t [ 1.20113738  5.88016015  6.          0.57839075 15.          0.60752162]. \t  -0.5267223663659426 \t -0.42952247663467025\n",
            "16     \t [1.7958895  9.19371155 5.         0.54448974 5.         0.93868386]. \t  -0.4669880318809955 \t -0.42952247663467025\n",
            "17     \t [ 5.45979531  0.54581357  9.          0.57112993 12.          0.48604953]. \t  -0.5634903169461377 \t -0.42952247663467025\n",
            "18     \t [ 5.39926331  9.71583432  5.          0.62833441 13.          0.6940315 ]. \t  -0.5298229923692752 \t -0.42952247663467025\n",
            "19     \t [ 5.75277105  1.25995511 14.          0.97274397  8.          0.39753437]. \t  -0.6135363407765908 \t -0.42952247663467025\n",
            "20     \t [ 2.00664037  5.88682269 14.          0.94696775  4.          0.80981462]. \t  -0.4349577325456077 \t -0.42952247663467025\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.227629657200072"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NvkuHKlQuxRy",
        "outputId": "179bb3c5-fe84-4b41-fb16-44f31a5440fa"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 20 \r\n",
        "\r\n",
        "np.random.seed(run_num_20)\r\n",
        "surrogate_winner_20 = dtStudentProcess(d_cov_func, nu = df)\r\n",
        "\r\n",
        "X_train20, X_test20, y_train20, y_test20 = train_test_split(X, y, test_size=test_perc, random_state=run_num_20)\r\n",
        "\r\n",
        "def f_syn_polarity20(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\r\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\r\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_20, objective = 'reg:squarederror')\r\n",
        "    score = np.array(cross_val_score(reg, X=X_train20, y=y_train20).mean())\r\n",
        "    return operator * score\r\n",
        "\r\n",
        "winner_20 = dGPGO_stp(surrogate_winner_20, Acquisition_new(util_winner), f_syn_polarity20, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_20.run(max_iter = max_iter, init_evals = n_init) # run\r\n",
        "\r\n",
        "### Return optimal parameters' set:\r\n",
        "params_winner_20 = winner_20.getResult()[0]\r\n",
        "params_winner_20['max_depth'] = int(params_winner_20['max_depth'])\r\n",
        "params_winner_20['min_child_weight'] = int(params_winner_20['min_child_weight'])\r\n",
        "\r\n",
        "### Re-train with optimal parameters, run predictons:\r\n",
        "dX_winner_train20 = xgb.DMatrix(X_train20, y_train20)\r\n",
        "dX_winner_test20 = xgb.DMatrix(X_test20, y_test20)\r\n",
        "model_winner_20 = xgb.train(params_winner_20, dX_winner_train20)\r\n",
        "pred_winner_20 = model_winner_20.predict(dX_winner_test20)\r\n",
        "\r\n",
        "rmse_winner_20 = np.sqrt(mean_squared_error(pred_winner_20, y_test20))\r\n",
        "rmse_winner_20"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 5.70517285  5.64528755 11.          0.51213999 19.          0.57883228]. \t  -0.5510634045248362 \t -0.4667910180825121\n",
            "init   \t [0.68106288 5.8452906  6.         0.58037829 8.         0.87974899]. \t  -0.4667910180825121 \t -0.4667910180825121\n",
            "init   \t [ 4.04121426  7.17971417 11.          0.57603584 10.          0.19954225]. \t  -0.6541702754539586 \t -0.4667910180825121\n",
            "init   \t [ 5.27556761  8.67655329 13.          0.54252273  4.          0.48570796]. \t  -0.5813476654605825 \t -0.4667910180825121\n",
            "init   \t [ 5.30003889  5.73946822 12.          0.52666698 16.          0.77467019]. \t  -0.5078849923055367 \t -0.4667910180825121\n",
            "1      \t [ 9.25190708  0.65043072 10.          0.56676233  5.          0.41289847]. \t  -0.6164858434894066 \t -0.4667910180825121\n",
            "2      \t [8.65689736 8.11684598 5.         0.53907072 1.         0.87574215]. \t  -0.47576906935129115 \t -0.4667910180825121\n",
            "3      \t [6.49728107e-01 7.72862043e-04 7.00000000e+00 5.71905117e-01\n",
            " 2.00000000e+00 1.02014940e-01]. \t  -0.6533990377801203 \t -0.4667910180825121\n",
            "4      \t [ 9.51806562  0.44900923  6.          0.62997227 16.          0.40028974]. \t  -0.6257141325107729 \t -0.4667910180825121\n",
            "5      \t [ 0.2775003   0.70654237  9.6325783   0.5        13.6325783   0.1       ]. \t  -0.6558976811169778 \t -0.4667910180825121\n",
            "6      \t [ 9.98029391  8.23632437  6.          0.63606408 13.          0.22908429]. \t  -0.6532745897130707 \t -0.4667910180825121\n",
            "7      \t [ 3.9245879   0.87058505 14.          0.68269183  1.          0.34048922]. \t  -0.623283664424315 \t -0.4667910180825121\n",
            "8      \t [ 8.41335534  1.9352266  13.          0.56525976 11.          0.46727958]. \t  -0.5794110082695386 \t -0.4667910180825121\n",
            "9      \t [4.59177231 0.         5.         0.5        8.98562033 0.1       ]. \t  -0.6534970625971244 \t -0.4667910180825121\n",
            "10     \t [ 0.26650768  8.81821255  6.          0.54118329 18.          0.20228777]. \t  -0.6522760722738631 \t -0.4667910180825121\n",
            "11     \t [ 0.33634523  2.44291758 12.          0.57848895  7.          0.29263108]. \t  -0.6166486249516908 \t -0.4667910180825121\n",
            "12     \t [ 2.26383924  0.61219945  6.          0.98670211 19.          0.4135094 ]. \t  -0.6248884235837333 \t -0.4667910180825121\n",
            "13     \t [6.49958993 1.98622034 6.         0.83635209 1.         0.2931143 ]. \t  -0.6276302072456058 \t -0.4667910180825121\n",
            "14     \t [1.36051555 9.95957797 7.         0.9242116  2.         0.87097462]. \t  \u001b[92m-0.449167771194887\u001b[0m \t -0.449167771194887\n",
            "15     \t [ 0.20978943  8.45447688 14.          0.77351134 17.          0.2656195 ]. \t  -0.6586945947582249 \t -0.449167771194887\n",
            "16     \t [7.55121074 9.86496533 6.         0.88828535 7.         0.3097795 ]. \t  -0.6258969685789838 \t -0.449167771194887\n",
            "17     \t [ 8.37038069  9.78705631 13.          0.55669631 13.          0.41750259]. \t  -0.616216391496974 \t -0.449167771194887\n",
            "18     \t [ 1.00455473  4.86452269 10.          0.65242434  1.          0.22603818]. \t  -0.6579989052523156 \t -0.449167771194887\n",
            "19     \t [ 3.60883526  5.92252212  6.          0.67482552 14.          0.25122461]. \t  -0.6541629726875307 \t -0.449167771194887\n",
            "20     \t [ 8.17732484  5.22697425 11.          0.61303379  1.          0.32672413]. \t  -0.6191817873624627 \t -0.449167771194887\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.664426285968519"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KFKuwvS3uzrs",
        "outputId": "c766c71a-5244-4604-c578-07d69a622f7f"
      },
      "source": [
        "end_win = time.time()\r\n",
        "end_win\r\n",
        "\r\n",
        "time_win = end_win - start_win\r\n",
        "time_win"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4759.922830104828"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CU2FlhY4vHUk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51d31e4c-34bd-440a-976c-cae89202e595"
      },
      "source": [
        "rmse_loser = [rmse_loser_1,\n",
        "rmse_loser_2,\n",
        "rmse_loser_3,\n",
        "rmse_loser_4,\n",
        "rmse_loser_5,\n",
        "rmse_loser_6,\n",
        "rmse_loser_7,\n",
        "rmse_loser_8,\n",
        "rmse_loser_9,\n",
        "rmse_loser_10,\n",
        "rmse_loser_11,\n",
        "rmse_loser_12,\n",
        "rmse_loser_13,\n",
        "rmse_loser_14,\n",
        "rmse_loser_15,\n",
        "rmse_loser_16,\n",
        "rmse_loser_17,\n",
        "rmse_loser_18,\n",
        "rmse_loser_19,\n",
        "rmse_loser_20]\n",
        "\n",
        "np.mean(rmse_loser)"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.335494497141253"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iZ53FsWXu3J1",
        "outputId": "8ff90bd8-5e32-4f2a-d1c9-669622d0bd48"
      },
      "source": [
        "rmse_winner = [rmse_winner_1,\r\n",
        "rmse_winner_2,\r\n",
        "rmse_winner_3,\r\n",
        "rmse_winner_4,\r\n",
        "rmse_winner_5,\r\n",
        "rmse_winner_6,\r\n",
        "rmse_winner_7,\r\n",
        "rmse_winner_8,\r\n",
        "rmse_winner_9,\r\n",
        "rmse_winner_10,\r\n",
        "rmse_winner_11,\r\n",
        "rmse_winner_12,\r\n",
        "rmse_winner_13,\r\n",
        "rmse_winner_14,\r\n",
        "rmse_winner_15,\r\n",
        "rmse_winner_16,\r\n",
        "rmse_winner_17,\r\n",
        "rmse_winner_18,\r\n",
        "rmse_winner_19,\r\n",
        "rmse_winner_20]\r\n",
        "\r\n",
        "np.mean(rmse_winner)"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.356508515428805"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9FOyoH8u5Wx",
        "outputId": "50114e89-d54c-4935-df19-8db8cb22e99a"
      },
      "source": [
        "min_rmse_loser = min_max_array(rmse_loser)\r\n",
        "min_rmse_loser, len(min_rmse_loser)"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([4.87018129255363,\n",
              "  4.343487868410211,\n",
              "  4.343487868410211,\n",
              "  4.343487868410211,\n",
              "  4.31722690080873,\n",
              "  4.1451215046465775,\n",
              "  4.1451215046465775,\n",
              "  4.1451215046465775,\n",
              "  4.130636454660636,\n",
              "  4.130636454660636,\n",
              "  4.130636454660636,\n",
              "  3.9962203731848227,\n",
              "  3.9962203731848227,\n",
              "  3.9962203731848227,\n",
              "  3.9962203731848227,\n",
              "  3.9962203731848227,\n",
              "  3.9962203731848227,\n",
              "  3.937438334950126,\n",
              "  3.937438334950126,\n",
              "  3.937438334950126],\n",
              " 20)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unXOpKHcvO15",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4bfc28b7-96e2-41b8-8bec-88c87b15a919"
      },
      "source": [
        "min_rmse_winner = min_max_array(rmse_winner)\n",
        "min_rmse_winner, len(min_rmse_winner)"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([4.485430541260721,\n",
              "  4.316024710268154,\n",
              "  4.316024710268154,\n",
              "  4.316024710268154,\n",
              "  4.229607302259219,\n",
              "  4.151672275228334,\n",
              "  4.151672275228334,\n",
              "  4.151672275228334,\n",
              "  4.130636454660636,\n",
              "  4.130636454660636,\n",
              "  4.130636454660636,\n",
              "  4.130636454660636,\n",
              "  4.130636454660636,\n",
              "  4.130636454660636,\n",
              "  4.130636454660636,\n",
              "  4.130636454660636,\n",
              "  3.734749153689431,\n",
              "  3.734749153689431,\n",
              "  3.734749153689431,\n",
              "  3.734749153689431],\n",
              " 20)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yxo85-HEvRPi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "outputId": "e8d4ef22-f30a-4d09-91d1-4e00a8a36609"
      },
      "source": [
        "### Visualise!\n",
        "\n",
        "title = obj_func\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(min_rmse_loser, color = 'Green', label='RMSE: GP dEI ')\n",
        "plt.plot(min_rmse_winner, color = 'Blue', label='RMSE: STP dEI ')# r'($\\nu$' ' = {})'.format(df))\n",
        "\n",
        "plt.title(title, weight = 'bold', family = 'Arial')\n",
        "plt.xlabel('Experiment(s)', weight = 'bold', family = 'Arial') # x-axis label\n",
        "plt.ylabel('RMSE ($)', weight = 'bold', family = 'Arial') # y-axis label\n",
        "plt.legend(loc=0) # add plot legend\n",
        "\n",
        "### Make the x-ticks integers, not floats:\n",
        "count = len(min_rmse_loser)\n",
        "plt.xticks(np.arange(count), np.arange(1, count + 1))\n",
        "plt.grid(b=None)\n",
        "plt.show() #visualize!\n"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwyO7_iZvT7a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02775613-f001-4238-9ac3-a9916b89b314"
      },
      "source": [
        "time_lose, time_win"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1569.190397977829, 4759.922830104828)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 97
        }
      ]
    }
  ]
}