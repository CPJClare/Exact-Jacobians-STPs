{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Hartmann3__STP__dERM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RaQo_XsusPbm"
      },
      "source": [
        "Hartmann3 synthetic function:\r\n",
        "\r\n",
        "GP EI: (exact GP EI gradients) vs. STP ERM: (exact STP ERM gradients)\r\n",
        "\r\n",
        "https://www.sfu.ca/~ssurjano/hart3.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bhgucjJgsYfm",
        "outputId": "aabac440-bb84-48c0-c512-121ca7c2fea8"
      },
      "source": [
        "!pip install pyGPGO"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyGPGO\n",
            "  Downloading https://files.pythonhosted.org/packages/8d/87/a113c91ba014708114f7635d5c0f6a5e5c773480c5f0a537b257a02d180d/pyGPGO-0.4.0.dev1.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pyGPGO) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from pyGPGO) (1.4.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from pyGPGO) (1.0.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from pyGPGO) (0.22.2.post1)\n",
            "Requirement already satisfied: theano in /usr/local/lib/python3.6/dist-packages (from pyGPGO) (1.0.5)\n",
            "Requirement already satisfied: pyMC3 in /usr/local/lib/python3.6/dist-packages (from pyGPGO) (3.7)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from theano->pyGPGO) (1.15.0)\n",
            "Requirement already satisfied: patsy>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from pyMC3->pyGPGO) (0.5.1)\n",
            "Requirement already satisfied: tqdm>=4.8.4 in /usr/local/lib/python3.6/dist-packages (from pyMC3->pyGPGO) (4.41.1)\n",
            "Requirement already satisfied: pandas>=0.18.0 in /usr/local/lib/python3.6/dist-packages (from pyMC3->pyGPGO) (1.1.5)\n",
            "Requirement already satisfied: h5py>=2.7.0 in /usr/local/lib/python3.6/dist-packages (from pyMC3->pyGPGO) (2.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.18.0->pyMC3->pyGPGO) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.18.0->pyMC3->pyGPGO) (2018.9)\n",
            "Building wheels for collected packages: pyGPGO\n",
            "  Building wheel for pyGPGO (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyGPGO: filename=pyGPGO-0.4.0.dev1-cp36-none-any.whl size=19867 sha256=48d257186a6782a257146191e61250218cb389a8b23ca42a19a4905ca007fdbf\n",
            "  Stored in directory: /root/.cache/pip/wheels/d1/27/04/c4fa3bfe194d36e3cd51542132f43415a6813114a5e8301acb\n",
            "Successfully built pyGPGO\n",
            "Installing collected packages: pyGPGO\n",
            "Successfully installed pyGPGO-0.4.0.dev1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E99qMZverfrP"
      },
      "source": [
        "### Import modules:\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "import scipy as sp\r\n",
        "import pandas as pd\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import warnings\r\n",
        "import time\r\n",
        "\r\n",
        "from pyGPGO.logger import EventLogger\r\n",
        "from pyGPGO.GPGO import GPGO\r\n",
        "from pyGPGO.surrogates.GaussianProcess import GaussianProcess\r\n",
        "from pyGPGO.surrogates.tStudentProcess import tStudentProcess\r\n",
        "from pyGPGO.acquisition import Acquisition\r\n",
        "from pyGPGO.covfunc import squaredExponential\r\n",
        "\r\n",
        "from collections import OrderedDict\r\n",
        "from joblib import Parallel, delayed\r\n",
        "from numpy.linalg import slogdet, inv, cholesky, solve\r\n",
        "from scipy.optimize import minimize\r\n",
        "from scipy.spatial.distance import cdist\r\n",
        "from scipy.special import gamma\r\n",
        "from scipy.stats import norm, t\r\n",
        "from matplotlib.pyplot import rc\r\n",
        "from mpl_toolkits.axes_grid1.inset_locator import inset_axes, mark_inset\r\n",
        "\r\n",
        "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xiJSpz2P9qXK"
      },
      "source": [
        "n_start_AcqFunc = 250 #multi-start iterations to avoid local optima in AcqFunc optimization"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzPUbCs3s9nu"
      },
      "source": [
        "### Inputs:\r\n",
        "\r\n",
        "obj_func = 'Hartmann3'\r\n",
        "n_test = n_start_AcqFunc # test points\r\n",
        "df = 3 # nu\r\n",
        "\r\n",
        "util_loser = 'dEI_GP'\r\n",
        "util_winner = 'dERM_STP'\r\n",
        "n_init = 5 # random initialisations"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-AMqjmpbtAb1"
      },
      "source": [
        "### Objective function:\r\n",
        "\r\n",
        "if obj_func == 'Hartmann3': # 3-D\r\n",
        "            \r\n",
        "    # True y bounds:\r\n",
        "    y_lb = -3.86278\r\n",
        "    operator = -1 # targets global minimum \r\n",
        "    y_global_orig = y_lb * operator # targets global minimum\r\n",
        "            \r\n",
        "    # Constraints:\r\n",
        "    lb = 0\r\n",
        "    ub = 1\r\n",
        "    \r\n",
        "    # Input array dimension(s):\r\n",
        "    dim = 3\r\n",
        "\r\n",
        "    # 3-D inputs' parameter bounds:\r\n",
        "    param = {'x1_training': ('cont', [lb, ub]),\r\n",
        "             'x2_training': ('cont', [lb, ub]),\r\n",
        "             'x3_training': ('cont', [lb, ub])}\r\n",
        "    \r\n",
        "    max_iter = (10 * dim)*0 + 100  # iterations of Bayesian optimisation\r\n",
        "    \r\n",
        "    # Test data:\r\n",
        "    x1_test = np.linspace(lb, ub, n_test) \r\n",
        "    x2_test = np.linspace(lb, ub, n_test)\r\n",
        "    x3_test = np.linspace(lb, ub, n_test)\r\n",
        "    Xstar_d = np.column_stack((x1_test, x2_test, x3_test))\r\n",
        "    \r\n",
        "    def f_syn_polarity(x1_training, x2_training, x3_training):\r\n",
        "       \r\n",
        "        value = np.array([x1_training, x2_training, x3_training])\r\n",
        "      \r\n",
        "        a = np.array([[3.0, 10, 30],\r\n",
        "                      [0.1, 10, 35],\r\n",
        "                      [3.0, 10, 30],\r\n",
        "                      [0.1, 10, 35]])\r\n",
        "        \r\n",
        "        alpha = np.array([1.0, 1.2, 3.0, 3.2])\r\n",
        "      \r\n",
        "        p = np.array([[.3689, .1170, .2673],\r\n",
        "                      [.4699, .4387, .7470],\r\n",
        "                      [.1091, .8732, .5547],\r\n",
        "                      [.3810, .5743, .8828]])\r\n",
        "  \r\n",
        "        s = 0\r\n",
        "        for i in [0,1,2,3]:\r\n",
        "            sm = a[i,0]*(value[0]-p[i,0])**2\r\n",
        "            sm += a[i,1]*(value[1]-p[i,1])**2\r\n",
        "            sm += a[i,2]*(value[2]-p[i,2])**2\r\n",
        "            s += alpha[i]*np.exp(-sm)\r\n",
        "        result = -s\r\n",
        "        \r\n",
        "        return operator * result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyQX-59DtC7-"
      },
      "source": [
        "### Cumulative Regret Calculator:\r\n",
        "\r\n",
        "def min_max_array(x):\r\n",
        "    new_list = []\r\n",
        "    for i, num in enumerate(x):\r\n",
        "            new_list.append(np.min(x[0:i+1]))\r\n",
        "    return new_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dE_I_YSntHxl"
      },
      "source": [
        "### Set-seeds:\r\n",
        "\r\n",
        "run_num_1 = 111\r\n",
        "run_num_2 = 113\r\n",
        "run_num_3 = 3333\r\n",
        "run_num_4 = 444\r\n",
        "run_num_5 = 5555\r\n",
        "run_num_6 = 6\r\n",
        "run_num_7 = 777\r\n",
        "run_num_8 = 887\r\n",
        "run_num_9 = 999\r\n",
        "run_num_10 = 1000\r\n",
        "run_num_11 = 1113\r\n",
        "run_num_12 = 1234\r\n",
        "run_num_13 = 2345\r\n",
        "run_num_14 = 88\r\n",
        "run_num_15 = 1556\r\n",
        "run_num_16 = 1666\r\n",
        "run_num_17 = 717\r\n",
        "run_num_18 = 8\r\n",
        "run_num_19 = 1998\r\n",
        "run_num_20 = 2000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oe29v5N5tKjF"
      },
      "source": [
        "### Derivatives - Squared-exponential covariance function:\r\n",
        "\r\n",
        "def l2norm_(X, Xstar):\r\n",
        "    \r\n",
        "    return cdist(X, Xstar)\r\n",
        "\r\n",
        "def kronDelta(X, Xstar):\r\n",
        "\r\n",
        "    return cdist(X, Xstar) < np.finfo(np.float32).eps\r\n",
        "\r\n",
        "class squaredExponentialDeriv(squaredExponential):\r\n",
        "    \r\n",
        "    def K(self, X, Xstar):\r\n",
        "        \r\n",
        "        r = (l2norm_(X, Xstar)/self.l)\r\n",
        "        K = self.sigmaf * np.exp(-1/2*r **2) + self.sigman * kronDelta(X, Xstar)\r\n",
        "        return K\r\n",
        "    \r\n",
        "    def dK(self, X, Xstar):\r\n",
        "        \r\n",
        "        r = (l2norm_(X, Xstar)/self.l)\r\n",
        "        dK = self.sigmaf/self.l**2 * np.exp(-1/2 * r **2) * l2norm_(X, Xstar)\r\n",
        "        return dK\r\n",
        "    \r\n",
        "        \r\n",
        "    def d2K(self, X, Xstar):\r\n",
        "        \r\n",
        "        r = (l2norm_(X, Xstar)/self.l)\r\n",
        "        d2K = self.sigmaf/self.l**2 * np.exp(-1/2 * r **2) * (r**2-1)\r\n",
        "        return d2K\r\n",
        "    \r\n",
        "cov_func = squaredExponential()\r\n",
        "d_cov_func = squaredExponentialDeriv()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEBX5l7TtNSN"
      },
      "source": [
        "class Acquisition_new(Acquisition):    \r\n",
        "    def __init__(self, mode, eps=1e-08, **params):\r\n",
        "        \r\n",
        "        self.params = params\r\n",
        "        self.eps = eps\r\n",
        "\r\n",
        "        mode_dict = {\r\n",
        "            'dEI_GP': self.dEI_GP,\r\n",
        "            'dERM_STP': self.dERM_STP\r\n",
        "        }\r\n",
        "\r\n",
        "        self.f = mode_dict[mode]\r\n",
        "    \r\n",
        "    def dEI_GP(self, tau, mean, std, ds, dm, dvdv, d2v, d2m):\r\n",
        "        z = -1 * (tau - mean - self.eps) / (std + self.eps)\r\n",
        "        \r\n",
        "        dsdx = ds / 2 * (std + self.eps)\r\n",
        "        d2sdx = -dsdx**2 / ((std + self.eps)) - dvdv / (std + self.eps) - d2v / (std + self.eps)\r\n",
        "        dmdx = (dm - z * dsdx) / (std + self.eps)\r\n",
        "        d2mdx = (d2m - (z * d2sdx + 2 * dmdx * dsdx)) / (std + self.eps)\r\n",
        "        \r\n",
        "        f = (std + self.eps) * (z * norm.cdf(z) + norm.pdf(z)[0])\r\n",
        "        df = (f / (std + self.eps) * dsdx + (std + self.eps) * norm.cdf(z) * dmdx)\r\n",
        "        d2f = (f / (std + self.eps) * d2sdx + dsdx * dmdx * norm.cdf(z) \\\r\n",
        "            + d2mdx * (std + self.eps) * norm.cdf(z) + dsdx * norm.cdf(z) * dmdx \\\r\n",
        "            + norm.pdf(z)[0] * (std + self.eps) * dmdx)\r\n",
        "            \r\n",
        "        return f, df, d2f\r\n",
        "\r\n",
        "    def dERM_STP(self, tau, mean, std, ds, dm, dvdv, d2v, d2m, nu=3.0):\r\n",
        "    \r\n",
        "        gamma = -1 * (y_global_orig - mean - self.eps) / (std + self.eps)\r\n",
        "        dsdx = ds / 2 * (std + self.eps)\r\n",
        "        dmdx = (dm - gamma * dsdx) / (std + self.eps)\r\n",
        "        f = (std + self.eps) * (gamma * t.cdf(gamma, df=nu) + (nu + gamma ** 2)/(nu - 1) * t.pdf(gamma, df=nu))\r\n",
        "        df = (gamma * t.cdf(gamma, df=nu) + (nu + gamma ** 2)/(nu - 1) * t.pdf(gamma,df=nu)[0]) * dsdx \\\r\n",
        "             + (std + self.eps) * (t.cdf(gamma,df=nu) * dmdx + gamma * t.pdf(gamma, df=nu) * \\\r\n",
        "             (1 - (nu + gamma ** 2)/(nu - 1) + 2/(nu - 1) * dmdx))\r\n",
        "        return f, df\r\n",
        "    \r\n",
        "    def _eval(self, tau, mean, std):\r\n",
        "    \r\n",
        "        return self.f(tau, mean, std, **self.params)\r\n",
        "    \r\n",
        "    def d_eval(self, tau, mean, std, ds, dm, dvdv, d2v, d2m):\r\n",
        "    \r\n",
        "        return self.f(tau, mean, std, ds, dm, dvdv, d2v, d2m, **self.params)\r\n",
        "\r\n",
        "    def d_eval_stp(self, tau, mean, std, ds, dm, dvdv, d2v, d2m, nu=3.0):\r\n",
        "    \r\n",
        "        return self.f(tau, mean, std, ds, dm, dvdv, d2v, d2m, nu=3.0, **self.params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SdY8rHwtRJ_"
      },
      "source": [
        "### Surrogate derivatives: \r\n",
        "\r\n",
        "from scipy.linalg import cholesky, solve\r\n",
        "\r\n",
        "class dGaussianProcess(GaussianProcess):\r\n",
        "    l = 1\r\n",
        "    sigmaf = 1\r\n",
        "    sigman = 1e-6\r\n",
        "\r\n",
        "    def AcqGrad(self, Xstar, return_std=False):\r\n",
        "        r_X = l2norm_(self.X, self.X)/self.l\r\n",
        "        K = self.sigmaf * np.exp(-1/2*r_X **2) + self.sigman * kronDelta(self.X, self.X)\r\n",
        "        L = cholesky(K).T\r\n",
        "        alpha = solve(L.T, solve(L, self.y))\r\n",
        "        Xstar = np.atleast_2d(Xstar)\r\n",
        "        Kstar = squaredExponentialDeriv.K(self, self.X, Xstar).T\r\n",
        "        dKstar = squaredExponentialDeriv.dK(self, self.X, Xstar).T\r\n",
        "        d2Kstar = squaredExponentialDeriv.d2K(self, self.X, Xstar).T\r\n",
        "        v = solve(self.L, Kstar.T)\r\n",
        "        dv = solve(self.L, dKstar.T)\r\n",
        "        d2v = solve(self.L, d2Kstar.T)\r\n",
        "        \r\n",
        "        ds = -2 * np.dot(dv.T, v)\r\n",
        "        dvdv = np.dot(dv.T, dv)\r\n",
        "        d2s = -2 * (dvdv + d2v)\r\n",
        "        \r\n",
        "        dm = np.dot(dKstar, alpha)\r\n",
        "        d2m = np.dot(d2Kstar, alpha)\r\n",
        "        return ds, dm, dvdv, d2v, d2m\r\n",
        "\r\n",
        "class dtStudentProcess(tStudentProcess):\r\n",
        "    l = 1\r\n",
        "    sigmaf = 1\r\n",
        "    sigman = 1e-6\r\n",
        "    \r\n",
        "    def AcqGrad(self, Xstar, return_std=False):\r\n",
        "        r_X = l2norm_(self.X, self.X)/self.l\r\n",
        "        K = self.sigmaf * np.exp(-1/2*r_X **2) + self.sigman * kronDelta(self.X, self.X)\r\n",
        "        L = cholesky(self.K11).T\r\n",
        "        alpha = solve(L.T, solve(L, self.y))\r\n",
        "        Xstar = np.atleast_2d(Xstar)\r\n",
        "        Kstar = squaredExponentialDeriv.K(self, self.X, Xstar).T\r\n",
        "        dKstar = squaredExponentialDeriv.dK(self, self.X, Xstar).T\r\n",
        "        d2Kstar = squaredExponentialDeriv.d2K(self, self.X, Xstar).T\r\n",
        "        v = solve(L, Kstar.T)\r\n",
        "        dv = solve(L, dKstar.T)\r\n",
        "        d2v = solve(L, d2Kstar.T)\r\n",
        "        \r\n",
        "        ds = -2 * np.dot(dv.T, v)\r\n",
        "        dvdv = np.dot(dv.T, dv)\r\n",
        "        d2s = -2 * (dvdv + d2v)\r\n",
        "        \r\n",
        "        dm = np.dot(dKstar, alpha)\r\n",
        "        d2m = np.dot(d2Kstar, alpha)\r\n",
        "        return ds, dm, dvdv, d2v, d2m"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNv4Z0EqtUi2"
      },
      "source": [
        "class dGPGO(GPGO):  \r\n",
        "    n_start = n_start_AcqFunc\r\n",
        "    eps = 1e-08\r\n",
        "        \r\n",
        "    def func(self, xnew):\r\n",
        "        new_mean, new_var = self.GP.predict(xnew, return_std=True)\r\n",
        "        new_std = np.sqrt(new_var + 1e-6)\r\n",
        "        ds, dm, dvdv, d2v, d2m = self.GP.AcqGrad(xnew, return_std=True)\r\n",
        "        f  = np.empty((self.n_start,))\r\n",
        "        df = np.empty((self.n_start,))\r\n",
        "        f  = -self.A.d_eval(self.tau, new_mean, new_std, ds=ds, dm=dm, dvdv=dvdv, d2v=d2v, d2m=d2m)[0]\r\n",
        "        df = -self.A.d_eval(self.tau, new_mean, new_std, ds=ds, dm=dm, dvdv=dvdv, d2v=d2v, d2m=d2m)[1]\r\n",
        "        df_array = np.full((len(xnew),),df)\r\n",
        "        return f, df_array\r\n",
        "        \r\n",
        "    def d_optimizeAcq(self, method='L-BFGS-B', n_start=n_start):\r\n",
        "        start_points_dict = [self._sampleParam() for i in range(n_start)]\r\n",
        "        start_points_arr = np.array([list(s.values())\r\n",
        "                                     for s in start_points_dict])\r\n",
        "        x_best = np.empty((n_start, len(self.parameter_key)))\r\n",
        "        f_best = np.empty((n_start,))\r\n",
        "        opt = Parallel(n_jobs=self.n_jobs)(delayed(minimize)(self.func,\r\n",
        "                                                                 x0=start_point,\r\n",
        "                                                                 method=method,\r\n",
        "                                                                 jac = True,\r\n",
        "                                                                 bounds=self.parameter_range) for start_point in\r\n",
        "                                               start_points_arr)\r\n",
        "        x_best = np.array([res.x for res in opt])\r\n",
        "        f_best = np.array([np.atleast_1d(res.fun)[0] for res in opt])\r\n",
        "\r\n",
        "        self.x_best = x_best\r\n",
        "        self.f_best = f_best\r\n",
        "        self.best = x_best[np.argmin(f_best)]\r\n",
        "    \r\n",
        "    def run(self, max_iter=10, init_evals=3, resume=False):\r\n",
        "        \r\n",
        "        if not resume:\r\n",
        "            self.init_evals = init_evals\r\n",
        "            self._firstRun(self.init_evals)\r\n",
        "            self.logger._printInit(self)\r\n",
        "        for iteration in range(max_iter):\r\n",
        "            self.d_optimizeAcq()\r\n",
        "            self.updateGP()\r\n",
        "            self.logger._printCurrent(self)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7vea4uj-GOi"
      },
      "source": [
        "## dGPGO_stp - BayesOpt derivatives' class: Student's-t\r\n",
        "\r\n",
        "class dGPGO_stp(GPGO):  \r\n",
        "    n_start = 100\r\n",
        "        \r\n",
        "    def func_stp(self, xnew):\r\n",
        "        new_mean, new_var = self.GP.predict(xnew, return_std=True)\r\n",
        "        new_std = np.sqrt(new_var + 1e-6)\r\n",
        "        ds, dm, dvdv, d2v, d2m = self.GP.AcqGrad(xnew, return_std=True)\r\n",
        "        f  = np.empty((self.n_start,))\r\n",
        "        df = np.empty((self.n_start,))\r\n",
        "        f  = -self.A.d_eval_stp(self.tau, new_mean, new_std, ds=ds, dm=dm, dvdv=dvdv, d2v=d2v, d2m=d2m, nu=df)[0]\r\n",
        "        df = -self.A.d_eval_stp(self.tau, new_mean, new_std, ds=ds, dm=dm, dvdv=dvdv, d2v=d2v, d2m=d2m, nu=df)[1]\r\n",
        "        df_array = np.full((len(xnew),),df)\r\n",
        "        return f, df_array\r\n",
        "        \r\n",
        "    def d_optimizeAcq_stp(self, method='L-BFGS-B', n_start=n_start):\r\n",
        "        start_points_dict = [self._sampleParam() for i in range(n_start)]\r\n",
        "        start_points_arr = np.array([list(s.values())\r\n",
        "                                     for s in start_points_dict])\r\n",
        "        x_best = np.empty((n_start, len(self.parameter_key)))\r\n",
        "        f_best = np.empty((n_start,))\r\n",
        "        opt = Parallel(n_jobs=self.n_jobs)(delayed(minimize)(self.func_stp,\r\n",
        "                                                                 x0=start_point,\r\n",
        "                                                                 method=method,\r\n",
        "                                                                 jac = True,\r\n",
        "                                                                 bounds=self.parameter_range) for start_point in\r\n",
        "                                               start_points_arr)\r\n",
        "        x_best = np.array([res.x for res in opt])\r\n",
        "        f_best = np.array([np.atleast_1d(res.fun)[0] for res in opt])\r\n",
        "\r\n",
        "        self.x_best = x_best\r\n",
        "        self.f_best = f_best\r\n",
        "        self.best = x_best[np.argmin(f_best)]\r\n",
        "    \r\n",
        "    def run(self, max_iter=10, init_evals=3, resume=False):\r\n",
        "        \r\n",
        "        if not resume:\r\n",
        "            self.init_evals = init_evals\r\n",
        "            self._firstRun(self.init_evals)\r\n",
        "            self.logger._printInit(self)\r\n",
        "        for iteration in range(max_iter):\r\n",
        "            self.d_optimizeAcq_stp()\r\n",
        "            self.updateGP()\r\n",
        "            self.logger._printCurrent(self)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "expMSaSUtcB-"
      },
      "source": [
        "### d2GPGO - BayesOpt class: \r\n",
        "\r\n",
        "class d2GPGO(GPGO):  \r\n",
        "    n_start = 100\r\n",
        "    p = np.full((n_start,1),1)\r\n",
        "    \r\n",
        "    def func(self, xnew):\r\n",
        "        new_mean, new_var = self.GP.predict(xnew, return_std=True)\r\n",
        "        new_std = np.sqrt(new_var + 1e-6)\r\n",
        "        ds, dm, dvdv, d2s, d2m = self.GP.AcqGrad(xnew, return_std=True)\r\n",
        "        f  = np.empty((self.n_start,))\r\n",
        "        df = np.empty((self.n_start,))\r\n",
        "        f  = -self.A.d_eval(self.tau, new_mean, new_std, ds=ds, dm=dm, dvdv=dvdv, d2s=d2s, d2m=d2m)[0]\r\n",
        "        df = -self.A.d_eval(self.tau, new_mean, new_std, ds=ds, dm=dm, dvdv=dvdv, d2s=d2s, d2m=d2m)[1]\r\n",
        "        df_array = np.full((len(xnew),),df)\r\n",
        "        return f, df_array\r\n",
        "    \r\n",
        "    def hessp_nonzero(self, xnew, p):\r\n",
        "        new_mean, new_var = self.GP.predict(xnew, return_std=True)\r\n",
        "        new_std = np.sqrt(new_var + 1e-6)\r\n",
        "        ds, dm, dvdv, d2s, d2m = self.GP.AcqGrad(xnew, return_std=True)\r\n",
        "        df2 = np.empty((self.n_start,))\r\n",
        "        df2 = -self.A.d_eval(self.tau, new_mean, new_std, ds=ds, dm=dm, dvdv=dvdv, d2s=d2s, d2m=d2m)[2]\r\n",
        "        H2 = np.empty((self.n_start,))\r\n",
        "        df2 = np.asarray(df2)\r\n",
        "        p = np.asarray(p)\r\n",
        "        H2 = np.multiply(df2,p)\r\n",
        "        return H2\r\n",
        "    \r\n",
        "    def d_optimizeAcq(self, method='Newton-CG', n_start=n_start):\r\n",
        "        start_points_dict = [self._sampleParam() for i in range(n_start)]\r\n",
        "        start_points_arr = np.array([list(s.values())\r\n",
        "                                     for s in start_points_dict])\r\n",
        "        x_best = np.empty((n_start, len(self.parameter_key)))\r\n",
        "        f_best = np.empty((n_start,))\r\n",
        "        opt = Parallel(n_jobs=self.n_jobs)(delayed(minimize)(self.func,\r\n",
        "                                                                 x0=start_point,\r\n",
        "                                                                 method=method,\r\n",
        "                                                                 jac = True,\r\n",
        "                                                                 hessp = self.hessp_nonzero,\r\n",
        "                                                                 bounds=self.parameter_range) for start_point in\r\n",
        "                                               start_points_arr)\r\n",
        "        \r\n",
        "        x_best = np.array([res.x for res in opt])\r\n",
        "        f_best = np.array([np.atleast_1d(res.fun)[0] for res in opt])\r\n",
        "\r\n",
        "        self.best = x_best[np.argmin(f_best)]     \r\n",
        "    \r\n",
        "    def run(self, max_iter=10, init_evals=3, resume=False):\r\n",
        "        \r\n",
        "        if not resume:\r\n",
        "            self.init_evals = init_evals\r\n",
        "            self._firstRun(self.init_evals)\r\n",
        "            self.logger._printInit(self)\r\n",
        "        for iteration in range(max_iter):\r\n",
        "            self.d_optimizeAcq()\r\n",
        "            self.updateGP()\r\n",
        "            self.logger._printCurrent(self)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "umKPgDuEtcxH",
        "outputId": "a8620976-edcd-4d56-dbe3-0ee9c180f94a"
      },
      "source": [
        "start_lose = time.time()\r\n",
        "start_lose"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1613721841.0769222"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mfEuNIqvtdSv",
        "outputId": "705d64b9-c3ea-460c-c16f-7012428b2810"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 1\r\n",
        "\r\n",
        "np.random.seed(run_num_1)\r\n",
        "surrogate_loser_1 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "loser_1 = dGPGO(surrogate_loser_1, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_1.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.61217018 0.16906975 0.43605902]. \t  0.37345684462559386 \t 2.3951473341797507\n",
            "init   \t [0.76926247 0.2953253  0.14916296]. \t  0.296162062221701 \t 2.3951473341797507\n",
            "init   \t [0.02247832 0.42022449 0.23868214]. \t  0.2904113287153621 \t 2.3951473341797507\n",
            "init   \t [0.33765619 0.99071246 0.23772645]. \t  0.11013785080555143 \t 2.3951473341797507\n",
            "init   \t [0.08119266 0.66960024 0.62124292]. \t  2.3951473341797507 \t 2.3951473341797507\n",
            "1      \t [0.00485804 0.68286261 0.96018946]. \t  \u001b[92m2.420367869177603\u001b[0m \t 2.420367869177603\n",
            "2      \t [0.02880528 0.86879906 0.77145199]. \t  1.7601887902025246 \t 2.420367869177603\n",
            "3      \t [0.07492704 0.49363721 0.85736657]. \t  \u001b[92m3.6977257680054527\u001b[0m \t 3.6977257680054527\n",
            "4      \t [0.01690823 0.20303658 0.75373379]. \t  1.1282788852081818 \t 3.6977257680054527\n",
            "5      \t [0.8642259  0.77437694 0.99435991]. \t  1.4015971324220011 \t 3.6977257680054527\n",
            "6      \t [0.45635757 0.34106775 0.99893775]. \t  1.2764378468913773 \t 3.6977257680054527\n",
            "7      \t [0.51811283 0.98946856 0.87150225]. \t  0.67902146488023 \t 3.6977257680054527\n",
            "8      \t [0.99661639 0.78557754 0.7341533 ]. \t  1.357762610045871 \t 3.6977257680054527\n",
            "9      \t [0.23891206 0.52279384 0.744595  ]. \t  2.9890323710727635 \t 3.6977257680054527\n",
            "10     \t [0.81264104 0.02508176 0.05331337]. \t  0.1288747745884618 \t 3.6977257680054527\n",
            "11     \t [0.04939583 0.93739076 0.03475733]. \t  0.0010293495994891455 \t 3.6977257680054527\n",
            "12     \t [0.98025994 0.94493063 0.93523017]. \t  0.7397702819810867 \t 3.6977257680054527\n",
            "13     \t [0.98287372 0.41064655 0.75548703]. \t  2.5063594077603826 \t 3.6977257680054527\n",
            "14     \t [0.00356336 0.52795408 0.85605193]. \t  \u001b[92m3.784229583259046\u001b[0m \t 3.784229583259046\n",
            "15     \t [0.0224553  0.43458959 0.97377221]. \t  2.141981244272377 \t 3.784229583259046\n",
            "16     \t [0.89463363 0.84820422 0.04541421]. \t  0.0006702216295780916 \t 3.784229583259046\n",
            "17     \t [0.44206035 0.23963528 0.53817297]. \t  0.32404118535996623 \t 3.784229583259046\n",
            "18     \t [0.95149581 0.20190542 0.27573327]. \t  0.3360499969635434 \t 3.784229583259046\n",
            "19     \t [0.0256771  0.06257825 0.03723693]. \t  0.13933363855005157 \t 3.784229583259046\n",
            "20     \t [9.74869324e-01 5.78320164e-04 8.56021504e-01]. \t  0.2252628604478815 \t 3.784229583259046\n",
            "21     \t [0.0315056  0.47574841 0.74104847]. \t  2.7939271436222963 \t 3.784229583259046\n",
            "22     \t [0.28843827 0.47548806 0.0361873 ]. \t  0.05481802798153683 \t 3.784229583259046\n",
            "23     \t [0.96154876 0.6577064  0.49507711]. \t  0.28915685733921626 \t 3.784229583259046\n",
            "24     \t [0.07414375 0.54304308 0.82810303]. \t  3.77544594612987 \t 3.784229583259046\n",
            "25     \t [0.02938756 0.45830168 0.01424345]. \t  0.03241094855247772 \t 3.784229583259046\n",
            "26     \t [0.15112422 0.57169865 0.8695877 ]. \t  \u001b[92m3.8131284252695528\u001b[0m \t 3.8131284252695528\n",
            "27     \t [0.15043162 0.60970521 0.85780357]. \t  3.7470332461824105 \t 3.8131284252695528\n",
            "28     \t [0.74361257 0.16587207 0.9127195 ]. \t  0.7937697608354773 \t 3.8131284252695528\n",
            "29     \t [0.33554799 0.1960815  0.29285402]. \t  0.9219313064285296 \t 3.8131284252695528\n",
            "30     \t [0.11181592 0.64508821 0.86932481]. \t  3.552307619911147 \t 3.8131284252695528\n",
            "31     \t [0.34104117 0.1003848  0.35929984]. \t  0.7758886749514444 \t 3.8131284252695528\n",
            "32     \t [0.74081242 0.2689585  0.55926302]. \t  0.3561168332330422 \t 3.8131284252695528\n",
            "33     \t [0.78657803 0.00727757 0.24876982]. \t  0.5199892114983524 \t 3.8131284252695528\n",
            "34     \t [0.71467268 0.1858588  0.43815269]. \t  0.30645954895088945 \t 3.8131284252695528\n",
            "35     \t [0.35345103 0.44407979 0.5372887 ]. \t  0.7310458138475027 \t 3.8131284252695528\n",
            "36     \t [0.99817061 0.28845339 0.96459069]. \t  1.2542554717761243 \t 3.8131284252695528\n",
            "37     \t [0.13551095 0.6147752  0.86099234]. \t  3.7217456557246953 \t 3.8131284252695528\n",
            "38     \t [0.10902695 0.23131956 0.48553758]. \t  0.2882119729872925 \t 3.8131284252695528\n",
            "39     \t [0.02291614 0.84016591 0.0677154 ]. \t  0.0034917025704461944 \t 3.8131284252695528\n",
            "40     \t [0.73759773 0.4973644  0.27133928]. \t  0.17695216392903418 \t 3.8131284252695528\n",
            "41     \t [0.09340409 0.63120493 0.81880677]. \t  3.5500371462617144 \t 3.8131284252695528\n",
            "42     \t [0.15975379 0.0859061  0.84242869]. \t  0.5263639474321362 \t 3.8131284252695528\n",
            "43     \t [0.67406653 0.50991186 0.8188701 ]. \t  3.624086501118737 \t 3.8131284252695528\n",
            "44     \t [0.56610359 0.55232818 0.85012893]. \t  \u001b[92m3.825256006651254\u001b[0m \t 3.825256006651254\n",
            "45     \t [0.58436425 0.14790159 0.82402201]. \t  0.8773608434753184 \t 3.825256006651254\n",
            "46     \t [0.63902865 0.60026654 0.85043859]. \t  3.722030318046956 \t 3.825256006651254\n",
            "47     \t [0.17251606 0.20286351 0.62992262]. \t  0.5516254039236159 \t 3.825256006651254\n",
            "48     \t [0.17397627 0.42220867 0.13521323]. \t  0.2102484680473599 \t 3.825256006651254\n",
            "49     \t [0.01465029 0.57513231 0.85630011]. \t  3.801418874241632 \t 3.825256006651254\n",
            "50     \t [0.09463249 0.74337459 0.42976448]. \t  1.6086984706306757 \t 3.825256006651254\n",
            "51     \t [0.55394923 0.60701978 0.86924581]. \t  3.7135686672921917 \t 3.825256006651254\n",
            "52     \t [0.15384586 0.58434252 0.88508595]. \t  3.7217618326684545 \t 3.825256006651254\n",
            "53     \t [0.97344354 0.58114841 0.77316942]. \t  2.992698046283317 \t 3.825256006651254\n",
            "54     \t [0.5634305  0.72109247 0.84012619]. \t  2.9223035948746707 \t 3.825256006651254\n",
            "55     \t [0.94325517 0.68353852 0.09056374]. \t  0.006284268308091021 \t 3.825256006651254\n",
            "56     \t [0.52111688 0.40014383 0.78101946]. \t  2.817593833921735 \t 3.825256006651254\n",
            "57     \t [0.55152553 0.57153675 0.89655024]. \t  3.6490884963887 \t 3.825256006651254\n",
            "58     \t [0.50107907 0.74984541 0.40285632]. \t  0.8316034757573986 \t 3.825256006651254\n",
            "59     \t [0.6360143  0.17643848 0.6802233 ]. \t  0.6809071664843318 \t 3.825256006651254\n",
            "60     \t [0.80083048 0.87131534 0.70079956]. \t  0.954051341659558 \t 3.825256006651254\n",
            "61     \t [0.82945898 0.45413776 0.05193529]. \t  0.04229423308324152 \t 3.825256006651254\n",
            "62     \t [0.00645321 0.41674021 0.89383033]. \t  3.012380711862429 \t 3.825256006651254\n",
            "63     \t [0.03893406 0.41065296 0.22052005]. \t  0.29746538350005025 \t 3.825256006651254\n",
            "64     \t [0.41043802 0.7724817  0.31190719]. \t  0.36545761370116336 \t 3.825256006651254\n",
            "65     \t [0.10812468 0.72529452 0.25465347]. \t  0.18203581392260854 \t 3.825256006651254\n",
            "66     \t [0.02337902 0.57170695 0.84412219]. \t  3.802239441730874 \t 3.825256006651254\n",
            "67     \t [0.36281072 0.18520399 0.67829577]. \t  0.7170562744784025 \t 3.825256006651254\n",
            "68     \t [0.15911236 0.11059501 0.65089183]. \t  0.3669042997391726 \t 3.825256006651254\n",
            "69     \t [0.86350529 0.8473428  0.26259772]. \t  0.04416400359341738 \t 3.825256006651254\n",
            "70     \t [0.91551351 0.96186868 0.09233347]. \t  0.0007755993199785646 \t 3.825256006651254\n",
            "71     \t [0.54915703 0.56985442 0.26726008]. \t  0.173083197347087 \t 3.825256006651254\n",
            "72     \t [0.25627324 0.96587806 0.97601747]. \t  0.5330995829102597 \t 3.825256006651254\n",
            "73     \t [0.94115139 0.34761178 0.07356107]. \t  0.07136458178756505 \t 3.825256006651254\n",
            "74     \t [0.48760414 0.88948427 0.93965469]. \t  1.1227699240858733 \t 3.825256006651254\n",
            "75     \t [0.11304749 0.9094962  0.33213995]. \t  0.671613628713174 \t 3.825256006651254\n",
            "76     \t [0.27651602 0.0953211  0.81854326]. \t  0.5872636327758214 \t 3.825256006651254\n",
            "77     \t [0.85890336 0.09590625 0.90600383]. \t  0.4619459676508832 \t 3.825256006651254\n",
            "78     \t [0.23733868 0.82048291 0.73915572]. \t  2.124101861946425 \t 3.825256006651254\n",
            "79     \t [0.14510227 0.95514932 0.67105549]. \t  2.084284345766824 \t 3.825256006651254\n",
            "80     \t [0.62299085 0.06122236 0.99608914]. \t  0.17879623427674915 \t 3.825256006651254\n",
            "81     \t [0.29088308 0.7915848  0.41403377]. \t  1.4173589192745526 \t 3.825256006651254\n",
            "82     \t [0.04076394 0.57291442 0.54014621]. \t  1.474778439507063 \t 3.825256006651254\n",
            "83     \t [0.75370983 0.59648656 0.59667461]. \t  0.9825631878460681 \t 3.825256006651254\n",
            "84     \t [0.59983715 0.47183885 0.69463084]. \t  2.06949302073836 \t 3.825256006651254\n",
            "85     \t [0.33242802 0.83723309 0.23185022]. \t  0.11719684568063422 \t 3.825256006651254\n",
            "86     \t [0.07444371 0.91348219 0.27435046]. \t  0.27969467234132755 \t 3.825256006651254\n",
            "87     \t [0.12414229 0.18733278 0.12349369]. \t  0.4276920407976786 \t 3.825256006651254\n",
            "88     \t [0.04087888 0.24199679 0.91266509]. \t  1.323603308572735 \t 3.825256006651254\n",
            "89     \t [0.82437346 0.62993189 0.86780455]. \t  3.530389788348192 \t 3.825256006651254\n",
            "90     \t [0.51211929 0.25967277 0.9643693 ]. \t  1.1073875208142783 \t 3.825256006651254\n",
            "91     \t [0.37851489 0.68753796 0.86748997]. \t  3.2710525685540577 \t 3.825256006651254\n",
            "92     \t [0.91092111 0.42303455 0.93094604]. \t  2.642199576880523 \t 3.825256006651254\n",
            "93     \t [0.84348377 0.94747191 0.42779077]. \t  0.35056261602863853 \t 3.825256006651254\n",
            "94     \t [0.91441912 0.00287177 0.42191247]. \t  0.18004712031761186 \t 3.825256006651254\n",
            "95     \t [0.73026186 0.35943098 0.6904105 ]. \t  1.5865167617832956 \t 3.825256006651254\n",
            "96     \t [0.49324108 0.4044849  0.28058902]. \t  0.43862141669775573 \t 3.825256006651254\n",
            "97     \t [0.7886927  0.92922132 0.01214836]. \t  0.00022036803397238445 \t 3.825256006651254\n",
            "98     \t [0.80472456 0.53074051 0.03159763]. \t  0.01934647763876607 \t 3.825256006651254\n",
            "99     \t [0.39965895 0.49225993 0.45506877]. \t  0.5537815478508364 \t 3.825256006651254\n",
            "100    \t [0.32115229 0.55739422 0.943681  ]. \t  3.080532019803937 \t 3.825256006651254\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1j6UBhkdtdVe",
        "outputId": "49fbb44c-be2d-440d-829a-d7122c58694d"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 2\r\n",
        "\r\n",
        "np.random.seed(run_num_2)\r\n",
        "surrogate_loser_2 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "loser_2 = dGPGO(surrogate_loser_2, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_2.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.85198549 0.0739036  0.89493176]. \t  0.3999083566189884 \t 2.6229838112516717\n",
            "init   \t [0.43649355 0.12767773 0.57585787]. \t  0.24461577848211966 \t 2.6229838112516717\n",
            "init   \t [0.84047092 0.43512055 0.69591056]. \t  1.8897715258798413 \t 2.6229838112516717\n",
            "init   \t [0.6846381  0.70064837 0.77969426]. \t  2.6229838112516717 \t 2.6229838112516717\n",
            "init   \t [0.64274937 0.96102617 0.10846489]. \t  0.003309399496320042 \t 2.6229838112516717\n",
            "1      \t [0.84869923 0.93241717 0.93297728]. \t  0.833544515077231 \t 2.6229838112516717\n",
            "2      \t [0.02123335 0.81643687 0.92868758]. \t  1.7643065017177344 \t 2.6229838112516717\n",
            "3      \t [0.53772176 0.60822836 0.78939081]. \t  \u001b[92m3.334335833419579\u001b[0m \t 3.334335833419579\n",
            "4      \t [0.41955576 0.63153118 0.95115261]. \t  2.832879013202632 \t 3.334335833419579\n",
            "5      \t [0.04108525 0.77996289 0.25475702]. \t  0.19144274065115907 \t 3.334335833419579\n",
            "6      \t [0.33921093 0.6247264  0.68984899]. \t  2.4018311508594015 \t 3.334335833419579\n",
            "7      \t [0.4698131 1.        1.       ]. \t  0.3328206922837246 \t 3.334335833419579\n",
            "8      \t [0.5556955  0.49822112 0.91801822]. \t  3.306460837857002 \t 3.334335833419579\n",
            "9      \t [0.55486262 0.48000381 0.80443181]. \t  \u001b[92m3.45892051020631\u001b[0m \t 3.45892051020631\n",
            "10     \t [0.49254267 0.51666065 0.09211929]. \t  0.07789428322196056 \t 3.45892051020631\n",
            "11     \t [0.56427197 0.47315337 0.80769217]. \t  3.4524476308700676 \t 3.45892051020631\n",
            "12     \t [0.20076159 0.94802777 0.08215082]. \t  0.0037369434913272913 \t 3.45892051020631\n",
            "13     \t [0.59511958 0.52829156 0.7703814 ]. \t  3.2000191832757725 \t 3.45892051020631\n",
            "14     \t [0.88475754 0.03414516 0.00250664]. \t  0.05128069141159111 \t 3.45892051020631\n",
            "15     \t [0.93786595 0.53122158 0.05330172]. \t  0.017297868965155307 \t 3.45892051020631\n",
            "16     \t [0.55954194 0.47340423 0.8693643 ]. \t  \u001b[92m3.5813279149003128\u001b[0m \t 3.5813279149003128\n",
            "17     \t [0.04768557 0.32850307 0.06774003]. \t  0.14217133957014558 \t 3.5813279149003128\n",
            "18     \t [0.526396   0.39045094 0.86418002]. \t  2.984508448221251 \t 3.5813279149003128\n",
            "19     \t [0.43142122 0.02467197 0.00728288]. \t  0.11940469697941827 \t 3.5813279149003128\n",
            "20     \t [0.74314632 0.5090107  0.94480178]. \t  2.9362200818931545 \t 3.5813279149003128\n",
            "21     \t [0.22895586 0.32638851 0.44386193]. \t  0.38289739220068636 \t 3.5813279149003128\n",
            "22     \t [0.46913283 0.82262434 0.20066583]. \t  0.05199322975955362 \t 3.5813279149003128\n",
            "23     \t [0.09778912 0.09784369 0.99987747]. \t  0.24248164089538843 \t 3.5813279149003128\n",
            "24     \t [0.03443249 0.4132959  0.79333907]. \t  2.9934704340345526 \t 3.5813279149003128\n",
            "25     \t [0.38506754 0.63776631 0.47773801]. \t  1.2393662793255629 \t 3.5813279149003128\n",
            "26     \t [0.31206494 0.14585568 0.16799338]. \t  0.7307676260265302 \t 3.5813279149003128\n",
            "27     \t [0.01981209 0.39317072 0.99526576]. \t  1.5953649003670003 \t 3.5813279149003128\n",
            "28     \t [0. 0. 0.]. \t  0.06797411659013229 \t 3.5813279149003128\n",
            "29     \t [0.27452343 0.49067769 0.19135809]. \t  0.21490083226807472 \t 3.5813279149003128\n",
            "30     \t [0.41783706 0.46348636 0.08625525]. \t  0.11238386633298683 \t 3.5813279149003128\n",
            "31     \t [0.00644994 0.58259736 0.77864353]. \t  3.3564867701344787 \t 3.5813279149003128\n",
            "32     \t [0.44338236 0.20986102 0.51086629]. \t  0.28470762022492946 \t 3.5813279149003128\n",
            "33     \t [0.85132587 0.65806184 0.68313386]. \t  1.5772789254037718 \t 3.5813279149003128\n",
            "34     \t [0.72357086 0.17284508 0.5742109 ]. \t  0.2758162501943998 \t 3.5813279149003128\n",
            "35     \t [0.38871972 0.08593287 0.46946098]. \t  0.31822054482285544 \t 3.5813279149003128\n",
            "36     \t [2.55343109e-01 6.77527185e-01 3.57837085e-04]. \t  0.005091131363578613 \t 3.5813279149003128\n",
            "37     \t [0.03071868 0.06496602 0.57653961]. \t  0.15761028036633287 \t 3.5813279149003128\n",
            "38     \t [0.66009546 0.39983393 0.51405981]. \t  0.37461569752794827 \t 3.5813279149003128\n",
            "39     \t [0.59449511 0.41620195 0.91113527]. \t  2.8805701809722963 \t 3.5813279149003128\n",
            "40     \t [0.2256699  0.07992419 0.41748843]. \t  0.4819630201952553 \t 3.5813279149003128\n",
            "41     \t [0.97615885 0.05411726 0.32503146]. \t  0.28831141983778286 \t 3.5813279149003128\n",
            "42     \t [0.53328464 0.57774717 0.88782334]. \t  \u001b[92m3.7094133224093757\u001b[0m \t 3.7094133224093757\n",
            "43     \t [0.52246356 0.82266054 0.13911684]. \t  0.013759219160063875 \t 3.7094133224093757\n",
            "44     \t [0.72982604 0.28184031 0.86459949]. \t  1.9044776108865855 \t 3.7094133224093757\n",
            "45     \t [0.63818509 0.39198514 0.45536791]. \t  0.2892748188116343 \t 3.7094133224093757\n",
            "46     \t [0.73136775 0.30934836 0.92232944]. \t  1.8276447749873623 \t 3.7094133224093757\n",
            "47     \t [0.77047443 0.64214483 0.12903927]. \t  0.024102114086029674 \t 3.7094133224093757\n",
            "48     \t [0.4634485  0.65534339 0.23827586]. \t  0.11593556414242427 \t 3.7094133224093757\n",
            "49     \t [0.26527368 0.43158941 0.30440152]. \t  0.4071890908589547 \t 3.7094133224093757\n",
            "50     \t [0.98012153 0.29487901 0.54063701]. \t  0.273707971859134 \t 3.7094133224093757\n",
            "51     \t [0.20844235 0.33272232 0.11427776]. \t  0.2883861055346616 \t 3.7094133224093757\n",
            "52     \t [0.83243348 0.27176422 0.0498824 ]. \t  0.10004439130921053 \t 3.7094133224093757\n",
            "53     \t [0.31228377 0.9497917  0.78925978]. \t  1.1372129515685026 \t 3.7094133224093757\n",
            "54     \t [0.81204389 0.10745564 0.99319514]. \t  0.2793664207698889 \t 3.7094133224093757\n",
            "55     \t [0.30559737 0.74140756 0.20001864]. \t  0.0690608605636907 \t 3.7094133224093757\n",
            "56     \t [0.30355771 0.60105121 0.66701023]. \t  2.232680275582264 \t 3.7094133224093757\n",
            "57     \t [0.32655384 0.45283861 0.70914601]. \t  2.3157027494336946 \t 3.7094133224093757\n",
            "58     \t [0.28164058 0.91846989 0.13751527]. \t  0.015473546478944136 \t 3.7094133224093757\n",
            "59     \t [0.04941912 0.54241983 0.36607595]. \t  0.4385741473197778 \t 3.7094133224093757\n",
            "60     \t [0.17888481 0.27261652 0.57983518]. \t  0.5075266432155109 \t 3.7094133224093757\n",
            "61     \t [0.60451084 0.31746966 0.12296527]. \t  0.30343594735798185 \t 3.7094133224093757\n",
            "62     \t [0.69919355 0.08866689 0.64966157]. \t  0.30694751763473066 \t 3.7094133224093757\n",
            "63     \t [0.89315899 0.15618225 0.85935459]. \t  0.8735852961978772 \t 3.7094133224093757\n",
            "64     \t [0.1521253  0.35901651 0.22965336]. \t  0.4723852558929425 \t 3.7094133224093757\n",
            "65     \t [0.23231678 0.10972766 0.93641936]. \t  0.44882914638217647 \t 3.7094133224093757\n",
            "66     \t [0.84188226 0.70723005 0.54045426]. \t  0.6264810020583895 \t 3.7094133224093757\n",
            "67     \t [0.01124263 0.9495282  0.89764946]. \t  0.8860680765708808 \t 3.7094133224093757\n",
            "68     \t [0.64184498 0.26595085 0.80861442]. \t  1.7950464127622898 \t 3.7094133224093757\n",
            "69     \t [0.0229726  0.00499224 0.67044876]. \t  0.17729951913373557 \t 3.7094133224093757\n",
            "70     \t [0.51570772 0.65260972 0.10114952]. \t  0.025592110981376887 \t 3.7094133224093757\n",
            "71     \t [0.60347441 0.897619   0.45446129]. \t  1.06949486043582 \t 3.7094133224093757\n",
            "72     \t [0.95758577 0.64651273 0.9322479 ]. \t  2.9288989872413875 \t 3.7094133224093757\n",
            "73     \t [0.78857677 0.49823768 0.51203495]. \t  0.38734467751295526 \t 3.7094133224093757\n",
            "74     \t [0.43165387 0.02353938 0.45456709]. \t  0.3284207744430256 \t 3.7094133224093757\n",
            "75     \t [0.93712698 0.40753674 0.39799078]. \t  0.1357588290930862 \t 3.7094133224093757\n",
            "76     \t [0.75852101 0.56650411 0.29588659]. \t  0.12717945507032985 \t 3.7094133224093757\n",
            "77     \t [0.65699665 0.67754593 0.95225407]. \t  2.573110674137345 \t 3.7094133224093757\n",
            "78     \t [0.21411235 0.92073944 0.18720244]. \t  0.050558991438194395 \t 3.7094133224093757\n",
            "79     \t [0.81186963 0.95466875 0.19513996]. \t  0.013622693124886443 \t 3.7094133224093757\n",
            "80     \t [0.13321787 0.16889317 0.15110243]. \t  0.549728293542864 \t 3.7094133224093757\n",
            "81     \t [0.50055678 0.40552636 0.35889602]. \t  0.39460953216206407 \t 3.7094133224093757\n",
            "82     \t [0.68937348 0.74734445 0.93696005]. \t  2.2621603595408324 \t 3.7094133224093757\n",
            "83     \t [0.24422687 0.25258948 0.42558462]. \t  0.43458421843040185 \t 3.7094133224093757\n",
            "84     \t [0.33290521 0.92785349 0.70086949]. \t  1.7092276465727636 \t 3.7094133224093757\n",
            "85     \t [0.23066553 0.98634866 0.51216754]. \t  2.405127214497184 \t 3.7094133224093757\n",
            "86     \t [0.77062047 0.44235163 0.41154312]. \t  0.2070073279439252 \t 3.7094133224093757\n",
            "87     \t [0.89972384 0.5416144  0.91950053]. \t  3.316834149142124 \t 3.7094133224093757\n",
            "88     \t [0.35886225 0.27348523 0.98584931]. \t  1.0168568786796437 \t 3.7094133224093757\n",
            "89     \t [0.24008624 0.71862237 0.60323517]. \t  2.5243228209177246 \t 3.7094133224093757\n",
            "90     \t [0.70227469 0.71073892 0.38639049]. \t  0.3630229515433469 \t 3.7094133224093757\n",
            "91     \t [0.04630934 0.53729892 0.00134421]. \t  0.015083670432843367 \t 3.7094133224093757\n",
            "92     \t [0.68416238 0.83798606 0.92513195]. \t  1.5835707271787056 \t 3.7094133224093757\n",
            "93     \t [0.80857863 0.30322871 0.42341   ]. \t  0.23276557630011466 \t 3.7094133224093757\n",
            "94     \t [0.87240962 0.53505616 0.5612704 ]. \t  0.5766218070407124 \t 3.7094133224093757\n",
            "95     \t [0.37075559 0.35467445 0.40266831]. \t  0.4292705700245347 \t 3.7094133224093757\n",
            "96     \t [0.07623101 0.7603222  0.69765839]. \t  2.4874462002268505 \t 3.7094133224093757\n",
            "97     \t [0.52298903 0.07651244 0.54207474]. \t  0.17715826053064726 \t 3.7094133224093757\n",
            "98     \t [0.18967587 0.49028662 0.51106987]. \t  0.8680665472248712 \t 3.7094133224093757\n",
            "99     \t [0.77481162 0.28016538 0.68979909]. \t  1.2006144140525907 \t 3.7094133224093757\n",
            "100    \t [0.34253365 0.56155319 0.19047772]. \t  0.13390384213700912 \t 3.7094133224093757\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eWnL1rwztdYG",
        "outputId": "5f6d5a0f-7b1b-46c1-db4e-3feb28043e48"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 3\r\n",
        "\r\n",
        "np.random.seed(run_num_3)\r\n",
        "surrogate_loser_3 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "loser_3 = dGPGO(surrogate_loser_3, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_3.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.75157561 0.10925128 0.48612128]. \t  0.19395982560567754 \t 0.5647137279144399\n",
            "init   \t [0.49983118 0.65711    0.23588471]. \t  0.10633623603471726 \t 0.5647137279144399\n",
            "init   \t [0.61279489 0.1196524  0.7122023 ]. \t  0.5647137279144399 \t 0.5647137279144399\n",
            "init   \t [0.37256054 0.52476827 0.26328708]. \t  0.24645583811739444 \t 0.5647137279144399\n",
            "init   \t [0.62163122 0.44909976 0.21906361]. \t  0.2633149120115605 \t 0.5647137279144399\n",
            "1      \t [0.03146046 0.20692188 0.93064958]. \t  \u001b[92m0.9684227062893344\u001b[0m \t 0.9684227062893344\n",
            "2      \t [0.22074887 0.05835947 0.90763313]. \t  0.3319318657983171 \t 0.9684227062893344\n",
            "3      \t [0.80398301 0.59732101 0.97755957]. \t  \u001b[92m2.4285425171285553\u001b[0m \t 2.4285425171285553\n",
            "4      \t [0.5136305  0.6806365  0.98913253]. \t  2.0107525433877536 \t 2.4285425171285553\n",
            "5      \t [0.96047188 0.84635755 0.98454941]. \t  1.059610295269561 \t 2.4285425171285553\n",
            "6      \t [0.73881103 0.2852016  0.92866174]. \t  1.569439758670843 \t 2.4285425171285553\n",
            "7      \t [0.71874866 0.52230578 0.95817827]. \t  \u001b[92m2.7596541737687836\u001b[0m \t 2.7596541737687836\n",
            "8      \t [0.58383333 0.51359399 0.72217957]. \t  2.534851203896329 \t 2.7596541737687836\n",
            "9      \t [0.69948316 0.50135082 0.96939443]. \t  2.5149949368150564 \t 2.7596541737687836\n",
            "10     \t [2.21903490e-15 3.33185517e-14 2.60365980e-15]. \t  0.06797411659014074 \t 2.7596541737687836\n",
            "11     \t [0.08297588 0.58449891 0.0494654 ]. \t  0.021801490436882155 \t 2.7596541737687836\n",
            "12     \t [0.72187606 0.64625932 0.88228903]. \t  \u001b[92m3.435042406543167\u001b[0m \t 3.435042406543167\n",
            "13     \t [0.69808759 0.73788415 0.86826237]. \t  2.744041156678493 \t 3.435042406543167\n",
            "14     \t [0.93887588 0.6613653  0.69546595]. \t  1.6275056193081119 \t 3.435042406543167\n",
            "15     \t [0.31568132 0.43634948 0.00142586]. \t  0.04293482559337013 \t 3.435042406543167\n",
            "16     \t [0.16842711 0.275181   0.2593233 ]. \t  0.6951641784284915 \t 3.435042406543167\n",
            "17     \t [0.         0.06579622 0.28490418]. \t  0.642256134812402 \t 3.435042406543167\n",
            "18     \t [0.71655162 0.49206118 0.79554346]. \t  3.3736815291998044 \t 3.435042406543167\n",
            "19     \t [0.75518888 0.55697264 0.75159371]. \t  2.855037242391764 \t 3.435042406543167\n",
            "20     \t [0.70228575 0.97471212 0.02582005]. \t  0.00029310748513941917 \t 3.435042406543167\n",
            "21     \t [0.99752384 0.064151   0.1703046 ]. \t  0.22410412246044556 \t 3.435042406543167\n",
            "22     \t [0.0228455  0.90537222 0.38683236]. \t  1.2492930756187308 \t 3.435042406543167\n",
            "23     \t [0.13378523 0.95916986 0.14208018]. \t  0.017267101982617633 \t 3.435042406543167\n",
            "24     \t [0.15772068 0.87127358 0.74372625]. \t  1.8724552511399 \t 3.435042406543167\n",
            "25     \t [0.14574635 0.38726702 0.24731617]. \t  0.42664930173697285 \t 3.435042406543167\n",
            "26     \t [0.04777267 0.86009005 0.91542791]. \t  1.4808874257581612 \t 3.435042406543167\n",
            "27     \t [0.63444916 0.36426605 0.85539499]. \t  2.7493518034550775 \t 3.435042406543167\n",
            "28     \t [0.73479632 0.49575709 0.51112471]. \t  0.4256175209454758 \t 3.435042406543167\n",
            "29     \t [0.30819472 0.01158196 0.12503412]. \t  0.48221799946723587 \t 3.435042406543167\n",
            "30     \t [0.74662962 0.58954966 0.84542697]. \t  \u001b[92m3.7070039032667954\u001b[0m \t 3.7070039032667954\n",
            "31     \t [0.42596168 0.35197751 0.56469788]. \t  0.5906940593598009 \t 3.7070039032667954\n",
            "32     \t [0.80411671 0.9277025  0.12495525]. \t  0.0031149831092267023 \t 3.7070039032667954\n",
            "33     \t [0.66688846 0.22554472 0.58509531]. \t  0.3956915884135331 \t 3.7070039032667954\n",
            "34     \t [0.96402316 0.80627495 0.02410483]. \t  0.0005753484237622942 \t 3.7070039032667954\n",
            "35     \t [0.46624611 0.63255562 0.52234503]. \t  1.2945035042624384 \t 3.7070039032667954\n",
            "36     \t [0.75738113 0.92448556 0.82374733]. \t  1.0051096970615077 \t 3.7070039032667954\n",
            "37     \t [0.13401842 0.59394694 0.07117342]. \t  0.02871568488814516 \t 3.7070039032667954\n",
            "38     \t [0.5921114  0.60036898 0.98206343]. \t  2.377598838026082 \t 3.7070039032667954\n",
            "39     \t [0.79172764 0.90784428 0.70914857]. \t  0.8431319054674943 \t 3.7070039032667954\n",
            "40     \t [0.1529027  0.26265257 0.99129722]. \t  0.9064177756037938 \t 3.7070039032667954\n",
            "41     \t [0.62518867 0.7121498  0.30259927]. \t  0.17816033134214168 \t 3.7070039032667954\n",
            "42     \t [0.24439409 0.39110835 0.57603432]. \t  0.8043123257896625 \t 3.7070039032667954\n",
            "43     \t [0.23812336 0.07427496 0.89625034]. \t  0.40560748042148276 \t 3.7070039032667954\n",
            "44     \t [0.04418732 0.53134902 0.61276949]. \t  1.6534094783377127 \t 3.7070039032667954\n",
            "45     \t [0.89190225 0.97916204 0.0016274 ]. \t  7.543539199836043e-05 \t 3.7070039032667954\n",
            "46     \t [0.8607458  0.29914426 0.76705565]. \t  1.8822648550157148 \t 3.7070039032667954\n",
            "47     \t [0.48787013 0.40950719 0.58412121]. \t  0.8188835841989224 \t 3.7070039032667954\n",
            "48     \t [0.6043061  0.76298401 0.62408145]. \t  1.5630631055365873 \t 3.7070039032667954\n",
            "49     \t [0.40684603 0.86167236 0.56044735]. \t  2.3905743122833742 \t 3.7070039032667954\n",
            "50     \t [0.62111368 0.52704732 0.84922528]. \t  \u001b[92m3.7896923749467715\u001b[0m \t 3.7896923749467715\n",
            "51     \t [0.08214015 0.55596698 0.52951788]. \t  1.3249204381796214 \t 3.7896923749467715\n",
            "52     \t [0.49073662 0.44830916 0.62947485]. \t  1.3035237788368443 \t 3.7896923749467715\n",
            "53     \t [0.61633848 0.39414783 0.60448986]. \t  0.8720175831755752 \t 3.7896923749467715\n",
            "54     \t [0.09632054 0.64536185 0.89614376]. \t  3.407552915077267 \t 3.7896923749467715\n",
            "55     \t [0.92134716 0.78477873 0.66050099]. \t  0.9011550630172612 \t 3.7896923749467715\n",
            "56     \t [0.57293523 0.53361841 0.30639247]. \t  0.22796920303046153 \t 3.7896923749467715\n",
            "57     \t [0.21923538 0.73686176 0.76555192]. \t  2.631803742382601 \t 3.7896923749467715\n",
            "58     \t [0.28290393 0.40273006 0.21517779]. \t  0.40796869278351705 \t 3.7896923749467715\n",
            "59     \t [0.7297733  0.40900862 0.17685063]. \t  0.2271653108899221 \t 3.7896923749467715\n",
            "60     \t [0.2230724  0.51649058 0.0899105 ]. \t  0.07523571607849378 \t 3.7896923749467715\n",
            "61     \t [0.47269005 0.57372622 0.30365754]. \t  0.24086025892892132 \t 3.7896923749467715\n",
            "62     \t [0.19179956 0.04726392 0.22522958]. \t  0.8223003040779117 \t 3.7896923749467715\n",
            "63     \t [0.00454622 0.61282583 0.98623381]. \t  2.2601194402741687 \t 3.7896923749467715\n",
            "64     \t [0.21059232 0.42055126 0.12793231]. \t  0.20770560044238096 \t 3.7896923749467715\n",
            "65     \t [0.34021681 0.82098355 0.4395948 ]. \t  1.6861927233521885 \t 3.7896923749467715\n",
            "66     \t [0.39788785 0.59683922 0.81430545]. \t  3.6427668029762037 \t 3.7896923749467715\n",
            "67     \t [0.46904421 0.31390676 0.81104597]. \t  2.257544468526626 \t 3.7896923749467715\n",
            "68     \t [0.84478817 0.17962491 0.48977645]. \t  0.17730670216691743 \t 3.7896923749467715\n",
            "69     \t [0.65164826 0.42992198 0.58646353]. \t  0.786818072813842 \t 3.7896923749467715\n",
            "70     \t [0.61884566 0.31746878 0.49688504]. \t  0.29554206320383436 \t 3.7896923749467715\n",
            "71     \t [0.61376377 0.70204201 0.35125962]. \t  0.3258708768234984 \t 3.7896923749467715\n",
            "72     \t [0.70209444 0.16663276 0.15167109]. \t  0.46828875679206056 \t 3.7896923749467715\n",
            "73     \t [0.26262437 0.44250019 0.03060716]. \t  0.06252148954452384 \t 3.7896923749467715\n",
            "74     \t [0.16253149 0.66009353 0.2770664 ]. \t  0.2333070716240491 \t 3.7896923749467715\n",
            "75     \t [0.54743938 0.95930507 0.37960054]. \t  0.6252381735775987 \t 3.7896923749467715\n",
            "76     \t [0.16019912 0.90675208 0.41598391]. \t  1.6567154722530448 \t 3.7896923749467715\n",
            "77     \t [0.14125313 0.56127449 0.28467157]. \t  0.24526769934086157 \t 3.7896923749467715\n",
            "78     \t [0.68087076 0.47864738 0.02789553]. \t  0.03623559685115433 \t 3.7896923749467715\n",
            "79     \t [0.84363628 0.39010202 0.36788822]. \t  0.20606981964993962 \t 3.7896923749467715\n",
            "80     \t [0.88060534 0.81553029 0.87952935]. \t  1.9182660692088596 \t 3.7896923749467715\n",
            "81     \t [0.89877928 0.3459699  0.20278669]. \t  0.22577900317369806 \t 3.7896923749467715\n",
            "82     \t [0.45857582 0.75096334 0.18121827]. \t  0.04132361494283127 \t 3.7896923749467715\n",
            "83     \t [0.63085465 0.39247245 0.10795824]. \t  0.1782535802299659 \t 3.7896923749467715\n",
            "84     \t [0.95316593 0.55188064 0.16964544]. \t  0.04218875364955372 \t 3.7896923749467715\n",
            "85     \t [0.85684935 0.75766339 0.033176  ]. \t  0.0017000811369795696 \t 3.7896923749467715\n",
            "86     \t [0.78716689 0.04587408 0.94080521]. \t  0.239651019612562 \t 3.7896923749467715\n",
            "87     \t [0.43911044 0.6981408  0.64567541]. \t  2.053757934020994 \t 3.7896923749467715\n",
            "88     \t [0.8694776  0.36424848 0.71022565]. \t  1.7943091250771404 \t 3.7896923749467715\n",
            "89     \t [0.13016603 0.16838239 0.61643476]. \t  0.4054373001335664 \t 3.7896923749467715\n",
            "90     \t [0.02729288 0.32701445 0.32086059]. \t  0.4466395844396876 \t 3.7896923749467715\n",
            "91     \t [0.37397447 0.81674824 0.92999278]. \t  1.7677722791097792 \t 3.7896923749467715\n",
            "92     \t [0.18208795 0.60697419 0.59659828]. \t  1.9675248424084217 \t 3.7896923749467715\n",
            "93     \t [0.23646816 0.25281473 0.09853109]. \t  0.3358113649828574 \t 3.7896923749467715\n",
            "94     \t [0.33836395 0.50746848 0.85995754]. \t  3.776428935520155 \t 3.7896923749467715\n",
            "95     \t [0.01149402 0.12150612 0.0622546 ]. \t  0.19307208285276578 \t 3.7896923749467715\n",
            "96     \t [0.54711447 0.76849605 0.56712612]. \t  1.70301075667317 \t 3.7896923749467715\n",
            "97     \t [0.89431243 0.73140871 0.6123873 ]. \t  0.803263834874783 \t 3.7896923749467715\n",
            "98     \t [0.18324495 0.1323387  0.43675612]. \t  0.404632292432565 \t 3.7896923749467715\n",
            "99     \t [0.38974735 0.09538962 0.96336092]. \t  0.3290183762762645 \t 3.7896923749467715\n",
            "100    \t [0.19820904 0.06434005 0.77592979]. \t  0.4449318315806874 \t 3.7896923749467715\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zciLjgNltda2",
        "outputId": "a107569a-74df-49f6-d7ce-86df7e925276"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 4\r\n",
        "\r\n",
        "np.random.seed(run_num_4)\r\n",
        "surrogate_loser_4 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "loser_4 = dGPGO(surrogate_loser_4, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_4.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.83936105 0.82129497 0.64220716]. \t  0.8796727111154451 \t 1.9592421489197056\n",
            "init   \t [0.66722262 0.03913991 0.06567239]. \t  0.21283931850025994 \t 1.9592421489197056\n",
            "init   \t [0.27650064 0.3163605  0.19359241]. \t  0.5590312289203825 \t 1.9592421489197056\n",
            "init   \t [0.19709288 0.84920429 0.84235809]. \t  1.818649590001362 \t 1.9592421489197056\n",
            "init   \t [0.34736473 0.7397423  0.49340914]. \t  1.9592421489197056 \t 1.9592421489197056\n",
            "1      \t [0.22412884 0.99744654 0.22062981]. \t  0.08723427566372202 \t 1.9592421489197056\n",
            "2      \t [0.4057286  0.31242791 0.80779792]. \t  \u001b[92m2.2369433002019576\u001b[0m \t 2.2369433002019576\n",
            "3      \t [0.32214328 0.501755   0.69916371]. \t  \u001b[92m2.347550186167049\u001b[0m \t 2.347550186167049\n",
            "4      \t [0.39005329 0.54644944 0.69817317]. \t  \u001b[92m2.3848842680130833\u001b[0m \t 2.3848842680130833\n",
            "5      \t [0.34533898 0.52489446 0.79943823]. \t  \u001b[92m3.5836920400367456\u001b[0m \t 3.5836920400367456\n",
            "6      \t [0.63960296 0.91555258 0.0555591 ]. \t  0.0010744359811289052 \t 3.5836920400367456\n",
            "7      \t [0.22351983 0.28690065 0.99397278]. \t  1.0190572439287129 \t 3.5836920400367456\n",
            "8      \t [1. 1. 1.]. \t  0.31688362070415665 \t 3.5836920400367456\n",
            "9      \t [0.77141999 0.13456388 0.98918372]. \t  0.3672430639616408 \t 3.5836920400367456\n",
            "10     \t [9.77453872e-17 1.90403794e-16 2.67441435e-16]. \t  0.06797411659013262 \t 3.5836920400367456\n",
            "11     \t [0.51437176 0.48035947 0.99743479]. \t  1.9786849166252316 \t 3.5836920400367456\n",
            "12     \t [0.84655016 0.99679085 0.6174574 ]. \t  0.5215294903522869 \t 3.5836920400367456\n",
            "13     \t [0.17365811 0.53915787 0.02836154]. \t  0.027309993474375557 \t 3.5836920400367456\n",
            "14     \t [0.29207719 0.024576   0.05725103]. \t  0.24009054578274416 \t 3.5836920400367456\n",
            "15     \t [0.98578944 0.13581165 0.27179492]. \t  0.3182599136266915 \t 3.5836920400367456\n",
            "16     \t [0.34593306 0.65138162 0.95430717]. \t  2.7032184610475998 \t 3.5836920400367456\n",
            "17     \t [0.99991374 0.24441493 0.03350525]. \t  0.0499541826127307 \t 3.5836920400367456\n",
            "18     \t [0.99429147 0.7531398  0.66991977]. \t  0.9772652638068244 \t 3.5836920400367456\n",
            "19     \t [0.27805725 0.59470274 0.85747296]. \t  \u001b[92m3.8051821097914003\u001b[0m \t 3.8051821097914003\n",
            "20     \t [0.94383316 0.48004807 0.41985768]. \t  0.12403116135075266 \t 3.8051821097914003\n",
            "21     \t [0.27516191 0.5050454  0.91115678]. \t  3.4233821782694847 \t 3.8051821097914003\n",
            "22     \t [0.88210647 0.22206517 0.85819382]. \t  1.3626645528941668 \t 3.8051821097914003\n",
            "23     \t [0.465022   0.28549004 0.02474828]. \t  0.12537651819404955 \t 3.8051821097914003\n",
            "24     \t [0.77736605 0.42924543 0.32339966]. \t  0.23232259737210692 \t 3.8051821097914003\n",
            "25     \t [0.48323297 0.97953335 0.58107143]. \t  1.7743492083378223 \t 3.8051821097914003\n",
            "26     \t [0.07390852 0.47026935 0.4418145 ]. \t  0.5388868547402469 \t 3.8051821097914003\n",
            "27     \t [0.43391942 0.96545355 0.6232254 ]. \t  1.8531866038798177 \t 3.8051821097914003\n",
            "28     \t [0.72373636 0.89800689 0.19750102]. \t  0.022219851170923014 \t 3.8051821097914003\n",
            "29     \t [0.23399885 0.62804062 0.83057931]. \t  3.6327766749305987 \t 3.8051821097914003\n",
            "30     \t [0.47646467 0.31690246 0.92015299]. \t  1.9335877861944526 \t 3.8051821097914003\n",
            "31     \t [0.26793312 0.20835569 0.39616386]. \t  0.5675650435515753 \t 3.8051821097914003\n",
            "32     \t [0.15449961 0.0739904  0.02383857]. \t  0.1444824248167775 \t 3.8051821097914003\n",
            "33     \t [0.22224128 0.58835294 0.54227105]. \t  1.5620019707976203 \t 3.8051821097914003\n",
            "34     \t [0.7624105  0.83740933 0.4174219 ]. \t  0.475673205419742 \t 3.8051821097914003\n",
            "35     \t [0.38226498 0.56576795 0.31580928]. \t  0.2940891935717929 \t 3.8051821097914003\n",
            "36     \t [0.42122065 0.03334281 0.87861693]. \t  0.2979774737400446 \t 3.8051821097914003\n",
            "37     \t [0.95577186 0.2331745  0.18906347]. \t  0.25888078497782724 \t 3.8051821097914003\n",
            "38     \t [0.77686775 0.19817622 0.50516741]. \t  0.20303404687426588 \t 3.8051821097914003\n",
            "39     \t [0.02572599 0.03736815 0.77135148]. \t  0.34568347538666316 \t 3.8051821097914003\n",
            "40     \t [0.53961985 0.04018105 0.36712348]. \t  0.6427412747817036 \t 3.8051821097914003\n",
            "41     \t [0.09117621 0.44826011 0.64659412]. \t  1.6004301056836554 \t 3.8051821097914003\n",
            "42     \t [0.69797215 0.92699324 0.60669458]. \t  1.068196523028918 \t 3.8051821097914003\n",
            "43     \t [0.14716524 0.6846356  0.55273114]. \t  2.331277322632874 \t 3.8051821097914003\n",
            "44     \t [0.94460183 0.39841865 0.20035792]. \t  0.14743471184946214 \t 3.8051821097914003\n",
            "45     \t [0.02849758 0.52533294 0.99914079]. \t  2.0416369587013725 \t 3.8051821097914003\n",
            "46     \t [0.59525019 0.69610244 0.04470115]. \t  0.0072205823831998734 \t 3.8051821097914003\n",
            "47     \t [0.18193296 0.12830195 0.57593137]. \t  0.24216357204494532 \t 3.8051821097914003\n",
            "48     \t [0.20414017 0.28332865 0.57594635]. \t  0.5157278007739652 \t 3.8051821097914003\n",
            "49     \t [0.51788579 0.81395536 0.34779211]. \t  0.4928912781654505 \t 3.8051821097914003\n",
            "50     \t [0.19029052 0.2022316  0.06057696]. \t  0.23450252136128893 \t 3.8051821097914003\n",
            "51     \t [0.31404645 0.34552945 0.79524926]. \t  2.489876466905594 \t 3.8051821097914003\n",
            "52     \t [0.38784961 0.87753762 0.74012079]. \t  1.6471620810320635 \t 3.8051821097914003\n",
            "53     \t [0.86772633 0.88541181 0.69753257]. \t  0.7934431092025374 \t 3.8051821097914003\n",
            "54     \t [0.46112737 0.30964653 0.05452543]. \t  0.17298332743033695 \t 3.8051821097914003\n",
            "55     \t [0.26064166 0.75821426 0.03807247]. \t  0.0040868577728126795 \t 3.8051821097914003\n",
            "56     \t [0.54614286 0.98388172 0.59399202]. \t  1.4879629764781568 \t 3.8051821097914003\n",
            "57     \t [0.45408303 0.80825061 0.6365093 ]. \t  2.0677163745827047 \t 3.8051821097914003\n",
            "58     \t [0.70741816 0.07928674 0.66920884]. \t  0.32737925886271296 \t 3.8051821097914003\n",
            "59     \t [0.06057931 0.81118412 0.58648205]. \t  2.9844350007817253 \t 3.8051821097914003\n",
            "60     \t [0.18077291 0.11516388 0.30186734]. \t  0.8693688111325402 \t 3.8051821097914003\n",
            "61     \t [0.78261206 0.08739428 0.30479699]. \t  0.569307619631024 \t 3.8051821097914003\n",
            "62     \t [0.20286778 0.48979907 0.2163436 ]. \t  0.2338871807716341 \t 3.8051821097914003\n",
            "63     \t [0.5386693  0.27612135 0.13522479]. \t  0.4221550183648063 \t 3.8051821097914003\n",
            "64     \t [0.22997948 0.52205122 0.23892099]. \t  0.220726732621446 \t 3.8051821097914003\n",
            "65     \t [0.85129755 0.30898539 0.42632346]. \t  0.20407290980443352 \t 3.8051821097914003\n",
            "66     \t [0.5290956  0.06631674 0.6350847 ]. \t  0.23937763515635238 \t 3.8051821097914003\n",
            "67     \t [0.32225612 0.58867405 0.66683327]. \t  2.187367589850706 \t 3.8051821097914003\n",
            "68     \t [0.70417805 0.87920468 0.75425424]. \t  1.185947839625982 \t 3.8051821097914003\n",
            "69     \t [0.68707938 0.97773471 0.3911807 ]. \t  0.44383116056510996 \t 3.8051821097914003\n",
            "70     \t [0.36894736 0.56217032 0.37926925]. \t  0.4739893548207636 \t 3.8051821097914003\n",
            "71     \t [0.91906465 0.12271966 0.32488807]. \t  0.3661618263517013 \t 3.8051821097914003\n",
            "72     \t [0.79562402 0.78140355 0.99146553]. \t  1.4024275536421427 \t 3.8051821097914003\n",
            "73     \t [0.19952009 0.46582031 0.97637385]. \t  2.2772004000300323 \t 3.8051821097914003\n",
            "74     \t [0.42921714 0.21734282 0.37283993]. \t  0.6569917965717333 \t 3.8051821097914003\n",
            "75     \t [0.58785692 0.23925236 0.34858759]. \t  0.6224387919467267 \t 3.8051821097914003\n",
            "76     \t [0.83910257 0.94330969 0.78787869]. \t  0.7862412827987283 \t 3.8051821097914003\n",
            "77     \t [0.56118624 0.59986808 0.49463569]. \t  0.8250381800002857 \t 3.8051821097914003\n",
            "78     \t [0.7662562  0.14790208 0.78952916]. \t  0.8577485172452937 \t 3.8051821097914003\n",
            "79     \t [0.21712483 0.17339281 0.04403016]. \t  0.20263511578268303 \t 3.8051821097914003\n",
            "80     \t [0.34812389 0.0410566  0.25745782]. \t  0.9402321835937894 \t 3.8051821097914003\n",
            "81     \t [0.80787489 0.49984609 0.19371546]. \t  0.11358820295056764 \t 3.8051821097914003\n",
            "82     \t [0.77992988 0.67256643 0.99579517]. \t  1.9093788624256178 \t 3.8051821097914003\n",
            "83     \t [0.74233078 0.51867866 0.07880477]. \t  0.04543925368190092 \t 3.8051821097914003\n",
            "84     \t [0.42375063 0.18059118 0.13005468]. \t  0.5409662204364257 \t 3.8051821097914003\n",
            "85     \t [0.82159906 0.2629594  0.99301144]. \t  0.8829369483160052 \t 3.8051821097914003\n",
            "86     \t [0.87004636 0.48103157 0.64604538]. \t  1.304640200884708 \t 3.8051821097914003\n",
            "87     \t [0.00611828 0.39700184 0.56633479]. \t  0.7582299138150631 \t 3.8051821097914003\n",
            "88     \t [0.66081675 0.24908963 0.36671335]. \t  0.49741112338686794 \t 3.8051821097914003\n",
            "89     \t [0.37145448 0.12239177 0.85745563]. \t  0.6941282348959765 \t 3.8051821097914003\n",
            "90     \t [0.50921909 0.11168804 0.67675224]. \t  0.44129452824360094 \t 3.8051821097914003\n",
            "91     \t [0.12022965 0.95893321 0.59043519]. \t  2.751577852315629 \t 3.8051821097914003\n",
            "92     \t [0.61692084 0.26266657 0.40584368]. \t  0.41060339066604384 \t 3.8051821097914003\n",
            "93     \t [0.38132105 0.65477671 0.33897365]. \t  0.41884361706401735 \t 3.8051821097914003\n",
            "94     \t [0.90274566 0.39309968 0.42848717]. \t  0.1538058248859996 \t 3.8051821097914003\n",
            "95     \t [0.3874207  0.91625586 0.45246377]. \t  1.7139535209488614 \t 3.8051821097914003\n",
            "96     \t [0.9037247  0.67953797 0.64761112]. \t  1.108383505018141 \t 3.8051821097914003\n",
            "97     \t [0.2129587  0.5704972  0.34719778]. \t  0.42129348276748696 \t 3.8051821097914003\n",
            "98     \t [0.5736689  0.22695589 0.48933553]. \t  0.27844968171053286 \t 3.8051821097914003\n",
            "99     \t [0.27718184 0.48726663 0.59624794]. \t  1.2942319378416134 \t 3.8051821097914003\n",
            "100    \t [0.77226186 0.20972114 0.79326813]. \t  1.2850883206415245 \t 3.8051821097914003\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GH76hlE2tddm",
        "outputId": "cc110863-289c-4e81-baa0-bab9f573bb17"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 5\r\n",
        "\r\n",
        "np.random.seed(run_num_5)\r\n",
        "surrogate_loser_5 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "loser_5 = dGPGO(surrogate_loser_5, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_5.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.55735327 0.54176063 0.56042378]. \t  0.9599610140567721 \t 0.9810564697651996\n",
            "init   \t [0.38204809 0.11909418 0.84599912]. \t  0.6913364116577386 \t 0.9810564697651996\n",
            "init   \t [0.73974161 0.93235756 0.85932801]. \t  0.9810564697651996 \t 0.9810564697651996\n",
            "init   \t [0.70810463 0.68096875 0.4086023 ]. \t  0.4017443293787304 \t 0.9810564697651996\n",
            "init   \t [0.77458462 0.9844762  0.96548486]. \t  0.4769494155906823 \t 0.9810564697651996\n",
            "1      \t [0.0655006  0.90661895 0.81035589]. \t  \u001b[92m1.403888408720423\u001b[0m \t 1.403888408720423\n",
            "2      \t [0.41215072 0.7779011  0.74763494]. \t  \u001b[92m2.1757126346478852\u001b[0m \t 2.1757126346478852\n",
            "3      \t [0.34717117 0.77095612 0.78484957]. \t  \u001b[92m2.3966577054831983\u001b[0m \t 2.3966577054831983\n",
            "4      \t [0.30370554 0.5420168  0.98229967]. \t  \u001b[92m2.396700843210034\u001b[0m \t 2.396700843210034\n",
            "5      \t [0.03830972 0.41319886 0.87948681]. \t  \u001b[92m3.0868801903159286\u001b[0m \t 3.0868801903159286\n",
            "6      \t [0.0550351  0.47493583 0.91186515]. \t  \u001b[92m3.247821453024615\u001b[0m \t 3.247821453024615\n",
            "7      \t [0.13007588 0.76889517 0.96291702]. \t  1.8354815792825336 \t 3.247821453024615\n",
            "8      \t [0.06020638 0.52263597 0.83656287]. \t  \u001b[92m3.772434542681439\u001b[0m \t 3.772434542681439\n",
            "9      \t [0.05709412 0.58527165 0.69772483]. \t  2.5312887636385204 \t 3.772434542681439\n",
            "10     \t [0.91025839 0.97028742 0.12792309]. \t  0.0018457322577184834 \t 3.772434542681439\n",
            "11     \t [0.99573914 0.49153797 0.75591514]. \t  2.7894953486682232 \t 3.772434542681439\n",
            "12     \t [0.10059082 0.55459117 0.835178  ]. \t  \u001b[92m3.8123132323574875\u001b[0m \t 3.8123132323574875\n",
            "13     \t [0.96087615 0.67807239 0.2789962 ]. \t  0.03903754399449888 \t 3.8123132323574875\n",
            "14     \t [0.85683623 0.07037409 0.99016484]. \t  0.20334698494229897 \t 3.8123132323574875\n",
            "15     \t [0.97447876 0.84968796 0.99975702]. \t  0.9204545784891927 \t 3.8123132323574875\n",
            "16     \t [0.61918291 0.63706631 0.36531518]. \t  0.31507380690837034 \t 3.8123132323574875\n",
            "17     \t [0.60892365 0.82768428 0.00560017]. \t  0.0008542513403068642 \t 3.8123132323574875\n",
            "18     \t [0.88794259 0.11985922 0.05976312]. \t  0.12240316278904899 \t 3.8123132323574875\n",
            "19     \t [0.18565789 0.41975985 0.79881607]. \t  3.104257557926961 \t 3.8123132323574875\n",
            "20     \t [0.54787183 0.252763   0.54415079]. \t  0.33307288848698713 \t 3.8123132323574875\n",
            "21     \t [0.20787217 0.61004851 0.74845814]. \t  3.0360388299352037 \t 3.8123132323574875\n",
            "22     \t [0.98303924 0.28190876 0.89179177]. \t  1.7480026944637124 \t 3.8123132323574875\n",
            "23     \t [0.61780318 0.02581508 0.05799669]. \t  0.20530916988886677 \t 3.8123132323574875\n",
            "24     \t [0.65433974 0.02262776 0.44688952]. \t  0.28211827697204606 \t 3.8123132323574875\n",
            "25     \t [0.02006712 0.54361265 0.85674407]. \t  3.8106114172417715 \t 3.8123132323574875\n",
            "26     \t [0.04408369 0.57798277 0.84008442]. \t  3.792111144405652 \t 3.8123132323574875\n",
            "27     \t [0.14501681 0.47075112 0.91461001]. \t  3.2110042352024024 \t 3.8123132323574875\n",
            "28     \t [0.03558969 0.66343753 0.93451965]. \t  2.8921565718998465 \t 3.8123132323574875\n",
            "29     \t [0.01404798 0.87419782 0.65725577]. \t  2.4792575908001058 \t 3.8123132323574875\n",
            "30     \t [0.18633038 0.99262283 0.31785779]. \t  0.4753520265355485 \t 3.8123132323574875\n",
            "31     \t [0.89852877 0.01203974 0.63453993]. \t  0.14481656909288962 \t 3.8123132323574875\n",
            "32     \t [0.97150247 0.69294377 0.22199652]. \t  0.019920135017441068 \t 3.8123132323574875\n",
            "33     \t [0.46085152 0.56645311 0.57553687]. \t  1.2863944466686195 \t 3.8123132323574875\n",
            "34     \t [0.82964158 0.57980175 0.14424181]. \t  0.041147105173632396 \t 3.8123132323574875\n",
            "35     \t [0.27448468 0.17200083 0.38396597]. \t  0.6423027740941983 \t 3.8123132323574875\n",
            "36     \t [0.26566642 0.00681334 0.42585266]. \t  0.40953511308472856 \t 3.8123132323574875\n",
            "37     \t [0.06013269 0.35026807 0.49586356]. \t  0.3954287956437817 \t 3.8123132323574875\n",
            "38     \t [0.65855735 0.7339546  0.17842301]. \t  0.027927969983747716 \t 3.8123132323574875\n",
            "39     \t [0.75833171 0.253738   0.0292391 ]. \t  0.09612694912065668 \t 3.8123132323574875\n",
            "40     \t [0.56494099 0.01288046 0.30065057]. \t  0.7736377060560666 \t 3.8123132323574875\n",
            "41     \t [0.44126691 0.35921091 0.76431648]. \t  2.3882885446221644 \t 3.8123132323574875\n",
            "42     \t [0.09772268 0.39730314 0.0571514 ]. \t  0.09736614631874568 \t 3.8123132323574875\n",
            "43     \t [0.04390447 0.99697316 0.73397787]. \t  1.2650016562313509 \t 3.8123132323574875\n",
            "44     \t [0.99299479 0.54594283 0.06288426]. \t  0.014164106367212341 \t 3.8123132323574875\n",
            "45     \t [0.83645947 0.11483328 0.79232853]. \t  0.671516469465977 \t 3.8123132323574875\n",
            "46     \t [0.25559256 0.4677096  0.20778315]. \t  0.26763763133381235 \t 3.8123132323574875\n",
            "47     \t [0.52134778 0.64412547 0.66476584]. \t  1.9387526401965522 \t 3.8123132323574875\n",
            "48     \t [0.97960237 0.58509312 0.39486473]. \t  0.0980438150330066 \t 3.8123132323574875\n",
            "49     \t [0.93425858 0.61751435 0.93636713]. \t  3.0007595366600857 \t 3.8123132323574875\n",
            "50     \t [0.93665032 0.83303117 0.3717566 ]. \t  0.14220193723144822 \t 3.8123132323574875\n",
            "51     \t [0.01043746 0.06645344 0.0582733 ]. \t  0.17874671309400159 \t 3.8123132323574875\n",
            "52     \t [0.42146344 0.43093162 0.21313343]. \t  0.34858766310202843 \t 3.8123132323574875\n",
            "53     \t [0.85740045 0.72358599 0.10595874]. \t  0.0067127424362161785 \t 3.8123132323574875\n",
            "54     \t [0.74536496 0.74030997 0.81142443]. \t  2.5236996121014146 \t 3.8123132323574875\n",
            "55     \t [0.48596649 0.05583247 0.71267398]. \t  0.34836328475177203 \t 3.8123132323574875\n",
            "56     \t [0.27943898 0.74737658 0.31328012]. \t  0.42639692017161174 \t 3.8123132323574875\n",
            "57     \t [0.7121061  0.70213367 0.23237757]. \t  0.05544100754439492 \t 3.8123132323574875\n",
            "58     \t [0.89629735 0.50531801 0.27715699]. \t  0.10830442188564005 \t 3.8123132323574875\n",
            "59     \t [0.81710177 0.67511305 0.7989285 ]. \t  2.909475046834837 \t 3.8123132323574875\n",
            "60     \t [0.27827065 0.70878759 0.22002966]. \t  0.10050040713970884 \t 3.8123132323574875\n",
            "61     \t [0.12217544 0.78496978 0.79026058]. \t  2.37095832378724 \t 3.8123132323574875\n",
            "62     \t [0.65576377 0.4536732  0.46632376]. \t  0.32528800321453955 \t 3.8123132323574875\n",
            "63     \t [0.78302288 0.13617894 0.70367363]. \t  0.5995147367720821 \t 3.8123132323574875\n",
            "64     \t [0.27565291 0.35679019 0.68211396]. \t  1.5721358806728192 \t 3.8123132323574875\n",
            "65     \t [0.50685866 0.05015652 0.57968208]. \t  0.1581913686140206 \t 3.8123132323574875\n",
            "66     \t [0.21613274 0.78748947 0.61979404]. \t  2.752520587780271 \t 3.8123132323574875\n",
            "67     \t [0.97683169 0.98537602 0.47034241]. \t  0.22879582575819743 \t 3.8123132323574875\n",
            "68     \t [0.8247913  0.00865746 0.77607339]. \t  0.2671791607087722 \t 3.8123132323574875\n",
            "69     \t [0.6154278  0.16102298 0.72380069]. \t  0.7868765266505973 \t 3.8123132323574875\n",
            "70     \t [0.44639332 0.52624246 0.94752527]. \t  2.9776268007255116 \t 3.8123132323574875\n",
            "71     \t [0.35265186 0.07195568 0.39129923]. \t  0.622943735926077 \t 3.8123132323574875\n",
            "72     \t [0.01915566 0.7514324  0.80350619]. \t  2.641516303394991 \t 3.8123132323574875\n",
            "73     \t [0.31166647 0.75193678 0.43966625]. \t  1.5656143444144268 \t 3.8123132323574875\n",
            "74     \t [0.057032   0.85634764 0.70514785]. \t  2.17220634986867 \t 3.8123132323574875\n",
            "75     \t [0.17449207 0.38170487 0.29543257]. \t  0.4687919094460895 \t 3.8123132323574875\n",
            "76     \t [0.18262366 0.40234101 0.78174162]. \t  2.853310454318601 \t 3.8123132323574875\n",
            "77     \t [0.23326724 0.12923415 0.21844802]. \t  0.8800126024126476 \t 3.8123132323574875\n",
            "78     \t [0.76291281 0.1939433  0.15790554]. \t  0.4132222812659086 \t 3.8123132323574875\n",
            "79     \t [0.09675512 0.75527026 0.79408307]. \t  2.6065132144451044 \t 3.8123132323574875\n",
            "80     \t [0.32685191 0.0862388  0.628197  ]. \t  0.2658594374175428 \t 3.8123132323574875\n",
            "81     \t [0.26988867 0.88735757 0.70824977]. \t  1.9302015782523108 \t 3.8123132323574875\n",
            "82     \t [0.84299487 0.1543878  0.18070156]. \t  0.4012743622864063 \t 3.8123132323574875\n",
            "83     \t [0.41130472 0.07171761 0.33969365]. \t  0.8345360731124587 \t 3.8123132323574875\n",
            "84     \t [7.35487395e-04 7.71143033e-01 7.78163031e-01]. \t  2.4185376828203515 \t 3.8123132323574875\n",
            "85     \t [0.27421342 0.3677449  0.62104297]. \t  1.042391069581968 \t 3.8123132323574875\n",
            "86     \t [0.02541217 0.25237601 0.45267658]. \t  0.29580436684080935 \t 3.8123132323574875\n",
            "87     \t [0.44317316 0.12260709 0.52398733]. \t  0.22575510537096177 \t 3.8123132323574875\n",
            "88     \t [0.57293859 0.99880182 0.4018766 ]. \t  0.6680010837087829 \t 3.8123132323574875\n",
            "89     \t [0.8022128  0.10624647 0.516911  ]. \t  0.15447209300219936 \t 3.8123132323574875\n",
            "90     \t [0.29358989 0.02299719 0.73290056]. \t  0.2828304241174419 \t 3.8123132323574875\n",
            "91     \t [0.54505805 0.52661682 0.47490715]. \t  0.5606793615622767 \t 3.8123132323574875\n",
            "92     \t [0.77323979 0.16021315 0.81059028]. \t  0.9486529552671926 \t 3.8123132323574875\n",
            "93     \t [0.45302664 0.21908675 0.15855595]. \t  0.618922530291222 \t 3.8123132323574875\n",
            "94     \t [0.82084058 0.40124898 0.96684758]. \t  2.0325833187159366 \t 3.8123132323574875\n",
            "95     \t [0.09455294 0.52870413 0.13348204]. \t  0.0900712093873404 \t 3.8123132323574875\n",
            "96     \t [0.44929156 0.89821783 0.27772   ]. \t  0.21315354397159983 \t 3.8123132323574875\n",
            "97     \t [0.28590341 0.55800049 0.51488452]. \t  1.1719963153531516 \t 3.8123132323574875\n",
            "98     \t [0.11525554 0.82021185 0.11016824]. \t  0.010565660616481393 \t 3.8123132323574875\n",
            "99     \t [0.13037669 0.19765145 0.80752148]. \t  1.2195795915290009 \t 3.8123132323574875\n",
            "100    \t [0.35073261 0.92595667 0.04860803]. \t  0.0014692202765782152 \t 3.8123132323574875\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fh_Amb8TtdgO",
        "outputId": "698c5a91-8405-4180-d4c4-a469aff308dc"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 6\r\n",
        "\r\n",
        "np.random.seed(run_num_6)\r\n",
        "surrogate_loser_6 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "loser_6 = dGPGO(surrogate_loser_6, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_6.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.89286015 0.33197981 0.82122912]. \t  2.3879127825536575 \t 2.5106636917702634\n",
            "init   \t [0.04169663 0.10765668 0.59505206]. \t  0.2321934630074273 \t 2.5106636917702634\n",
            "init   \t [0.52981736 0.41880743 0.33540785]. \t  0.37989557867087576 \t 2.5106636917702634\n",
            "init   \t [0.62251943 0.43814143 0.73588211]. \t  2.5106636917702634 \t 2.5106636917702634\n",
            "init   \t [0.51803641 0.5788586  0.6453551 ]. \t  1.7289203948034264 \t 2.5106636917702634\n",
            "1      \t [0.62339102 0.39276721 0.97743955]. \t  1.8557550530508597 \t 2.5106636917702634\n",
            "2      \t [0.81081535 0.27720993 0.64760969]. \t  0.8549986775341755 \t 2.5106636917702634\n",
            "3      \t [0.97311854 0.60304551 0.92068359]. \t  \u001b[92m3.2276797271364055\u001b[0m \t 3.2276797271364055\n",
            "4      \t [0.89264006 0.58864963 0.95652948]. \t  2.776092518207796 \t 3.2276797271364055\n",
            "5      \t [0.96507948 0.60085005 0.82529255]. \t  \u001b[92m3.479322232477214\u001b[0m \t 3.479322232477214\n",
            "6      \t [0.95842992 0.48714277 0.81520669]. \t  3.4274222523101296 \t 3.479322232477214\n",
            "7      \t [0.95032843 0.60164973 0.75457865]. \t  2.678691870274754 \t 3.479322232477214\n",
            "8      \t [0. 0. 0.]. \t  0.06797411659013229 \t 3.479322232477214\n",
            "9      \t [0.01270824 0.5371693  0.06605772]. \t  0.0354315091022392 \t 3.479322232477214\n",
            "10     \t [0.20140086 0.25986668 0.00643016]. \t  0.09731674626066539 \t 3.479322232477214\n",
            "11     \t [0.9954704  0.56038921 0.8728146 ]. \t  \u001b[92m3.6483342701470103\u001b[0m \t 3.6483342701470103\n",
            "12     \t [0.08776956 0.94212277 0.01891421]. \t  0.0006566083725651407 \t 3.6483342701470103\n",
            "13     \t [0.4346958  0.60889655 0.8201429 ]. \t  3.631042399892129 \t 3.6483342701470103\n",
            "14     \t [0.01161386 0.66515041 0.99551219]. \t  1.9498416196162505 \t 3.6483342701470103\n",
            "15     \t [0.33254032 0.89414231 0.98713051]. \t  0.8152211979376042 \t 3.6483342701470103\n",
            "16     \t [0.22420658 0.65264203 0.9182928 ]. \t  3.176475020832948 \t 3.6483342701470103\n",
            "17     \t [0.38995695 0.50459032 0.97495208]. \t  2.4537818811544887 \t 3.6483342701470103\n",
            "18     \t [0.89731428 0.0644819  0.04485822]. \t  0.09540218126323545 \t 3.6483342701470103\n",
            "19     \t [0.5430287  0.62248852 0.87089541]. \t  3.6481824403125525 \t 3.6483342701470103\n",
            "20     \t [0.02245446 0.71257964 0.70832644]. \t  2.542735102304769 \t 3.6483342701470103\n",
            "21     \t [0.84916036 0.43499921 0.05964426]. \t  0.050000621163526424 \t 3.6483342701470103\n",
            "22     \t [0.72328122 0.78476196 0.91791194]. \t  2.0917540572107587 \t 3.6483342701470103\n",
            "23     \t [0.50671568 0.59326383 0.87247359]. \t  \u001b[92m3.7574239741369033\u001b[0m \t 3.7574239741369033\n",
            "24     \t [0.80771617 0.77253684 0.56420031]. \t  0.8074089229060448 \t 3.7574239741369033\n",
            "25     \t [0.82280098 0.98264258 0.03503644]. \t  0.00023447364217251603 \t 3.7574239741369033\n",
            "26     \t [0.36120819 0.01883246 0.12314037]. \t  0.48675579172091765 \t 3.7574239741369033\n",
            "27     \t [0.61004954 0.97795743 0.5380338 ]. \t  1.2796657235618687 \t 3.7574239741369033\n",
            "28     \t [0.50645476 0.26659896 0.96461242]. \t  1.1510964734131832 \t 3.7574239741369033\n",
            "29     \t [0.00523641 0.84890647 0.76701844]. \t  1.8904386191124445 \t 3.7574239741369033\n",
            "30     \t [0.07455069 0.71285656 0.57962799]. \t  2.5839320858154777 \t 3.7574239741369033\n",
            "31     \t [0.83401196 0.42203083 0.26259397]. \t  0.2125427448756169 \t 3.7574239741369033\n",
            "32     \t [0.00550112 0.79531694 0.3026345 ]. \t  0.4132763468583321 \t 3.7574239741369033\n",
            "33     \t [0.33510451 0.26649245 0.92074688]. \t  1.4902139984299103 \t 3.7574239741369033\n",
            "34     \t [0.69362147 0.34044285 0.45542876]. \t  0.2582487569586175 \t 3.7574239741369033\n",
            "35     \t [0.27408507 0.37107772 0.1495041 ]. \t  0.33823696322386365 \t 3.7574239741369033\n",
            "36     \t [0.14167772 0.6583676  0.23549375]. \t  0.13307732649588877 \t 3.7574239741369033\n",
            "37     \t [0.36784136 0.65945815 0.86312928]. \t  3.4849121667963368 \t 3.7574239741369033\n",
            "38     \t [0.50130668 0.46810664 0.92565854]. \t  3.071990928694588 \t 3.7574239741369033\n",
            "39     \t [0.11506585 0.08818824 0.01786167]. \t  0.1264166391740304 \t 3.7574239741369033\n",
            "40     \t [0.61125555 0.20673819 0.86929837]. \t  1.2342396193127003 \t 3.7574239741369033\n",
            "41     \t [0.11552913 0.937178   0.91319381]. \t  0.9229498004302739 \t 3.7574239741369033\n",
            "42     \t [0.73894287 0.33413814 0.6103939 ]. \t  0.7455446737025775 \t 3.7574239741369033\n",
            "43     \t [0.58721719 0.38510632 0.8374942 ]. \t  2.96017619594543 \t 3.7574239741369033\n",
            "44     \t [0.2924251  0.24847362 0.29458142]. \t  0.8162192034311126 \t 3.7574239741369033\n",
            "45     \t [0.9399308  0.69448975 0.89984114]. \t  2.934326827015603 \t 3.7574239741369033\n",
            "46     \t [0.49109484 0.33504597 0.0306225 ]. \t  0.11075140000922437 \t 3.7574239741369033\n",
            "47     \t [0.95946823 0.76273733 0.74217599]. \t  1.6013158015827202 \t 3.7574239741369033\n",
            "48     \t [0.00768901 0.57624884 0.45984971]. \t  1.006739146423282 \t 3.7574239741369033\n",
            "49     \t [0.7588967  0.6338851  0.46007792]. \t  0.4302286953977035 \t 3.7574239741369033\n",
            "50     \t [0.43681923 0.26771217 0.525546  ]. \t  0.3357713031640264 \t 3.7574239741369033\n",
            "51     \t [0.84650032 0.32968555 0.67756352]. \t  1.3030194973795366 \t 3.7574239741369033\n",
            "52     \t [0.29116669 0.1959375  0.57236862]. \t  0.33835299363392896 \t 3.7574239741369033\n",
            "53     \t [0.49086338 0.2402136  0.66310662]. \t  0.8580888362708491 \t 3.7574239741369033\n",
            "54     \t [0.67174643 0.13687066 0.29850672]. \t  0.7358259286026887 \t 3.7574239741369033\n",
            "55     \t [0.90824711 0.51686926 0.94068471]. \t  2.9773967891772 \t 3.7574239741369033\n",
            "56     \t [0.96313273 0.38871968 0.58126843]. \t  0.56783893931115 \t 3.7574239741369033\n",
            "57     \t [0.47678022 0.58904368 0.60675061]. \t  1.527985140761224 \t 3.7574239741369033\n",
            "58     \t [0.61939634 0.49537322 0.21904615]. \t  0.19585169709126193 \t 3.7574239741369033\n",
            "59     \t [0.89125536 0.52848028 0.37504456]. \t  0.12161102080979641 \t 3.7574239741369033\n",
            "60     \t [0.2492909  0.99861842 0.46834362]. \t  1.9369879716347638 \t 3.7574239741369033\n",
            "61     \t [0.89311458 0.0260461  0.41271687]. \t  0.21862146592906784 \t 3.7574239741369033\n",
            "62     \t [0.75365534 0.64361142 0.56627093]. \t  0.8495271886852758 \t 3.7574239741369033\n",
            "63     \t [0.10353124 0.02964342 0.15018766]. \t  0.4970848309262672 \t 3.7574239741369033\n",
            "64     \t [0.38652703 0.09628612 0.01688402]. \t  0.15160735034832432 \t 3.7574239741369033\n",
            "65     \t [0.90753459 0.45836172 0.5257814 ]. \t  0.3378386372294143 \t 3.7574239741369033\n",
            "66     \t [0.97786691 0.23969996 0.30313812]. \t  0.2737430610896056 \t 3.7574239741369033\n",
            "67     \t [0.2985376  0.45515157 0.72666153]. \t  2.5519283059711437 \t 3.7574239741369033\n",
            "68     \t [0.71962825 0.37640665 0.92594521]. \t  2.3788782644785234 \t 3.7574239741369033\n",
            "69     \t [0.91095124 0.26026786 0.08100662]. \t  0.11910272542970807 \t 3.7574239741369033\n",
            "70     \t [0.2007796  0.40410126 0.04414278]. \t  0.09057280874521183 \t 3.7574239741369033\n",
            "71     \t [0.98066823 0.17618395 0.30887276]. \t  0.29940407941322894 \t 3.7574239741369033\n",
            "72     \t [0.26486167 0.03322341 0.3011884 ]. \t  0.8724455649378886 \t 3.7574239741369033\n",
            "73     \t [0.19341254 0.9115757  0.8193853 ]. \t  1.3477910671526903 \t 3.7574239741369033\n",
            "74     \t [0.85383639 0.303693   0.58484458]. \t  0.4983868149187136 \t 3.7574239741369033\n",
            "75     \t [0.11261631 0.64295778 0.35291107]. \t  0.5654453165628954 \t 3.7574239741369033\n",
            "76     \t [0.25255531 0.58296163 0.5355288 ]. \t  1.4636644975046686 \t 3.7574239741369033\n",
            "77     \t [0.69044902 0.1928956  0.86895197]. \t  1.1234012728940739 \t 3.7574239741369033\n",
            "78     \t [0.06808589 0.44438348 0.31510407]. \t  0.33025696989363273 \t 3.7574239741369033\n",
            "79     \t [0.1535016  0.47575467 0.38377442]. \t  0.4277339308242742 \t 3.7574239741369033\n",
            "80     \t [0.88950651 0.15575891 0.5753877 ]. \t  0.23678865995367532 \t 3.7574239741369033\n",
            "81     \t [0.60563334 0.94996264 0.53201211]. \t  1.357208075848746 \t 3.7574239741369033\n",
            "82     \t [0.03640684 0.90685607 0.13933935]. \t  0.017359262999842685 \t 3.7574239741369033\n",
            "83     \t [0.83622549 0.00479024 0.15238126]. \t  0.30811922728605484 \t 3.7574239741369033\n",
            "84     \t [0.03047936 0.45130896 0.09831493]. \t  0.0994418319705768 \t 3.7574239741369033\n",
            "85     \t [0.56096791 0.42054039 0.53609475]. \t  0.5375442750723753 \t 3.7574239741369033\n",
            "86     \t [0.69089327 0.41434753 0.52576339]. \t  0.41222705943601856 \t 3.7574239741369033\n",
            "87     \t [0.33303682 0.85963986 0.88246611]. \t  1.6270249322213164 \t 3.7574239741369033\n",
            "88     \t [0.76694449 0.44906852 0.62817908]. \t  1.1229951985849844 \t 3.7574239741369033\n",
            "89     \t [0.38506973 0.2254537  0.89158306]. \t  1.3124017684339946 \t 3.7574239741369033\n",
            "90     \t [0.94760925 0.81277586 0.24814905]. \t  0.02384324621445827 \t 3.7574239741369033\n",
            "91     \t [0.22321952 0.85183336 0.71042249]. \t  2.116278041457136 \t 3.7574239741369033\n",
            "92     \t [0.63232264 0.79212972 0.83959709]. \t  2.2158432287888115 \t 3.7574239741369033\n",
            "93     \t [0.89311012 0.27852264 0.10452753]. \t  0.15260228430094222 \t 3.7574239741369033\n",
            "94     \t [0.65225194 0.56959118 0.85237454]. \t  \u001b[92m3.7923072368609834\u001b[0m \t 3.7923072368609834\n",
            "95     \t [0.45988746 0.15648535 0.31912992]. \t  0.889225446032996 \t 3.7923072368609834\n",
            "96     \t [0.91897039 0.23558463 0.23921164]. \t  0.34277234372304527 \t 3.7923072368609834\n",
            "97     \t [0.84404778 0.5475815  0.87225978]. \t  3.714655484967224 \t 3.7923072368609834\n",
            "98     \t [0.22581407 0.36439774 0.63953126]. \t  1.194698309876963 \t 3.7923072368609834\n",
            "99     \t [0.36129501 0.29655172 0.46130896]. \t  0.3620060373190372 \t 3.7923072368609834\n",
            "100    \t [0.21910047 0.55932934 0.67552251]. \t  2.2680994506603955 \t 3.7923072368609834\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XNmoDTuUtdi3",
        "outputId": "7814ffa2-acbe-4e79-e93b-724138aa3ca3"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 7\r\n",
        "\r\n",
        "np.random.seed(run_num_7)\r\n",
        "surrogate_loser_7 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "loser_7 = dGPGO(surrogate_loser_7, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_7.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.15266373 0.30235661 0.06203641]. \t  0.1742288077468587 \t 1.6237282255098657\n",
            "init   \t [0.45986034 0.83525338 0.92699705]. \t  1.6237282255098657 \t 1.6237282255098657\n",
            "init   \t [0.72698898 0.76849622 0.26920507]. \t  0.08405715787489784 \t 1.6237282255098657\n",
            "init   \t [0.64402929 0.09337326 0.07968589]. \t  0.2756481743251193 \t 1.6237282255098657\n",
            "init   \t [0.58961375 0.34334054 0.98887615]. \t  1.4020548914621052 \t 1.6237282255098657\n",
            "1      \t [0.32658881 0.90655956 0.99955954]. \t  0.6793796294845356 \t 1.6237282255098657\n",
            "2      \t [0.97122775 0.93199907 0.87171651]. \t  0.9308657054768225 \t 1.6237282255098657\n",
            "3      \t [0.57909004 0.58214012 0.79783263]. \t  \u001b[92m3.478089696581643\u001b[0m \t 3.478089696581643\n",
            "4      \t [0.49507021 0.39113672 0.52467305]. \t  0.478096743295257 \t 3.478089696581643\n",
            "5      \t [0.96188519 0.49079304 0.94964007]. \t  2.739322132683216 \t 3.478089696581643\n",
            "6      \t [0.90946938 0.60810834 0.78435192]. \t  3.0774531590511494 \t 3.478089696581643\n",
            "7      \t [0.06261914 0.66505939 0.93875608]. \t  2.8328149285258104 \t 3.478089696581643\n",
            "8      \t [0.63975981 0.37291359 0.00880596]. \t  0.05617173979970511 \t 3.478089696581643\n",
            "9      \t [0.76047284 0.60730235 0.87877328]. \t  \u001b[92m3.623712367146717\u001b[0m \t 3.623712367146717\n",
            "10     \t [0.6876877  0.55744025 0.93478593]. \t  3.1829751880829815 \t 3.623712367146717\n",
            "11     \t [0.08676052 0.79807152 0.87850186]. \t  2.2208290897125167 \t 3.623712367146717\n",
            "12     \t [0.28916277 0.04583535 0.00261902]. \t  0.11401832354434507 \t 3.623712367146717\n",
            "13     \t [0.71667679 0.57664501 0.81395165]. \t  3.5779368756411625 \t 3.623712367146717\n",
            "14     \t [0.11508561 0.71452061 0.02534668]. \t  0.004527603857494236 \t 3.623712367146717\n",
            "15     \t [0.7138761  0.68377075 0.77379694]. \t  2.655945564466352 \t 3.623712367146717\n",
            "16     \t [0.62595126 0.94264969 0.00840659]. \t  0.00028614683822522104 \t 3.623712367146717\n",
            "17     \t [0.99588291 0.81961183 0.15742965]. \t  0.003958213795357282 \t 3.623712367146717\n",
            "18     \t [0. 0. 0.]. \t  0.06797411659013229 \t 3.623712367146717\n",
            "19     \t [0.87761517 0.03556698 0.76435623]. \t  0.3351948214917791 \t 3.623712367146717\n",
            "20     \t [0.97857549 0.00446178 0.16841397]. \t  0.2154343354374641 \t 3.623712367146717\n",
            "21     \t [0.81221252 0.525542   0.85463545]. \t  \u001b[92m3.7301762932112674\u001b[0m \t 3.7301762932112674\n",
            "22     \t [0.900185   0.75296151 0.1168608 ]. \t  0.005072749512409333 \t 3.7301762932112674\n",
            "23     \t [0.81353052 0.51280105 0.82744971]. \t  3.6314788950239585 \t 3.7301762932112674\n",
            "24     \t [0.96913426 0.69361638 0.55129864]. \t  0.45446883683488787 \t 3.7301762932112674\n",
            "25     \t [0.45611335 0.03406024 0.61887374]. \t  0.1704663937816772 \t 3.7301762932112674\n",
            "26     \t [0.59223331 0.04267828 0.1338271 ]. \t  0.47744590290110217 \t 3.7301762932112674\n",
            "27     \t [0.83943545 0.77914392 0.08044774]. \t  0.0029026804888624066 \t 3.7301762932112674\n",
            "28     \t [0.20846012 0.53348377 0.51992205]. \t  1.1202841961222842 \t 3.7301762932112674\n",
            "29     \t [0.67681719 0.52661603 0.85870288]. \t  \u001b[92m3.7745252416454917\u001b[0m \t 3.7745252416454917\n",
            "30     \t [0.12509163 0.87757482 0.5510732 ]. \t  3.0681829219005508 \t 3.7745252416454917\n",
            "31     \t [0.14369607 0.95138149 0.31002503]. \t  0.4675814613706676 \t 3.7745252416454917\n",
            "32     \t [0.03528407 0.40941311 0.95717746]. \t  2.236590588082838 \t 3.7745252416454917\n",
            "33     \t [0.44280206 0.63711349 0.86091196]. \t  3.611710810406915 \t 3.7745252416454917\n",
            "34     \t [0.26180585 0.73568205 0.73537597]. \t  2.5128286579035257 \t 3.7745252416454917\n",
            "35     \t [0.00487776 0.79900419 0.58464229]. \t  2.8879111571102887 \t 3.7745252416454917\n",
            "36     \t [0.62391776 0.3903563  0.68428808]. \t  1.6717627713433323 \t 3.7745252416454917\n",
            "37     \t [0.22203301 0.58076122 0.99938986]. \t  2.090782773049574 \t 3.7745252416454917\n",
            "38     \t [0.0497901  0.70348162 0.40504456]. \t  1.1607301643572914 \t 3.7745252416454917\n",
            "39     \t [0.08902865 0.97575397 0.57315788]. \t  2.714889081910986 \t 3.7745252416454917\n",
            "40     \t [0.71455558 0.69017883 0.69063379]. \t  1.73744604695138 \t 3.7745252416454917\n",
            "41     \t [0.0498949  0.78122685 0.55605748]. \t  2.879384773660573 \t 3.7745252416454917\n",
            "42     \t [0.45781108 0.26174588 0.94199186]. \t  1.2974802200354303 \t 3.7745252416454917\n",
            "43     \t [0.80913553 0.35598418 0.04574654]. \t  0.07245009830167098 \t 3.7745252416454917\n",
            "44     \t [0.39885319 0.30427766 0.15303229]. \t  0.4753994581941886 \t 3.7745252416454917\n",
            "45     \t [0.83497665 0.25408047 0.74194063]. \t  1.4079264679802885 \t 3.7745252416454917\n",
            "46     \t [0.05275221 0.73612313 0.71259125]. \t  2.516696422081942 \t 3.7745252416454917\n",
            "47     \t [0.03962876 0.04670778 0.97027546]. \t  0.19387205368595167 \t 3.7745252416454917\n",
            "48     \t [0.86384956 0.12288378 0.74114512]. \t  0.6383704123542232 \t 3.7745252416454917\n",
            "49     \t [0.79372708 0.35345592 0.53009712]. \t  0.32798013380319835 \t 3.7745252416454917\n",
            "50     \t [0.97791284 0.38396962 0.41962348]. \t  0.1246402598256812 \t 3.7745252416454917\n",
            "51     \t [0.64270236 0.75777217 0.68904066]. \t  1.6447147713498858 \t 3.7745252416454917\n",
            "52     \t [0.59139013 0.74227825 0.5157312 ]. \t  1.2993767437115655 \t 3.7745252416454917\n",
            "53     \t [0.0323653 0.3292039 0.1426402]. \t  0.2856535148959515 \t 3.7745252416454917\n",
            "54     \t [0.14383193 0.24141099 0.57065719]. \t  0.40695734141747664 \t 3.7745252416454917\n",
            "55     \t [0.89946978 0.08606762 0.85056182]. \t  0.5104618044370799 \t 3.7745252416454917\n",
            "56     \t [0.51976017 0.22889251 0.35232833]. \t  0.6751039301685013 \t 3.7745252416454917\n",
            "57     \t [0.34184268 0.52468794 0.68221237]. \t  2.1899374017129496 \t 3.7745252416454917\n",
            "58     \t [0.8893635  0.6882076  0.46426269]. \t  0.3181762541715076 \t 3.7745252416454917\n",
            "59     \t [0.24686786 0.27282795 0.58364829]. \t  0.5249811572852917 \t 3.7745252416454917\n",
            "60     \t [0.70830618 0.04660946 0.32106337]. \t  0.6183033089960998 \t 3.7745252416454917\n",
            "61     \t [0.65447025 0.27748361 0.42481929]. \t  0.33403303258545897 \t 3.7745252416454917\n",
            "62     \t [0.37683677 0.11525415 0.83630861]. \t  0.6800501825362579 \t 3.7745252416454917\n",
            "63     \t [0.6491765  0.27384865 0.4162992 ]. \t  0.3572171488569962 \t 3.7745252416454917\n",
            "64     \t [0.56966596 0.46622878 0.01058221]. \t  0.036280012296470564 \t 3.7745252416454917\n",
            "65     \t [0.45698753 0.27685337 0.04188856]. \t  0.16480651689762704 \t 3.7745252416454917\n",
            "66     \t [0.78088958 0.25464311 0.24869187]. \t  0.49327180578225444 \t 3.7745252416454917\n",
            "67     \t [0.61253894 0.36137669 0.63838783]. \t  1.0866471780065128 \t 3.7745252416454917\n",
            "68     \t [0.73972948 0.36316492 0.22201953]. \t  0.3420994600975986 \t 3.7745252416454917\n",
            "69     \t [0.598202   0.82681672 0.4747132 ]. \t  1.208649785030718 \t 3.7745252416454917\n",
            "70     \t [0.46091194 0.43140288 0.24909247]. \t  0.377258888183895 \t 3.7745252416454917\n",
            "71     \t [0.30980585 0.49272807 0.36154795]. \t  0.3955171481806316 \t 3.7745252416454917\n",
            "72     \t [0.96726563 0.89399728 0.96826019]. \t  0.8901370600589371 \t 3.7745252416454917\n",
            "73     \t [0.94643135 0.6245116  0.67643341]. \t  1.505106316787355 \t 3.7745252416454917\n",
            "74     \t [0.60589302 0.39114068 0.79202229]. \t  2.8228352062288518 \t 3.7745252416454917\n",
            "75     \t [0.82920723 0.53231563 0.33423074]. \t  0.13146137802733474 \t 3.7745252416454917\n",
            "76     \t [0.40343961 0.54806134 0.28058575]. \t  0.23948447099976833 \t 3.7745252416454917\n",
            "77     \t [0.69097311 0.11912294 0.61050187]. \t  0.27857093327605525 \t 3.7745252416454917\n",
            "78     \t [0.25755065 0.94546291 0.61372001]. \t  2.5138857218938946 \t 3.7745252416454917\n",
            "79     \t [0.09052689 0.51850689 0.63509928]. \t  1.7795558465731527 \t 3.7745252416454917\n",
            "80     \t [0.08479463 0.43788564 0.85789907]. \t  3.3742250385770323 \t 3.7745252416454917\n",
            "81     \t [0.70973134 0.52561147 0.93043662]. \t  3.200528830327633 \t 3.7745252416454917\n",
            "82     \t [0.07887098 0.62444953 0.75322688]. \t  3.047981909596093 \t 3.7745252416454917\n",
            "83     \t [0.43462338 0.85841275 0.40296966]. \t  1.097896948788411 \t 3.7745252416454917\n",
            "84     \t [0.07294121 0.01268299 0.75749395]. \t  0.27074762988187523 \t 3.7745252416454917\n",
            "85     \t [0.44020526 0.37645287 0.00968987]. \t  0.06863582851943462 \t 3.7745252416454917\n",
            "86     \t [0.96569043 0.0641876  0.43360957]. \t  0.15547161313291824 \t 3.7745252416454917\n",
            "87     \t [0.9198246  0.75940701 0.3705377 ]. \t  0.14052092081423212 \t 3.7745252416454917\n",
            "88     \t [0.22721022 0.26302799 0.52202655]. \t  0.3376790780837995 \t 3.7745252416454917\n",
            "89     \t [0.49843932 0.33475206 0.39853827]. \t  0.4193223537377658 \t 3.7745252416454917\n",
            "90     \t [0.88988416 0.29059567 0.72660669]. \t  1.5339975868716995 \t 3.7745252416454917\n",
            "91     \t [0.72225574 0.33315413 0.03442952]. \t  0.08471335964138589 \t 3.7745252416454917\n",
            "92     \t [0.43095451 0.12771229 0.9532665 ]. \t  0.46890119473138564 \t 3.7745252416454917\n",
            "93     \t [0.89322154 0.53536312 0.32746133]. \t  0.10282724257581978 \t 3.7745252416454917\n",
            "94     \t [0.77737971 0.27780371 0.64434659]. \t  0.8377082090623511 \t 3.7745252416454917\n",
            "95     \t [0.35604936 0.29752458 0.55250137]. \t  0.44763709345367153 \t 3.7745252416454917\n",
            "96     \t [0.30570847 0.99814714 0.78796821]. \t  0.8833022204035772 \t 3.7745252416454917\n",
            "97     \t [0.00304443 0.23869801 0.80155107]. \t  1.5294429905123614 \t 3.7745252416454917\n",
            "98     \t [0.23894336 0.95513749 0.52767956]. \t  2.6336435430210194 \t 3.7745252416454917\n",
            "99     \t [0.96334931 0.53329215 0.96711868]. \t  2.5687415955445196 \t 3.7745252416454917\n",
            "100    \t [0.07733048 0.16211484 0.24032112]. \t  0.7439261134149052 \t 3.7745252416454917\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_21yVprtdlu",
        "outputId": "571a5225-bfb9-41ec-aa00-612a26f7dc6c"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 8\r\n",
        "\r\n",
        "np.random.seed(run_num_8)\r\n",
        "surrogate_loser_8 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "loser_8 = dGPGO(surrogate_loser_8, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_8.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.10949987 0.04221501 0.59969258]. \t  0.15732379460832624 \t 0.8830091449513892\n",
            "init   \t [0.29987071 0.79661178 0.36619613]. \t  0.8830091449513892 \t 0.8830091449513892\n",
            "init   \t [0.88060834 0.29784163 0.32910326]. \t  0.2992873424905509 \t 0.8830091449513892\n",
            "init   \t [0.56871692 0.74392742 0.05385289]. \t  0.0051668955191694075 \t 0.8830091449513892\n",
            "init   \t [0.60207437 0.42931858 0.20547361]. \t  0.2908249205255991 \t 0.8830091449513892\n",
            "1      \t [0.19816249 0.92572195 0.98825083]. \t  0.653164991364588 \t 0.8830091449513892\n",
            "2      \t [0.03762084 0.88738442 0.32955494]. \t  0.6464733859302134 \t 0.8830091449513892\n",
            "3      \t [0.59298739 0.68240117 0.69116296]. \t  \u001b[92m1.9678379589841422\u001b[0m \t 1.9678379589841422\n",
            "4      \t [0.99676486 0.73834616 0.85474135]. \t  \u001b[92m2.622597831543382\u001b[0m \t 2.622597831543382\n",
            "5      \t [0.93909608 0.58044829 0.98333517]. \t  2.313423464876202 \t 2.622597831543382\n",
            "6      \t [0.99228465 0.88181182 0.94936817]. \t  1.067160942216205 \t 2.622597831543382\n",
            "7      \t [0.99710209 0.50230383 0.74747112]. \t  \u001b[92m2.685143360380625\u001b[0m \t 2.685143360380625\n",
            "8      \t [0.9896991  0.65501731 0.64538838]. \t  1.0539030471951414 \t 2.685143360380625\n",
            "9      \t [0.7541667  0.03842059 0.03250692]. \t  0.11522480748951529 \t 2.685143360380625\n",
            "10     \t [0.07190656 0.53540888 0.79364133]. \t  \u001b[92m3.5327551500263175\u001b[0m \t 3.5327551500263175\n",
            "11     \t [0.05636162 0.68631151 0.69982546]. \t  2.571862442260311 \t 3.5327551500263175\n",
            "12     \t [0.01897474 0.65584092 0.8751561 ]. \t  3.4459419056992657 \t 3.5327551500263175\n",
            "13     \t [0.2026131  0.90973368 0.02599246]. \t  0.0009568874318973229 \t 3.5327551500263175\n",
            "14     \t [0.02015943 0.17546544 0.86007435]. \t  1.0094731274153572 \t 3.5327551500263175\n",
            "15     \t [0.19305313 0.4204086  0.91237006]. \t  2.90434651604119 \t 3.5327551500263175\n",
            "16     \t [0.01195417 0.52698999 0.94314404]. \t  3.0097456026265337 \t 3.5327551500263175\n",
            "17     \t [0.25596225 0.39703781 0.71712214]. \t  2.163721458320439 \t 3.5327551500263175\n",
            "18     \t [0.25710802 0.65710627 0.8981127 ]. \t  3.3435551924240294 \t 3.5327551500263175\n",
            "19     \t [0.16048547 0.50023439 0.88621337]. \t  \u001b[92m3.6214125818552776\u001b[0m \t 3.6214125818552776\n",
            "20     \t [0.1447954  0.99892044 0.43056541]. \t  1.609250144807879 \t 3.6214125818552776\n",
            "21     \t [0.11841363 0.58660831 0.83267808]. \t  \u001b[92m3.772577454290076\u001b[0m \t 3.772577454290076\n",
            "22     \t [0.97987785 0.13714279 0.00248463]. \t  0.039646571425264975 \t 3.772577454290076\n",
            "23     \t [0.92705413 0.12267879 0.87792272]. \t  0.6413473884501208 \t 3.772577454290076\n",
            "24     \t [0.54587708 0.31433642 0.57322102]. \t  0.5247106459907515 \t 3.772577454290076\n",
            "25     \t [0.65244562 0.21314933 0.18035824]. \t  0.5712228373905991 \t 3.772577454290076\n",
            "26     \t [0.04984912 0.5167762  0.42371492]. \t  0.6001261842172103 \t 3.772577454290076\n",
            "27     \t [0.49843812 0.60544155 0.73993733]. \t  2.788419670954484 \t 3.772577454290076\n",
            "28     \t [0.00250769 0.18540452 0.04799546]. \t  0.15072785648040404 \t 3.772577454290076\n",
            "29     \t [0.14683982 0.04030608 0.72601804]. \t  0.3195768296436863 \t 3.772577454290076\n",
            "30     \t [0.6154916  0.22834525 0.33458037]. \t  0.649724837300166 \t 3.772577454290076\n",
            "31     \t [0.94447738 0.03541163 0.59211034]. \t  0.12339061425489205 \t 3.772577454290076\n",
            "32     \t [0.80518828 0.21820939 0.23047591]. \t  0.4900798351740443 \t 3.772577454290076\n",
            "33     \t [0.49180021 0.67289655 0.09640609]. \t  0.020477956936715228 \t 3.772577454290076\n",
            "34     \t [0.18869189 0.24753615 0.03695648]. \t  0.15575914337109642 \t 3.772577454290076\n",
            "35     \t [0.2574242  0.34725895 0.79140007]. \t  2.4829744875783115 \t 3.772577454290076\n",
            "36     \t [0.56853046 0.35320536 0.40936881]. \t  0.3551657529796507 \t 3.772577454290076\n",
            "37     \t [0.45222453 0.34502697 0.26254166]. \t  0.5921939442113907 \t 3.772577454290076\n",
            "38     \t [0.37352425 0.49742226 0.6706283 ]. \t  1.9661671443461928 \t 3.772577454290076\n",
            "39     \t [0.33441288 0.55681958 0.67261602]. \t  2.1628781305061278 \t 3.772577454290076\n",
            "40     \t [0.8265361  0.4549186  0.12056968]. \t  0.08966259473256484 \t 3.772577454290076\n",
            "41     \t [0.01969536 0.35498542 0.88037469]. \t  2.548617770891581 \t 3.772577454290076\n",
            "42     \t [0.37671893 0.37852214 0.4858566 ]. \t  0.41712957272792933 \t 3.772577454290076\n",
            "43     \t [0.94598866 0.17874027 0.78746887]. \t  1.0359074807983142 \t 3.772577454290076\n",
            "44     \t [0.49561461 0.44173046 0.18714477]. \t  0.27897904173413063 \t 3.772577454290076\n",
            "45     \t [0.65245155 0.41173725 0.90895695]. \t  2.858627770566837 \t 3.772577454290076\n",
            "46     \t [0.32817392 0.43930842 0.76576021]. \t  2.93773136755562 \t 3.772577454290076\n",
            "47     \t [0.47310704 0.73225946 0.77967682]. \t  2.567374504345819 \t 3.772577454290076\n",
            "48     \t [0.9492419  0.6437365  0.83270298]. \t  3.3208203778497323 \t 3.772577454290076\n",
            "49     \t [0.37866648 0.48483831 0.11681013]. \t  0.13267064162900638 \t 3.772577454290076\n",
            "50     \t [0.63776864 0.13811317 0.10232408]. \t  0.3542316780422566 \t 3.772577454290076\n",
            "51     \t [0.0226272  0.46039584 0.39306004]. \t  0.39248366552278097 \t 3.772577454290076\n",
            "52     \t [0.74226553 0.68447382 0.74109946]. \t  2.258201061656978 \t 3.772577454290076\n",
            "53     \t [0.73671863 0.51163238 0.43451977]. \t  0.2618360570450024 \t 3.772577454290076\n",
            "54     \t [0.28537774 0.79787034 0.66787496]. \t  2.4079245833904652 \t 3.772577454290076\n",
            "55     \t [0.52755091 0.87057573 0.11727033]. \t  0.007314436154976636 \t 3.772577454290076\n",
            "56     \t [0.29346589 0.65910737 0.11163608]. \t  0.029896622802024427 \t 3.772577454290076\n",
            "57     \t [0.2895177  0.92186079 0.05601142]. \t  0.0019237374875095135 \t 3.772577454290076\n",
            "58     \t [0.88082954 0.71100467 0.07799923]. \t  0.004986407593590679 \t 3.772577454290076\n",
            "59     \t [0.73226542 0.65192332 0.47391306]. \t  0.5463663067102064 \t 3.772577454290076\n",
            "60     \t [0.02333221 0.35878397 0.34093755]. \t  0.38738388979188215 \t 3.772577454290076\n",
            "61     \t [0.98356167 0.53763874 0.63483676]. \t  1.1181476260591983 \t 3.772577454290076\n",
            "62     \t [0.156046   0.59655081 0.86958024]. \t  3.7669941228717163 \t 3.772577454290076\n",
            "63     \t [0.88684696 0.12634136 0.08895455]. \t  0.17206619872067863 \t 3.772577454290076\n",
            "64     \t [0.18218602 0.45036847 0.09175881]. \t  0.1184082809363772 \t 3.772577454290076\n",
            "65     \t [0.34292031 0.68519737 0.09262511]. \t  0.018785249890164495 \t 3.772577454290076\n",
            "66     \t [0.7317639  0.74224622 0.95671144]. \t  2.077065612238471 \t 3.772577454290076\n",
            "67     \t [0.87137829 0.5695713  0.62329453]. \t  1.0607784657356756 \t 3.772577454290076\n",
            "68     \t [0.40862621 0.14201035 0.85863338]. \t  0.8060903991006787 \t 3.772577454290076\n",
            "69     \t [0.16367498 0.61225246 0.02678576]. \t  0.013724982809582762 \t 3.772577454290076\n",
            "70     \t [0.22148134 0.37110448 0.85048554]. \t  2.8364247037422916 \t 3.772577454290076\n",
            "71     \t [0.13958531 0.2856406  0.7196961 ]. \t  1.5031167436047488 \t 3.772577454290076\n",
            "72     \t [0.07848445 0.86996977 0.15270764]. \t  0.02527200462831359 \t 3.772577454290076\n",
            "73     \t [0.58561207 0.76780412 0.07594845]. \t  0.0055932062345625125 \t 3.772577454290076\n",
            "74     \t [0.43456286 0.62971159 0.41523235]. \t  0.7294054153517803 \t 3.772577454290076\n",
            "75     \t [0.70086648 0.12372512 0.64641762]. \t  0.38204950898303597 \t 3.772577454290076\n",
            "76     \t [0.91643672 0.5876769  0.04810105]. \t  0.010587036778547476 \t 3.772577454290076\n",
            "77     \t [0.51723626 0.13900936 0.15596207]. \t  0.6423475702077569 \t 3.772577454290076\n",
            "78     \t [0.08522595 0.01635146 0.36754278]. \t  0.5270746580713294 \t 3.772577454290076\n",
            "79     \t [0.78306988 0.08787704 0.37528958]. \t  0.4211244425314013 \t 3.772577454290076\n",
            "80     \t [0.19974885 0.85121287 0.97113527]. \t  1.1809105868252145 \t 3.772577454290076\n",
            "81     \t [0.90777519 0.74850197 0.25926154]. \t  0.03547903149967979 \t 3.772577454290076\n",
            "82     \t [0.1885716  0.12356481 0.45988309]. \t  0.3315845871223555 \t 3.772577454290076\n",
            "83     \t [0.63074269 0.83889432 0.31506937]. \t  0.23857930741350988 \t 3.772577454290076\n",
            "84     \t [0.7426569  0.81448563 0.09728448]. \t  0.003764932984621751 \t 3.772577454290076\n",
            "85     \t [0.15288894 0.84947162 0.48361687]. \t  2.5748938282286176 \t 3.772577454290076\n",
            "86     \t [0.14127014 0.02953937 0.32421009]. \t  0.7204917512255963 \t 3.772577454290076\n",
            "87     \t [0.89069861 0.32297623 0.05610986]. \t  0.07585405621148908 \t 3.772577454290076\n",
            "88     \t [0.89113566 0.81003517 0.79241111]. \t  1.7044650571138988 \t 3.772577454290076\n",
            "89     \t [0.29976282 0.72376121 0.38408178]. \t  0.9206494684935415 \t 3.772577454290076\n",
            "90     \t [0.21801703 0.69174281 0.33881151]. \t  0.5459252904266572 \t 3.772577454290076\n",
            "91     \t [0.32550266 0.96841791 0.94725396]. \t  0.6263481766596568 \t 3.772577454290076\n",
            "92     \t [0.67032488 0.96302862 0.87442259]. \t  0.7919522796466973 \t 3.772577454290076\n",
            "93     \t [0.16572149 0.0161577  0.12188675]. \t  0.4232172729959182 \t 3.772577454290076\n",
            "94     \t [0.31948346 0.05319824 0.01070769]. \t  0.13222931057489118 \t 3.772577454290076\n",
            "95     \t [0.8957484  0.2366864  0.66129404]. \t  0.7942865476787239 \t 3.772577454290076\n",
            "96     \t [0.6343747  0.42482422 0.42699233]. \t  0.28859725211452913 \t 3.772577454290076\n",
            "97     \t [0.09414835 0.47320232 0.48502237]. \t  0.6944438440132831 \t 3.772577454290076\n",
            "98     \t [0.35682058 0.62212976 0.54162852]. \t  1.5788029312392569 \t 3.772577454290076\n",
            "99     \t [0.48811289 0.59955038 0.18383557]. \t  0.09066010487259671 \t 3.772577454290076\n",
            "100    \t [0.52697289 0.4467923  0.3546839 ]. \t  0.34114377715747707 \t 3.772577454290076\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vkq32q1utdoO",
        "outputId": "6e42d564-25b8-4821-bcce-5403961827f9"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 9\r\n",
        "\r\n",
        "np.random.seed(run_num_9)\r\n",
        "surrogate_loser_9 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "loser_9 = dGPGO(surrogate_loser_9, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_9.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.80342804 0.5275223  0.11911147]. \t  0.05516938924925502 \t 1.6536488994056173\n",
            "init   \t [0.63968144 0.09092526 0.33222568]. \t  0.7039339223433662 \t 1.6536488994056173\n",
            "init   \t [0.42738095 0.55438581 0.62812652]. \t  1.6536488994056173 \t 1.6536488994056173\n",
            "init   \t [0.69739294 0.78994969 0.13189035]. \t  0.009151170523361311 \t 1.6536488994056173\n",
            "init   \t [0.34277045 0.20155961 0.70732423]. \t  0.9342632521680911 \t 1.6536488994056173\n",
            "1      \t [0.21003505 0.96880326 0.94623317]. \t  0.629144655843092 \t 1.6536488994056173\n",
            "2      \t [0.88822247 0.64717505 0.93325184]. \t  \u001b[92m2.936038130897624\u001b[0m \t 2.936038130897624\n",
            "3      \t [0.92309585 0.63136935 0.89713949]. \t  \u001b[92m3.36145188082496\u001b[0m \t 3.36145188082496\n",
            "4      \t [0.97078368 0.37827572 0.60407706]. \t  0.7223897216338022 \t 3.36145188082496\n",
            "5      \t [0. 0. 0.]. \t  0.06797411659013229 \t 3.36145188082496\n",
            "6      \t [1. 1. 1.]. \t  0.31688362070415665 \t 3.36145188082496\n",
            "7      \t [0.00148228 0.77610523 0.75410658]. \t  2.350920832258359 \t 3.36145188082496\n",
            "8      \t [0.94693831 0.61155431 0.92293946]. \t  3.1864363691427835 \t 3.36145188082496\n",
            "9      \t [0.98424851 0.89958172 0.51609175]. \t  0.31770001519740687 \t 3.36145188082496\n",
            "10     \t [0.93586997 0.71443262 0.87120637]. \t  2.8725659805480657 \t 3.36145188082496\n",
            "11     \t [0.77658981 0.35780449 0.82640336]. \t  2.6628596109839844 \t 3.36145188082496\n",
            "12     \t [0.03186592 0.79569369 0.94704453]. \t  1.7844095329589087 \t 3.36145188082496\n",
            "13     \t [0.86200319 0.56979155 0.74442387]. \t  2.6684475637901897 \t 3.36145188082496\n",
            "14     \t [0.41031621 0.0265955  0.03250833]. \t  0.17539991132785313 \t 3.36145188082496\n",
            "15     \t [0.99776157 0.64222621 0.87827287]. \t  \u001b[92m3.368594517628543\u001b[0m \t 3.368594517628543\n",
            "16     \t [0.01878772 0.99728483 0.09035177]. \t  0.004010356586168865 \t 3.368594517628543\n",
            "17     \t [0.97557082 0.95671533 0.08182595]. \t  0.0004614956657524075 \t 3.368594517628543\n",
            "18     \t [0.09961053 0.42373985 0.04458018]. \t  0.07105609479834335 \t 3.368594517628543\n",
            "19     \t [0.99727223 0.54122305 0.93669611]. \t  3.0519998910271773 \t 3.368594517628543\n",
            "20     \t [0.00683062 0.90479811 0.88456725]. \t  1.2373877556091002 \t 3.368594517628543\n",
            "21     \t [0.69338885 0.08627401 0.11065029]. \t  0.3459448233643284 \t 3.368594517628543\n",
            "22     \t [0.18895708 0.29736843 0.64769767]. \t  0.9953405067578573 \t 3.368594517628543\n",
            "23     \t [0.35292396 0.7284181  0.45097298]. \t  1.5102668359383205 \t 3.368594517628543\n",
            "24     \t [0.01964039 0.59122522 0.4011626 ]. \t  0.7098580300055227 \t 3.368594517628543\n",
            "25     \t [0.04125021 0.48258325 0.94515344]. \t  2.837141442408747 \t 3.368594517628543\n",
            "26     \t [0.00398344 0.15635549 0.93625284]. \t  0.6489238761176981 \t 3.368594517628543\n",
            "27     \t [0.21856344 0.9647262  0.58236059]. \t  2.6596853529896296 \t 3.368594517628543\n",
            "28     \t [0.65705179 0.74970073 0.56481824]. \t  1.2542485516850428 \t 3.368594517628543\n",
            "29     \t [0.43860812 0.29570706 0.13229548]. \t  0.41484876224588224 \t 3.368594517628543\n",
            "30     \t [0.81559219 0.47198787 0.91297578]. \t  3.189328460116302 \t 3.368594517628543\n",
            "31     \t [0.17228086 0.74038486 0.67007788]. \t  2.552227852029529 \t 3.368594517628543\n",
            "32     \t [0.82225239 0.45869837 0.27291106]. \t  0.17904623993483504 \t 3.368594517628543\n",
            "33     \t [0.52240711 0.30817363 0.33234368]. \t  0.5886953591025507 \t 3.368594517628543\n",
            "34     \t [0.03322266 0.99462012 0.91350755]. \t  0.5965323152717071 \t 3.368594517628543\n",
            "35     \t [0.48284383 0.60851186 0.30178257]. \t  0.2274639721561767 \t 3.368594517628543\n",
            "36     \t [0.45149601 0.13284041 0.40736616]. \t  0.5555766641190214 \t 3.368594517628543\n",
            "37     \t [0.02936286 0.50790832 0.37360011]. \t  0.40799229236381035 \t 3.368594517628543\n",
            "38     \t [0.21818567 0.06360072 0.46128035]. \t  0.31401638914978247 \t 3.368594517628543\n",
            "39     \t [0.34508795 0.46351135 0.42728305]. \t  0.4658177758162148 \t 3.368594517628543\n",
            "40     \t [0.80047097 0.20977166 0.93964803]. \t  0.9353536138601612 \t 3.368594517628543\n",
            "41     \t [0.56338529 0.40970159 0.87966518]. \t  3.0816785177566506 \t 3.368594517628543\n",
            "42     \t [0.62898453 0.15398445 0.99820052]. \t  0.3995217044582213 \t 3.368594517628543\n",
            "43     \t [0.0953194  0.12889631 0.9974975 ]. \t  0.3258998308602872 \t 3.368594517628543\n",
            "44     \t [0.74579503 0.9702184  0.92701043]. \t  0.6504016745915788 \t 3.368594517628543\n",
            "45     \t [0.82514889 0.47849315 0.0029895 ]. \t  0.01784129005495299 \t 3.368594517628543\n",
            "46     \t [0.9188371  0.24794378 0.49868342]. \t  0.1764476238190525 \t 3.368594517628543\n",
            "47     \t [0.54600719 0.5851521  0.59197347]. \t  1.2946530885733964 \t 3.368594517628543\n",
            "48     \t [0.89827105 0.52168441 0.30821513]. \t  0.10285301763675626 \t 3.368594517628543\n",
            "49     \t [0.81484174 0.21135661 0.82836122]. \t  1.3201867902358642 \t 3.368594517628543\n",
            "50     \t [0.04297924 0.14489111 0.80397184]. \t  0.8487009222388502 \t 3.368594517628543\n",
            "51     \t [0.44974115 0.01155191 0.59013511]. \t  0.12822442104215265 \t 3.368594517628543\n",
            "52     \t [0.56402957 0.15836296 0.81883483]. \t  0.9473434746978434 \t 3.368594517628543\n",
            "53     \t [0.05298445 0.79463477 0.40427988]. \t  1.4275269907996242 \t 3.368594517628543\n",
            "54     \t [0.32126796 0.23252415 0.7501739 ]. \t  1.3344198696765737 \t 3.368594517628543\n",
            "55     \t [0.73366163 0.85545537 0.01165897]. \t  0.0005379679255610535 \t 3.368594517628543\n",
            "56     \t [0.54033511 0.08211717 0.30454888]. \t  0.8685149917997927 \t 3.368594517628543\n",
            "57     \t [0.32280099 0.4509323  0.4857151 ]. \t  0.5797761220824894 \t 3.368594517628543\n",
            "58     \t [0.48645451 0.20290869 0.15632495]. \t  0.6160579007606918 \t 3.368594517628543\n",
            "59     \t [0.88657382 0.0909298  0.49189932]. \t  0.13633706368341955 \t 3.368594517628543\n",
            "60     \t [0.68439215 0.37020731 0.72050967]. \t  1.9833113696201004 \t 3.368594517628543\n",
            "61     \t [0.51141405 0.09418599 0.10047457]. \t  0.4061489572814179 \t 3.368594517628543\n",
            "62     \t [0.31198933 0.70608232 0.40206586]. \t  1.0247202121973575 \t 3.368594517628543\n",
            "63     \t [0.33741312 0.5207626  0.95663166]. \t  2.8153421842141757 \t 3.368594517628543\n",
            "64     \t [0.03344787 0.8869391  0.50351697]. \t  2.7488240138915274 \t 3.368594517628543\n",
            "65     \t [0.82589378 0.70949834 0.42831321]. \t  0.32973691478035394 \t 3.368594517628543\n",
            "66     \t [0.10956208 0.51557668 0.38425112]. \t  0.47166276244020716 \t 3.368594517628543\n",
            "67     \t [0.10601178 0.96121496 0.08958546]. \t  0.004469271519085479 \t 3.368594517628543\n",
            "68     \t [0.84485844 0.91589973 0.97923942]. \t  0.7251132535909458 \t 3.368594517628543\n",
            "69     \t [0.00710543 0.95040531 0.98600257]. \t  0.550192477085139 \t 3.368594517628543\n",
            "70     \t [0.85744293 0.25559295 0.24291441]. \t  0.3969420390316026 \t 3.368594517628543\n",
            "71     \t [0.26519166 0.0887628  0.46017331]. \t  0.3395092315991357 \t 3.368594517628543\n",
            "72     \t [0.90395729 0.3567291  0.37436871]. \t  0.18962674617183883 \t 3.368594517628543\n",
            "73     \t [0.05179127 0.23950252 0.93598409]. \t  1.1625248997381643 \t 3.368594517628543\n",
            "74     \t [0.20215366 0.23546802 0.21831932]. \t  0.74571077099277 \t 3.368594517628543\n",
            "75     \t [0.32964147 0.6739022  0.07081528]. \t  0.015614063513558365 \t 3.368594517628543\n",
            "76     \t [0.09664869 0.29738318 0.92270834]. \t  1.7252518603526286 \t 3.368594517628543\n",
            "77     \t [0.74898839 0.10693057 0.11347135]. \t  0.3184501015130546 \t 3.368594517628543\n",
            "78     \t [0.11971233 0.3297402  0.11591569]. \t  0.26592145385109955 \t 3.368594517628543\n",
            "79     \t [0.68666318 0.69387016 0.76306807]. \t  2.498359242000209 \t 3.368594517628543\n",
            "80     \t [0.21155027 0.908299   0.69610042]. \t  2.0048917301760367 \t 3.368594517628543\n",
            "81     \t [0.72726961 0.84442141 0.54620606]. \t  1.0287107371753577 \t 3.368594517628543\n",
            "82     \t [0.76275115 0.91814487 0.47922723]. \t  0.7010143297199012 \t 3.368594517628543\n",
            "83     \t [0.77735567 0.78843499 0.0312755 ]. \t  0.0014528375337805194 \t 3.368594517628543\n",
            "84     \t [0.75981294 0.37752905 0.47924214]. \t  0.2445766115717162 \t 3.368594517628543\n",
            "85     \t [0.32076759 0.78466335 0.89882487]. \t  2.267498681703352 \t 3.368594517628543\n",
            "86     \t [0.75002834 0.19657723 0.90564032]. \t  1.018964296156073 \t 3.368594517628543\n",
            "87     \t [0.89324058 0.30707588 0.98512393]. \t  1.1942851913691062 \t 3.368594517628543\n",
            "88     \t [0.22373454 0.24845131 0.00914723]. \t  0.10696785384108524 \t 3.368594517628543\n",
            "89     \t [0.80565659 0.50885834 0.7450086 ]. \t  2.7413748713011907 \t 3.368594517628543\n",
            "90     \t [0.48535488 0.48539508 0.44074141]. \t  0.4427910360182567 \t 3.368594517628543\n",
            "91     \t [0.70727185 0.052759   0.75253142]. \t  0.3848868135768166 \t 3.368594517628543\n",
            "92     \t [0.65551647 0.61255679 0.24423964]. \t  0.10058266221322114 \t 3.368594517628543\n",
            "93     \t [0.3117153  0.57133763 0.91604566]. \t  \u001b[92m3.467293904848227\u001b[0m \t 3.467293904848227\n",
            "94     \t [0.97957236 0.53099367 0.29827555]. \t  0.07146247475221883 \t 3.467293904848227\n",
            "95     \t [0.07078675 0.7785934  0.05120101]. \t  0.0037300677157083425 \t 3.467293904848227\n",
            "96     \t [0.61396047 0.44728502 0.34116334]. \t  0.29998891684476914 \t 3.467293904848227\n",
            "97     \t [0.65055637 0.16667546 0.59492917]. \t  0.32597291361489167 \t 3.467293904848227\n",
            "98     \t [0.14447002 0.2820046  0.62544465]. \t  0.7791423355461649 \t 3.467293904848227\n",
            "99     \t [0.42241034 0.37140917 0.48479234]. \t  0.3928200094367406 \t 3.467293904848227\n",
            "100    \t [0.94137119 0.19156    0.80927874]. \t  1.1500040915581455 \t 3.467293904848227\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5Aur-aRtdq-",
        "outputId": "5bed9bb8-c27e-4b42-db64-01d870aceedf"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 10\r\n",
        "\r\n",
        "np.random.seed(run_num_10)\r\n",
        "surrogate_loser_10 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "loser_10 = dGPGO(surrogate_loser_10, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_10.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.65358959 0.11500694 0.95028286]. \t  0.42730315147591735 \t 1.1029187088185965\n",
            "init   \t [0.4821914  0.87247454 0.21233268]. \t  0.06161964400032635 \t 1.1029187088185965\n",
            "init   \t [0.04070962 0.39719446 0.2331322 ]. \t  0.33269334660262956 \t 1.1029187088185965\n",
            "init   \t [0.84174072 0.20708234 0.74246953]. \t  1.1029187088185965 \t 1.1029187088185965\n",
            "init   \t [0.39215413 0.18225652 0.74353941]. \t  0.9779763535009853 \t 1.1029187088185965\n",
            "1      \t [0.86210958 0.05779215 0.34314786]. \t  0.3927532126432764 \t 1.1029187088185965\n",
            "2      \t [0.92518579 0.78050514 0.87881645]. \t  \u001b[92m2.2443644989048672\u001b[0m \t 2.2443644989048672\n",
            "3      \t [0.98338289 0.93392737 0.81033881]. \t  0.8330483590093913 \t 2.2443644989048672\n",
            "4      \t [0.24984974 0.81883701 0.99874938]. \t  1.135383544316759 \t 2.2443644989048672\n",
            "5      \t [0.84241011 0.46743734 0.97175067]. \t  \u001b[92m2.3195199773796715\u001b[0m \t 2.3195199773796715\n",
            "6      \t [0.77577588 0.62900507 0.95536967]. \t  \u001b[92m2.727536856652607\u001b[0m \t 2.727536856652607\n",
            "7      \t [0.65288565 0.62205817 0.86074034]. \t  \u001b[92m3.6353071006528688\u001b[0m \t 3.6353071006528688\n",
            "8      \t [0.66640478 0.61877439 0.58484337]. \t  1.0883761860789831 \t 3.6353071006528688\n",
            "9      \t [0.11836853 0.92421572 0.40077358]. \t  1.4382047023912417 \t 3.6353071006528688\n",
            "10     \t [6.24314263e-15 1.31537357e-14 1.20310445e-14]. \t  0.06797411659014845 \t 3.6353071006528688\n",
            "11     \t [0.91498214 0.04875353 0.00402212]. \t  0.048769558323538735 \t 3.6353071006528688\n",
            "12     \t [0.28501363 0.56069794 0.81105969]. \t  \u001b[92m3.7011399387928163\u001b[0m \t 3.7011399387928163\n",
            "13     \t [0.06191779 0.75578134 0.02041316]. \t  0.0025418667181896612 \t 3.7011399387928163\n",
            "14     \t [0.44724157 0.86154826 0.74155042]. \t  1.6436280199400723 \t 3.7011399387928163\n",
            "15     \t [0.32900204 0.5222541  0.95157111]. \t  2.9035028561512806 \t 3.7011399387928163\n",
            "16     \t [0.20016925 0.69643574 0.79177916]. \t  3.0240737795597763 \t 3.7011399387928163\n",
            "17     \t [0.42243564 0.66380211 0.71533146]. \t  2.4692385729881137 \t 3.7011399387928163\n",
            "18     \t [0.09177594 0.9678026  0.06127449]. \t  0.0020036168886189963 \t 3.7011399387928163\n",
            "19     \t [0.68362641 0.98533124 0.05418554]. \t  0.0006363090741918352 \t 3.7011399387928163\n",
            "20     \t [0.1121814  0.57163451 0.79125054]. \t  3.5216749058979495 \t 3.7011399387928163\n",
            "21     \t [0.5783509  0.49008829 0.81913902]. \t  3.5934784878492065 \t 3.7011399387928163\n",
            "22     \t [0.51884018 0.57083199 0.85833946]. \t  \u001b[92m3.825950075211796\u001b[0m \t 3.825950075211796\n",
            "23     \t [0.0093238  0.61751695 0.70112407]. \t  2.564612796557479 \t 3.825950075211796\n",
            "24     \t [0.0306889  0.97870848 0.73586423]. \t  1.337128891077002 \t 3.825950075211796\n",
            "25     \t [0.27980633 0.46148506 0.1752227 ]. \t  0.2378344066527602 \t 3.825950075211796\n",
            "26     \t [0.32699639 0.02315428 0.06128359]. \t  0.2549683775569132 \t 3.825950075211796\n",
            "27     \t [0.73893811 0.68093479 0.04192289]. \t  0.006243422255177505 \t 3.825950075211796\n",
            "28     \t [0.20139786 0.88767482 0.66395525]. \t  2.388005815326538 \t 3.825950075211796\n",
            "29     \t [0.02551235 0.3423593  0.75184502]. \t  2.1388202357199315 \t 3.825950075211796\n",
            "30     \t [0.6657422  0.53602642 0.87915674]. \t  3.7327606277135947 \t 3.825950075211796\n",
            "31     \t [0.53783974 0.37586609 0.11422795]. \t  0.23297846283288623 \t 3.825950075211796\n",
            "32     \t [0.17733175 0.19713417 0.53632996]. \t  0.27802306505192764 \t 3.825950075211796\n",
            "33     \t [0.03047332 0.0352073  0.14699465]. \t  0.42970513491749057 \t 3.825950075211796\n",
            "34     \t [0.92196866 0.20235519 0.22032218]. \t  0.34779631994302274 \t 3.825950075211796\n",
            "35     \t [0.41268804 0.21677931 0.07668479]. \t  0.3026346220661753 \t 3.825950075211796\n",
            "36     \t [0.28033917 0.9940199  0.44363132]. \t  1.6428151604376122 \t 3.825950075211796\n",
            "37     \t [0.91502671 0.69756974 0.91725515]. \t  2.787194121186799 \t 3.825950075211796\n",
            "38     \t [0.04794957 0.42614372 0.76621089]. \t  2.846087590501406 \t 3.825950075211796\n",
            "39     \t [0.84554329 0.19749171 0.53590731]. \t  0.21072762937007228 \t 3.825950075211796\n",
            "40     \t [0.10248424 0.96645507 0.94996451]. \t  0.6251604128490882 \t 3.825950075211796\n",
            "41     \t [0.54897872 0.42031727 0.66842942]. \t  1.6163134760994067 \t 3.825950075211796\n",
            "42     \t [0.94325126 0.54732528 0.85763938]. \t  3.69811452912248 \t 3.825950075211796\n",
            "43     \t [0.90677211 0.67028093 0.75452163]. \t  2.372125722503001 \t 3.825950075211796\n",
            "44     \t [0.25070052 0.63827166 0.40552541]. \t  0.884706677204184 \t 3.825950075211796\n",
            "45     \t [0.32103194 0.78721674 0.28324319]. \t  0.2781771042452093 \t 3.825950075211796\n",
            "46     \t [0.73990251 0.38721673 0.73946956]. \t  2.273626035083554 \t 3.825950075211796\n",
            "47     \t [0.61276184 0.23705712 0.22676888]. \t  0.6905085975860578 \t 3.825950075211796\n",
            "48     \t [0.79204942 0.18643486 0.93466089]. \t  0.8194979038041548 \t 3.825950075211796\n",
            "49     \t [0.2182822  0.82045885 0.34387207]. \t  0.7485559328161449 \t 3.825950075211796\n",
            "50     \t [0.79434093 0.75456143 0.80972959]. \t  2.3576544924574 \t 3.825950075211796\n",
            "51     \t [0.99685024 0.77141333 0.03923149]. \t  0.0009762900711944165 \t 3.825950075211796\n",
            "52     \t [0.33678804 0.22014992 0.00684788]. \t  0.11712495717363505 \t 3.825950075211796\n",
            "53     \t [0.41028139 0.5719381  0.26311632]. \t  0.19774787863788867 \t 3.825950075211796\n",
            "54     \t [0.60077419 0.77319619 0.97632717]. \t  1.6470965209265724 \t 3.825950075211796\n",
            "55     \t [0.95857662 0.47617844 0.80837472]. \t  3.338517307679115 \t 3.825950075211796\n",
            "56     \t [0.16839819 0.09440242 0.80593045]. \t  0.5819460844192055 \t 3.825950075211796\n",
            "57     \t [0.77126344 0.27787038 0.57095094]. \t  0.40678651464373805 \t 3.825950075211796\n",
            "58     \t [0.23622091 0.68363914 0.77638665]. \t  2.9986286541218723 \t 3.825950075211796\n",
            "59     \t [0.98691583 0.16799015 0.96810489]. \t  0.5602547142275782 \t 3.825950075211796\n",
            "60     \t [0.09323872 0.98788564 0.33246815]. \t  0.5978453646759858 \t 3.825950075211796\n",
            "61     \t [0.20585617 0.57446848 0.03650038]. \t  0.02341783038978431 \t 3.825950075211796\n",
            "62     \t [0.91977899 0.02827927 0.32915799]. \t  0.33213211617093846 \t 3.825950075211796\n",
            "63     \t [0.1744179  0.49781195 0.54077387]. \t  1.051091442156995 \t 3.825950075211796\n",
            "64     \t [0.99869586 0.95597948 0.00719737]. \t  6.747924827195337e-05 \t 3.825950075211796\n",
            "65     \t [0.05923735 0.65355236 0.73897894]. \t  2.847661076986637 \t 3.825950075211796\n",
            "66     \t [0.01269454 0.01430481 0.53274047]. \t  0.11687636895554003 \t 3.825950075211796\n",
            "67     \t [0.83162222 0.16189688 0.6821508 ]. \t  0.6204044967331243 \t 3.825950075211796\n",
            "68     \t [0.10307635 0.98704216 0.62230977]. \t  2.3851373682146537 \t 3.825950075211796\n",
            "69     \t [0.81310371 0.96461204 0.89116056]. \t  0.7398602162679827 \t 3.825950075211796\n",
            "70     \t [0.96392299 0.0732633  0.82991453]. \t  0.4700895298354437 \t 3.825950075211796\n",
            "71     \t [0.72813753 0.72226023 0.02144692]. \t  0.0029895386586784 \t 3.825950075211796\n",
            "72     \t [0.20147301 0.83427847 0.02792526]. \t  0.0016588547472213234 \t 3.825950075211796\n",
            "73     \t [0.96753452 0.40978965 0.46139521]. \t  0.14787896680659463 \t 3.825950075211796\n",
            "74     \t [0.24613548 0.25790695 0.36722679]. \t  0.6088691497095003 \t 3.825950075211796\n",
            "75     \t [0.61869238 0.30960007 0.75805921]. \t  1.9422037888060717 \t 3.825950075211796\n",
            "76     \t [0.06499414 0.21232159 0.37895357]. \t  0.497457472021624 \t 3.825950075211796\n",
            "77     \t [0.03885248 0.33913528 0.45254913]. \t  0.33612032404781683 \t 3.825950075211796\n",
            "78     \t [0.55379685 0.80032251 0.19912658]. \t  0.04278284053876489 \t 3.825950075211796\n",
            "79     \t [0.02021057 0.15528238 0.76939561]. \t  0.8700524306842632 \t 3.825950075211796\n",
            "80     \t [0.322192   0.09348805 0.37668228]. \t  0.6953889615688063 \t 3.825950075211796\n",
            "81     \t [0.09095127 0.93642432 0.61762491]. \t  2.685359651785647 \t 3.825950075211796\n",
            "82     \t [0.10159781 0.36047065 0.96154357]. \t  1.8417153323280768 \t 3.825950075211796\n",
            "83     \t [0.57007758 0.64436134 0.09782204]. \t  0.02497798409309806 \t 3.825950075211796\n",
            "84     \t [0.07585703 0.84169618 0.31840343]. \t  0.5586293380163749 \t 3.825950075211796\n",
            "85     \t [0.26852638 0.32867212 0.25219684]. \t  0.625031988431732 \t 3.825950075211796\n",
            "86     \t [0.03915346 0.47901975 0.39966145]. \t  0.4367926667230772 \t 3.825950075211796\n",
            "87     \t [0.79613901 0.22285573 0.8464027 ]. \t  1.401327231034421 \t 3.825950075211796\n",
            "88     \t [0.78634327 0.08272775 0.40411308]. \t  0.34048301845184237 \t 3.825950075211796\n",
            "89     \t [0.25075283 0.42774475 0.05874369]. \t  0.09926859078580283 \t 3.825950075211796\n",
            "90     \t [0.52020758 0.06642892 0.61019363]. \t  0.20301745333843152 \t 3.825950075211796\n",
            "91     \t [0.04145178 0.9982077  0.04650104]. \t  0.0011636459714390384 \t 3.825950075211796\n",
            "92     \t [0.6463775  0.3034504  0.95923746]. \t  1.4498903641763436 \t 3.825950075211796\n",
            "93     \t [0.21348632 0.41152009 0.08247436]. \t  0.14062703500889415 \t 3.825950075211796\n",
            "94     \t [0.52393133 0.71775999 0.69267821]. \t  2.0244943228113934 \t 3.825950075211796\n",
            "95     \t [0.92048307 0.02856064 0.60107968]. \t  0.12706036817059735 \t 3.825950075211796\n",
            "96     \t [0.55966008 0.00459803 0.75177789]. \t  0.2510841678353941 \t 3.825950075211796\n",
            "97     \t [0.06118166 0.51334582 0.17268385]. \t  0.12985995150060514 \t 3.825950075211796\n",
            "98     \t [0.67207969 0.29327723 0.95465068]. \t  1.416516516636988 \t 3.825950075211796\n",
            "99     \t [0.21093958 0.37233381 0.00276439]. \t  0.05926409704295324 \t 3.825950075211796\n",
            "100    \t [0.06026911 0.76609599 0.98141921]. \t  1.630173690362098 \t 3.825950075211796\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MMu7U9NNtdtn",
        "outputId": "8868a62f-c61b-4624-8f0d-613e2a5a684f"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 11\r\n",
        "\r\n",
        "np.random.seed(run_num_11)\r\n",
        "surrogate_loser_11 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "loser_11 = dGPGO(surrogate_loser_11, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_11.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.98103566 0.82124785 0.64589361]. \t  0.6570240879762975 \t 0.687459437576373\n",
            "init   \t [0.42368801 0.20231098 0.49190677]. \t  0.29901296656621285 \t 0.687459437576373\n",
            "init   \t [0.13855833 0.45252104 0.11373463]. \t  0.1378633538075966 \t 0.687459437576373\n",
            "init   \t [0.00292449 0.52342617 0.18997116]. \t  0.12299763462541902 \t 0.687459437576373\n",
            "init   \t [0.14171979 0.33586637 0.58369219]. \t  0.687459437576373 \t 0.687459437576373\n",
            "1      \t [0.23267662 0.55632866 0.98247629]. \t  \u001b[92m2.401417505244251\u001b[0m \t 2.401417505244251\n",
            "2      \t [0.1018956  0.95528749 0.95450529]. \t  0.6625688909370097 \t 2.401417505244251\n",
            "3      \t [0.49069589 0.32394686 0.99166679]. \t  1.2576491073928158 \t 2.401417505244251\n",
            "4      \t [0.99622896 0.50984357 0.00987988]. \t  0.008998496185923171 \t 2.401417505244251\n",
            "5      \t [0.17147829 0.44852386 0.99855404]. \t  1.8327027793895603 \t 2.401417505244251\n",
            "6      \t [0.52755702 0.61099275 0.96085651]. \t  \u001b[92m2.731594261003499\u001b[0m \t 2.731594261003499\n",
            "7      \t [0.50454698 0.68368804 0.91827122]. \t  \u001b[92m2.973325813164906\u001b[0m \t 2.973325813164906\n",
            "8      \t [0.45418709 0.73924568 0.73077065]. \t  2.2589019118704425 \t 2.973325813164906\n",
            "9      \t [0.53897689 0.95646638 0.88685629]. \t  0.8406578977033827 \t 2.973325813164906\n",
            "10     \t [0.60547932 0.56968479 0.81474854]. \t  \u001b[92m3.6411377654076196\u001b[0m \t 3.6411377654076196\n",
            "11     \t [0.9286639  0.28303617 0.64074614]. \t  0.8065492930206374 \t 3.6411377654076196\n",
            "12     \t [0.6863489  0.52266533 0.67959972]. \t  1.8802784961869148 \t 3.6411377654076196\n",
            "13     \t [0.81516659 0.04230252 0.0144339 ]. \t  0.07642119456660208 \t 3.6411377654076196\n",
            "14     \t [3.56002803e-02 9.17723598e-01 3.28278754e-04]. \t  0.0004253659195000631 \t 3.6411377654076196\n",
            "15     \t [0.93259193 0.59135126 0.8998208 ]. \t  3.4797007688347623 \t 3.6411377654076196\n",
            "16     \t [0.03180589 0.80324037 0.77495056]. \t  2.203713570050467 \t 3.6411377654076196\n",
            "17     \t [0.11708033 0.14671029 0.01423623]. \t  0.11999746361891822 \t 3.6411377654076196\n",
            "18     \t [0.50657018 0.49089223 0.85138264]. \t  \u001b[92m3.707332576217637\u001b[0m \t 3.707332576217637\n",
            "19     \t [0.80737161 0.0155614  0.66176037]. \t  0.18365620244113123 \t 3.707332576217637\n",
            "20     \t [0.37567124 0.48150598 0.85948258]. \t  3.6688440344570115 \t 3.707332576217637\n",
            "21     \t [0.48075247 0.4868558  0.84329412]. \t  3.6882293140094626 \t 3.707332576217637\n",
            "22     \t [0.41981534 0.53971767 0.86767719]. \t  \u001b[92m3.82609236093791\u001b[0m \t 3.82609236093791\n",
            "23     \t [0.02516017 0.58371562 0.30762436]. \t  0.28033234832751935 \t 3.82609236093791\n",
            "24     \t [0.48944405 0.6559092  0.83877525]. \t  3.459362722987617 \t 3.82609236093791\n",
            "25     \t [0.42029516 0.21429986 0.30920755]. \t  0.8618548387131872 \t 3.82609236093791\n",
            "26     \t [0.96711039 0.14576491 0.91452994]. \t  0.6616327147356602 \t 3.82609236093791\n",
            "27     \t [0.38311766 0.57716223 0.84585103]. \t  \u001b[92m3.832003952745011\u001b[0m \t 3.832003952745011\n",
            "28     \t [0.77891632 0.65166351 0.98580516]. \t  2.150894348089524 \t 3.832003952745011\n",
            "29     \t [0.9881304  0.83507513 0.9180588 ]. \t  1.588643416372116 \t 3.832003952745011\n",
            "30     \t [0.7384121  0.63332243 0.15380586]. \t  0.03551552148442237 \t 3.832003952745011\n",
            "31     \t [0.89618434 0.95859692 0.01322118]. \t  0.00011836310433250633 \t 3.832003952745011\n",
            "32     \t [0.3222753  0.04010627 0.03332996]. \t  0.18124540626821045 \t 3.832003952745011\n",
            "33     \t [0.88887881 0.39281112 0.92796296]. \t  2.4565050300574933 \t 3.832003952745011\n",
            "34     \t [0.31470288 0.14953304 0.93760765]. \t  0.6195643860286129 \t 3.832003952745011\n",
            "35     \t [0.75331242 0.27518024 0.74191406]. \t  1.5632474996030026 \t 3.832003952745011\n",
            "36     \t [0.01390206 0.79841187 0.82989979]. \t  2.270327964643763 \t 3.832003952745011\n",
            "37     \t [0.41503592 0.54752483 0.84909805]. \t  \u001b[92m3.8511165079641128\u001b[0m \t 3.8511165079641128\n",
            "38     \t [0.93771621 0.40085258 0.90840629]. \t  2.710212038381022 \t 3.8511165079641128\n",
            "39     \t [0.14965119 0.83004101 0.03678273]. \t  0.0020265420497056937 \t 3.8511165079641128\n",
            "40     \t [0.65825822 0.03070688 0.99081436]. \t  0.13820090658858034 \t 3.8511165079641128\n",
            "41     \t [0.81284622 0.54746547 0.93318464]. \t  3.1695915074968855 \t 3.8511165079641128\n",
            "42     \t [0.06732819 0.25070027 0.34887756]. \t  0.5420588033558534 \t 3.8511165079641128\n",
            "43     \t [0.53629411 0.64714885 0.64894173]. \t  1.7993921357365534 \t 3.8511165079641128\n",
            "44     \t [0.76632994 0.38500206 0.54507039]. \t  0.42354185795531757 \t 3.8511165079641128\n",
            "45     \t [0.86768148 0.71598166 0.27425688]. \t  0.05270532364446077 \t 3.8511165079641128\n",
            "46     \t [0.23880265 0.48264459 0.03340302]. \t  0.04854418552914468 \t 3.8511165079641128\n",
            "47     \t [0.57775579 0.25342925 0.84516579]. \t  1.6930206158719372 \t 3.8511165079641128\n",
            "48     \t [0.64910131 0.22567323 0.61880267]. \t  0.5435876480199389 \t 3.8511165079641128\n",
            "49     \t [0.01604554 0.33670665 0.18461822]. \t  0.3487165227529128 \t 3.8511165079641128\n",
            "50     \t [0.79408129 0.56766075 0.71696715]. \t  2.3064305635288314 \t 3.8511165079641128\n",
            "51     \t [0.7402549  0.3655645  0.54355901]. \t  0.406519646040674 \t 3.8511165079641128\n",
            "52     \t [0.3419478  0.37953654 0.70966463]. \t  1.978436778247392 \t 3.8511165079641128\n",
            "53     \t [0.88811151 0.16272661 0.61333689]. \t  0.3545873365654224 \t 3.8511165079641128\n",
            "54     \t [0.12063825 0.28654585 0.99348593]. \t  1.0168883573826288 \t 3.8511165079641128\n",
            "55     \t [0.60034281 0.33793455 0.11347122]. \t  0.2572321192658406 \t 3.8511165079641128\n",
            "56     \t [0.39737068 0.82353818 0.94749653]. \t  1.5740956107911792 \t 3.8511165079641128\n",
            "57     \t [0.41435179 0.284656   0.94117003]. \t  1.4810304390715325 \t 3.8511165079641128\n",
            "58     \t [0.28734283 0.70631795 0.44272901]. \t  1.4552116958940275 \t 3.8511165079641128\n",
            "59     \t [0.64582064 0.27048736 0.60267387]. \t  0.5685328343977809 \t 3.8511165079641128\n",
            "60     \t [0.00961796 0.99646683 0.43966302]. \t  1.6845678787749219 \t 3.8511165079641128\n",
            "61     \t [0.79067284 0.4889186  0.34277043]. \t  0.1720656681141432 \t 3.8511165079641128\n",
            "62     \t [0.73614964 0.08873081 0.08123764]. \t  0.23429503064823837 \t 3.8511165079641128\n",
            "63     \t [0.4074256  0.8972777  0.15489097]. \t  0.020428079250579424 \t 3.8511165079641128\n",
            "64     \t [0.37505358 0.98290616 0.57097691]. \t  2.1753048118193075 \t 3.8511165079641128\n",
            "65     \t [0.95340121 0.51222226 0.23034747]. \t  0.07641901044959246 \t 3.8511165079641128\n",
            "66     \t [0.66780369 0.70433721 0.20511346]. \t  0.04426141336992313 \t 3.8511165079641128\n",
            "67     \t [0.04098637 0.00125381 0.04984295]. \t  0.1533279300561005 \t 3.8511165079641128\n",
            "68     \t [0.00668649 0.08940948 0.4725365 ]. \t  0.21997486144380612 \t 3.8511165079641128\n",
            "69     \t [0.50161296 0.95814489 0.13143229]. \t  0.008605766789104212 \t 3.8511165079641128\n",
            "70     \t [0.89116531 0.99014832 0.88225675]. \t  0.5995239394899143 \t 3.8511165079641128\n",
            "71     \t [0.52987778 0.82918509 0.07807094]. \t  0.003878824742720693 \t 3.8511165079641128\n",
            "72     \t [0.46976662 0.07514661 0.19917063]. \t  0.8292924932649325 \t 3.8511165079641128\n",
            "73     \t [0.69407614 0.03869317 0.24545797]. \t  0.67522371915888 \t 3.8511165079641128\n",
            "74     \t [0.8785674  0.28201439 0.88658166]. \t  1.7952803792715935 \t 3.8511165079641128\n",
            "75     \t [0.11661158 0.57658081 0.89141671]. \t  3.6830251509630694 \t 3.8511165079641128\n",
            "76     \t [0.59244815 0.46942989 0.82981494]. \t  3.5506139509376005 \t 3.8511165079641128\n",
            "77     \t [0.15000929 0.83777228 0.81145404]. \t  1.947239797927471 \t 3.8511165079641128\n",
            "78     \t [0.03822197 0.16401539 0.05399344]. \t  0.17994921078638834 \t 3.8511165079641128\n",
            "79     \t [0.47399276 0.3863694  0.51186274]. \t  0.4426504618250182 \t 3.8511165079641128\n",
            "80     \t [0.07164216 0.1899446  0.51708782]. \t  0.2455051318790618 \t 3.8511165079641128\n",
            "81     \t [0.38550927 0.95537209 0.6460422 ]. \t  1.8994677417985926 \t 3.8511165079641128\n",
            "82     \t [0.62526033 0.68415627 0.34354801]. \t  0.2776421583690714 \t 3.8511165079641128\n",
            "83     \t [0.26733556 0.17048883 0.76671184]. \t  0.9703752437022648 \t 3.8511165079641128\n",
            "84     \t [0.0319908  0.90880231 0.79468209]. \t  1.423155602228559 \t 3.8511165079641128\n",
            "85     \t [0.89718587 0.84522385 0.93800549]. \t  1.412830789733427 \t 3.8511165079641128\n",
            "86     \t [0.45140418 0.1109365  0.8919736 ]. \t  0.5691786665236267 \t 3.8511165079641128\n",
            "87     \t [0.88238899 0.29590736 0.9282799 ]. \t  1.6420849187800834 \t 3.8511165079641128\n",
            "88     \t [0.51218717 0.00332253 0.90375347]. \t  0.1970479087957312 \t 3.8511165079641128\n",
            "89     \t [0.93373041 0.30008258 0.2868365 ]. \t  0.2738027942787708 \t 3.8511165079641128\n",
            "90     \t [0.82243595 0.81083886 0.19101491]. \t  0.015539666017811682 \t 3.8511165079641128\n",
            "91     \t [0.41487269 0.86355672 0.87969103]. \t  1.5871950857407389 \t 3.8511165079641128\n",
            "92     \t [0.85695397 0.31416628 0.88882694]. \t  2.089645940612437 \t 3.8511165079641128\n",
            "93     \t [0.30654317 0.68735427 0.25532736]. \t  0.1666041551928666 \t 3.8511165079641128\n",
            "94     \t [0.67643792 0.00669018 0.92556499]. \t  0.17923468057010708 \t 3.8511165079641128\n",
            "95     \t [0.41761799 0.16804838 0.19966897]. \t  0.8437008962970893 \t 3.8511165079641128\n",
            "96     \t [0.79041122 0.13903144 0.95899516]. \t  0.4865923987032969 \t 3.8511165079641128\n",
            "97     \t [0.27664727 0.08700359 0.81163344]. \t  0.5498838079004507 \t 3.8511165079641128\n",
            "98     \t [0.29130459 0.70467093 0.66824017]. \t  2.401895065242285 \t 3.8511165079641128\n",
            "99     \t [0.60433714 0.04489796 0.1987617 ]. \t  0.6982724795764844 \t 3.8511165079641128\n",
            "100    \t [0.92270353 0.34908719 0.75873467]. \t  2.179299016401862 \t 3.8511165079641128\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UEjKllHqtdwW",
        "outputId": "c8c42c77-7aa9-438f-ba81-0b95e044d01a"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 12\r\n",
        "\r\n",
        "np.random.seed(run_num_12)\r\n",
        "surrogate_loser_12 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "loser_12 = dGPGO(surrogate_loser_12, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_12.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.19151945 0.62210877 0.43772774]. \t  1.1006281843679786 \t 1.6482992955272024\n",
            "init   \t [0.78535858 0.77997581 0.27259261]. \t  0.07153771680480671 \t 1.6482992955272024\n",
            "init   \t [0.27646426 0.80187218 0.95813935]. \t  1.6482992955272024 \t 1.6482992955272024\n",
            "init   \t [0.87593263 0.35781727 0.50099513]. \t  0.2282556248207173 \t 1.6482992955272024\n",
            "init   \t [0.68346294 0.71270203 0.37025075]. \t  0.33032494760932407 \t 1.6482992955272024\n",
            "1      \t [0.01398415 0.60897714 0.98856638]. \t  \u001b[92m2.228026380269567\u001b[0m \t 2.228026380269567\n",
            "2      \t [0.03494356 0.11089546 0.89536938]. \t  0.5535687532914756 \t 2.228026380269567\n",
            "3      \t [0.00761147 0.89633798 0.99755295]. \t  0.7296905183544732 \t 2.228026380269567\n",
            "4      \t [0.42621529 0.52201143 0.94674436]. \t  \u001b[92m2.981393016278439\u001b[0m \t 2.981393016278439\n",
            "5      \t [0.62997572 0.44202894 0.99618939]. \t  1.8391155649095643 \t 2.981393016278439\n",
            "6      \t [0.32429872 0.48995801 0.97777927]. \t  2.3563074830909687 \t 2.981393016278439\n",
            "7      \t [0.99206005 0.87479422 0.59276177]. \t  0.4186473279209974 \t 2.981393016278439\n",
            "8      \t [0.05476847 0.53285389 0.64325851]. \t  1.8981988576929787 \t 2.981393016278439\n",
            "9      \t [1.36465618e-14 4.78754555e-14 1.19124674e-14]. \t  0.06797411659015497 \t 2.981393016278439\n",
            "10     \t [0.0322103  0.83671651 0.03102825]. \t  0.0015280586580840726 \t 2.981393016278439\n",
            "11     \t [0.7698327  0.69612096 0.89058708]. \t  \u001b[92m3.029485017097235\u001b[0m \t 3.029485017097235\n",
            "12     \t [0.43926768 0.5817004  0.86293835]. \t  \u001b[92m3.8177647120619196\u001b[0m \t 3.8177647120619196\n",
            "13     \t [0.94895416 0.92227183 0.04577195]. \t  0.00027661898739457985 \t 3.8177647120619196\n",
            "14     \t [0.96716075 0.55712453 0.89266817]. \t  3.560435248735423 \t 3.8177647120619196\n",
            "15     \t [0.9867346  0.37982781 0.09809661]. \t  0.06760397508437056 \t 3.8177647120619196\n",
            "16     \t [0.90017763 0.51696429 0.78507178]. \t  3.237173868723097 \t 3.8177647120619196\n",
            "17     \t [0.27326543 0.62896984 0.83586619]. \t  3.6455175745982786 \t 3.8177647120619196\n",
            "18     \t [0.41927477 0.56180083 0.80926929]. \t  3.6659304271249677 \t 3.8177647120619196\n",
            "19     \t [0.35657319 0.69697249 0.11065344]. \t  0.021505444175504602 \t 3.8177647120619196\n",
            "20     \t [0.19309422 0.2903732  0.41339479]. \t  0.4298102396014697 \t 3.8177647120619196\n",
            "21     \t [0.20463233 0.04324562 0.65697062]. \t  0.23108242657891664 \t 3.8177647120619196\n",
            "22     \t [1.57842901e-02 8.20918142e-01 7.03733502e-04]. \t  0.0008601507897846089 \t 3.8177647120619196\n",
            "23     \t [0.60248017 0.3231838  0.54524734]. \t  0.40827501689705414 \t 3.8177647120619196\n",
            "24     \t [0.95934255 0.13483385 0.58978918]. \t  0.23503660187818645 \t 3.8177647120619196\n",
            "25     \t [0.944549   0.01940069 0.0303243 ]. \t  0.062405931694092066 \t 3.8177647120619196\n",
            "26     \t [0.55017482 0.2753227  0.00227453]. \t  0.08574825192552826 \t 3.8177647120619196\n",
            "27     \t [0.6303606  0.91787344 0.54542479]. \t  1.3455171557446375 \t 3.8177647120619196\n",
            "28     \t [0.41436159 0.5956259  0.79046636]. \t  3.439291049398898 \t 3.8177647120619196\n",
            "29     \t [0.43817054 0.11696402 0.3778372 ]. \t  0.6896644481940144 \t 3.8177647120619196\n",
            "30     \t [0.06346045 0.63738689 0.33134359]. \t  0.4292681917320401 \t 3.8177647120619196\n",
            "31     \t [0.02637106 0.98958525 0.7432182 ]. \t  1.225135757871817 \t 3.8177647120619196\n",
            "32     \t [0.82033973 0.79716428 0.08591558]. \t  0.0028307305418085256 \t 3.8177647120619196\n",
            "33     \t [0.45938261 0.75250997 0.30932276]. \t  0.3116825248599445 \t 3.8177647120619196\n",
            "34     \t [0.14857774 0.26493928 0.00777946]. \t  0.09209710617458892 \t 3.8177647120619196\n",
            "35     \t [0.60832238 0.55700038 0.81874419]. \t  3.683568939368378 \t 3.8177647120619196\n",
            "36     \t [0.17079159 0.75188383 0.20204625]. \t  0.07527308098946896 \t 3.8177647120619196\n",
            "37     \t [0.88857253 0.13831501 0.99785855]. \t  0.3461011837569411 \t 3.8177647120619196\n",
            "38     \t [0.43495971 0.82343517 0.56848465]. \t  2.260262249034812 \t 3.8177647120619196\n",
            "39     \t [0.05922251 0.19985131 0.18276069]. \t  0.5656147149471785 \t 3.8177647120619196\n",
            "40     \t [0.17134421 0.86725833 0.08159063]. \t  0.004730173871140647 \t 3.8177647120619196\n",
            "41     \t [0.42662425 0.69689899 0.79793902]. \t  2.9774239054093257 \t 3.8177647120619196\n",
            "42     \t [0.18872382 0.51514048 0.22488052]. \t  0.20745664175205186 \t 3.8177647120619196\n",
            "43     \t [0.06999489 0.27249309 0.77138987]. \t  1.7229732668908813 \t 3.8177647120619196\n",
            "44     \t [0.42434339 0.83062154 0.07494507]. \t  0.004198432986741109 \t 3.8177647120619196\n",
            "45     \t [0.52873134 0.48130385 0.74748806]. \t  2.8460017521193435 \t 3.8177647120619196\n",
            "46     \t [0.85544522 0.01913392 0.08899941]. \t  0.17209892135155425 \t 3.8177647120619196\n",
            "47     \t [0.94509336 0.52652727 0.52765881]. \t  0.355756562588529 \t 3.8177647120619196\n",
            "48     \t [0.21204997 0.52165853 0.10561627]. \t  0.08444321506372225 \t 3.8177647120619196\n",
            "49     \t [0.49512097 0.96958735 0.07743071]. \t  0.0021082109387776248 \t 3.8177647120619196\n",
            "50     \t [0.86690057 0.3390747  0.56690559]. \t  0.4488104603551687 \t 3.8177647120619196\n",
            "51     \t [0.43918792 0.76714066 0.46667066]. \t  1.5678915085471918 \t 3.8177647120619196\n",
            "52     \t [0.98846208 0.47663677 0.92545313]. \t  3.0092802227686923 \t 3.8177647120619196\n",
            "53     \t [0.47038939 0.8521396  0.9974098 ]. \t  0.9628504485489497 \t 3.8177647120619196\n",
            "54     \t [0.77998072 0.34352677 0.73868039]. \t  1.9942376878878827 \t 3.8177647120619196\n",
            "55     \t [0.68813783 0.09659201 0.92520935]. \t  0.4257988157189452 \t 3.8177647120619196\n",
            "56     \t [0.50350165 0.82297981 0.52319174]. \t  1.8476111133622661 \t 3.8177647120619196\n",
            "57     \t [0.70155505 0.48281139 0.71915125]. \t  2.381860570908207 \t 3.8177647120619196\n",
            "58     \t [0.55925288 0.44245054 0.32789276]. \t  0.3357905619753193 \t 3.8177647120619196\n",
            "59     \t [0.49978886 0.66873457 0.25703783]. \t  0.13282998981277314 \t 3.8177647120619196\n",
            "60     \t [0.84105879 0.5629506  0.34627985]. \t  0.1243490147263995 \t 3.8177647120619196\n",
            "61     \t [0.12603402 0.63989199 0.71126356]. \t  2.6777207524288986 \t 3.8177647120619196\n",
            "62     \t [0.39964433 0.99658814 0.45459576]. \t  1.484373314649264 \t 3.8177647120619196\n",
            "63     \t [0.44241317 0.17151992 0.84918487]. \t  1.0160410107130888 \t 3.8177647120619196\n",
            "64     \t [0.18133544 0.67887524 0.60018155]. \t  2.3929382970522894 \t 3.8177647120619196\n",
            "65     \t [0.81359004 0.76129887 0.49975164]. \t  0.6095207961373732 \t 3.8177647120619196\n",
            "66     \t [0.06363343 0.5422766  0.31167274]. \t  0.2877952793680176 \t 3.8177647120619196\n",
            "67     \t [0.63145594 0.62557868 0.9352136 ]. \t  3.0671100960742277 \t 3.8177647120619196\n",
            "68     \t [0.50259193 0.833834   0.27032893]. \t  0.16973072351208715 \t 3.8177647120619196\n",
            "69     \t [0.86423622 0.32921701 0.78258116]. \t  2.2149879950678066 \t 3.8177647120619196\n",
            "70     \t [0.09070466 0.39075461 0.30718995]. \t  0.40511813609791275 \t 3.8177647120619196\n",
            "71     \t [0.92027701 0.90035824 0.55900314]. \t  0.4813358852570495 \t 3.8177647120619196\n",
            "72     \t [0.07396428 0.72569041 0.44195972]. \t  1.6725069055792048 \t 3.8177647120619196\n",
            "73     \t [0.38433519 0.8620133  0.47753883]. \t  2.017900529756957 \t 3.8177647120619196\n",
            "74     \t [0.70212302 0.08661498 0.22402478]. \t  0.671412664775161 \t 3.8177647120619196\n",
            "75     \t [0.43888827 0.73929972 0.25561194]. \t  0.14414037023096218 \t 3.8177647120619196\n",
            "76     \t [0.75182032 0.06531014 0.26174813]. \t  0.626724796742964 \t 3.8177647120619196\n",
            "77     \t [0.09764391 0.93899129 0.53823406]. \t  2.882858654763731 \t 3.8177647120619196\n",
            "78     \t [0.77977615 0.7150441  0.1566065 ]. \t  0.01689399623485657 \t 3.8177647120619196\n",
            "79     \t [0.1475432  0.68460258 0.02741935]. \t  0.006626606260917234 \t 3.8177647120619196\n",
            "80     \t [0.10978135 0.16938244 0.0893402 ]. \t  0.30763139312569293 \t 3.8177647120619196\n",
            "81     \t [0.91764548 0.85015027 0.7222717 ]. \t  0.9821056508060743 \t 3.8177647120619196\n",
            "82     \t [0.36101895 0.51007173 0.34447252]. \t  0.35858889577856073 \t 3.8177647120619196\n",
            "83     \t [0.55837414 0.61348561 0.73980716]. \t  2.7160211996294548 \t 3.8177647120619196\n",
            "84     \t [0.00460694 0.52167147 0.3961229 ]. \t  0.49170667493883247 \t 3.8177647120619196\n",
            "85     \t [0.31677985 0.2602402  0.8693305 ]. \t  1.7042213605703371 \t 3.8177647120619196\n",
            "86     \t [0.18255073 0.2406186  0.76028492]. \t  1.433703322006356 \t 3.8177647120619196\n",
            "87     \t [0.41735198 0.83179659 0.40232147]. \t  1.1129711832789138 \t 3.8177647120619196\n",
            "88     \t [0.79543712 0.53168433 0.72799723]. \t  2.503079766226982 \t 3.8177647120619196\n",
            "89     \t [0.53838599 0.95932856 0.43034474]. \t  1.0109630170128627 \t 3.8177647120619196\n",
            "90     \t [0.25203354 0.22257782 0.88271513]. \t  1.3217363185741255 \t 3.8177647120619196\n",
            "91     \t [0.60582574 0.32749757 0.65001159]. \t  1.0835379561877838 \t 3.8177647120619196\n",
            "92     \t [0.90664635 0.39572821 0.15957417]. \t  0.13677434322670798 \t 3.8177647120619196\n",
            "93     \t [0.0285213  0.76993271 0.50713399]. \t  2.540627034382265 \t 3.8177647120619196\n",
            "94     \t [0.55089586 0.71819762 0.38418863]. \t  0.5712272229635553 \t 3.8177647120619196\n",
            "95     \t [0.90917994 0.50707355 0.56146547]. \t  0.5385477977482964 \t 3.8177647120619196\n",
            "96     \t [0.41922349 0.57565327 0.70638927]. \t  2.4807373000665214 \t 3.8177647120619196\n",
            "97     \t [0.92676762 0.83831109 0.71541354]. \t  0.9938525433078214 \t 3.8177647120619196\n",
            "98     \t [0.67512551 0.55953668 0.47623847]. \t  0.47444785276726686 \t 3.8177647120619196\n",
            "99     \t [0.14963312 0.34455413 0.24765072]. \t  0.5208219028417973 \t 3.8177647120619196\n",
            "100    \t [0.72125536 0.92491891 0.11602707]. \t  0.0034592068050073 \t 3.8177647120619196\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QoONg4VEtdy-",
        "outputId": "c4606ff9-e839-4244-857f-064c4e6dfa72"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 13\r\n",
        "\r\n",
        "np.random.seed(run_num_13)\r\n",
        "surrogate_loser_13 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "loser_13 = dGPGO(surrogate_loser_13, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_13.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.66064431 0.41360065 0.66810256]. \t  1.5457680899590245 \t 2.6697919207500047\n",
            "init   \t [0.22994342 0.80767834 0.63681846]. \t  2.6697919207500047 \t 2.6697919207500047\n",
            "init   \t [0.17219385 0.26038587 0.91531999]. \t  1.4685599870430508 \t 2.6697919207500047\n",
            "init   \t [0.46281551 0.12019095 0.88088551]. \t  0.639297028201682 \t 2.6697919207500047\n",
            "init   \t [0.22621895 0.81144033 0.44587892]. \t  1.960293029067079 \t 2.6697919207500047\n",
            "1      \t [0.54200533 0.91610447 0.94072107]. \t  0.9345749070922738 \t 2.6697919207500047\n",
            "2      \t [0.02210984 0.92408402 0.84182838]. \t  1.1986876793966825 \t 2.6697919207500047\n",
            "3      \t [0.2551603  0.64940006 0.63764251]. \t  2.2608836438455646 \t 2.6697919207500047\n",
            "4      \t [0.66842278 0.8625381  0.47685558]. \t  0.9979464612823808 \t 2.6697919207500047\n",
            "5      \t [0.28295395 0.99213626 0.55559161]. \t  2.407164166226089 \t 2.6697919207500047\n",
            "6      \t [0.88103078 0.40276708 0.88651999]. \t  \u001b[92m2.915655348161583\u001b[0m \t 2.915655348161583\n",
            "7      \t [0.85186124 0.38309496 0.96481362]. \t  1.9341935747459023 \t 2.915655348161583\n",
            "8      \t [0.99269926 0.32021245 0.76400007]. \t  1.9945111951258503 \t 2.915655348161583\n",
            "9      \t [0.9725296  0.74703252 0.78270881]. \t  2.1046312358842396 \t 2.915655348161583\n",
            "10     \t [0.64921845 0.58807206 0.82377664]. \t  \u001b[92m3.6487778595796403\u001b[0m \t 3.6487778595796403\n",
            "11     \t [0.67136868 0.65587793 0.78364563]. \t  2.96634855335022 \t 3.6487778595796403\n",
            "12     \t [0.93628178 0.98787552 0.00269434]. \t  5.9873549531252935e-05 \t 3.6487778595796403\n",
            "13     \t [0.20452643 0.95991774 0.00283118]. \t  0.0003842936989728932 \t 3.6487778595796403\n",
            "14     \t [0.28742108 0.58414624 0.97025443]. \t  2.6197401819259363 \t 3.6487778595796403\n",
            "15     \t [0.00918113 0.97131325 0.42350288]. \t  1.5801725863686558 \t 3.6487778595796403\n",
            "16     \t [0.48726769 0.41055569 0.89191979]. \t  3.0159354059023435 \t 3.6487778595796403\n",
            "17     \t [0.96241204 0.4977143  0.74078178]. \t  2.599080513436684 \t 3.6487778595796403\n",
            "18     \t [0.74037132 0.56122025 0.89563392]. \t  3.6190597808260043 \t 3.6487778595796403\n",
            "19     \t [0.43409753 0.96598791 0.28398903]. \t  0.22325574543686724 \t 3.6487778595796403\n",
            "20     \t [6.20647375e-16 8.28149418e-14 1.08553013e-14]. \t  0.06797411659015741 \t 3.6487778595796403\n",
            "21     \t [0.07733708 0.51291034 0.03301615]. \t  0.03137606726735763 \t 3.6487778595796403\n",
            "22     \t [0.00463129 0.16486818 0.46556882]. \t  0.2530080813283786 \t 3.6487778595796403\n",
            "23     \t [0.00320505 0.76662497 0.0974604 ]. \t  0.009031250496607012 \t 3.6487778595796403\n",
            "24     \t [0.94297442 0.99829129 0.44240974]. \t  0.22086974728088565 \t 3.6487778595796403\n",
            "25     \t [0.21806645 0.88497201 0.6999076 ]. \t  2.063426247291155 \t 3.6487778595796403\n",
            "26     \t [0.20999599 0.09189476 0.71383494]. \t  0.4644809892599904 \t 3.6487778595796403\n",
            "27     \t [0.74413859 0.50011464 0.86989513]. \t  \u001b[92m3.6589177774387114\u001b[0m \t 3.6589177774387114\n",
            "28     \t [0.73999113 0.51658583 0.89231812]. \t  3.5895994430253797 \t 3.6589177774387114\n",
            "29     \t [2.42898347e-04 6.38186582e-01 9.27581282e-01]. \t  3.1001233077485315 \t 3.6589177774387114\n",
            "30     \t [0.59203947 0.80806188 0.30643635]. \t  0.23209380349430725 \t 3.6589177774387114\n",
            "31     \t [0.99851265 0.8170673  0.06665683]. \t  0.000890527374873991 \t 3.6589177774387114\n",
            "32     \t [0.68939258 0.52375207 0.88165166]. \t  \u001b[92m3.691592425297216\u001b[0m \t 3.691592425297216\n",
            "33     \t [0.71862106 0.57869395 0.48791097]. \t  0.4877635725523503 \t 3.691592425297216\n",
            "34     \t [0.71015982 0.06240761 0.63233308]. \t  0.22214275190933627 \t 3.691592425297216\n",
            "35     \t [0.29537222 0.35230617 0.91184422]. \t  2.3294800034017893 \t 3.691592425297216\n",
            "36     \t [0.12149322 0.05671003 0.94459882]. \t  0.26115399667729156 \t 3.691592425297216\n",
            "37     \t [0.99370289 0.94780616 0.6772205 ]. \t  0.4206480841228158 \t 3.691592425297216\n",
            "38     \t [0.79155526 0.75895341 0.35415099]. \t  0.20442672556046407 \t 3.691592425297216\n",
            "39     \t [0.41819026 0.01110224 0.42672782]. \t  0.42020265486539904 \t 3.691592425297216\n",
            "40     \t [0.78397085 0.65300818 0.7031766 ]. \t  1.901811970012256 \t 3.691592425297216\n",
            "41     \t [0.77590574 0.03251059 0.70689557]. \t  0.27454686586139604 \t 3.691592425297216\n",
            "42     \t [0.60493648 0.51070951 0.91896704]. \t  3.3321496123694563 \t 3.691592425297216\n",
            "43     \t [0.04359117 0.24712361 0.53820461]. \t  0.3207940100321773 \t 3.691592425297216\n",
            "44     \t [0.92602388 0.994073   0.32371181]. \t  0.07090802659903218 \t 3.691592425297216\n",
            "45     \t [0.57344005 0.61197114 0.18922437]. \t  0.07785242306504275 \t 3.691592425297216\n",
            "46     \t [0.12012265 0.14761798 0.26838165]. \t  0.8242627741310308 \t 3.691592425297216\n",
            "47     \t [0.27362221 0.36267462 0.50258601]. \t  0.44167122163239614 \t 3.691592425297216\n",
            "48     \t [0.46186667 0.12699026 0.51178451]. \t  0.23846173888788497 \t 3.691592425297216\n",
            "49     \t [0.76667116 0.6392036  0.80705852]. \t  3.2443713047496106 \t 3.691592425297216\n",
            "50     \t [0.13767661 0.66241837 0.82792657]. \t  3.425531063299217 \t 3.691592425297216\n",
            "51     \t [0.95793867 0.66615194 0.60097035]. \t  0.7192507904766757 \t 3.691592425297216\n",
            "52     \t [0.05145473 0.64851336 0.21348459]. \t  0.09474949345676345 \t 3.691592425297216\n",
            "53     \t [0.48623021 0.77377589 0.411413  ]. \t  0.9734191014544429 \t 3.691592425297216\n",
            "54     \t [0.78882542 0.13270961 0.90543571]. \t  0.6333344930577843 \t 3.691592425297216\n",
            "55     \t [0.24220402 0.25654683 0.99013943]. \t  0.8858897160755633 \t 3.691592425297216\n",
            "56     \t [0.22829701 0.05339116 0.54151275]. \t  0.16354623570708623 \t 3.691592425297216\n",
            "57     \t [0.76016192 0.74881222 0.56770784]. \t  0.9373503703113361 \t 3.691592425297216\n",
            "58     \t [0.2795376  0.58991046 0.2037006 ]. \t  0.12301042643614361 \t 3.691592425297216\n",
            "59     \t [0.31980344 0.82750827 0.11581836]. \t  0.011157555616638023 \t 3.691592425297216\n",
            "60     \t [0.20118285 0.96464589 0.24263923]. \t  0.14556622456810533 \t 3.691592425297216\n",
            "61     \t [0.51970053 0.84549219 0.4922885 ]. \t  1.6293240662063382 \t 3.691592425297216\n",
            "62     \t [0.82379254 0.81675594 0.85550943]. \t  1.9278312884785196 \t 3.691592425297216\n",
            "63     \t [0.40139966 0.0505134  0.44444517]. \t  0.38492138667391107 \t 3.691592425297216\n",
            "64     \t [0.41306454 0.59975919 0.95354358]. \t  2.885245533239425 \t 3.691592425297216\n",
            "65     \t [0.90762895 0.09043539 0.35045986]. \t  0.33954753185910946 \t 3.691592425297216\n",
            "66     \t [0.56437488 0.00299382 0.07478562]. \t  0.2575720757158853 \t 3.691592425297216\n",
            "67     \t [0.04035773 0.82036326 0.80315336]. \t  2.0799583721374217 \t 3.691592425297216\n",
            "68     \t [0.17023444 0.85258575 0.91493466]. \t  1.5565545405274104 \t 3.691592425297216\n",
            "69     \t [0.87734285 0.27734693 0.62186276]. \t  0.6661400065856827 \t 3.691592425297216\n",
            "70     \t [0.85442665 0.36765057 0.29538047]. \t  0.2636416785367353 \t 3.691592425297216\n",
            "71     \t [0.65398105 0.18685371 0.2556733 ]. \t  0.7441875730102886 \t 3.691592425297216\n",
            "72     \t [0.16483906 0.02722539 0.07838903]. \t  0.27911575215119816 \t 3.691592425297216\n",
            "73     \t [0.88555469 0.94382983 0.28432556]. \t  0.05271556107938427 \t 3.691592425297216\n",
            "74     \t [0.68988585 0.09996215 0.53871476]. \t  0.1713866911101003 \t 3.691592425297216\n",
            "75     \t [0.0180464  0.85478503 0.24884179]. \t  0.17919690815782152 \t 3.691592425297216\n",
            "76     \t [0.78321123 0.04993389 0.03396936]. \t  0.11155547807075329 \t 3.691592425297216\n",
            "77     \t [0.83488428 0.17038079 0.32578126]. \t  0.4593399119267525 \t 3.691592425297216\n",
            "78     \t [0.00337352 0.81377532 0.47141789]. \t  2.3007426133611486 \t 3.691592425297216\n",
            "79     \t [0.44659059 0.34840865 0.05190856]. \t  0.14299750465571026 \t 3.691592425297216\n",
            "80     \t [0.82874866 0.46665737 0.88077643]. \t  3.426233378521397 \t 3.691592425297216\n",
            "81     \t [0.44960303 0.01595051 0.18324486]. \t  0.716359984072404 \t 3.691592425297216\n",
            "82     \t [0.24540929 0.96089703 0.58222289]. \t  2.6288618583053487 \t 3.691592425297216\n",
            "83     \t [0.60404809 0.72598803 0.35607217]. \t  0.37369385522183546 \t 3.691592425297216\n",
            "84     \t [0.48995829 0.47442484 0.90045138]. \t  3.391988206652351 \t 3.691592425297216\n",
            "85     \t [0.86483266 0.77760985 0.20264433]. \t  0.017360610657446037 \t 3.691592425297216\n",
            "86     \t [0.96077586 0.70534906 0.83842714]. \t  2.8847770418582708 \t 3.691592425297216\n",
            "87     \t [0.46111202 0.66460214 0.78780807]. \t  3.0911449545854603 \t 3.691592425297216\n",
            "88     \t [0.17321434 0.20357407 0.55873031]. \t  0.3164822550587725 \t 3.691592425297216\n",
            "89     \t [0.03865646 0.34254306 0.56508583]. \t  0.5978670329887741 \t 3.691592425297216\n",
            "90     \t [0.79060194 0.25759698 0.43351173]. \t  0.24940990598258378 \t 3.691592425297216\n",
            "91     \t [0.32236939 0.42199787 0.28337111]. \t  0.42707895490724906 \t 3.691592425297216\n",
            "92     \t [0.67223202 0.36264268 0.17064459]. \t  0.31461328905573827 \t 3.691592425297216\n",
            "93     \t [0.62734209 0.15873131 0.39466884]. \t  0.505366693689942 \t 3.691592425297216\n",
            "94     \t [0.86554748 0.19661497 0.24919996]. \t  0.4438912830916337 \t 3.691592425297216\n",
            "95     \t [0.98606367 0.93178489 0.99407399]. \t  0.5702372182084068 \t 3.691592425297216\n",
            "96     \t [0.13527758 0.02125589 0.12677246]. \t  0.42834497445761877 \t 3.691592425297216\n",
            "97     \t [0.6710944  0.88514049 0.09061597]. \t  0.0026314471908069825 \t 3.691592425297216\n",
            "98     \t [0.13400302 0.73596897 0.96113073]. \t  2.091712838707545 \t 3.691592425297216\n",
            "99     \t [0.8586582  0.53204264 0.85304921]. \t  \u001b[92m3.721611532889103\u001b[0m \t 3.721611532889103\n",
            "100    \t [0.86204299 0.60518289 0.98541934]. \t  2.265846033509685 \t 3.721611532889103\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CoQHSe2Dtd1m",
        "outputId": "fa3378c8-1192-4c30-b927-057cf0eba184"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 14\r\n",
        "\r\n",
        "np.random.seed(run_num_14)\r\n",
        "surrogate_loser_14 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "loser_14 = dGPGO(surrogate_loser_14, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_14.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.64755105 0.50714969 0.52834138]. \t  0.5963212059988954 \t 2.610000357863649\n",
            "init   \t [0.8962852  0.69999119 0.7142971 ]. \t  1.7197848290620104 \t 2.610000357863649\n",
            "init   \t [0.71733838 0.22281946 0.17515452]. \t  0.48166052848103497 \t 2.610000357863649\n",
            "init   \t [0.45684149 0.92873843 0.00988589]. \t  0.0004588015757462679 \t 2.610000357863649\n",
            "init   \t [0.08992219 0.85020027 0.48562106]. \t  2.610000357863649 \t 2.610000357863649\n",
            "1      \t [0.07426471 0.99752623 0.88021278]. \t  0.6630933245090485 \t 2.610000357863649\n",
            "2      \t [0.00180211 0.46271645 0.11240184]. \t  0.09986378420438345 \t 2.610000357863649\n",
            "3      \t [0.04945322 0.96746298 0.34336512]. \t  0.71192198606886 \t 2.610000357863649\n",
            "4      \t [0.95164465 0.9953009  0.00385032]. \t  5.429819046242939e-05 \t 2.610000357863649\n",
            "5      \t [0.9979422  0.99655151 0.98186954]. \t  0.37591226277898976 \t 2.610000357863649\n",
            "6      \t [0.95837713 0.62910703 0.12515201]. \t  0.014715862521621976 \t 2.610000357863649\n",
            "7      \t [0.05380284 0.69099212 0.61559705]. \t  2.477025978003247 \t 2.610000357863649\n",
            "8      \t [0.4099993  0.92486099 0.52611019]. \t  2.2038453653786894 \t 2.610000357863649\n",
            "9      \t [0.12502002 0.82889781 0.49304721]. \t  \u001b[92m2.6590433374920455\u001b[0m \t 2.6590433374920455\n",
            "10     \t [0.0432031  0.81394199 0.52604994]. \t  \u001b[92m2.863126326791752\u001b[0m \t 2.863126326791752\n",
            "11     \t [0.93728378 0.01172871 0.05842301]. \t  0.0917317165535674 \t 2.863126326791752\n",
            "12     \t [0.04830445 0.79599636 0.56926789]. \t  \u001b[92m2.94888161804253\u001b[0m \t 2.94888161804253\n",
            "13     \t [0.96219208 0.92550704 0.48515165]. \t  0.2980789003451671 \t 2.94888161804253\n",
            "14     \t [0.49305204 0.0469595  0.        ]. \t  0.10658903029135289 \t 2.94888161804253\n",
            "15     \t [0.9725237  0.48844653 0.87837032]. \t  \u001b[92m3.4955070864224314\u001b[0m \t 3.4955070864224314\n",
            "16     \t [0.97765512 0.28908983 0.8882185 ]. \t  1.8331935020921892 \t 3.4955070864224314\n",
            "17     \t [0.82336931 0.69094476 0.99535318]. \t  1.8317234802335283 \t 3.4955070864224314\n",
            "18     \t [0.96873907 0.56322882 0.9383951 ]. \t  3.050598972616365 \t 3.4955070864224314\n",
            "19     \t [0.98414681 0.41566923 0.49281313]. \t  0.19462008725546937 \t 3.4955070864224314\n",
            "20     \t [0.55403896 0.41378808 0.03051479]. \t  0.06960499772290545 \t 3.4955070864224314\n",
            "21     \t [0.82438027 0.50574454 0.9092123 ]. \t  3.376368871912553 \t 3.4955070864224314\n",
            "22     \t [0.50527634 0.4021167  0.905708  ]. \t  2.827435466911807 \t 3.4955070864224314\n",
            "23     \t [0.33973592 0.83048357 0.79975323]. \t  1.9524561078382523 \t 3.4955070864224314\n",
            "24     \t [0.65056615 0.4367664  0.9827399 ]. \t  2.025405392267641 \t 3.4955070864224314\n",
            "25     \t [0.15264283 0.02107091 0.5549017 ]. \t  0.12891552497090242 \t 3.4955070864224314\n",
            "26     \t [0.5351411  0.09445077 0.68338355]. \t  0.40519482656621714 \t 3.4955070864224314\n",
            "27     \t [0.28528739 0.46336557 0.26731631]. \t  0.33819052205087036 \t 3.4955070864224314\n",
            "28     \t [0.14266977 0.41799958 0.07239393]. \t  0.11124283833397586 \t 3.4955070864224314\n",
            "29     \t [0.55257834 0.99265052 0.26703074]. \t  0.1208697033520126 \t 3.4955070864224314\n",
            "30     \t [0.01806789 0.96107533 0.70194425]. \t  1.71026065747257 \t 3.4955070864224314\n",
            "31     \t [0.6096993  0.99899235 0.85987141]. \t  0.6218218838754161 \t 3.4955070864224314\n",
            "32     \t [0.47334064 0.29370186 0.6061317 ]. \t  0.6727834909940495 \t 3.4955070864224314\n",
            "33     \t [0.07255508 0.10238236 0.99487251]. \t  0.2646432290137243 \t 3.4955070864224314\n",
            "34     \t [0.07668192 0.4967238  0.99851593]. \t  1.9951281424451577 \t 3.4955070864224314\n",
            "35     \t [0.16316254 0.77607392 0.10225318]. \t  0.01087632488374639 \t 3.4955070864224314\n",
            "36     \t [0.17653826 0.21738289 0.96215749]. \t  0.8595982659733136 \t 3.4955070864224314\n",
            "37     \t [0. 0. 0.]. \t  0.06797411659013229 \t 3.4955070864224314\n",
            "38     \t [0.91865577 0.36275365 0.1912791 ]. \t  0.1862391981836753 \t 3.4955070864224314\n",
            "39     \t [0.41527166 0.64813462 0.16672562]. \t  0.058604953951955356 \t 3.4955070864224314\n",
            "40     \t [0.79917452 0.98775728 0.7286888 ]. \t  0.5596406347526663 \t 3.4955070864224314\n",
            "41     \t [0.39789933 0.57798059 0.91184543]. \t  \u001b[92m3.509310013992248\u001b[0m \t 3.509310013992248\n",
            "42     \t [0.07907219 0.0441213  0.49501061]. \t  0.18632550644606571 \t 3.509310013992248\n",
            "43     \t [0.43329081 0.40589903 0.72027082]. \t  2.2226473633261703 \t 3.509310013992248\n",
            "44     \t [0.12907142 0.51616038 0.51373335]. \t  1.0164915943293173 \t 3.509310013992248\n",
            "45     \t [0.4413456  0.07346512 0.83536804]. \t  0.48157224856520825 \t 3.509310013992248\n",
            "46     \t [0.42542469 0.33852438 0.46777118]. \t  0.35843440517809333 \t 3.509310013992248\n",
            "47     \t [0.45205372 0.86171738 0.8772896 ]. \t  1.6020784611943335 \t 3.509310013992248\n",
            "48     \t [0.06500231 0.74732896 0.95425446]. \t  2.0866199126866483 \t 3.509310013992248\n",
            "49     \t [0.40675235 0.20604412 0.32647895]. \t  0.835147485697324 \t 3.509310013992248\n",
            "50     \t [0.51629496 0.44238532 0.11428872]. \t  0.1618554098435086 \t 3.509310013992248\n",
            "51     \t [0.01841433 0.66213843 0.44030181]. \t  1.3100501508645412 \t 3.509310013992248\n",
            "52     \t [0.42750177 0.08095008 0.22881845]. \t  0.934718699821992 \t 3.509310013992248\n",
            "53     \t [0.99132776 0.56247312 0.87898254]. \t  \u001b[92m3.6264090638744766\u001b[0m \t 3.6264090638744766\n",
            "54     \t [0.19229006 0.27583495 0.78979266]. \t  1.8392142767687603 \t 3.6264090638744766\n",
            "55     \t [0.99607341 0.95060923 0.07884714]. \t  0.00040076247048857424 \t 3.6264090638744766\n",
            "56     \t [0.72236324 0.20397123 0.68392677]. \t  0.809154011084921 \t 3.6264090638744766\n",
            "57     \t [0.82726544 0.56984414 0.91488448]. \t  3.402460992571767 \t 3.6264090638744766\n",
            "58     \t [0.4496368  0.09232231 0.65260425]. \t  0.3286036761854578 \t 3.6264090638744766\n",
            "59     \t [0.66686898 0.36122784 0.22139213]. \t  0.3992469319683874 \t 3.6264090638744766\n",
            "60     \t [0.84343985 0.47811337 0.0260408 ]. \t  0.024123015022092978 \t 3.6264090638744766\n",
            "61     \t [0.50786571 0.30890895 0.09449962]. \t  0.26672971047047095 \t 3.6264090638744766\n",
            "62     \t [0.15524704 0.78149988 0.03254962]. \t  0.002786228877873154 \t 3.6264090638744766\n",
            "63     \t [0.29530089 0.8267771  0.50007408]. \t  2.4621184302121764 \t 3.6264090638744766\n",
            "64     \t [0.73916819 0.92720124 0.92738278]. \t  0.8970108912251867 \t 3.6264090638744766\n",
            "65     \t [0.67444173 0.44494752 0.87215447]. \t  3.372482011479299 \t 3.6264090638744766\n",
            "66     \t [0.56649389 0.50233033 0.51945521]. \t  0.6376902576687437 \t 3.6264090638744766\n",
            "67     \t [0.28076717 0.11678024 0.47315713]. \t  0.3132157828560709 \t 3.6264090638744766\n",
            "68     \t [0.34975716 0.55735162 0.44800429]. \t  0.7645979471726574 \t 3.6264090638744766\n",
            "69     \t [0.79661882 0.26738063 0.53155921]. \t  0.2656598661584797 \t 3.6264090638744766\n",
            "70     \t [0.14360166 0.87285564 0.22607183]. \t  0.11979374408608724 \t 3.6264090638744766\n",
            "71     \t [0.20273741 0.68270988 0.68133879]. \t  2.5067715263777925 \t 3.6264090638744766\n",
            "72     \t [0.10236774 0.30777171 0.08361548]. \t  0.2042348819686146 \t 3.6264090638744766\n",
            "73     \t [0.67858085 0.6219549  0.55325466]. \t  0.9070086533155038 \t 3.6264090638744766\n",
            "74     \t [0.14445184 0.18590658 0.93168239]. \t  0.8378508615508395 \t 3.6264090638744766\n",
            "75     \t [0.33188654 0.9244444  0.23513361]. \t  0.11906325846609561 \t 3.6264090638744766\n",
            "76     \t [0.48097666 0.13009226 0.29282477]. \t  0.9441138008571816 \t 3.6264090638744766\n",
            "77     \t [0.32990587 0.58124315 0.16500403]. \t  0.09588791985837758 \t 3.6264090638744766\n",
            "78     \t [0.55142752 0.00357512 0.70672671]. \t  0.21495598343963285 \t 3.6264090638744766\n",
            "79     \t [0.49708493 0.55071354 0.15142348]. \t  0.10212397980838571 \t 3.6264090638744766\n",
            "80     \t [0.83839049 0.61356518 0.90770868]. \t  3.37999894933908 \t 3.6264090638744766\n",
            "81     \t [0.10514481 0.86268084 0.29104502]. \t  0.3755481141119861 \t 3.6264090638744766\n",
            "82     \t [0.77926204 0.03190372 0.28453967]. \t  0.5564609157951522 \t 3.6264090638744766\n",
            "83     \t [0.0844148  0.96013624 0.30520214]. \t  0.42971674692981515 \t 3.6264090638744766\n",
            "84     \t [0.91312378 0.47041951 0.20526919]. \t  0.1073063230476932 \t 3.6264090638744766\n",
            "85     \t [0.90271173 0.30248265 0.60457918]. \t  0.6061828342528154 \t 3.6264090638744766\n",
            "86     \t [0.56818296 0.13607622 0.98090366]. \t  0.40439730435804294 \t 3.6264090638744766\n",
            "87     \t [0.02036094 0.91618381 0.91600987]. \t  1.0458528763005754 \t 3.6264090638744766\n",
            "88     \t [0.58408506 0.22139976 0.15178313]. \t  0.5231401031426556 \t 3.6264090638744766\n",
            "89     \t [0.7460948 0.9881278 0.4693557]. \t  0.6309118965643825 \t 3.6264090638744766\n",
            "90     \t [0.64502969 0.7036153  0.33497261]. \t  0.24718341667757182 \t 3.6264090638744766\n",
            "91     \t [0.81336943 0.7955296  0.2408704 ]. \t  0.038691247046843534 \t 3.6264090638744766\n",
            "92     \t [0.22908182 0.34958393 0.70349495]. \t  1.7538685921576378 \t 3.6264090638744766\n",
            "93     \t [0.02556752 0.98745888 0.72362373]. \t  1.3883167758583452 \t 3.6264090638744766\n",
            "94     \t [0.59237836 0.93434985 0.51741072]. \t  1.4000596558201313 \t 3.6264090638744766\n",
            "95     \t [0.07210507 0.7999767  0.69767257]. \t  2.4017222558308857 \t 3.6264090638744766\n",
            "96     \t [0.01540501 0.18169681 0.53582139]. \t  0.23750339174042914 \t 3.6264090638744766\n",
            "97     \t [0.38957153 0.32283297 0.30482957]. \t  0.6454892445601828 \t 3.6264090638744766\n",
            "98     \t [0.38533378 0.41887816 0.54780256]. \t  0.6877116008288234 \t 3.6264090638744766\n",
            "99     \t [0.14901495 0.8771433  0.04026063]. \t  0.001634100766366568 \t 3.6264090638744766\n",
            "100    \t [0.62120192 0.76496167 0.20277625]. \t  0.04053907768932854 \t 3.6264090638744766\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aq4J_FTytd4O",
        "outputId": "94978c39-d5c4-45be-9256-7188435045f4"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 15\r\n",
        "\r\n",
        "np.random.seed(run_num_15)\r\n",
        "surrogate_loser_15 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "loser_15 = dGPGO(surrogate_loser_15, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_15.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.57788186 0.25550133 0.25252687]. \t  0.7217338409961529 \t 1.540625560354162\n",
            "init   \t [0.70990435 0.44755236 0.22694296]. \t  0.23199690080875204 \t 1.540625560354162\n",
            "init   \t [0.40170957 0.88225774 0.43721347]. \t  1.540625560354162 \t 1.540625560354162\n",
            "init   \t [0.87842518 0.78052775 0.53421429]. \t  0.5653855252190516 \t 1.540625560354162\n",
            "init   \t [0.01173301 0.26575648 0.3311941 ]. \t  0.5019682819961726 \t 1.540625560354162\n",
            "1      \t [0.45870705 0.97826428 0.97993811]. \t  0.46749494069699443 \t 1.540625560354162\n",
            "2      \t [0.66075471 0.         0.61312367]. \t  0.12159642826997123 \t 1.540625560354162\n",
            "3      \t [0.46645428 0.99752683 0.41295545]. \t  0.9605705723922775 \t 1.540625560354162\n",
            "4      \t [0.34390964 0.64896828 0.49739212]. \t  1.5093166326574845 \t 1.540625560354162\n",
            "5      \t [0.05653148 0.81602006 0.23121467]. \t  0.13016751165670606 \t 1.540625560354162\n",
            "6      \t [0.29839982 0.94779509 0.61815286]. \t  \u001b[92m2.376883919188496\u001b[0m \t 2.376883919188496\n",
            "7      \t [0.20225734 0.91762416 0.5993738 ]. \t  \u001b[92m2.8143502335642547\u001b[0m \t 2.8143502335642547\n",
            "8      \t [0.05726953 0.95949547 0.73838485]. \t  1.4281001613955855 \t 2.8143502335642547\n",
            "9      \t [0.96497544 0.07697573 0.01139424]. \t  0.04752173213497603 \t 2.8143502335642547\n",
            "10     \t [1. 1. 1.]. \t  0.31688362070415665 \t 2.8143502335642547\n",
            "11     \t [0.06083111 0.95090286 0.46023173]. \t  2.1523413244692358 \t 2.8143502335642547\n",
            "12     \t [0.29544804 0.00749755 0.1138019 ]. \t  0.4304462537200079 \t 2.8143502335642547\n",
            "13     \t [0.9788367  0.13663849 0.46305662]. \t  0.1333115859356235 \t 2.8143502335642547\n",
            "14     \t [0.99248475 0.89393169 0.25379208]. \t  0.019773995845805473 \t 2.8143502335642547\n",
            "15     \t [0.12141814 0.77615604 0.59627157]. \t  \u001b[92m2.88255169201502\u001b[0m \t 2.88255169201502\n",
            "16     \t [0.09484896 0.05628241 0.76753651]. \t  0.40772636461818845 \t 2.88255169201502\n",
            "17     \t [0.98855647 0.64396695 0.96994006]. \t  2.387841970988428 \t 2.88255169201502\n",
            "18     \t [0.10907278 0.82392297 0.59561025]. \t  \u001b[92m3.000256288747202\u001b[0m \t 3.000256288747202\n",
            "19     \t [0.96921191 0.20641809 0.99642329]. \t  0.5856509656909898 \t 3.000256288747202\n",
            "20     \t [0.99071045 0.11175798 0.0756385 ]. \t  0.10411761062601754 \t 3.000256288747202\n",
            "21     \t [0.17049127 0.30955908 0.5032451 ]. \t  0.36574896563945947 \t 3.000256288747202\n",
            "22     \t [0.35626726 0.30263896 0.91762246]. \t  1.8275297317309531 \t 3.000256288747202\n",
            "23     \t [0.26082213 0.52658403 0.99305453]. \t  2.176598638221769 \t 3.000256288747202\n",
            "24     \t [0.75812964 0.4744     0.99163431]. \t  2.031412034788625 \t 3.000256288747202\n",
            "25     \t [0.30805774 0.03183635 0.97626546]. \t  0.1605391479077385 \t 3.000256288747202\n",
            "26     \t [0.67703224 0.08606007 0.20809089]. \t  0.6706776152497954 \t 3.000256288747202\n",
            "27     \t [0.22027058 0.82222382 0.71245758]. \t  2.222960151813486 \t 3.000256288747202\n",
            "28     \t [0.04551279 0.41571432 0.91993079]. \t  2.7629813981502034 \t 3.000256288747202\n",
            "29     \t [0.02701892 0.73247455 0.54921471]. \t  2.5873896378178856 \t 3.000256288747202\n",
            "30     \t [0.16202041 0.86821179 0.56604579]. \t  \u001b[92m3.062704393865933\u001b[0m \t 3.062704393865933\n",
            "31     \t [0.37857704 0.4008916  0.57095718]. \t  0.7635747625375403 \t 3.062704393865933\n",
            "32     \t [0.09185293 0.74550382 0.61485494]. \t  2.7270945552334847 \t 3.062704393865933\n",
            "33     \t [0.81464089 0.20410306 0.19092096]. \t  0.42888437233317794 \t 3.062704393865933\n",
            "34     \t [0.85842146 0.29924661 0.15859815]. \t  0.24543263868828105 \t 3.062704393865933\n",
            "35     \t [0.03667396 0.16455783 0.93639795]. \t  0.6920956476994375 \t 3.062704393865933\n",
            "36     \t [0.01042273 0.54901765 0.83205998]. \t  \u001b[92m3.77490607109067\u001b[0m \t 3.77490607109067\n",
            "37     \t [0.09235068 0.59453282 0.5299634 ]. \t  1.5829305238168287 \t 3.77490607109067\n",
            "38     \t [0.13771917 0.43744959 0.88943158]. \t  3.232777206779901 \t 3.77490607109067\n",
            "39     \t [0.05362766 0.30928959 0.06249417]. \t  0.14577172423795093 \t 3.77490607109067\n",
            "40     \t [0.03454882 0.5530963  0.43818732]. \t  0.7889596293821415 \t 3.77490607109067\n",
            "41     \t [0.06175869 0.84298002 0.17074262]. \t  0.03836987173902284 \t 3.77490607109067\n",
            "42     \t [0.17364572 0.64082997 0.5845864 ]. \t  2.1333882315447505 \t 3.77490607109067\n",
            "43     \t [0.19228284 0.34767431 0.35689909]. \t  0.48325991939194524 \t 3.77490607109067\n",
            "44     \t [0.23354636 0.46147258 0.26207807]. \t  0.32929384743697176 \t 3.77490607109067\n",
            "45     \t [0.55513085 0.78585576 0.17222904]. \t  0.026846057895913045 \t 3.77490607109067\n",
            "46     \t [0.32640468 0.99936566 0.57634998]. \t  2.227868485959561 \t 3.77490607109067\n",
            "47     \t [0.10589905 0.5469575  0.0158781 ]. \t  0.019376708817783993 \t 3.77490607109067\n",
            "48     \t [0.72025457 0.42821853 0.89108759]. \t  3.1298633504098006 \t 3.77490607109067\n",
            "49     \t [0.95041933 0.47369464 0.77930951]. \t  3.0573049834269135 \t 3.77490607109067\n",
            "50     \t [0.37463577 0.52589471 0.87634371]. \t  3.7726339942689924 \t 3.77490607109067\n",
            "51     \t [0.80954807 0.70749514 0.87223495]. \t  2.9788339110325452 \t 3.77490607109067\n",
            "52     \t [0.04936964 0.76643056 0.27365458]. \t  0.25866039619980574 \t 3.77490607109067\n",
            "53     \t [0.20524069 0.76502703 0.87035564]. \t  2.5771584611667055 \t 3.77490607109067\n",
            "54     \t [0.44579854 0.10044851 0.46713745]. \t  0.32543707520716264 \t 3.77490607109067\n",
            "55     \t [0.76353136 0.02023589 0.33067034]. \t  0.5065642586272455 \t 3.77490607109067\n",
            "56     \t [0.05689745 0.55302702 0.21793852]. \t  0.139296626823076 \t 3.77490607109067\n",
            "57     \t [0.93704072 0.31603255 0.75654058]. \t  1.9235605761300654 \t 3.77490607109067\n",
            "58     \t [0.92634453 0.55250125 0.37701181]. \t  0.10624015191215301 \t 3.77490607109067\n",
            "59     \t [0.98566888 0.93332484 0.38748483]. \t  0.12626546681121031 \t 3.77490607109067\n",
            "60     \t [0.52099922 0.16674173 0.54409066]. \t  0.2500879587637723 \t 3.77490607109067\n",
            "61     \t [0.45686738 0.7185135  0.97018309]. \t  2.0936312622869466 \t 3.77490607109067\n",
            "62     \t [0.38554434 0.52484819 0.3410793 ]. \t  0.3446974468763136 \t 3.77490607109067\n",
            "63     \t [0.48744492 0.75878045 0.40262775]. \t  0.8724685464007343 \t 3.77490607109067\n",
            "64     \t [0.32494146 0.55551894 0.11106917]. \t  0.07247095796871711 \t 3.77490607109067\n",
            "65     \t [0.22437916 0.18511023 0.39884955]. \t  0.554974513879724 \t 3.77490607109067\n",
            "66     \t [0.66195118 0.83031165 0.1263408 ]. \t  0.007416368719395564 \t 3.77490607109067\n",
            "67     \t [0.45277534 0.90837962 0.37948517]. \t  0.8302928560503484 \t 3.77490607109067\n",
            "68     \t [0.11030779 0.93358032 0.79663281]. \t  1.2673113215586038 \t 3.77490607109067\n",
            "69     \t [0.75689381 0.26626376 0.40769996]. \t  0.3093906641862753 \t 3.77490607109067\n",
            "70     \t [0.25249093 0.86169084 0.93775836]. \t  1.3487665227408552 \t 3.77490607109067\n",
            "71     \t [0.07417741 0.78342391 0.77384638]. \t  2.3549929045130416 \t 3.77490607109067\n",
            "72     \t [0.47843585 0.0947401  0.45417235]. \t  0.35892328654689665 \t 3.77490607109067\n",
            "73     \t [0.0028475  0.80972295 0.83880453]. \t  2.1616271984147324 \t 3.77490607109067\n",
            "74     \t [0.18412308 0.40611329 0.4560012 ]. \t  0.4478611342438572 \t 3.77490607109067\n",
            "75     \t [0.72015424 0.33382483 0.25526604]. \t  0.43357330396654686 \t 3.77490607109067\n",
            "76     \t [0.63260961 0.21704913 0.29129282]. \t  0.7245423555057698 \t 3.77490607109067\n",
            "77     \t [0.55578338 0.43008045 0.87967857]. \t  3.2470912244888375 \t 3.77490607109067\n",
            "78     \t [0.12989691 0.37828551 0.63240513]. \t  1.1871928238269298 \t 3.77490607109067\n",
            "79     \t [0.53736537 0.60544502 0.00838678]. \t  0.01141987847330488 \t 3.77490607109067\n",
            "80     \t [0.53955518 0.92113759 0.71411087]. \t  1.251299842896169 \t 3.77490607109067\n",
            "81     \t [0.78266961 0.80364241 0.95875221]. \t  1.5914169852142543 \t 3.77490607109067\n",
            "82     \t [0.98124241 0.27066251 0.49880464]. \t  0.16785373269959117 \t 3.77490607109067\n",
            "83     \t [0.86908715 0.7723944  0.27931136]. \t  0.05584375404926373 \t 3.77490607109067\n",
            "84     \t [0.18191085 0.87279776 0.06389839]. \t  0.0030067773074108337 \t 3.77490607109067\n",
            "85     \t [0.68267667 0.23806959 0.53615028]. \t  0.27687692743916364 \t 3.77490607109067\n",
            "86     \t [0.51580088 0.11030196 0.4589715 ]. \t  0.33837634771568686 \t 3.77490607109067\n",
            "87     \t [0.96852447 0.15481291 0.01517424]. \t  0.049788937359754426 \t 3.77490607109067\n",
            "88     \t [0.57673346 0.28305391 0.46648613]. \t  0.3037232717970031 \t 3.77490607109067\n",
            "89     \t [0.74398022 0.26145588 0.77234634]. \t  1.630304906430959 \t 3.77490607109067\n",
            "90     \t [0.9634963  0.27587429 0.56715608]. \t  0.35600452235267915 \t 3.77490607109067\n",
            "91     \t [0.73619882 0.71342445 0.18271428]. \t  0.026606237937590572 \t 3.77490607109067\n",
            "92     \t [0.57409829 0.41833827 0.3448306 ]. \t  0.3538976036382143 \t 3.77490607109067\n",
            "93     \t [0.56102639 0.06679605 0.64037298]. \t  0.2483887387972291 \t 3.77490607109067\n",
            "94     \t [0.93882985 0.30550525 0.23690212]. \t  0.25813917911444156 \t 3.77490607109067\n",
            "95     \t [0.05289148 0.90095206 0.18316854]. \t  0.04819088234293035 \t 3.77490607109067\n",
            "96     \t [0.47581993 0.98761237 0.71369248]. \t  1.0932033178536977 \t 3.77490607109067\n",
            "97     \t [0.79326914 0.88886517 0.95242994]. \t  1.0291534985796806 \t 3.77490607109067\n",
            "98     \t [0.90504706 0.25651419 0.73046914]. \t  1.3447868969921708 \t 3.77490607109067\n",
            "99     \t [0.35541618 0.87374982 0.17466265]. \t  0.03535399187079531 \t 3.77490607109067\n",
            "100    \t [0.61162121 0.13982352 0.67154308]. \t  0.513200668672658 \t 3.77490607109067\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4tJ-ta9Gtd7H",
        "outputId": "05e7fddc-daf8-43b8-a600-b71f651279b9"
      },
      "source": [
        "\r\n",
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 16\r\n",
        "\r\n",
        "np.random.seed(run_num_16)\r\n",
        "surrogate_loser_16 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "loser_16 = dGPGO(surrogate_loser_16, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_16.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.86394833 0.316366   0.67215078]. \t  1.1947633822631252 \t 3.8084053754826726\n",
            "init   \t [0.50791167 0.78166043 0.28368168]. \t  0.20052765717741963 \t 3.8084053754826726\n",
            "init   \t [0.23370878 0.56379969 0.87502436]. \t  3.8084053754826726 \t 3.8084053754826726\n",
            "init   \t [0.71894292 0.18213174 0.24380041]. \t  0.6532826728302027 \t 3.8084053754826726\n",
            "init   \t [0.08605673 0.53424539 0.83969965]. \t  3.8072565344663847 \t 3.8084053754826726\n",
            "1      \t [0.12378065 0.3168353  0.9881258 ]. \t  1.2451880833929392 \t 3.8084053754826726\n",
            "2      \t [0.27777137 0.94321124 0.96390235]. \t  0.6864412627036871 \t 3.8084053754826726\n",
            "3      \t [0.04467565 0.50685234 0.49507307]. \t  0.8672168540674181 \t 3.8084053754826726\n",
            "4      \t [0.00628562 0.62733695 0.98683809]. \t  2.216178855902818 \t 3.8084053754826726\n",
            "5      \t [0.24302952 0.56104258 0.71810923]. \t  2.714059998275844 \t 3.8084053754826726\n",
            "6      \t [0.96778771 0.7812586  0.04735995]. \t  0.0011023859383055771 \t 3.8084053754826726\n",
            "7      \t [0.94597768 0.01177013 0.21221742]. \t  0.30096480916717455 \t 3.8084053754826726\n",
            "8      \t [1. 1. 1.]. \t  0.3168836207042708 \t 3.8084053754826726\n",
            "9      \t [0.86553754 0.59748584 0.96361124]. \t  2.6531081511639076 \t 3.8084053754826726\n",
            "10     \t [0.64363474 0.74123114 0.01847141]. \t  0.002719251460432877 \t 3.8084053754826726\n",
            "11     \t [0.50952739 0.70630666 0.95920627]. \t  2.3194290643909032 \t 3.8084053754826726\n",
            "12     \t [0.85522449 0.97974245 0.20836416]. \t  0.014056808087911507 \t 3.8084053754826726\n",
            "13     \t [0.90774286 0.35181844 0.01944449]. \t  0.03818621659189494 \t 3.8084053754826726\n",
            "14     \t [0.10442144 0.99757424 0.15075461]. \t  0.019461290813885892 \t 3.8084053754826726\n",
            "15     \t [0.14617975 0.59194485 0.82341198]. \t  3.724750123953375 \t 3.8084053754826726\n",
            "16     \t [1.         0.50084316 0.99314177]. \t  2.040382279489931 \t 3.8084053754826726\n",
            "17     \t [0.98906708 0.49449814 0.55793286]. \t  0.4720653973501239 \t 3.8084053754826726\n",
            "18     \t [0.62536537 0.37354973 0.93829796]. \t  2.2286937559849958 \t 3.8084053754826726\n",
            "19     \t [0.18821051 0.55334529 0.97120305]. \t  2.600081052146005 \t 3.8084053754826726\n",
            "20     \t [1.46554816e-14 1.46554816e-14 1.46554816e-14]. \t  0.06797411659015279 \t 3.8084053754826726\n",
            "21     \t [0.04213704 0.63940159 0.82388213]. \t  3.5206016329706498 \t 3.8084053754826726\n",
            "22     \t [0.05848135 0.47135776 0.81994148]. \t  3.5213504013127426 \t 3.8084053754826726\n",
            "23     \t [0.02438986 0.40362393 0.08475438]. \t  0.11377877239053279 \t 3.8084053754826726\n",
            "24     \t [0.9949863  0.74522997 0.86488097]. \t  2.5691256948847254 \t 3.8084053754826726\n",
            "25     \t [0.0918845  0.35222564 0.4215209 ]. \t  0.3685063985974734 \t 3.8084053754826726\n",
            "26     \t [0.52488244 0.425859   0.11355576]. \t  0.17691971367879086 \t 3.8084053754826726\n",
            "27     \t [0.6074091  0.5651644  0.76113511]. \t  3.062024712880875 \t 3.8084053754826726\n",
            "28     \t [0.22418673 0.0710134  0.17935842]. \t  0.7291417604546632 \t 3.8084053754826726\n",
            "29     \t [0.23621156 0.77273158 0.72314536]. \t  2.3689595603050706 \t 3.8084053754826726\n",
            "30     \t [0.77679747 0.65878278 0.78907801]. \t  2.9409868761613063 \t 3.8084053754826726\n",
            "31     \t [0.45034395 0.31184179 0.49118223]. \t  0.34019795084950083 \t 3.8084053754826726\n",
            "32     \t [0.46134655 0.08631294 0.92137812]. \t  0.4002426920815167 \t 3.8084053754826726\n",
            "33     \t [0.19064436 0.41679247 0.29594489]. \t  0.4111794760569056 \t 3.8084053754826726\n",
            "34     \t [0.75430526 0.46631548 0.45988832]. \t  0.2589634023281027 \t 3.8084053754826726\n",
            "35     \t [0.13569583 0.80181312 0.26061919]. \t  0.2203440587860694 \t 3.8084053754826726\n",
            "36     \t [0.0210929  0.27206192 0.20612286]. \t  0.490978954096385 \t 3.8084053754826726\n",
            "37     \t [0.02629512 0.25758702 0.77117475]. \t  1.5962871548899078 \t 3.8084053754826726\n",
            "38     \t [0.0730912  0.3120373  0.68606544]. \t  1.373801062907681 \t 3.8084053754826726\n",
            "39     \t [0.60526256 0.23723161 0.17647999]. \t  0.5717865799947605 \t 3.8084053754826726\n",
            "40     \t [0.72925291 0.17985652 0.1038405 ]. \t  0.292122585001201 \t 3.8084053754826726\n",
            "41     \t [0.79270642 0.632309   0.21490531]. \t  0.050736446991021644 \t 3.8084053754826726\n",
            "42     \t [0.74023708 0.53345148 0.69811026]. \t  2.097988154667948 \t 3.8084053754826726\n",
            "43     \t [0.93793031 0.3497155  0.18397546]. \t  0.1792599539864782 \t 3.8084053754826726\n",
            "44     \t [0.86734098 0.14622375 0.46534638]. \t  0.1795843969354928 \t 3.8084053754826726\n",
            "45     \t [0.32042685 0.21356485 0.69082321]. \t  0.909072075181484 \t 3.8084053754826726\n",
            "46     \t [0.93574892 0.27989918 0.01027377]. \t  0.04031023584034215 \t 3.8084053754826726\n",
            "47     \t [0.81706479 0.81081815 0.35408109]. \t  0.1967663213350807 \t 3.8084053754826726\n",
            "48     \t [0.87105988 0.83328345 0.27765357]. \t  0.054612293237242315 \t 3.8084053754826726\n",
            "49     \t [0.80936618 0.41185184 0.98653548]. \t  1.8141680762717804 \t 3.8084053754826726\n",
            "50     \t [0.19768881 0.46814887 0.84863148]. \t  3.6004712755916812 \t 3.8084053754826726\n",
            "51     \t [0.96048891 0.86684945 0.3033962 ]. \t  0.05267070671118978 \t 3.8084053754826726\n",
            "52     \t [0.12271592 0.1474196  0.20314984]. \t  0.7305270817652886 \t 3.8084053754826726\n",
            "53     \t [0.17349131 0.99299808 0.26561668]. \t  0.20964188064412295 \t 3.8084053754826726\n",
            "54     \t [0.10685981 0.53430589 0.85641253]. \t  \u001b[92m3.823263664347254\u001b[0m \t 3.823263664347254\n",
            "55     \t [0.11058698 0.97447688 0.3993736 ]. \t  1.3143788435188235 \t 3.823263664347254\n",
            "56     \t [0.87576718 0.49481146 0.55244252]. \t  0.5009209227040916 \t 3.823263664347254\n",
            "57     \t [0.02274508 0.95137123 0.03512004]. \t  0.0009698546243275209 \t 3.823263664347254\n",
            "58     \t [0.38999471 0.77210159 0.1036472 ]. \t  0.010896907604143387 \t 3.823263664347254\n",
            "59     \t [0.21582292 0.48375857 0.98977518]. \t  2.1204190952489697 \t 3.823263664347254\n",
            "60     \t [0.73989052 0.66110466 0.04476188]. \t  0.007994844924721287 \t 3.823263664347254\n",
            "61     \t [0.6488681  0.22512697 0.23753612]. \t  0.6857983232125278 \t 3.823263664347254\n",
            "62     \t [0.14359178 0.42114215 0.06835466]. \t  0.10418286952472619 \t 3.823263664347254\n",
            "63     \t [0.00618998 0.96069648 0.70577431]. \t  1.666715409499641 \t 3.823263664347254\n",
            "64     \t [0.064971   0.79011856 0.66386657]. \t  2.5880114255305275 \t 3.823263664347254\n",
            "65     \t [0.13804909 0.39060505 0.22599701]. \t  0.3945123996524405 \t 3.823263664347254\n",
            "66     \t [0.97278609 0.91514663 0.22301319]. \t  0.01214201106358989 \t 3.823263664347254\n",
            "67     \t [0.47805976 0.44904643 0.69911163]. \t  2.1228685999717154 \t 3.823263664347254\n",
            "68     \t [0.77591688 0.99971211 0.4089057 ]. \t  0.3571896826106656 \t 3.823263664347254\n",
            "69     \t [0.20130951 0.18090192 0.01361808]. \t  0.1280031127309551 \t 3.823263664347254\n",
            "70     \t [0.17274771 0.89530659 0.84319324]. \t  1.4260445211381512 \t 3.823263664347254\n",
            "71     \t [0.44817697 0.83618646 0.21842817]. \t  0.07567909537410877 \t 3.823263664347254\n",
            "72     \t [0.63705785 0.06685462 0.37546693]. \t  0.5564580121519138 \t 3.823263664347254\n",
            "73     \t [0.34473282 0.70500744 0.23790905]. \t  0.12499189981633492 \t 3.823263664347254\n",
            "74     \t [0.01471892 0.52767853 0.83725391]. \t  3.770355627665735 \t 3.823263664347254\n",
            "75     \t [0.8195422  0.67247422 0.25780942]. \t  0.056289828958829925 \t 3.823263664347254\n",
            "76     \t [0.45033804 0.7720043  0.64231289]. \t  2.072030997389843 \t 3.823263664347254\n",
            "77     \t [0.95281713 0.4814589  0.13972786]. \t  0.05889852967393788 \t 3.823263664347254\n",
            "78     \t [0.21029326 0.08949757 0.61269778]. \t  0.24228973974732076 \t 3.823263664347254\n",
            "79     \t [0.97854859 0.24215962 0.51359038]. \t  0.17767339073826924 \t 3.823263664347254\n",
            "80     \t [0.27415217 0.58686992 0.96646935]. \t  2.6831589861332583 \t 3.823263664347254\n",
            "81     \t [0.38697907 0.29536366 0.09405804]. \t  0.29552336972369 \t 3.823263664347254\n",
            "82     \t [0.38041894 0.28113978 0.19138337]. \t  0.6436855569505461 \t 3.823263664347254\n",
            "83     \t [0.73644648 0.74409984 0.0593703 ]. \t  0.004067044093233711 \t 3.823263664347254\n",
            "84     \t [0.43974686 0.25576428 0.76727338]. \t  1.5860201622412489 \t 3.823263664347254\n",
            "85     \t [0.91716939 0.23206351 0.2338737 ]. \t  0.3441867231335853 \t 3.823263664347254\n",
            "86     \t [0.928106   0.36398451 0.2170875 ]. \t  0.19819281824603735 \t 3.823263664347254\n",
            "87     \t [0.55496943 0.67175689 0.39539542]. \t  0.5496100823873051 \t 3.823263664347254\n",
            "88     \t [0.10679858 0.88619389 0.87102049]. \t  1.436988468303008 \t 3.823263664347254\n",
            "89     \t [0.42338151 0.41695061 0.16612036]. \t  0.2995029456411814 \t 3.823263664347254\n",
            "90     \t [0.89484097 0.34203198 0.19629526]. \t  0.22656051317356457 \t 3.823263664347254\n",
            "91     \t [0.17349676 0.41012199 0.94005107]. \t  2.4938462280056983 \t 3.823263664347254\n",
            "92     \t [0.19771752 0.31239557 0.2045076 ]. \t  0.558662218988743 \t 3.823263664347254\n",
            "93     \t [0.33178305 0.02142237 0.89524347]. \t  0.24704410756571965 \t 3.823263664347254\n",
            "94     \t [0.33225438 0.53569611 0.07988812]. \t  0.06111318267713783 \t 3.823263664347254\n",
            "95     \t [0.65412864 0.03015545 0.90688896]. \t  0.2532532659952299 \t 3.823263664347254\n",
            "96     \t [0.96827564 0.09360594 0.34086681]. \t  0.28907922102817385 \t 3.823263664347254\n",
            "97     \t [0.50114949 0.9151915  0.28738893]. \t  0.21956923397295164 \t 3.823263664347254\n",
            "98     \t [0.4219465  0.87052368 0.10971162]. \t  0.007494253147581695 \t 3.823263664347254\n",
            "99     \t [0.64684064 0.68675623 0.41936934]. \t  0.5458447520385002 \t 3.823263664347254\n",
            "100    \t [0.28560638 0.15564314 0.33671172]. \t  0.8403431815373047 \t 3.823263664347254\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NqjDLo0otd9-",
        "outputId": "0146729a-a1a6-4ad6-e4d4-ded6e6a221ab"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 17\r\n",
        "\r\n",
        "np.random.seed(run_num_17)\r\n",
        "surrogate_loser_17 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "loser_17 = dGPGO(surrogate_loser_17, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_17.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.65182403 0.73051962 0.19336736]. \t  0.03562358530655929 \t 3.1179188940604616\n",
            "init   \t [0.51967724 0.67638327 0.80444487]. \t  3.1179188940604616 \t 3.1179188940604616\n",
            "init   \t [0.53351286 0.90803418 0.44348113]. \t  1.1984486017622078 \t 3.1179188940604616\n",
            "init   \t [0.25597221 0.27866502 0.04332186]. \t  0.1645724004847893 \t 3.1179188940604616\n",
            "init   \t [0.46897499 0.9416138  0.60040624]. \t  1.9192522196289552 \t 3.1179188940604616\n",
            "1      \t [0.24286651 0.38597556 0.96454773]. \t  1.996297956656514 \t 3.1179188940604616\n",
            "2      \t [0.78722    0.79439557 0.98615055]. \t  1.382220738619041 \t 3.1179188940604616\n",
            "3      \t [0.48019116 0.49032971 0.69126343]. \t  2.1360120328634014 \t 3.1179188940604616\n",
            "4      \t [0.4607031  0.72735048 0.97289599]. \t  2.000642401584277 \t 3.1179188940604616\n",
            "5      \t [0.00166169 0.44190378 0.82055392]. \t  \u001b[92m3.336924169425464\u001b[0m \t 3.336924169425464\n",
            "6      \t [0.84992296 0.83200048 0.62767412]. \t  0.8027149864585417 \t 3.336924169425464\n",
            "7      \t [0.0340026  0.68308541 0.48965537]. \t  1.8929631780470242 \t 3.336924169425464\n",
            "8      \t [0.02659308 0.43512057 0.74588651]. \t  2.6717059421663465 \t 3.336924169425464\n",
            "9      \t [0.05231581 0.5113275  0.93934183]. \t  3.03637678220694 \t 3.336924169425464\n",
            "10     \t [0.78110492 0.5287478  0.92088201]. \t  3.3164288980141534 \t 3.336924169425464\n",
            "11     \t [0.7581868  0.59054205 0.89327271]. \t  \u001b[92m3.5935769448692607\u001b[0m \t 3.5935769448692607\n",
            "12     \t [0.06540998 0.93970118 0.11022951]. \t  0.0080286044197451 \t 3.5935769448692607\n",
            "13     \t [0.00917304 0.02388783 0.90208087]. \t  0.24124246128516022 \t 3.5935769448692607\n",
            "14     \t [0.76092022 0.54245962 0.7960535 ]. \t  3.430452623032548 \t 3.5935769448692607\n",
            "15     \t [0.06151538 0.96523417 0.47660316]. \t  2.287919808243954 \t 3.5935769448692607\n",
            "16     \t [0.70429838 0.55004718 0.88602409]. \t  \u001b[92m3.696554861371928\u001b[0m \t 3.696554861371928\n",
            "17     \t [0.63769391 0.60924651 0.82227901]. \t  3.571667524633763 \t 3.696554861371928\n",
            "18     \t [0.84564485 0.31890278 0.00286951]. \t  0.04128989362046553 \t 3.696554861371928\n",
            "19     \t [0.68465033 0.98359513 0.32227619]. \t  0.19492681141269716 \t 3.696554861371928\n",
            "20     \t [0.63440592 0.59892124 0.85253504]. \t  \u001b[92m3.7306976893459707\u001b[0m \t 3.7306976893459707\n",
            "21     \t [0.82399351 0.15394836 0.64722767]. \t  0.4584541049637956 \t 3.7306976893459707\n",
            "22     \t [0.00093538 0.63875692 0.84784539]. \t  3.577111350024265 \t 3.7306976893459707\n",
            "23     \t [0.13352918 0.94357418 0.30028195]. \t  0.4097630621694289 \t 3.7306976893459707\n",
            "24     \t [0.03451205 0.12497542 0.02756137]. \t  0.1274157801200702 \t 3.7306976893459707\n",
            "25     \t [0.76576681 0.43251344 0.11100496]. \t  0.111032154670387 \t 3.7306976893459707\n",
            "26     \t [0.71302946 0.65428918 0.85710419]. \t  3.4312930079457056 \t 3.7306976893459707\n",
            "27     \t [0.71440553 0.77407405 0.36272561]. \t  0.3093169795364798 \t 3.7306976893459707\n",
            "28     \t [0.70784051 0.97993163 0.67028909]. \t  0.7892816451607718 \t 3.7306976893459707\n",
            "29     \t [0.51800861 0.54551968 0.26487654]. \t  0.19936679977656882 \t 3.7306976893459707\n",
            "30     \t [0.03243126 0.94999519 0.69930194]. \t  1.8007352842172186 \t 3.7306976893459707\n",
            "31     \t [0.48660742 0.02568772 0.01930355]. \t  0.1394555186161855 \t 3.7306976893459707\n",
            "32     \t [0.78903081 0.82775419 0.1624179 ]. \t  0.009969463208157872 \t 3.7306976893459707\n",
            "33     \t [0.34639487 0.50220626 0.23128726]. \t  0.24563542914317918 \t 3.7306976893459707\n",
            "34     \t [0.73921642 0.46775549 0.67290683]. \t  1.6954635060896477 \t 3.7306976893459707\n",
            "35     \t [0.78764978 0.06042556 0.8302592 ]. \t  0.4267724380537717 \t 3.7306976893459707\n",
            "36     \t [0.70383577 0.57096565 0.17725708]. \t  0.0771285228845155 \t 3.7306976893459707\n",
            "37     \t [0.08224507 0.52206253 0.92520623]. \t  3.274999790654435 \t 3.7306976893459707\n",
            "38     \t [0.0620436  0.42801267 0.25147661]. \t  0.3106822479949355 \t 3.7306976893459707\n",
            "39     \t [0.20240843 0.223458   0.35299039]. \t  0.6751471570789034 \t 3.7306976893459707\n",
            "40     \t [0.78411066 0.99428025 0.80895205]. \t  0.5882277001490892 \t 3.7306976893459707\n",
            "41     \t [0.90256261 0.37869663 0.54763987]. \t  0.3839181168949045 \t 3.7306976893459707\n",
            "42     \t [0.82027636 0.03502204 0.24575806]. \t  0.5004688146837654 \t 3.7306976893459707\n",
            "43     \t [0.85121567 0.22893522 0.27913839]. \t  0.438480566219128 \t 3.7306976893459707\n",
            "44     \t [0.26433048 0.1597651  0.85691673]. \t  0.9205910915577684 \t 3.7306976893459707\n",
            "45     \t [0.76691622 0.51337129 0.38827815]. \t  0.19408491849855966 \t 3.7306976893459707\n",
            "46     \t [0.55298012 0.12193723 0.8363212 ]. \t  0.7153673496834476 \t 3.7306976893459707\n",
            "47     \t [0.98453676 0.40768679 0.23852634]. \t  0.13626639106252103 \t 3.7306976893459707\n",
            "48     \t [0.25856198 0.52084654 0.77467825]. \t  3.3395602090894543 \t 3.7306976893459707\n",
            "49     \t [0.45296663 0.78267689 0.08376546]. \t  0.006740911098779007 \t 3.7306976893459707\n",
            "50     \t [0.58567443 0.59245542 0.05416328]. \t  0.02355784289594982 \t 3.7306976893459707\n",
            "51     \t [0.59592109 0.97796742 0.08147182]. \t  0.0017791627784233446 \t 3.7306976893459707\n",
            "52     \t [0.02516885 0.60819578 0.81362045]. \t  3.592382920232589 \t 3.7306976893459707\n",
            "53     \t [0.81432498 0.1020756  0.43962605]. \t  0.2412751609616757 \t 3.7306976893459707\n",
            "54     \t [0.64815103 0.29806231 0.44364782]. \t  0.2969447837152171 \t 3.7306976893459707\n",
            "55     \t [0.56051781 0.51183617 0.49888347]. \t  0.5888244819252172 \t 3.7306976893459707\n",
            "56     \t [0.48517218 0.99134974 0.49422206]. \t  1.5385809578318683 \t 3.7306976893459707\n",
            "57     \t [0.05816553 0.45763475 0.69376408]. \t  2.153215054180613 \t 3.7306976893459707\n",
            "58     \t [0.26909419 0.74643037 0.04197797]. \t  0.004915713440641152 \t 3.7306976893459707\n",
            "59     \t [0.25910482 0.33170441 0.73636218]. \t  1.9550234372979356 \t 3.7306976893459707\n",
            "60     \t [0.86860015 0.22907423 0.02897167]. \t  0.07587325391055157 \t 3.7306976893459707\n",
            "61     \t [0.26429574 0.80177263 0.59003997]. \t  2.7847853706041117 \t 3.7306976893459707\n",
            "62     \t [0.3464018  0.83103848 0.63003205]. \t  2.4356311999821503 \t 3.7306976893459707\n",
            "63     \t [0.33390264 0.9444325  0.98442306]. \t  0.5888996464899308 \t 3.7306976893459707\n",
            "64     \t [0.71600419 0.11410615 0.07313597]. \t  0.22480895578597396 \t 3.7306976893459707\n",
            "65     \t [0.86916049 0.615922   0.70458205]. \t  1.9599722542591864 \t 3.7306976893459707\n",
            "66     \t [0.53336509 0.84486317 0.59531914]. \t  1.8388020734686554 \t 3.7306976893459707\n",
            "67     \t [0.73978342 0.65624186 0.80203376]. \t  3.109129084832267 \t 3.7306976893459707\n",
            "68     \t [0.46908484 0.29016108 0.48622976]. \t  0.32457236537058454 \t 3.7306976893459707\n",
            "69     \t [0.8055574  0.75264993 0.77470985]. \t  2.0918877698505494 \t 3.7306976893459707\n",
            "70     \t [0.71930082 0.69451456 0.87152847]. \t  3.121077480785742 \t 3.7306976893459707\n",
            "71     \t [0.23683334 0.00284899 0.28194082]. \t  0.8279866835289357 \t 3.7306976893459707\n",
            "72     \t [0.1040646  0.38199153 0.39295203]. \t  0.3873126590831428 \t 3.7306976893459707\n",
            "73     \t [0.30928002 0.42841219 0.01410685]. \t  0.054880482195661766 \t 3.7306976893459707\n",
            "74     \t [0.97649113 0.50950127 0.40146832]. \t  0.1005319691968133 \t 3.7306976893459707\n",
            "75     \t [0.8338393  0.74114505 0.04995728]. \t  0.002826701835849269 \t 3.7306976893459707\n",
            "76     \t [0.03921711 0.24384318 0.59904097]. \t  0.5135943190545953 \t 3.7306976893459707\n",
            "77     \t [0.66007928 0.85332333 0.76544453]. \t  1.4290146320216675 \t 3.7306976893459707\n",
            "78     \t [0.89236848 0.35997672 0.13321747]. \t  0.14218906731735056 \t 3.7306976893459707\n",
            "79     \t [0.78621447 0.86215128 0.27846026]. \t  0.07912757823474001 \t 3.7306976893459707\n",
            "80     \t [0.80024897 0.32727763 0.05355603]. \t  0.09341169596865143 \t 3.7306976893459707\n",
            "81     \t [0.93128779 0.68743986 0.38818922]. \t  0.13885246555159952 \t 3.7306976893459707\n",
            "82     \t [0.47095339 0.96251407 0.29195505]. \t  0.23654792306024855 \t 3.7306976893459707\n",
            "83     \t [0.14768116 0.94840828 0.44541929]. \t  1.9774934950818177 \t 3.7306976893459707\n",
            "84     \t [0.86850914 0.70841359 0.22112106]. \t  0.027852132025478437 \t 3.7306976893459707\n",
            "85     \t [0.3953896  0.57065299 0.56860651]. \t  1.3740985016426173 \t 3.7306976893459707\n",
            "86     \t [0.45464463 0.93392166 0.39989463]. \t  0.9872177351317891 \t 3.7306976893459707\n",
            "87     \t [0.83370591 0.80028876 0.03974119]. \t  0.0012445130735553001 \t 3.7306976893459707\n",
            "88     \t [0.3954636 0.4760783 0.9305186]. \t  3.0539262472087465 \t 3.7306976893459707\n",
            "89     \t [0.20014603 0.64918314 0.30455259]. \t  0.32378189219673037 \t 3.7306976893459707\n",
            "90     \t [0.81105369 0.1425165  0.27709311]. \t  0.5516169326058925 \t 3.7306976893459707\n",
            "91     \t [0.51933244 0.17350192 0.01046866]. \t  0.12509653075594532 \t 3.7306976893459707\n",
            "92     \t [0.20641137 0.07008326 0.74426415]. \t  0.43692938093535527 \t 3.7306976893459707\n",
            "93     \t [0.26813454 0.33962522 0.85618197]. \t  2.5218159508499016 \t 3.7306976893459707\n",
            "94     \t [0.34518094 0.31846439 0.72401148]. \t  1.7566215036092712 \t 3.7306976893459707\n",
            "95     \t [0.72152361 0.5511943  0.11487202]. \t  0.05310531285038199 \t 3.7306976893459707\n",
            "96     \t [0.88422074 0.77691097 0.8119611 ]. \t  2.122276340715377 \t 3.7306976893459707\n",
            "97     \t [0.88363888 0.3470758  0.85211001]. \t  2.540494522342128 \t 3.7306976893459707\n",
            "98     \t [0.8354768  0.44173748 0.23095813]. \t  0.17848714396589418 \t 3.7306976893459707\n",
            "99     \t [0.04158365 0.95493571 0.1420074 ]. \t  0.01712143442770344 \t 3.7306976893459707\n",
            "100    \t [0.87034875 0.38981482 0.60959142]. \t  0.8117138118498288 \t 3.7306976893459707\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jOqNmgSIteAw",
        "outputId": "947d24d8-307f-4772-ed0c-a831bcc2ce64"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 18\r\n",
        "\r\n",
        "np.random.seed(run_num_18)\r\n",
        "surrogate_loser_18 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "loser_18 = dGPGO(surrogate_loser_18, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_18.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.8734294  0.96854066 0.86919454]. \t  0.722531189286755 \t 1.1210522139432408\n",
            "init   \t [0.53085569 0.23272833 0.0113988 ]. \t  0.1133646794700679 \t 1.1210522139432408\n",
            "init   \t [0.43046882 0.40235136 0.52267467]. \t  0.5232646985053151 \t 1.1210522139432408\n",
            "init   \t [0.4783918  0.55535647 0.54338602]. \t  1.0390748854353227 \t 1.1210522139432408\n",
            "init   \t [0.76089558 0.71237457 0.6196821 ]. \t  1.1210522139432408 \t 1.1210522139432408\n",
            "1      \t [0.28982083 0.95201409 0.18404427]. \t  0.04220734680078784 \t 1.1210522139432408\n",
            "2      \t [0.12883542 0.98258307 0.90180597]. \t  0.6910738837771888 \t 1.1210522139432408\n",
            "3      \t [0.49148239 0.8099037  0.6976846 ]. \t  \u001b[92m1.8373634823765186\u001b[0m \t 1.8373634823765186\n",
            "4      \t [0.46136123 0.85569499 0.88610921]. \t  1.63167944843026 \t 1.8373634823765186\n",
            "5      \t [0.49454713 0.93328953 0.56170327]. \t  \u001b[92m1.9054625615284428\u001b[0m \t 1.9054625615284428\n",
            "6      \t [0.41435422 0.93277915 0.58517174]. \t  \u001b[92m2.210826503581639\u001b[0m \t 2.210826503581639\n",
            "7      \t [0.80890325 0.0109094  0.32544314]. \t  0.45214607547986396 \t 2.210826503581639\n",
            "8      \t [0.34819219 0.99739422 0.56097608]. \t  2.1934316775664433 \t 2.210826503581639\n",
            "9      \t [0. 0. 0.]. \t  0.06797411659013229 \t 2.210826503581639\n",
            "10     \t [0.43963141 0.98143234 0.73039565]. \t  1.0945002305811715 \t 2.210826503581639\n",
            "11     \t [0.02531226 0.36418907 0.95948581]. \t  1.8848202094359148 \t 2.210826503581639\n",
            "12     \t [0.9798975  0.65782747 1.        ]. \t  1.8577503855843776 \t 2.210826503581639\n",
            "13     \t [0.99970731 0.70062909 0.0567806 ]. \t  0.0027811478146734293 \t 2.210826503581639\n",
            "14     \t [0.02495978 0.79442693 0.58527601]. \t  \u001b[92m2.9049684298182346\u001b[0m \t 2.9049684298182346\n",
            "15     \t [0.08533529 0.90729061 0.49773317]. \t  2.706790721132025 \t 2.9049684298182346\n",
            "16     \t [0.5421836  0.01602986 0.01723476]. \t  0.1264373214115636 \t 2.9049684298182346\n",
            "17     \t [0.13495713 0.71802934 0.77968976]. \t  2.822467058101586 \t 2.9049684298182346\n",
            "18     \t [0.03434562 0.05142528 0.46605694]. \t  0.22909651602977216 \t 2.9049684298182346\n",
            "19     \t [0.33535147 0.59732357 0.98448116]. \t  2.350249163604855 \t 2.9049684298182346\n",
            "20     \t [0.1926461  0.75814723 0.55309241]. \t  2.740424052375752 \t 2.9049684298182346\n",
            "21     \t [0.06859238 0.70957122 0.98703068]. \t  1.888144648409867 \t 2.9049684298182346\n",
            "22     \t [0.48419506 0.01549588 0.89987476]. \t  0.22772306298785155 \t 2.9049684298182346\n",
            "23     \t [0.10489585 0.81005677 0.60815803]. \t  \u001b[92m2.927816116398253\u001b[0m \t 2.927816116398253\n",
            "24     \t [0.04431098 0.71713489 0.71201417]. \t  2.554798272340415 \t 2.927816116398253\n",
            "25     \t [0.18247832 0.87344963 0.60022047]. \t  \u001b[92m2.938319453370552\u001b[0m \t 2.938319453370552\n",
            "26     \t [0.06030413 0.11138544 0.92178117]. \t  0.49124076807692607 \t 2.938319453370552\n",
            "27     \t [0.12645112 0.80261085 0.63044693]. \t  2.800126405517988 \t 2.938319453370552\n",
            "28     \t [0.95024252 0.95266361 0.40567717]. \t  0.17505838902345658 \t 2.938319453370552\n",
            "29     \t [0.77931996 0.70884583 0.55471014]. \t  0.8143693810560588 \t 2.938319453370552\n",
            "30     \t [0.98265583 0.00159317 0.96059378]. \t  0.1290059718409104 \t 2.938319453370552\n",
            "31     \t [0.01807392 0.81368688 0.98082843]. \t  1.3266707709398322 \t 2.938319453370552\n",
            "32     \t [0.91169582 0.26398139 0.02446877]. \t  0.056762094606577974 \t 2.938319453370552\n",
            "33     \t [0.80542036 0.93073344 0.58505498]. \t  0.7409700208735187 \t 2.938319453370552\n",
            "34     \t [0.08887828 0.80400539 0.6141851 ]. \t  2.8865270941268943 \t 2.938319453370552\n",
            "35     \t [0.64262746 0.50518779 0.79424085]. \t  \u001b[92m3.4202876002328155\u001b[0m \t 3.4202876002328155\n",
            "36     \t [0.86634869 0.14229106 0.77209624]. \t  0.7956734891111156 \t 3.4202876002328155\n",
            "37     \t [0.79991674 0.35631883 0.97903321]. \t  1.5825716542459531 \t 3.4202876002328155\n",
            "38     \t [0.52897407 0.54925181 0.88068636]. \t  \u001b[92m3.7659958974425827\u001b[0m \t 3.7659958974425827\n",
            "39     \t [0.96662041 0.17014501 0.54127832]. \t  0.17694808258495845 \t 3.7659958974425827\n",
            "40     \t [0.61165389 0.07896505 0.17290464]. \t  0.6322229024122338 \t 3.7659958974425827\n",
            "41     \t [0.15815988 0.67675116 0.59988159]. \t  2.396099008710307 \t 3.7659958974425827\n",
            "42     \t [0.55307155 0.83113468 0.65867287]. \t  1.659746827776479 \t 3.7659958974425827\n",
            "43     \t [0.10993072 0.74821578 0.46707571]. \t  2.077590778069325 \t 3.7659958974425827\n",
            "44     \t [0.62939252 0.56615106 0.881394  ]. \t  3.7398318014237706 \t 3.7659958974425827\n",
            "45     \t [0.792842   0.95433978 0.03948485]. \t  0.0003512453288885632 \t 3.7659958974425827\n",
            "46     \t [0.45353138 0.15999598 0.48404493]. \t  0.29717680074648767 \t 3.7659958974425827\n",
            "47     \t [0.22384738 0.1550626  0.19208477]. \t  0.7812168459064405 \t 3.7659958974425827\n",
            "48     \t [0.48298156 0.28940149 0.37160245]. \t  0.5464451043545906 \t 3.7659958974425827\n",
            "49     \t [0.70331652 0.53653374 0.90614469]. \t  3.518286282461717 \t 3.7659958974425827\n",
            "50     \t [0.60261663 0.44378932 0.49433609]. \t  0.40890372618870746 \t 3.7659958974425827\n",
            "51     \t [0.5967522  0.98799453 0.06163179]. \t  0.0009981258980179624 \t 3.7659958974425827\n",
            "52     \t [0.71091254 0.69983393 0.23784189]. \t  0.059899164199538016 \t 3.7659958974425827\n",
            "53     \t [0.20273128 0.58872086 0.10767453]. \t  0.04954895770137224 \t 3.7659958974425827\n",
            "54     \t [0.66317934 0.93525471 0.68586192]. \t  0.9970043927416965 \t 3.7659958974425827\n",
            "55     \t [0.58764246 0.33637294 0.31484846]. \t  0.5169073769604036 \t 3.7659958974425827\n",
            "56     \t [0.86629224 0.04252853 0.05942087]. \t  0.1231852269548608 \t 3.7659958974425827\n",
            "57     \t [0.57283024 0.38759573 0.9278396 ]. \t  2.4697606835699473 \t 3.7659958974425827\n",
            "58     \t [0.27813955 0.37995999 0.75926053]. \t  2.5025501735589653 \t 3.7659958974425827\n",
            "59     \t [0.41186934 0.23513372 0.39403079]. \t  0.562511567165831 \t 3.7659958974425827\n",
            "60     \t [0.32108626 0.38704859 0.13114269]. \t  0.27577883497062416 \t 3.7659958974425827\n",
            "61     \t [0.30102575 0.18571067 0.69279238]. \t  0.7864759815082917 \t 3.7659958974425827\n",
            "62     \t [0.65089824 0.92138798 0.79668312]. \t  1.0511102182778886 \t 3.7659958974425827\n",
            "63     \t [0.0107059  0.76416342 0.50836861]. \t  2.499686990361062 \t 3.7659958974425827\n",
            "64     \t [0.77860221 0.52231521 0.39471157]. \t  0.19287222570968438 \t 3.7659958974425827\n",
            "65     \t [0.36787302 0.17657212 0.1280795 ]. \t  0.5396635722306211 \t 3.7659958974425827\n",
            "66     \t [0.79104461 0.10531206 0.92057209]. \t  0.4680412116273659 \t 3.7659958974425827\n",
            "67     \t [0.81682994 0.62089582 0.08571168]. \t  0.01656009674352917 \t 3.7659958974425827\n",
            "68     \t [0.88480013 0.54827671 0.58088944]. \t  0.6976109027858441 \t 3.7659958974425827\n",
            "69     \t [0.25229893 0.85664729 0.91213272]. \t  1.5378716112082156 \t 3.7659958974425827\n",
            "70     \t [0.50164155 0.6093058  0.23688983]. \t  0.12733227359775642 \t 3.7659958974425827\n",
            "71     \t [0.68805769 0.04971805 0.07463285]. \t  0.2312012001698145 \t 3.7659958974425827\n",
            "72     \t [0.16447824 0.05546928 0.91134554]. \t  0.31617261110468226 \t 3.7659958974425827\n",
            "73     \t [0.67234226 0.0349185  0.15057444]. \t  0.4712658901470614 \t 3.7659958974425827\n",
            "74     \t [0.3134     0.67794811 0.1465898 ]. \t  0.03974076975009795 \t 3.7659958974425827\n",
            "75     \t [0.30640295 0.92612678 0.86974016]. \t  1.119825071673948 \t 3.7659958974425827\n",
            "76     \t [0.17382888 0.31318679 0.10265048]. \t  0.26947136899597673 \t 3.7659958974425827\n",
            "77     \t [0.75990013 0.21311832 0.99415522]. \t  0.6388463637914399 \t 3.7659958974425827\n",
            "78     \t [0.89047902 0.64491292 0.32921552]. \t  0.08812881948180289 \t 3.7659958974425827\n",
            "79     \t [0.98696351 0.18431559 0.75848459]. \t  1.0022842918950319 \t 3.7659958974425827\n",
            "80     \t [0.35343701 0.18275473 0.59440178]. \t  0.3720688142423593 \t 3.7659958974425827\n",
            "81     \t [0.31539192 0.7792746  0.79955247]. \t  2.3896674916440004 \t 3.7659958974425827\n",
            "82     \t [0.14508814 0.18143484 0.64846608]. \t  0.5653094985500519 \t 3.7659958974425827\n",
            "83     \t [0.40737253 0.82678737 0.820015  ]. \t  1.9662984601658695 \t 3.7659958974425827\n",
            "84     \t [0.92766229 0.18593304 0.06503856]. \t  0.10954397157902122 \t 3.7659958974425827\n",
            "85     \t [0.32869146 0.05835603 0.47108057]. \t  0.2996102459165014 \t 3.7659958974425827\n",
            "86     \t [0.02684041 0.70539556 0.80612601]. \t  3.0104249940434444 \t 3.7659958974425827\n",
            "87     \t [0.05193446 0.06965373 0.43434944]. \t  0.3262841943372766 \t 3.7659958974425827\n",
            "88     \t [0.81004504 0.44320829 0.10766096]. \t  0.0898625192921795 \t 3.7659958974425827\n",
            "89     \t [0.61289151 0.38869684 0.43681509]. \t  0.29952453091027376 \t 3.7659958974425827\n",
            "90     \t [0.86832659 0.12322599 0.86359209]. \t  0.6746635330832248 \t 3.7659958974425827\n",
            "91     \t [0.86847025 0.1642101  0.21030744]. \t  0.4197267969060901 \t 3.7659958974425827\n",
            "92     \t [0.09465095 0.7470169  0.71808077]. \t  2.503304736154796 \t 3.7659958974425827\n",
            "93     \t [0.29236438 0.26645893 0.82895882]. \t  1.829975409816589 \t 3.7659958974425827\n",
            "94     \t [0.92417251 0.24429314 0.62430697]. \t  0.5906192293138568 \t 3.7659958974425827\n",
            "95     \t [0.55557028 0.3141471  0.73791255]. \t  1.8287978040338055 \t 3.7659958974425827\n",
            "96     \t [0.90070846 0.18634617 0.11644003]. \t  0.20613038331249936 \t 3.7659958974425827\n",
            "97     \t [0.92852018 0.18809886 0.72942986]. \t  0.9291753024647236 \t 3.7659958974425827\n",
            "98     \t [0.98478221 0.01874909 0.39303239]. \t  0.18371725118779617 \t 3.7659958974425827\n",
            "99     \t [0.5440732  0.8788577  0.07824594]. \t  0.002815235884962438 \t 3.7659958974425827\n",
            "100    \t [0.12195859 0.52278805 0.62882059]. \t  1.749290705274293 \t 3.7659958974425827\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i4HnKuoqteDo",
        "outputId": "bf56ad7b-6e6f-49e9-db04-4260e33f5cef"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 19\r\n",
        "\r\n",
        "np.random.seed(run_num_19)\r\n",
        "surrogate_loser_19 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "loser_19 = dGPGO(surrogate_loser_19, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_19.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.55944558 0.76149199 0.24453155]. \t  0.09433080659504385 \t 2.524990008735946\n",
            "init   \t [0.77168947 0.35447208 0.7966974 ]. \t  2.524990008735946 \t 2.524990008735946\n",
            "init   \t [0.34600002 0.44387444 0.30330359]. \t  0.3914194506273641 \t 2.524990008735946\n",
            "init   \t [0.44211392 0.57320936 0.06566402]. \t  0.036928944981920925 \t 2.524990008735946\n",
            "init   \t [0.02408889 0.82062263 0.36513765]. \t  0.9772047942100802 \t 2.524990008735946\n",
            "1      \t [0.96499061 0.19185662 0.98563058]. \t  0.5815385407774518 \t 2.524990008735946\n",
            "2      \t [0.35021227 0.89949578 0.95419986]. \t  0.9824452595236306 \t 2.524990008735946\n",
            "3      \t [0.8364035  0.47078086 0.70932116]. \t  2.156884519592091 \t 2.524990008735946\n",
            "4      \t [0.46047313 0.25352007 0.98033154]. \t  0.9460814398415793 \t 2.524990008735946\n",
            "5      \t [0.87336482 0.09118839 0.45818428]. \t  0.17563272101768643 \t 2.524990008735946\n",
            "6      \t [0.70411181 0.42279097 0.83675882]. \t  \u001b[92m3.2477068869485466\u001b[0m \t 3.2477068869485466\n",
            "7      \t [0.74803474 0.60533411 0.93779535]. \t  3.0703841077028233 \t 3.2477068869485466\n",
            "8      \t [0.57560304 0.47085767 0.99523987]. \t  1.978350792868597 \t 3.2477068869485466\n",
            "9      \t [0.8489015  0.05888081 0.        ]. \t  0.05678630591260754 \t 3.2477068869485466\n",
            "10     \t [0.57851022 0.66686718 0.73448346]. \t  2.4468307948849954 \t 3.2477068869485466\n",
            "11     \t [0.01381987 0.98972179 0.81693901]. \t  0.8543229768829992 \t 3.2477068869485466\n",
            "12     \t [0.23320797 0.98878694 0.03639898]. \t  0.0008881824743265614 \t 3.2477068869485466\n",
            "13     \t [0.98986012 0.68494222 0.00835794]. \t  0.0016982651496915456 \t 3.2477068869485466\n",
            "14     \t [0.49101384 0.3773169  0.69597041]. \t  1.7869436007858153 \t 3.2477068869485466\n",
            "15     \t [0.55409487 0.02109967 0.87866785]. \t  0.26380236860584677 \t 3.2477068869485466\n",
            "16     \t [0.01606225 0.5779807  0.00526043]. \t  0.01062082046094439 \t 3.2477068869485466\n",
            "17     \t [0.99767452 0.26448242 0.08286597]. \t  0.08857016652594893 \t 3.2477068869485466\n",
            "18     \t [0.74305045 0.94567471 0.91399927]. \t  0.8206436917127765 \t 3.2477068869485466\n",
            "19     \t [0.00140369 0.98879627 0.22223029]. \t  0.09232469302045357 \t 3.2477068869485466\n",
            "20     \t [0.01163211 0.08638709 0.6501099 ]. \t  0.30126735795355003 \t 3.2477068869485466\n",
            "21     \t [0.79458708 0.54059858 0.95886999]. \t  2.764205728180439 \t 3.2477068869485466\n",
            "22     \t [0.68655261 0.47258864 0.84533407]. \t  \u001b[92m3.58096390901099\u001b[0m \t 3.58096390901099\n",
            "23     \t [0.09645332 0.95171827 0.70824437]. \t  1.7335812415162595 \t 3.58096390901099\n",
            "24     \t [0.29698544 0.17077213 0.37241439]. \t  0.6982054319324602 \t 3.58096390901099\n",
            "25     \t [0. 0. 0.]. \t  0.06797411659013229 \t 3.58096390901099\n",
            "26     \t [0.05937236 0.02006886 0.20927426]. \t  0.617373006092504 \t 3.58096390901099\n",
            "27     \t [0.56925757 0.90150448 0.25170707]. \t  0.10227822235163753 \t 3.58096390901099\n",
            "28     \t [0.05486895 0.31756584 0.13401811]. \t  0.2926632371936038 \t 3.58096390901099\n",
            "29     \t [0.50329508 0.22209549 0.64275971]. \t  0.6699701094542871 \t 3.58096390901099\n",
            "30     \t [0.61332035 0.09483347 0.84810284]. \t  0.563234333851149 \t 3.58096390901099\n",
            "31     \t [0.8527929  0.94871274 0.00283516]. \t  0.00011822558074294612 \t 3.58096390901099\n",
            "32     \t [0.56351103 0.61250042 0.93764616]. \t  3.0875063529706157 \t 3.58096390901099\n",
            "33     \t [0.2471381  0.01618808 0.14396125]. \t  0.547463513490101 \t 3.58096390901099\n",
            "34     \t [0.97547624 0.00351099 0.13726235]. \t  0.17554002587598122 \t 3.58096390901099\n",
            "35     \t [0.01895849 0.29888338 0.15063556]. \t  0.33152174386330074 \t 3.58096390901099\n",
            "36     \t [0.04503366 0.22594075 0.7378816 ]. \t  1.2153461880632406 \t 3.58096390901099\n",
            "37     \t [0.28985676 0.99895607 0.97563001]. \t  0.40933801513369666 \t 3.58096390901099\n",
            "38     \t [0.82309201 0.72860314 0.42102276]. \t  0.3286530780947166 \t 3.58096390901099\n",
            "39     \t [0.42547655 0.0600468  0.47533445]. \t  0.28650314809862054 \t 3.58096390901099\n",
            "40     \t [0.12523936 0.71993174 0.23134444]. \t  0.12419919234462012 \t 3.58096390901099\n",
            "41     \t [0.96160152 0.36463765 0.76840526]. \t  2.3586836952155927 \t 3.58096390901099\n",
            "42     \t [0.19076268 0.78430101 0.32881315]. \t  0.5982041585220237 \t 3.58096390901099\n",
            "43     \t [0.40813466 0.66810217 0.94607042]. \t  2.739423609810528 \t 3.58096390901099\n",
            "44     \t [0.9962501  0.90959763 0.21364914]. \t  0.009053813831887955 \t 3.58096390901099\n",
            "45     \t [0.89691926 0.09320847 0.15243687]. \t  0.2900082957339025 \t 3.58096390901099\n",
            "46     \t [0.08587334 0.74904582 0.5635006 ]. \t  2.7671860688773733 \t 3.58096390901099\n",
            "47     \t [0.08933768 0.62692057 0.7161244 ]. \t  2.717688340274245 \t 3.58096390901099\n",
            "48     \t [0.77119201 0.03859479 0.34265034]. \t  0.4890468547987831 \t 3.58096390901099\n",
            "49     \t [0.23163943 0.83329578 0.03876278]. \t  0.0021264459665956 \t 3.58096390901099\n",
            "50     \t [0.22967869 0.13702735 0.05796596]. \t  0.2523975141805462 \t 3.58096390901099\n",
            "51     \t [0.11237253 0.65966214 0.42607608]. \t  1.1995499965462453 \t 3.58096390901099\n",
            "52     \t [0.40230869 0.63370855 0.44560545]. \t  0.9785715601738311 \t 3.58096390901099\n",
            "53     \t [0.63854359 0.86124614 0.40485685]. \t  0.6643958568560102 \t 3.58096390901099\n",
            "54     \t [0.08013076 0.54133813 0.42400428]. \t  0.6870736161624138 \t 3.58096390901099\n",
            "55     \t [0.47446429 0.75289979 0.96713271]. \t  1.9044832473946198 \t 3.58096390901099\n",
            "56     \t [0.63207101 0.35509364 0.00158214]. \t  0.055427795107536884 \t 3.58096390901099\n",
            "57     \t [0.01274147 0.03203166 0.9689726 ]. \t  0.16871854597066624 \t 3.58096390901099\n",
            "58     \t [0.96799084 0.6911544  0.91876818]. \t  2.8023998550294 \t 3.58096390901099\n",
            "59     \t [0.9030704  0.69926889 0.30648406]. \t  0.06705165906432335 \t 3.58096390901099\n",
            "60     \t [0.37250422 0.30495954 0.09332783]. \t  0.28344377600126297 \t 3.58096390901099\n",
            "61     \t [0.1500338  0.2151008  0.13895462]. \t  0.48014866379633925 \t 3.58096390901099\n",
            "62     \t [0.72429142 0.87225717 0.0389365 ]. \t  0.0008069837359671329 \t 3.58096390901099\n",
            "63     \t [0.86377943 0.88216093 0.87175809]. \t  1.329082134525974 \t 3.58096390901099\n",
            "64     \t [0.52780961 0.26488685 0.19576341]. \t  0.6398531580442676 \t 3.58096390901099\n",
            "65     \t [0.60830622 0.53552546 0.48498891]. \t  0.5388088371927547 \t 3.58096390901099\n",
            "66     \t [0.43756068 0.48625546 0.57947842]. \t  1.0478624634428502 \t 3.58096390901099\n",
            "67     \t [0.5947457  0.26791511 0.43075178]. \t  0.3584817684980289 \t 3.58096390901099\n",
            "68     \t [0.3857746  0.28585579 0.81923471]. \t  2.0091479654578412 \t 3.58096390901099\n",
            "69     \t [0.36378236 0.43626916 0.27521403]. \t  0.3957733306512729 \t 3.58096390901099\n",
            "70     \t [0.31011784 0.11476409 0.94703201]. \t  0.4384406165894201 \t 3.58096390901099\n",
            "71     \t [0.96106242 0.25484353 0.2852708 ]. \t  0.28735363514336076 \t 3.58096390901099\n",
            "72     \t [0.67275161 0.90584988 0.19440821]. \t  0.024581555798376124 \t 3.58096390901099\n",
            "73     \t [0.42856543 0.24123473 0.06572751]. \t  0.2506139230144343 \t 3.58096390901099\n",
            "74     \t [0.11795803 0.82308563 0.88447371]. \t  1.9627816582310302 \t 3.58096390901099\n",
            "75     \t [0.63786628 0.39515356 0.74653427]. \t  2.4223442758118603 \t 3.58096390901099\n",
            "76     \t [0.24354281 0.26780065 0.85639567]. \t  1.8094612962111003 \t 3.58096390901099\n",
            "77     \t [0.66767182 0.18181134 0.57205183]. \t  0.28984354895333153 \t 3.58096390901099\n",
            "78     \t [0.30974426 0.6692461  0.01863442]. \t  0.007649779620783134 \t 3.58096390901099\n",
            "79     \t [0.39396999 0.01359379 0.42209846]. \t  0.44288926847849175 \t 3.58096390901099\n",
            "80     \t [0.00622778 0.70489465 0.76489267]. \t  2.78904023721274 \t 3.58096390901099\n",
            "81     \t [0.18417019 0.65820939 0.99740273]. \t  1.9631796307825014 \t 3.58096390901099\n",
            "82     \t [0.58584054 0.3550544  0.10439691]. \t  0.2224736789213607 \t 3.58096390901099\n",
            "83     \t [0.87431571 0.63954261 0.41195434]. \t  0.19574496210540726 \t 3.58096390901099\n",
            "84     \t [0.69349169 0.58534044 0.62972631]. \t  1.3302088711861788 \t 3.58096390901099\n",
            "85     \t [0.15209794 0.15202414 0.68155881]. \t  0.5943639086588272 \t 3.58096390901099\n",
            "86     \t [0.79200461 0.63794397 0.74960302]. \t  2.5584025352025037 \t 3.58096390901099\n",
            "87     \t [0.34459962 0.41262583 0.27340987]. \t  0.4449259337296729 \t 3.58096390901099\n",
            "88     \t [0.20535494 0.65975039 0.7603884 ]. \t  3.0016424961664874 \t 3.58096390901099\n",
            "89     \t [0.46426209 0.42973631 0.24545965]. \t  0.3772305907675988 \t 3.58096390901099\n",
            "90     \t [0.19736085 0.98220752 0.1427098 ]. \t  0.016314629956979514 \t 3.58096390901099\n",
            "91     \t [0.66980287 0.41687428 0.04706623]. \t  0.07243416592273004 \t 3.58096390901099\n",
            "92     \t [0.04429268 0.08927876 0.88714029]. \t  0.47583883365716306 \t 3.58096390901099\n",
            "93     \t [0.79558746 0.93614399 0.12043335]. \t  0.0028177863330248122 \t 3.58096390901099\n",
            "94     \t [0.41132162 0.13810393 0.80963589]. \t  0.8209804264571146 \t 3.58096390901099\n",
            "95     \t [0.6393976  0.57288184 0.79072809]. \t  3.3957518147380443 \t 3.58096390901099\n",
            "96     \t [0.40173052 0.16642951 0.80461023]. \t  1.0008823398027678 \t 3.58096390901099\n",
            "97     \t [0.91045104 0.13395119 0.72920788]. \t  0.6572687768834146 \t 3.58096390901099\n",
            "98     \t [0.98245798 0.35708702 0.41811415]. \t  0.12973870484146707 \t 3.58096390901099\n",
            "99     \t [0.38571383 0.86707224 0.2595399 ]. \t  0.17830519849965276 \t 3.58096390901099\n",
            "100    \t [0.00128665 0.16274988 0.01869641]. \t  0.10224037162700153 \t 3.58096390901099\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FfiDmH_bteGg",
        "outputId": "3ba96ddc-107a-4448-90cf-38ed1e5b1f75"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 20\r\n",
        "\r\n",
        "np.random.seed(run_num_20)\r\n",
        "surrogate_loser_20 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "loser_20 = dGPGO(surrogate_loser_20, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_20.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.57051729 0.56452876 0.48844183]. \t  0.675391399411646 \t 0.675391399411646\n",
            "init   \t [0.33647775 0.37586818 0.53203587]. \t  0.5331349538596052 \t 0.675391399411646\n",
            "init   \t [0.06810629 0.58452906 0.23789776]. \t  0.14747335095307315 \t 0.675391399411646\n",
            "init   \t [0.16075658 0.15211915 0.12706922]. \t  0.48089725804912437 \t 0.675391399411646\n",
            "init   \t [0.32744117 0.69415387 0.35896647]. \t  0.6289408047543804 \t 0.675391399411646\n",
            "1      \t [0.837611   0.97862966 0.10314233]. \t  0.0013418779384681767 \t 0.675391399411646\n",
            "2      \t [0.4192146  0.99013113 0.80759551]. \t  \u001b[92m0.8040311121195758\u001b[0m \t 0.8040311121195758\n",
            "3      \t [0.58490171 0.         0.        ]. \t  0.08889155882157919 \t 0.8040311121195758\n",
            "4      \t [0.56803068 0.99098437 0.50767036]. \t  \u001b[92m1.310904866486966\u001b[0m \t 1.310904866486966\n",
            "5      \t [0.84822682 0.9788853  0.85466061]. \t  0.6702188994918671 \t 1.310904866486966\n",
            "6      \t [0.84509259 0.         0.89862994]. \t  0.19194384543565085 \t 1.310904866486966\n",
            "7      \t [0.67407812 0.99571082 0.58414561]. \t  1.010438400626982 \t 1.310904866486966\n",
            "8      \t [0.25706606 0.99173014 0.15932733]. \t  0.02275764294278305 \t 1.310904866486966\n",
            "9      \t [0.         0.         0.31158648]. \t  0.5470921834405232 \t 1.310904866486966\n",
            "10     \t [0.97947894 0.27434002 0.93560419]. \t  \u001b[92m1.395891124057239\u001b[0m \t 1.395891124057239\n",
            "11     \t [0.87557865 0.50601557 0.95614793]. \t  \u001b[92m2.7139777944223322\u001b[0m \t 2.7139777944223322\n",
            "12     \t [0.75793425 0.69772209 0.98756189]. \t  1.927540683132728 \t 2.7139777944223322\n",
            "13     \t [0.12414169 0.99832635 0.576206  ]. \t  2.566437553023501 \t 2.7139777944223322\n",
            "14     \t [0.75104891 0.2645525  0.9551541 ]. \t  1.199937865485019 \t 2.7139777944223322\n",
            "15     \t [0.97780808 0.58615943 0.81664382]. \t  \u001b[92m3.457183993275378\u001b[0m \t 3.457183993275378\n",
            "16     \t [0.98736861 0.72749542 0.80627068]. \t  2.471770357321162 \t 3.457183993275378\n",
            "17     \t [0.89238206 0.48690832 0.72463641]. \t  2.3802531328000907 \t 3.457183993275378\n",
            "18     \t [0.0105289  0.05708276 0.89134199]. \t  0.34912691446217564 \t 3.457183993275378\n",
            "19     \t [0.37352692 0.01567742 0.7968013 ]. \t  0.2930995645423669 \t 3.457183993275378\n",
            "20     \t [0.03867006 0.69475133 0.7534113 ]. \t  2.7906836037923957 \t 3.457183993275378\n",
            "21     \t [0.02486294 0.57208497 0.57980776]. \t  1.6658299552603169 \t 3.457183993275378\n",
            "22     \t [0.04885327 0.69232283 0.89481865]. \t  3.0944266830502576 \t 3.457183993275378\n",
            "23     \t [0.00853456 0.53126337 0.97882715]. \t  2.411907692500336 \t 3.457183993275378\n",
            "24     \t [0.0286142  0.81411405 0.82827108]. \t  2.131555367069342 \t 3.457183993275378\n",
            "25     \t [1.05734595e-14 1.67322102e-14 2.84724772e-14]. \t  0.06797411659016761 \t 3.457183993275378\n",
            "26     \t [0.23771443 0.50552904 0.94685943]. \t  2.927759953411534 \t 3.457183993275378\n",
            "27     \t [0.98694874 0.41002003 0.01130985]. \t  0.018869161976546287 \t 3.457183993275378\n",
            "28     \t [0.16058444 0.61398367 0.90177234]. \t  \u001b[92m3.5144420137944152\u001b[0m \t 3.5144420137944152\n",
            "29     \t [0.20739069 0.06601235 0.08339898]. \t  0.3266638276141411 \t 3.5144420137944152\n",
            "30     \t [0.09177414 0.25874783 0.46582282]. \t  0.3098176725068744 \t 3.5144420137944152\n",
            "31     \t [0.59223853 0.49019339 0.514645  ]. \t  0.5636584653373499 \t 3.5144420137944152\n",
            "32     \t [0.72667423 0.33162898 0.13623505]. \t  0.25692809763748053 \t 3.5144420137944152\n",
            "33     \t [0.82268004 0.38351431 0.59432374]. \t  0.6940951772220839 \t 3.5144420137944152\n",
            "34     \t [0.68789817 0.31596021 0.63752013]. \t  0.9215222723989691 \t 3.5144420137944152\n",
            "35     \t [0.57752401 0.80350725 0.18556134]. \t  0.0312731366325119 \t 3.5144420137944152\n",
            "36     \t [0.19787002 0.1977767  0.83017448]. \t  1.2277701990258327 \t 3.5144420137944152\n",
            "37     \t [0.16067943 0.02817582 0.75723975]. \t  0.3137567177172803 \t 3.5144420137944152\n",
            "38     \t [0.72541457 0.57787923 0.33583694]. \t  0.16900693522232732 \t 3.5144420137944152\n",
            "39     \t [0.47827067 0.1364753  0.25670492]. \t  0.9585843576107357 \t 3.5144420137944152\n",
            "40     \t [0.23343479 0.59138584 0.30642539]. \t  0.3000119022807604 \t 3.5144420137944152\n",
            "41     \t [0.43976237 0.92250638 0.44201957]. \t  1.447177776885933 \t 3.5144420137944152\n",
            "42     \t [0.01103052 0.19701301 0.73397797]. \t  1.013933980043001 \t 3.5144420137944152\n",
            "43     \t [0.24307251 0.26563776 0.48045205]. \t  0.33360810781241124 \t 3.5144420137944152\n",
            "44     \t [0.26541441 0.20442255 0.21154234]. \t  0.8182007654284428 \t 3.5144420137944152\n",
            "45     \t [0.17598597 0.39637633 0.93595256]. \t  2.4420997621465594 \t 3.5144420137944152\n",
            "46     \t [0.37043523 0.84876641 0.75157091]. \t  1.8070542759678743 \t 3.5144420137944152\n",
            "47     \t [0.65497411 0.21224035 0.76263526]. \t  1.2312315716964708 \t 3.5144420137944152\n",
            "48     \t [0.40726517 0.24250585 0.70979656]. \t  1.1742944616992397 \t 3.5144420137944152\n",
            "49     \t [0.37505632 0.11231425 0.63764472]. \t  0.3404462066127 \t 3.5144420137944152\n",
            "50     \t [0.06155421 0.86108384 0.97645898]. \t  1.069465683646719 \t 3.5144420137944152\n",
            "51     \t [0.64534578 0.18739021 0.25470415]. \t  0.7539909915086813 \t 3.5144420137944152\n",
            "52     \t [0.28313966 0.41353302 0.27735699]. \t  0.4382617694804395 \t 3.5144420137944152\n",
            "53     \t [0.38503878 0.50627476 0.87408068]. \t  \u001b[92m3.727265148396073\u001b[0m \t 3.727265148396073\n",
            "54     \t [0.65515322 0.48343052 0.95854661]. \t  2.6390659543517048 \t 3.727265148396073\n",
            "55     \t [0.35869692 0.89270219 0.01228564]. \t  0.0007101787817227387 \t 3.727265148396073\n",
            "56     \t [0.61181179 0.36417187 0.10662379]. \t  0.20987490090205413 \t 3.727265148396073\n",
            "57     \t [0.93400767 0.1910905  0.00216429]. \t  0.04407628331929119 \t 3.727265148396073\n",
            "58     \t [0.26830483 0.30305693 0.19889753]. \t  0.5988107042239119 \t 3.727265148396073\n",
            "59     \t [0.27043163 0.2608227  0.27860543]. \t  0.7938419092915375 \t 3.727265148396073\n",
            "60     \t [0.34468722 0.65659973 0.86325841]. \t  3.5060239611180912 \t 3.727265148396073\n",
            "61     \t [0.16877957 0.60131644 0.76137919]. \t  3.1874531127871046 \t 3.727265148396073\n",
            "62     \t [0.8598495  0.09687106 0.70544899]. \t  0.45449322998722325 \t 3.727265148396073\n",
            "63     \t [0.16356561 0.91951492 0.89582497]. \t  1.104444384511528 \t 3.727265148396073\n",
            "64     \t [0.09449532 0.3713226  0.38575202]. \t  0.3889635066766884 \t 3.727265148396073\n",
            "65     \t [0.95270096 0.07614477 0.80783348]. \t  0.4895141696876869 \t 3.727265148396073\n",
            "66     \t [0.97256952 0.53652074 0.09495664]. \t  0.023834490049896122 \t 3.727265148396073\n",
            "67     \t [0.04743825 0.43780258 0.59339554]. \t  1.092959659135892 \t 3.727265148396073\n",
            "68     \t [0.37743864 0.46176713 0.30895875]. \t  0.36325840663334485 \t 3.727265148396073\n",
            "69     \t [0.20933675 0.14745675 0.3301066 ]. \t  0.8199469925635752 \t 3.727265148396073\n",
            "70     \t [0.09873953 0.08344149 0.80692709]. \t  0.5296855557748921 \t 3.727265148396073\n",
            "71     \t [0.81629061 0.76645914 0.85429327]. \t  2.4207171980182767 \t 3.727265148396073\n",
            "72     \t [0.50729649 0.54241612 0.82395397]. \t  \u001b[92m3.7483005109548158\u001b[0m \t 3.7483005109548158\n",
            "73     \t [0.84389819 0.06320748 0.9842202 ]. \t  0.20065465243633962 \t 3.7483005109548158\n",
            "74     \t [0.23833763 0.14661407 0.50546705]. \t  0.25480671405943567 \t 3.7483005109548158\n",
            "75     \t [0.42865986 0.34679638 0.33070755]. \t  0.5504560083351115 \t 3.7483005109548158\n",
            "76     \t [0.49552011 0.19991211 0.82820314]. \t  1.2497044559822954 \t 3.7483005109548158\n",
            "77     \t [0.41202769 0.53309334 0.45145642]. \t  0.6403108764028697 \t 3.7483005109548158\n",
            "78     \t [0.96633121 0.2334966  0.14465115]. \t  0.19060165348368643 \t 3.7483005109548158\n",
            "79     \t [0.83820039 0.8749031  0.41691695]. \t  0.3498651032944783 \t 3.7483005109548158\n",
            "80     \t [0.86449676 0.79579375 0.91435755]. \t  1.9827797305941421 \t 3.7483005109548158\n",
            "81     \t [0.93400194 0.03894668 0.94800781]. \t  0.2100098811139738 \t 3.7483005109548158\n",
            "82     \t [0.00313698 0.46054941 0.47015367]. \t  0.5733329196058139 \t 3.7483005109548158\n",
            "83     \t [0.29671487 0.24949484 0.68979245]. \t  1.0839801349581415 \t 3.7483005109548158\n",
            "84     \t [0.76412824 0.85355953 0.26043253]. \t  0.06421503493392128 \t 3.7483005109548158\n",
            "85     \t [0.72595035 0.82629906 0.78520751]. \t  1.643169958970201 \t 3.7483005109548158\n",
            "86     \t [0.21010636 0.94073467 0.28099994]. \t  0.2948632701610779 \t 3.7483005109548158\n",
            "87     \t [0.47684429 0.79057807 0.68083692]. \t  1.937658177452781 \t 3.7483005109548158\n",
            "88     \t [7.93175689e-01 2.53034714e-01 1.34429246e-04]. \t  0.05690454852728574 \t 3.7483005109548158\n",
            "89     \t [0.20677644 0.64188221 0.66110577]. \t  2.3710065060725087 \t 3.7483005109548158\n",
            "90     \t [0.59187883 0.47656673 0.53049912]. \t  0.5999908176225892 \t 3.7483005109548158\n",
            "91     \t [0.01786845 0.04463263 0.66603289]. \t  0.24235692844368947 \t 3.7483005109548158\n",
            "92     \t [0.58293223 0.27863226 0.96653618]. \t  1.2122270629110494 \t 3.7483005109548158\n",
            "93     \t [0.86670758 0.64452272 0.24326527]. \t  0.04633779647468605 \t 3.7483005109548158\n",
            "94     \t [0.23969353 0.81773721 0.53651922]. \t  2.8242970536633676 \t 3.7483005109548158\n",
            "95     \t [0.30600453 0.74269544 0.00722497]. \t  0.002870525365569 \t 3.7483005109548158\n",
            "96     \t [0.84062088 0.07715726 0.05192499]. \t  0.1255534664247606 \t 3.7483005109548158\n",
            "97     \t [0.0562497  0.94139369 0.10719894]. \t  0.007371544767094565 \t 3.7483005109548158\n",
            "98     \t [0.10695383 0.05917109 0.42824318]. \t  0.37253454997322105 \t 3.7483005109548158\n",
            "99     \t [0.16625909 0.72276611 0.68183679]. \t  2.5378207743566707 \t 3.7483005109548158\n",
            "100    \t [0.49060885 0.6520218  0.17341417]. \t  0.05712173825425049 \t 3.7483005109548158\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9kM4bcwSteJO",
        "outputId": "3e3adf37-91e0-412f-d262-132f00324497"
      },
      "source": [
        "end_lose = time.time()\r\n",
        "end_lose\r\n",
        "\r\n",
        "time_lose = end_lose - start_lose\r\n",
        "time_lose\r\n",
        "\r\n",
        "start_win = time.time()\r\n",
        "start_win"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1613725409.1479833"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8NMzLM7nteQH",
        "outputId": "881a9e0c-cb2f-4b51-aa3e-c68b17071b25"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 1 \r\n",
        "\r\n",
        "np.random.seed(run_num_1)\r\n",
        "surrogate_winner_1 = dtStudentProcess(d_cov_func, nu = df)\r\n",
        "\r\n",
        "winner_1 = dGPGO_stp(surrogate_winner_1, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_1.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.61217018 0.16906975 0.43605902]. \t  0.37345684462559386 \t 2.3951473341797507\n",
            "init   \t [0.76926247 0.2953253  0.14916296]. \t  0.296162062221701 \t 2.3951473341797507\n",
            "init   \t [0.02247832 0.42022449 0.23868214]. \t  0.2904113287153621 \t 2.3951473341797507\n",
            "init   \t [0.33765619 0.99071246 0.23772645]. \t  0.11013785080555143 \t 2.3951473341797507\n",
            "init   \t [0.08119266 0.66960024 0.62124292]. \t  2.3951473341797507 \t 2.3951473341797507\n",
            "1      \t [1. 1. 1.]. \t  0.31688362070415665 \t 2.3951473341797507\n",
            "2      \t [0.06043968 0.93959724 0.93680048]. \t  0.8159694620909308 \t 2.3951473341797507\n",
            "3      \t [0.15721395 0.18840441 0.9937264 ]. \t  0.5424005963869378 \t 2.3951473341797507\n",
            "4      \t [0.98784352 0.41322054 0.77223499]. \t  \u001b[92m2.694929315346602\u001b[0m \t 2.694929315346602\n",
            "5      \t [0.98182536 0.86519171 0.35977712]. \t  0.09962299605130942 \t 2.694929315346602\n",
            "6      \t [0.62982928 0.55846488 1.        ]. \t  2.073126909058266 \t 2.694929315346602\n",
            "7      \t [0.86133272 0.02398225 0.01774984]. \t  0.06840903089257366 \t 2.694929315346602\n",
            "8      \t [1.         0.31628966 1.        ]. \t  1.0855433487775297 \t 2.694929315346602\n",
            "9      \t [0.00463698 0.91765106 0.34106446]. \t  0.7252518770302266 \t 2.694929315346602\n",
            "10     \t [0.0171388  0.49996885 0.95713766]. \t  \u001b[92m2.7097234110494783\u001b[0m \t 2.7097234110494783\n",
            "11     \t [0.99771704 0.09710732 0.40644106]. \t  0.17689893728090691 \t 2.7097234110494783\n",
            "12     \t [0.03845036 0.02318382 0.63693803]. \t  0.1681242498375306 \t 2.7097234110494783\n",
            "13     \t [0.02039239 0.00959954 0.12446507]. \t  0.33562676449223156 \t 2.7097234110494783\n",
            "14     \t [0.65228008 1.         0.67835417]. \t  0.8298466905863778 \t 2.7097234110494783\n",
            "15     \t [0.99962872 0.66217227 0.0082572 ]. \t  0.00209607984584796 \t 2.7097234110494783\n",
            "16     \t [0.42634223 0.56247224 0.        ]. \t  0.016040159768401487 \t 2.7097234110494783\n",
            "17     \t [1.         0.67031455 0.90663565]. \t  \u001b[92m3.037333824572172\u001b[0m \t 3.037333824572172\n",
            "18     \t [0.69712585 0.6785152  0.61239848]. \t  1.2356669002214458 \t 3.037333824572172\n",
            "19     \t [0.88703559 0.94561824 0.05102635]. \t  0.0003439409731806934 \t 3.037333824572172\n",
            "20     \t [0.         0.77263701 0.        ]. \t  0.0013154422838324048 \t 3.037333824572172\n",
            "21     \t [0.44855 1.      1.     ]. \t  0.33313383110560796 \t 3.037333824572172\n",
            "22     \t [0.26074899 0.46048002 0.84360366]. \t  \u001b[92m3.559609028243813\u001b[0m \t 3.559609028243813\n",
            "23     \t [0.27670214 0.58226199 0.99551306]. \t  2.1633285515671576 \t 3.559609028243813\n",
            "24     \t [0.         0.18291483 0.        ]. \t  0.0746342648585666 \t 3.559609028243813\n",
            "25     \t [4.00233546e-01 1.20843046e-10 5.13791244e-03]. \t  0.11061462359099535 \t 3.559609028243813\n",
            "26     \t [0.14899862 0.37302288 0.78504588]. \t  2.6505203010874494 \t 3.559609028243813\n",
            "27     \t [0.58519976 0.08306968 0.92397612]. \t  0.3819933614864243 \t 3.559609028243813\n",
            "28     \t [0.99308826 0.02844135 0.81174796]. \t  0.3186508417293715 \t 3.559609028243813\n",
            "29     \t [2.26306909e-01 9.81221260e-01 3.11460686e-04]. \t  0.00031687642451543475 \t 3.559609028243813\n",
            "30     \t [0.32339285 0.77407455 0.81099656]. \t  2.4590052071464976 \t 3.559609028243813\n",
            "31     \t [0.47000814 0.45829877 0.77596508]. \t  3.1189368273749043 \t 3.559609028243813\n",
            "32     \t [0.9956356  0.99999997 0.77314007]. \t  0.43688101703474513 \t 3.559609028243813\n",
            "33     \t [0.26505249 0.         0.618948  ]. \t  0.1304409772570526 \t 3.559609028243813\n",
            "34     \t [0.56643285 0.92188569 0.00612328]. \t  0.0003642619010635195 \t 3.559609028243813\n",
            "35     \t [0.29491014 0.99622021 0.75832255]. \t  1.0368818951031464 \t 3.559609028243813\n",
            "36     \t [0.01366693 0.7846565  0.73124634]. \t  2.3193724818425965 \t 3.559609028243813\n",
            "37     \t [9.93255839e-01 1.49510172e-01 3.69849210e-04]. \t  0.03624054759257532 \t 3.559609028243813\n",
            "38     \t [0. 0. 0.]. \t  0.06797411659013229 \t 3.559609028243813\n",
            "39     \t [0.81284988 0.34529294 0.84330539]. \t  2.5493228517934905 \t 3.559609028243813\n",
            "40     \t [6.20173160e-01 3.02330781e-01 1.55958268e-09]. \t  0.06881777309675606 \t 3.559609028243813\n",
            "41     \t [0.16929119 0.54636084 0.88184887]. \t  \u001b[92m3.7623581112453506\u001b[0m \t 3.7623581112453506\n",
            "42     \t [0.13519666 0.00774926 0.9234129 ]. \t  0.18354705405777533 \t 3.7623581112453506\n",
            "43     \t [0.76500065 0.00536374 0.65831531]. \t  0.16531193738809585 \t 3.7623581112453506\n",
            "44     \t [0.86566154 0.7340515  0.9186304 ]. \t  2.4997174782692624 \t 3.7623581112453506\n",
            "45     \t [0.99999917 0.60129154 0.56114393]. \t  0.4835783690142424 \t 3.7623581112453506\n",
            "46     \t [0.24772762 0.54937467 0.83960919]. \t  \u001b[92m3.8432965347174646\u001b[0m \t 3.8432965347174646\n",
            "47     \t [0.44901018 0.56797468 0.87370157]. \t  3.8063049397209934 \t 3.8432965347174646\n",
            "48     \t [0.04340784 0.52459186 0.90000597]. \t  3.561646427154672 \t 3.8432965347174646\n",
            "49     \t [0.30558483 0.63264275 0.89111665]. \t  3.5310509527739695 \t 3.8432965347174646\n",
            "50     \t [0.00395011 0.50553556 0.8749233 ]. \t  3.6705417874251465 \t 3.8432965347174646\n",
            "51     \t [0.45033973 0.6061437  0.85493005]. \t  3.753688653015401 \t 3.8432965347174646\n",
            "52     \t [0.44882985 0.47201178 0.84380418]. \t  3.6209243685953503 \t 3.8432965347174646\n",
            "53     \t [0.26212341 0.54450552 0.85321929]. \t  \u001b[92m3.8569970369205837\u001b[0m \t 3.8569970369205837\n",
            "54     \t [0.58134218 0.51852794 0.88473422]. \t  3.6847088628110796 \t 3.8569970369205837\n",
            "55     \t [0.52904887 0.53032519 0.91471176]. \t  3.4454759547320055 \t 3.8569970369205837\n",
            "56     \t [0.07699087 0.50775658 0.87446323]. \t  3.7004565592772956 \t 3.8569970369205837\n",
            "57     \t [0.00281792 0.9722953  0.04792056]. \t  0.0012897774933416997 \t 3.8569970369205837\n",
            "58     \t [0.4073267  0.58302285 0.90208244]. \t  3.6022454628205707 \t 3.8569970369205837\n",
            "59     \t [0.01024721 0.46253483 0.84142556]. \t  3.524568944136317 \t 3.8569970369205837\n",
            "60     \t [0.07414375 0.54304308 0.82810303]. \t  3.77544594612987 \t 3.8569970369205837\n",
            "61     \t [0.0504329  0.68399416 0.86070774]. \t  3.2947751327981454 \t 3.8569970369205837\n",
            "62     \t [0.18486576 0.5247528  0.80260636]. \t  3.6140431162896736 \t 3.8569970369205837\n",
            "63     \t [5.68275160e-01 5.86693527e-08 1.96071682e-01]. \t  0.6647699026717498 \t 3.8569970369205837\n",
            "64     \t [0.15112422 0.57169865 0.8695877 ]. \t  3.8131284252695528 \t 3.8569970369205837\n",
            "65     \t [0.40411052 0.57824214 0.87792902]. \t  3.7806394894875766 \t 3.8569970369205837\n",
            "66     \t [0.15043162 0.60970521 0.85780357]. \t  3.7470332461824105 \t 3.8569970369205837\n",
            "67     \t [0.48825941 0.54786943 0.84111149]. \t  3.825718612689603 \t 3.8569970369205837\n",
            "68     \t [0.50699119 0.56642632 0.82044433]. \t  3.718089006813016 \t 3.8569970369205837\n",
            "69     \t [0.37285646 0.49706079 0.9291817 ]. \t  3.167455852781071 \t 3.8569970369205837\n",
            "70     \t [0.3872114  0.57612738 0.92320694]. \t  3.3737869899368236 \t 3.8569970369205837\n",
            "71     \t [0.87121634 0.51465973 0.8645512 ]. \t  3.675271968418247 \t 3.8569970369205837\n",
            "72     \t [0.57211671 0.59941484 0.88418673]. \t  3.675929337689049 \t 3.8569970369205837\n",
            "73     \t [0.23277105 0.57522965 0.8168357 ]. \t  3.726925377999931 \t 3.8569970369205837\n",
            "74     \t [0.79639925 0.02410353 0.99093815]. \t  0.12773014673226793 \t 3.8569970369205837\n",
            "75     \t [0.61272774 0.59809361 0.85115438]. \t  3.7383989269098388 \t 3.8569970369205837\n",
            "76     \t [0.7269977  0.57082385 0.88398796]. \t  3.695271412511484 \t 3.8569970369205837\n",
            "77     \t [0.47435262 0.46178673 0.78468431]. \t  3.2227779820521487 \t 3.8569970369205837\n",
            "78     \t [0.00891897 0.4759973  0.86118779]. \t  3.5887673574826495 \t 3.8569970369205837\n",
            "79     \t [0.0088676  0.54735681 0.79873144]. \t  3.5658461385896985 \t 3.8569970369205837\n",
            "80     \t [0.36501478 0.61058063 0.78513565]. \t  3.3609733830481874 \t 3.8569970369205837\n",
            "81     \t [0.43129676 0.5334239  0.82135133]. \t  3.742661626962778 \t 3.8569970369205837\n",
            "82     \t [0.44806228 0.58681829 0.86771351]. \t  3.7963371090209606 \t 3.8569970369205837\n",
            "83     \t [0.70277256 0.5752355  0.82244235]. \t  3.6494373971380174 \t 3.8569970369205837\n",
            "84     \t [0.19968569 0.65146239 0.79328599]. \t  3.2975905939014947 \t 3.8569970369205837\n",
            "85     \t [0.61722195 0.562291   0.84755359]. \t  3.804000829957553 \t 3.8569970369205837\n",
            "86     \t [0.71116404 0.57144986 0.81892155]. \t  3.6285611308829613 \t 3.8569970369205837\n",
            "87     \t [0.83826163 0.55417167 0.85918682]. \t  3.7413436192615395 \t 3.8569970369205837\n",
            "88     \t [0.4407848  0.44834125 0.8019104 ]. \t  3.3063598905333302 \t 3.8569970369205837\n",
            "89     \t [0.13983949 0.60982182 0.83636512]. \t  3.721257528703066 \t 3.8569970369205837\n",
            "90     \t [0.02036176 0.51903972 0.80253599]. \t  3.567291232755222 \t 3.8569970369205837\n",
            "91     \t [0.53143074 0.58221234 0.83241364]. \t  3.750253228179835 \t 3.8569970369205837\n",
            "92     \t [0.13551967 0.61477908 0.86099374]. \t  3.721729798800781 \t 3.8569970369205837\n",
            "93     \t [0.39429347 0.57616573 0.87345393]. \t  3.8035060808375962 \t 3.8569970369205837\n",
            "94     \t [0.75470308 0.53339764 0.85170202]. \t  3.760636200698413 \t 3.8569970369205837\n",
            "95     \t [0.78403698 0.5659761  0.87503501]. \t  3.722659538556718 \t 3.8569970369205837\n",
            "96     \t [0.24940432 0.48230513 0.79459484]. \t  3.4271425002510676 \t 3.8569970369205837\n",
            "97     \t [0.51412726 0.57571    0.8428894 ]. \t  3.8045115476003253 \t 3.8569970369205837\n",
            "98     \t [0.35321925 0.54229721 0.81064821]. \t  3.6913433884304307 \t 3.8569970369205837\n",
            "99     \t [0.05766926 0.62078636 0.81547445]. \t  3.5677144595940287 \t 3.8569970369205837\n",
            "100    \t [0.33421861 0.49739085 0.83747596]. \t  3.7323101500829483 \t 3.8569970369205837\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBLcW6tlteS2",
        "outputId": "963d9b5c-c189-4b59-8ba3-f207b7844b94"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 2 \r\n",
        "\r\n",
        "np.random.seed(run_num_2)\r\n",
        "surrogate_winner_2 = dtStudentProcess(d_cov_func, nu = df)\r\n",
        "\r\n",
        "winner_2 = dGPGO_stp(surrogate_winner_2, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_2.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.85198549 0.0739036  0.89493176]. \t  0.3999083566189884 \t 2.6229838112516717\n",
            "init   \t [0.43649355 0.12767773 0.57585787]. \t  0.24461577848211966 \t 2.6229838112516717\n",
            "init   \t [0.84047092 0.43512055 0.69591056]. \t  1.8897715258798413 \t 2.6229838112516717\n",
            "init   \t [0.6846381  0.70064837 0.77969426]. \t  2.6229838112516717 \t 2.6229838112516717\n",
            "init   \t [0.64274937 0.96102617 0.10846489]. \t  0.003309399496320042 \t 2.6229838112516717\n",
            "1      \t [1. 1. 1.]. \t  0.31688362070415665 \t 2.6229838112516717\n",
            "2      \t [0.14306915 0.79984146 0.95066451]. \t  1.7295279521051767 \t 2.6229838112516717\n",
            "3      \t [0.01071905 0.89780036 0.09392995]. \t  0.005584714744756498 \t 2.6229838112516717\n",
            "4      \t [0.12387761 0.14906975 0.98769341]. \t  0.42212207280543435 \t 2.6229838112516717\n",
            "5      \t [0.92029033 0.04265395 0.04009595]. \t  0.08078082202939513 \t 2.6229838112516717\n",
            "6      \t [0.08503078 0.11774937 0.01943383]. \t  0.12432388056630428 \t 2.6229838112516717\n",
            "7      \t [0.63099739 0.5250951  1.        ]. \t  2.0384981519812366 \t 2.6229838112516717\n",
            "8      \t [1.         1.         0.38167631]. \t  0.09682184641767629 \t 2.6229838112516717\n",
            "9      \t [0.02851891 0.49081837 0.58086585]. \t  1.2342513397499568 \t 2.6229838112516717\n",
            "10     \t [0.35031593 1.         0.5576755 ]. \t  2.172310886756859 \t 2.6229838112516717\n",
            "11     \t [0.49629512 1.         1.        ]. \t  0.33238563820330963 \t 2.6229838112516717\n",
            "12     \t [0.59755247 0.62273669 0.13461144]. \t  0.042990505986531764 \t 2.6229838112516717\n",
            "13     \t [1.         0.55106406 1.        ]. \t  2.0036809020457818 \t 2.6229838112516717\n",
            "14     \t [1.         0.73885233 0.21122933]. \t  0.012507983298002977 \t 2.6229838112516717\n",
            "15     \t [0.09414994 0.96539139 0.65476983]. \t  2.205389904508487 \t 2.6229838112516717\n",
            "16     \t [0.52547029 0.01008207 0.22076588]. \t  0.7766560905841401 \t 2.6229838112516717\n",
            "17     \t [0.       0.       0.299904]. \t  0.5619159736259453 \t 2.6229838112516717\n",
            "18     \t [0.9783558  0.1675202  0.45607717]. \t  0.1414004799632371 \t 2.6229838112516717\n",
            "19     \t [0.99802487 0.7638907  0.68774249]. \t  1.072500460595886 \t 2.6229838112516717\n",
            "20     \t [1.83908462e-13 1.83908462e-13 1.83908462e-13]. \t  0.06797411659038972 \t 2.6229838112516717\n",
            "21     \t [0.00989744 0.60537288 0.98253438]. \t  2.3404120901543712 \t 2.6229838112516717\n",
            "22     \t [0.98683068 0.08566433 1.        ]. \t  0.21093452056296386 \t 2.6229838112516717\n",
            "23     \t [0.93499869 0.82251594 0.01581566]. \t  0.0004573369370060813 \t 2.6229838112516717\n",
            "24     \t [0.         0.08722106 0.72411667]. \t  0.46068197561643626 \t 2.6229838112516717\n",
            "25     \t [0.3644009 0.        0.       ]. \t  0.10224041126598558 \t 2.6229838112516717\n",
            "26     \t [0.74298784 0.31660024 0.02271814]. \t  0.07333462233907706 \t 2.6229838112516717\n",
            "27     \t [0.70676853 1.         0.67070283]. \t  0.7329817437214419 \t 2.6229838112516717\n",
            "28     \t [0.002897   0.90104093 0.530301  ]. \t  \u001b[92m2.867814143620765\u001b[0m \t 2.867814143620765\n",
            "29     \t [0.47808053 0.00645933 0.98660663]. \t  0.11205696502821058 \t 2.867814143620765\n",
            "30     \t [0.28250958 1.         0.16389881]. \t  0.024185458126898592 \t 2.867814143620765\n",
            "31     \t [3.44825542e-23 5.86195917e-01 3.44825542e-23]. \t  0.008748611304749306 \t 2.867814143620765\n",
            "32     \t [0.82676193 0.82177611 1.        ]. \t  1.0820484844737897 \t 2.867814143620765\n",
            "33     \t [0.23085658 0.80349586 0.59117614]. \t  2.857853873094403 \t 2.867814143620765\n",
            "34     \t [0.12310792 0.98251499 0.99767229]. \t  0.39263949902846446 \t 2.867814143620765\n",
            "35     \t [0.77355296 0.         0.56192887]. \t  0.09535074667079736 \t 2.867814143620765\n",
            "36     \t [0.         0.49660527 0.24155336]. \t  0.19139309149315045 \t 2.867814143620765\n",
            "37     \t [0.14981227 0.         0.79930113]. \t  0.25005362093662176 \t 2.867814143620765\n",
            "38     \t [0.34320239 0.52735644 0.83971844]. \t  \u001b[92m3.819980895555412\u001b[0m \t 3.819980895555412\n",
            "39     \t [0.42087352 0.58831954 0.7729273 ]. \t  3.268167034198613 \t 3.819980895555412\n",
            "40     \t [0.30620724 0.49227166 0.88459375]. \t  3.613148050583227 \t 3.819980895555412\n",
            "41     \t [0.36528408 0.47340127 0.8443526 ]. \t  3.6346731634013745 \t 3.819980895555412\n",
            "42     \t [0.36250929 0.57206132 0.93587704]. \t  3.1999269985953034 \t 3.819980895555412\n",
            "43     \t [0.33993034 0.47446703 0.84539231]. \t  3.641706512961933 \t 3.819980895555412\n",
            "44     \t [0.98552166 0.2628534  0.06371711]. \t  0.07451850534278143 \t 3.819980895555412\n",
            "45     \t [0.99996521 0.31394741 0.88917603]. \t  2.054009699373671 \t 3.819980895555412\n",
            "46     \t [0.01941904 1.         0.31887818]. \t  0.4705253367332492 \t 3.819980895555412\n",
            "47     \t [0.42351448 0.56993188 0.827321  ]. \t  3.7735639744908713 \t 3.819980895555412\n",
            "48     \t [0.49099262 0.56053994 0.79665025]. \t  3.5350826660085533 \t 3.819980895555412\n",
            "49     \t [0.00244323 0.8512385  0.74890588]. \t  1.9269952534517976 \t 3.819980895555412\n",
            "50     \t [0.52116226 0.48450646 0.83035516]. \t  3.6378210596421616 \t 3.819980895555412\n",
            "51     \t [0.37659799 0.68574    0.00735869]. \t  0.005398218412755925 \t 3.819980895555412\n",
            "52     \t [0.27086107 0.58003252 0.81542543]. \t  3.7094113558392126 \t 3.819980895555412\n",
            "53     \t [0.20137189 0.30941326 0.91461563]. \t  1.9058559284679035 \t 3.819980895555412\n",
            "54     \t [0.25947307 0.54210737 0.81180878]. \t  3.7066657614149543 \t 3.819980895555412\n",
            "55     \t [0.4380889  0.55423771 0.76576446]. \t  3.2158386828219383 \t 3.819980895555412\n",
            "56     \t [0.3534429  0.58938161 0.86438732]. \t  3.8074817088616704 \t 3.819980895555412\n",
            "57     \t [0.41564466 0.50840698 0.91067924]. \t  3.442229036519346 \t 3.819980895555412\n",
            "58     \t [0.98348988 0.03450328 0.75010232]. \t  0.3188449392927411 \t 3.819980895555412\n",
            "59     \t [0.98095193 0.93872574 0.09112391]. \t  0.0006154511398731977 \t 3.819980895555412\n",
            "60     \t [0.27661532 0.6351806  0.82127146]. \t  3.5535755557149495 \t 3.819980895555412\n",
            "61     \t [0.48557621 0.53183717 0.83641186]. \t  3.799903161805562 \t 3.819980895555412\n",
            "62     \t [0.35456456 0.67678569 0.77831887]. \t  3.0027001473689525 \t 3.819980895555412\n",
            "63     \t [0.38994824 0.58782214 0.79244815]. \t  3.485896780788635 \t 3.819980895555412\n",
            "64     \t [0.41773608 0.60592883 0.87758468]. \t  3.7124195972062655 \t 3.819980895555412\n",
            "65     \t [0.42816897 0.46804587 0.90267452]. \t  3.3391451977633273 \t 3.819980895555412\n",
            "66     \t [0.55526622 0.51729954 0.81202233]. \t  3.627717778154402 \t 3.819980895555412\n",
            "67     \t [0.48361665 0.55251965 0.80167543]. \t  3.5894138785970564 \t 3.819980895555412\n",
            "68     \t [0.50248205 0.45177046 0.87077715]. \t  3.4525719671037494 \t 3.819980895555412\n",
            "69     \t [0.32782284 0.62768993 0.82896975]. \t  3.6205460057321788 \t 3.819980895555412\n",
            "70     \t [0.4822511  0.57479041 0.81533624]. \t  3.6784024362700958 \t 3.819980895555412\n",
            "71     \t [0.34524529 0.56222715 0.80580203]. \t  3.653136348505176 \t 3.819980895555412\n",
            "72     \t [0.47846732 0.62957361 0.87197169]. \t  3.62434765148135 \t 3.819980895555412\n",
            "73     \t [0.40402687 0.55994162 0.85126616]. \t  \u001b[92m3.8537144369426364\u001b[0m \t 3.8537144369426364\n",
            "74     \t [0.45362065 0.66572392 0.88072463]. \t  3.3810143989533374 \t 3.8537144369426364\n",
            "75     \t [0.35930461 0.51996166 0.89980917]. \t  3.590981993153152 \t 3.8537144369426364\n",
            "76     \t [0.40305831 0.59225605 0.81911673]. \t  3.686371657770671 \t 3.8537144369426364\n",
            "77     \t [0.12484429 0.78644951 0.23898211]. \t  0.14905678316327517 \t 3.8537144369426364\n",
            "78     \t [0.51232617 0.54004065 0.77664361]. \t  3.3166381920698456 \t 3.8537144369426364\n",
            "79     \t [0.41913437 0.55814139 0.76685943]. \t  3.2355046881531875 \t 3.8537144369426364\n",
            "80     \t [0.30502956 0.53584687 0.94313474]. \t  3.066830064552242 \t 3.8537144369426364\n",
            "81     \t [0.45449055 0.58273397 0.81921567]. \t  3.697342084337479 \t 3.8537144369426364\n",
            "82     \t [0.32703156 0.60536996 0.81654932]. \t  3.6453131899497673 \t 3.8537144369426364\n",
            "83     \t [0.98464349 0.01389699 0.17163263]. \t  0.2190903238151828 \t 3.8537144369426364\n",
            "84     \t [0.02081792 0.55126355 0.87281418]. \t  3.7763914838699835 \t 3.8537144369426364\n",
            "85     \t [0.0133277  0.61640636 0.79059533]. \t  3.3894603123311033 \t 3.8537144369426364\n",
            "86     \t [0.00271908 0.52013283 0.79671906]. \t  3.5150774131920617 \t 3.8537144369426364\n",
            "87     \t [0.10443614 0.61709583 0.87175505]. \t  3.6805680680282102 \t 3.8537144369426364\n",
            "88     \t [0.23314157 0.80974303 0.72055961]. \t  2.2294904160369335 \t 3.8537144369426364\n",
            "89     \t [0.33299929 0.61259981 0.80549341]. \t  3.5412527482888145 \t 3.8537144369426364\n",
            "90     \t [0.52641221 0.6558243  0.87397014]. \t  3.4577268497653932 \t 3.8537144369426364\n",
            "91     \t [0.37910087 0.46289261 0.87713804]. \t  3.5020574738172026 \t 3.8537144369426364\n",
            "92     \t [0.2599573  0.55844605 0.88172947]. \t  3.7775948504892 \t 3.8537144369426364\n",
            "93     \t [0.41348224 0.48904863 0.81573269]. \t  3.6005968534243884 \t 3.8537144369426364\n",
            "94     \t [0.33583282 0.5665216  0.93710784]. \t  3.183440089550424 \t 3.8537144369426364\n",
            "95     \t [0.56801558 0.55390071 0.81815122]. \t  3.694552573639544 \t 3.8537144369426364\n",
            "96     \t [0.3461939  0.65252073 0.83695075]. \t  3.510371912868934 \t 3.8537144369426364\n",
            "97     \t [0.23068339 0.61388633 0.92067648]. \t  3.3209896607107496 \t 3.8537144369426364\n",
            "98     \t [0.6734359  0.54426251 0.84344333]. \t  3.782378405394317 \t 3.8537144369426364\n",
            "99     \t [0.39625322 0.55524672 0.85757041]. \t  \u001b[92m3.8550010195296487\u001b[0m \t 3.8550010195296487\n",
            "100    \t [0.65212838 0.49904569 0.86472281]. \t  3.694638582705788 \t 3.8550010195296487\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "emgjXwfeuvRY",
        "outputId": "5c23a6fb-7d13-467e-8ea1-d1ac2ed07f6f"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 3 \r\n",
        "\r\n",
        "np.random.seed(run_num_3)\r\n",
        "surrogate_winner_3 = dtStudentProcess(d_cov_func, nu = df)\r\n",
        "\r\n",
        "winner_3 = dGPGO_stp(surrogate_winner_3, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_3.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.75157561 0.10925128 0.48612128]. \t  0.19395982560567754 \t 0.5647137279144399\n",
            "init   \t [0.49983118 0.65711    0.23588471]. \t  0.10633623603471726 \t 0.5647137279144399\n",
            "init   \t [0.61279489 0.1196524  0.7122023 ]. \t  0.5647137279144399 \t 0.5647137279144399\n",
            "init   \t [0.37256054 0.52476827 0.26328708]. \t  0.24645583811739444 \t 0.5647137279144399\n",
            "init   \t [0.62163122 0.44909976 0.21906361]. \t  0.2633149120115605 \t 0.5647137279144399\n",
            "1      \t [1. 1. 1.]. \t  0.31688362070415665 \t 0.5647137279144399\n",
            "2      \t [0.17674282 1.         0.95138054]. \t  0.4756407860985818 \t 0.5647137279144399\n",
            "3      \t [0.00798031 0.03065721 0.14136081]. \t  0.39018974818183505 \t 0.5647137279144399\n",
            "4      \t [1.        0.2211669 1.       ]. \t  \u001b[92m0.6245644227160623\u001b[0m \t 0.6245644227160623\n",
            "5      \t [0.0344408  0.23023841 0.96001912]. \t  \u001b[92m0.9416815988869073\u001b[0m \t 0.9416815988869073\n",
            "6      \t [1.         0.98085764 0.07410203]. \t  0.00029848708768459906 \t 0.9416815988869073\n",
            "7      \t [1.         0.57189754 0.65676583]. \t  \u001b[92m1.332128845938422\u001b[0m \t 1.332128845938422\n",
            "8      \t [0.11876587 0.87331495 0.03593827]. \t  0.0014804692647737315 \t 1.332128845938422\n",
            "9      \t [0.50833609 0.6043408  1.        ]. \t  \u001b[92m2.0570681056959756\u001b[0m \t 2.0570681056959756\n",
            "10     \t [0.00106696 0.70801824 0.41925762]. \t  1.2965752068906924 \t 2.0570681056959756\n",
            "11     \t [1.         1.         0.53264924]. \t  0.24963843279849424 \t 2.0570681056959756\n",
            "12     \t [0.95159084 0.2075399  0.10255089]. \t  0.1473766287671977 \t 2.0570681056959756\n",
            "13     \t [0.00195417 0.63804167 0.88531834]. \t  \u001b[92m3.494779716243495\u001b[0m \t 3.494779716243495\n",
            "14     \t [0.         0.32612386 0.        ]. \t  0.05034853925586329 \t 3.494779716243495\n",
            "15     \t [0.35416366 0.01441445 0.03538726]. \t  0.17917500076515638 \t 3.494779716243495\n",
            "16     \t [1.         0.61760412 1.        ]. \t  1.9593958319810247 \t 3.494779716243495\n",
            "17     \t [0.03098445 0.3284919  0.72794841]. \t  1.8380234055189004 \t 3.494779716243495\n",
            "18     \t [0.34741297 1.         0.31281229]. \t  0.37284212748941625 \t 3.494779716243495\n",
            "19     \t [0.69114474 0.97483285 0.00180176]. \t  0.00015815213819995807 \t 3.494779716243495\n",
            "20     \t [0.26520423 0.76762598 0.75930883]. \t  2.4028905973312984 \t 3.494779716243495\n",
            "21     \t [0.18406308 0.58647248 1.        ]. \t  2.0732407730847173 \t 3.494779716243495\n",
            "22     \t [0.         0.         0.70904913]. \t  0.20574378241188596 \t 3.494779716243495\n",
            "23     \t [0.03520389 0.87200476 0.79877103]. \t  1.676093267759886 \t 3.494779716243495\n",
            "24     \t [0.69493175 0.88226919 0.82158191]. \t  1.3402095012066195 \t 3.494779716243495\n",
            "25     \t [0.69687013 0.46624496 0.76570772]. \t  2.9685567609964183 \t 3.494779716243495\n",
            "26     \t [0. 0. 0.]. \t  0.06797411659013229 \t 3.494779716243495\n",
            "27     \t [0.56009247 0.46910479 0.        ]. \t  0.03044313172939404 \t 3.494779716243495\n",
            "28     \t [0.731236   0.         0.08231925]. \t  0.21070724840207578 \t 3.494779716243495\n",
            "29     \t [0.7004606  0.35972244 0.99802224]. \t  1.3796855888407242 \t 3.494779716243495\n",
            "30     \t [0.90892775 0.45998633 0.01580362]. \t  0.019290984364549738 \t 3.494779716243495\n",
            "31     \t [0.66449063 0.02336703 0.98041352]. \t  0.1409443961440923 \t 3.494779716243495\n",
            "32     \t [0.88055777 0.00816104 0.        ]. \t  0.047486095986767646 \t 3.494779716243495\n",
            "33     \t [0.90917741 0.14878148 0.80008161]. \t  0.8612117561151917 \t 3.494779716243495\n",
            "34     \t [0.92216757 0.00822464 0.50987968]. \t  0.08763355068063193 \t 3.494779716243495\n",
            "35     \t [0.89582336 0.01459834 0.948607  ]. \t  0.16379639678335906 \t 3.494779716243495\n",
            "36     \t [0.00217535 0.95155078 0.26184913]. \t  0.20871490035074294 \t 3.494779716243495\n",
            "37     \t [0.67853964 1.         1.        ]. \t  0.328212568291673 \t 3.494779716243495\n",
            "38     \t [0.62576396 1.         0.52940227]. \t  1.1414075379482846 \t 3.494779716243495\n",
            "39     \t [0.81157987 1.         0.272739  ]. \t  0.05377051005334174 \t 3.494779716243495\n",
            "40     \t [0.27794534 0.         0.50285857]. \t  0.18466049109516866 \t 3.494779716243495\n",
            "41     \t [0.         0.67469064 0.        ]. \t  0.0036670603094512712 \t 3.494779716243495\n",
            "42     \t [0.47870821 0.57889615 0.72564779]. \t  2.665605300449731 \t 3.494779716243495\n",
            "43     \t [0.04048759 0.53664351 0.82465506]. \t  \u001b[92m3.7442709214895076\u001b[0m \t 3.7442709214895076\n",
            "44     \t [0.00599853 0.45591357 0.79912712]. \t  3.296256227699394 \t 3.7442709214895076\n",
            "45     \t [0.99877468 0.51020561 0.87297528]. \t  3.5860251805882135 \t 3.7442709214895076\n",
            "46     \t [5.68595479e-06 4.93797802e-01 7.66168441e-01]. \t  3.1398939307974567 \t 3.7442709214895076\n",
            "47     \t [0.8994576  0.51299454 0.85801685]. \t  3.668579310691682 \t 3.7442709214895076\n",
            "48     \t [0.16708354 0.01949412 0.93764401]. \t  0.1895299450520082 \t 3.7442709214895076\n",
            "49     \t [0.8891927  0.53994909 0.86546082]. \t  3.709569198936615 \t 3.7442709214895076\n",
            "50     \t [0.80333003 0.52817538 0.81036234]. \t  3.5430556066299563 \t 3.7442709214895076\n",
            "51     \t [0.90916248 0.51630615 0.88142294]. \t  3.60266784360417 \t 3.7442709214895076\n",
            "52     \t [0.90868028 0.52316898 0.88655756]. \t  3.5894297728330002 \t 3.7442709214895076\n",
            "53     \t [0.94926641 0.48354739 0.86160982]. \t  3.539336550346743 \t 3.7442709214895076\n",
            "54     \t [0.95283373 0.50594766 0.85696194]. \t  3.6272277168104434 \t 3.7442709214895076\n",
            "55     \t [0.41990405 0.99999895 0.70277805]. \t  1.2063058653983219 \t 3.7442709214895076\n",
            "56     \t [0.88930467 0.57970161 0.85144931]. \t  3.6863269056260224 \t 3.7442709214895076\n",
            "57     \t [0.9440368  0.53348331 0.87188129]. \t  3.6633123087417303 \t 3.7442709214895076\n",
            "58     \t [0.00812068 0.59682319 0.86887982]. \t  3.73270157983922 \t 3.7442709214895076\n",
            "59     \t [0.28323107 0.5032867  0.86146702]. \t  \u001b[92m3.758198456873485\u001b[0m \t 3.758198456873485\n",
            "60     \t [0.206945   0.51187128 0.85567966]. \t  \u001b[92m3.7896848795369404\u001b[0m \t 3.7896848795369404\n",
            "61     \t [0.18432046 0.49270155 0.86234314]. \t  3.7051817907753026 \t 3.7896848795369404\n",
            "62     \t [0.07707203 0.49791812 0.88968304]. \t  3.570673941664955 \t 3.7896848795369404\n",
            "63     \t [0.09441302 0.58219301 0.81664536]. \t  3.698136880352131 \t 3.7896848795369404\n",
            "64     \t [0.22085321 0.62046185 0.82264409]. \t  3.63134988556917 \t 3.7896848795369404\n",
            "65     \t [0.05214006 0.54344191 0.82039534]. \t  3.731656266887608 \t 3.7896848795369404\n",
            "66     \t [0.88841287 0.51563126 0.86226063]. \t  3.675019149999458 \t 3.7896848795369404\n",
            "67     \t [0.99665607 0.54479258 0.91646956]. \t  3.3190281828508725 \t 3.7896848795369404\n",
            "68     \t [0.34688249 0.46735149 0.86771699]. \t  3.5705717541078696 \t 3.7896848795369404\n",
            "69     \t [0.94058104 0.99999999 0.80262388]. \t  0.5002870023253252 \t 3.7896848795369404\n",
            "70     \t [0.16230684 0.46997923 0.79398781]. \t  3.3619631346348 \t 3.7896848795369404\n",
            "71     \t [0.04278874 0.48416486 0.85163365]. \t  3.6523364322217025 \t 3.7896848795369404\n",
            "72     \t [0.19286379 0.66155319 0.83075219]. \t  3.445395577630209 \t 3.7896848795369404\n",
            "73     \t [0.99939443 0.02047863 0.11944457]. \t  0.1434793385060084 \t 3.7896848795369404\n",
            "74     \t [0.09380152 0.61595191 0.87006272]. \t  3.688364987311298 \t 3.7896848795369404\n",
            "75     \t [0.74662962 0.58954966 0.84542697]. \t  3.7070039032667954 \t 3.7896848795369404\n",
            "76     \t [0.56062839 0.55763694 0.84718871]. \t  \u001b[92m3.821437022903156\u001b[0m \t 3.821437022903156\n",
            "77     \t [0.23863015 0.59204473 0.86968537]. \t  3.7874536955201155 \t 3.821437022903156\n",
            "78     \t [0.16027679 0.55693706 0.8331693 ]. \t  3.8164941535911607 \t 3.821437022903156\n",
            "79     \t [0.57288193 0.55444539 0.85212306]. \t  \u001b[92m3.8250329807836327\u001b[0m \t 3.8250329807836327\n",
            "80     \t [0.12125842 0.59471263 0.86922824]. \t  3.766861277872451 \t 3.8250329807836327\n",
            "81     \t [0.3785927  0.54481006 0.86671442]. \t  \u001b[92m3.8366773651368113\u001b[0m \t 3.8366773651368113\n",
            "82     \t [0.56260449 0.56011595 0.87098775]. \t  3.801857616157844 \t 3.8366773651368113\n",
            "83     \t [0.53491861 0.54644165 0.85674596]. \t  3.8328136797286545 \t 3.8366773651368113\n",
            "84     \t [0.40808974 0.60235585 0.85281939]. \t  3.7730820595343673 \t 3.8366773651368113\n",
            "85     \t [0.50115509 0.52666646 0.83816215]. \t  3.7946438266424045 \t 3.8366773651368113\n",
            "86     \t [0.5214504  0.53958773 0.87181892]. \t  3.7992520347399177 \t 3.8366773651368113\n",
            "87     \t [0.42325143 0.51275926 0.86073251]. \t  3.7873070245278697 \t 3.8366773651368113\n",
            "88     \t [0.49526128 0.5228215  0.9041852 ]. \t  3.5480570578744803 \t 3.8366773651368113\n",
            "89     \t [0.60306924 0.55283111 0.88748424]. \t  3.7128765969706587 \t 3.8366773651368113\n",
            "90     \t [0.22236729 0.55234621 0.84877983]. \t  \u001b[92m3.856920034413695\u001b[0m \t 3.856920034413695\n",
            "91     \t [0.5768912  0.53351037 0.83965297]. \t  3.7915910194734117 \t 3.856920034413695\n",
            "92     \t [0.45347676 0.49928485 0.84919709]. \t  3.747217263848062 \t 3.856920034413695\n",
            "93     \t [0.45496791 0.54181496 0.8774682 ]. \t  3.7868641156635627 \t 3.856920034413695\n",
            "94     \t [0.54616665 0.64845873 0.87293478]. \t  3.502534495731788 \t 3.856920034413695\n",
            "95     \t [0.62992416 0.50135224 0.8284892 ]. \t  3.667051522443348 \t 3.856920034413695\n",
            "96     \t [0.00785194 0.68927322 0.96760693]. \t  2.2762713673194654 \t 3.856920034413695\n",
            "97     \t [0.82531367 0.52460042 0.84448228]. \t  3.712192099793732 \t 3.856920034413695\n",
            "98     \t [0.17207363 0.51920498 0.85439072]. \t  3.8064695194881484 \t 3.856920034413695\n",
            "99     \t [0.30441131 0.60621249 0.88492253]. \t  3.6794957935019257 \t 3.856920034413695\n",
            "100    \t [0.44713152 0.60560259 0.85921664]. \t  3.7552621906548973 \t 3.856920034413695\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8riJpBBKuvT4",
        "outputId": "9d743fe1-1cec-45eb-8ee9-a912f59c2ecd"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 4 \r\n",
        "\r\n",
        "np.random.seed(run_num_4)\r\n",
        "surrogate_winner_4 = dtStudentProcess(d_cov_func, nu = df)\r\n",
        "\r\n",
        "winner_4 = dGPGO_stp(surrogate_winner_4, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_4.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.83936105 0.82129497 0.64220716]. \t  0.8796727111154451 \t 1.9592421489197056\n",
            "init   \t [0.66722262 0.03913991 0.06567239]. \t  0.21283931850025994 \t 1.9592421489197056\n",
            "init   \t [0.27650064 0.3163605  0.19359241]. \t  0.5590312289203825 \t 1.9592421489197056\n",
            "init   \t [0.19709288 0.84920429 0.84235809]. \t  1.818649590001362 \t 1.9592421489197056\n",
            "init   \t [0.34736473 0.7397423  0.49340914]. \t  1.9592421489197056 \t 1.9592421489197056\n",
            "1      \t [0.72375065 0.15415572 0.98627436]. \t  0.4436681531252645 \t 1.9592421489197056\n",
            "2      \t [0.00568574 0.90362017 0.0072839 ]. \t  0.0005407899212332239 \t 1.9592421489197056\n",
            "3      \t [0.91444374 0.93067625 0.16188928]. \t  0.004441114262621269 \t 1.9592421489197056\n",
            "4      \t [0.21130366 0.05657913 0.88927897]. \t  0.3547902658844305 \t 1.9592421489197056\n",
            "5      \t [0.98597041 0.2856864  0.41065736]. \t  0.15284929019920618 \t 1.9592421489197056\n",
            "6      \t [0.4999548  0.53356661 1.        ]. \t  \u001b[92m2.0615730796175358\u001b[0m \t 2.0615730796175358\n",
            "7      \t [0.58744713 1.         0.97447813]. \t  0.40260980358626247 \t 2.0615730796175358\n",
            "8      \t [1.         0.63716856 1.        ]. \t  1.9145973168985835 \t 2.0615730796175358\n",
            "9      \t [0.04681794 0.56517096 0.99093683]. \t  \u001b[92m2.2288293321682677\u001b[0m \t 2.2288293321682677\n",
            "10     \t [0.51837673 0.52935167 0.00551262]. \t  0.02191995397414247 \t 2.2288293321682677\n",
            "11     \t [0.02858877 0.14789467 0.01450595]. \t  0.10288931401095262 \t 2.2288293321682677\n",
            "12     \t [0.0271757  0.45756503 0.75278759]. \t  \u001b[92m2.8585148930112485\u001b[0m \t 2.8585148930112485\n",
            "13     \t [1. 1. 1.]. \t  0.31688362070415665 \t 2.8585148930112485\n",
            "14     \t [0.02787663 0.66366164 0.51021483]. \t  1.9151235028676394 \t 2.8585148930112485\n",
            "15     \t [0.988023   0.02395959 0.60873828]. \t  0.12689093149567174 \t 2.8585148930112485\n",
            "16     \t [0.13662715 0.03635785 0.35887153]. \t  0.6218264756135358 \t 2.8585148930112485\n",
            "17     \t [0.2647458  1.         0.33470218]. \t  0.5565886241693958 \t 2.8585148930112485\n",
            "18     \t [0.996473   0.11479847 0.06709126]. \t  0.09217281441087458 \t 2.8585148930112485\n",
            "19     \t [1.         0.26923297 1.        ]. \t  0.8440211002333479 \t 2.8585148930112485\n",
            "20     \t [0.58260927 0.2657287  0.70752056]. \t  1.2820868553462472 \t 2.8585148930112485\n",
            "21     \t [0.54371658 0.96899499 0.02197771]. \t  0.00041722452452298 \t 2.8585148930112485\n",
            "22     \t [1.61248556e-01 1.38912816e-05 1.82582186e-03]. \t  0.09250344433213939 \t 2.8585148930112485\n",
            "23     \t [0.97422189 1.         0.44865831]. \t  0.19600912019862995 \t 2.8585148930112485\n",
            "24     \t [0.98729812 0.75484397 0.03715949]. \t  0.0011921357841584727 \t 2.8585148930112485\n",
            "25     \t [6.93889378e-18 2.83980352e-03 8.45173062e-01]. \t  0.2400393885767887 \t 2.8585148930112485\n",
            "26     \t [0.00148381 0.89117079 0.77956044]. \t  1.575520080441569 \t 2.8585148930112485\n",
            "27     \t [0.93341652 0.98195286 0.01379805]. \t  8.502821513842029e-05 \t 2.8585148930112485\n",
            "28     \t [0.04224489 0.43534671 0.        ]. \t  0.030941514924457507 \t 2.8585148930112485\n",
            "29     \t [0.0886894  0.23322895 0.99327489]. \t  0.7396794921275043 \t 2.8585148930112485\n",
            "30     \t [1.         0.55630243 0.67419077]. \t  1.5797848490043092 \t 2.8585148930112485\n",
            "31     \t [0.         0.35237064 0.41732444]. \t  0.3287829013708738 \t 2.8585148930112485\n",
            "32     \t [0.51345121 0.         0.54035583]. \t  0.12959877341603074 \t 2.8585148930112485\n",
            "33     \t [3.92628982e-12 9.85275199e-12 1.71604788e-11]. \t  0.06797411661099806 \t 2.8585148930112485\n",
            "34     \t [0.94527692 0.00430629 0.26153721]. \t  0.32483527044115357 \t 2.8585148930112485\n",
            "35     \t [0.51954325 1.         0.66703585]. \t  1.1986622422719675 \t 2.8585148930112485\n",
            "36     \t [0.51143822 0.6261951  0.79414823]. \t  \u001b[92m3.322430027359041\u001b[0m \t 3.322430027359041\n",
            "37     \t [6.18967857e-04 5.61939433e-01 7.87825480e-01]. \t  \u001b[92m3.463231087211141\u001b[0m \t 3.463231087211141\n",
            "38     \t [0.84044993 0.47996542 0.91279693]. \t  3.224877979754453 \t 3.463231087211141\n",
            "39     \t [0.28167866 0.638555   0.72422753]. \t  2.72793323073814 \t 3.463231087211141\n",
            "40     \t [0.19315352 0.79025438 0.10905113]. \t  0.011710646771429538 \t 3.463231087211141\n",
            "41     \t [0.6903902  0.77142568 0.94587436]. \t  1.978483842112595 \t 3.463231087211141\n",
            "42     \t [0.8640248  0.336286   0.11146822]. \t  0.14309814963347003 \t 3.463231087211141\n",
            "43     \t [1.         1.         0.82465066]. \t  0.5137593382988003 \t 3.463231087211141\n",
            "44     \t [2.98636951e-08 4.26018928e-08 2.73010639e-01]. \t  0.5793839204869329 \t 3.463231087211141\n",
            "45     \t [0.23692292 0.98429707 0.02557405]. \t  0.0006571567662146616 \t 3.463231087211141\n",
            "46     \t [0.03396911 0.98436057 0.97729861]. \t  0.45209442762873975 \t 3.463231087211141\n",
            "47     \t [0.64979433 0.54013746 0.50613043]. \t  0.5707918807958635 \t 3.463231087211141\n",
            "48     \t [0.97193625 0.39473107 0.83799101]. \t  2.948472848729675 \t 3.463231087211141\n",
            "49     \t [0.78399748 0.51898664 0.92776953]. \t  3.2032448602562287 \t 3.463231087211141\n",
            "50     \t [6.42075087e-04 6.68614398e-01 9.66789468e-01]. \t  2.3935410119533116 \t 3.463231087211141\n",
            "51     \t [0.13845779 0.55154144 0.8235816 ]. \t  \u001b[92m3.7718641215065203\u001b[0m \t 3.7718641215065203\n",
            "52     \t [0.22159922 0.56766273 0.74955434]. \t  3.0871801177913363 \t 3.7718641215065203\n",
            "53     \t [0.1709654  0.50367943 0.77390527]. \t  3.292186247201152 \t 3.7718641215065203\n",
            "54     \t [0.94995931 0.02579013 0.96588486]. \t  0.15995204942470498 \t 3.7718641215065203\n",
            "55     \t [0.02419428 0.50930592 0.8443374 ]. \t  3.742366743780555 \t 3.7718641215065203\n",
            "56     \t [0.00361563 0.61760219 0.85461235]. \t  3.6813319036242556 \t 3.7718641215065203\n",
            "57     \t [0.13623039 0.60497798 0.85241299]. \t  3.76301272444142 \t 3.7718641215065203\n",
            "58     \t [0.43775799 0.56235646 0.84066941]. \t  \u001b[92m3.830470986675787\u001b[0m \t 3.830470986675787\n",
            "59     \t [0.52390721 0.57116823 0.84955666]. \t  3.821561387387678 \t 3.830470986675787\n",
            "60     \t [0.18268352 0.56724212 0.85147867]. \t  \u001b[92m3.8492524690859415\u001b[0m \t 3.8492524690859415\n",
            "61     \t [0.48633675 0.62352658 0.8066559 ]. \t  3.456061298285689 \t 3.8492524690859415\n",
            "62     \t [0.10815124 0.58528302 0.84437937]. \t  3.803887502749966 \t 3.8492524690859415\n",
            "63     \t [0.13970047 0.52752678 0.84848446]. \t  3.819843286171226 \t 3.8492524690859415\n",
            "64     \t [0.1411614  0.6278393  0.84423655]. \t  3.6636201138727227 \t 3.8492524690859415\n",
            "65     \t [0.29638118 0.46240447 0.86898806]. \t  3.5351175252018945 \t 3.8492524690859415\n",
            "66     \t [0.35955485 0.51797221 0.82401753]. \t  3.741218642778799 \t 3.8492524690859415\n",
            "67     \t [0.45618316 0.50450917 0.85197972]. \t  3.765777030533203 \t 3.8492524690859415\n",
            "68     \t [0.12731815 0.52772681 0.88282385]. \t  3.7216356679991174 \t 3.8492524690859415\n",
            "69     \t [0.43243621 0.00505517 0.28184623]. \t  0.866302529903445 \t 3.8492524690859415\n",
            "70     \t [0.49550672 0.4808993  0.82965343]. \t  3.6238019484476287 \t 3.8492524690859415\n",
            "71     \t [0.51524657 0.51060444 0.88154452]. \t  3.6916091683005123 \t 3.8492524690859415\n",
            "72     \t [0.30852452 0.50793529 0.8719326 ]. \t  3.742814395299765 \t 3.8492524690859415\n",
            "73     \t [0.23399859 0.62803991 0.83057837]. \t  3.632776421799366 \t 3.8492524690859415\n",
            "74     \t [0.36679414 0.57647524 0.82817666]. \t  3.7778493585728956 \t 3.8492524690859415\n",
            "75     \t [0.41148259 0.54683101 0.85695995]. \t  \u001b[92m3.851987105943082\u001b[0m \t 3.851987105943082\n",
            "76     \t [0.45203417 0.53567756 0.8016389 ]. \t  3.5944493182566726 \t 3.851987105943082\n",
            "77     \t [0.08338416 0.55633864 0.78803777]. \t  3.4936421793865207 \t 3.851987105943082\n",
            "78     \t [0.31708664 0.59040173 0.839278  ]. \t  3.7960391732963865 \t 3.851987105943082\n",
            "79     \t [0.60985369 0.55447446 0.84832101]. \t  3.811386523445674 \t 3.851987105943082\n",
            "80     \t [0.54544629 0.49027786 0.93236976]. \t  3.084989589259835 \t 3.851987105943082\n",
            "81     \t [0.30980616 0.57104439 0.88094943]. \t  3.77757711883093 \t 3.851987105943082\n",
            "82     \t [0.34275941 0.51814184 0.82642831]. \t  3.7546182814173634 \t 3.851987105943082\n",
            "83     \t [0.23978321 0.58549462 0.83966553]. \t  3.810007924726116 \t 3.851987105943082\n",
            "84     \t [0.72757864 0.56533264 0.84228652]. \t  3.7527499647184523 \t 3.851987105943082\n",
            "85     \t [0.37562654 0.52557147 0.84283957]. \t  3.821162838022261 \t 3.851987105943082\n",
            "86     \t [0.05203226 0.48747327 0.79271973]. \t  3.40281571312646 \t 3.851987105943082\n",
            "87     \t [0.21953335 0.53655089 0.84336356]. \t  3.839209489026665 \t 3.851987105943082\n",
            "88     \t [0.03862915 0.46824181 0.91575503]. \t  3.1632634487539457 \t 3.851987105943082\n",
            "89     \t [0.33554709 0.58795255 0.791631  ]. \t  3.493034033110571 \t 3.851987105943082\n",
            "90     \t [0.07221495 0.56387775 0.8083205 ]. \t  3.6594399407679985 \t 3.851987105943082\n",
            "91     \t [0.39934261 0.62550969 0.87450724]. \t  3.647176496545304 \t 3.851987105943082\n",
            "92     \t [0.23136946 0.61210081 0.82572853]. \t  3.6796471171744676 \t 3.851987105943082\n",
            "93     \t [0.0368064  0.57418365 0.87739794]. \t  3.752782320177969 \t 3.851987105943082\n",
            "94     \t [0.35697267 0.49298132 0.81787027]. \t  3.6338928690176866 \t 3.851987105943082\n",
            "95     \t [0.36220889 0.52669001 0.82819997]. \t  3.777945071365579 \t 3.851987105943082\n",
            "96     \t [0.31736928 0.61633425 0.74698437]. \t  2.97027031084742 \t 3.851987105943082\n",
            "97     \t [0.40193588 0.64511727 0.80837861]. \t  3.3936069223844445 \t 3.851987105943082\n",
            "98     \t [0.0485262  0.62554319 0.82240134]. \t  3.581926022792689 \t 3.851987105943082\n",
            "99     \t [0.05970461 0.53394763 0.87106289]. \t  3.7765887954417665 \t 3.851987105943082\n",
            "100    \t [0.40074291 0.47767872 0.81693758]. \t  3.561391081117814 \t 3.851987105943082\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mrvkDXP0uvWU",
        "outputId": "290cebcd-9fe5-42c3-eda4-11d47882626c"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 5 \r\n",
        "\r\n",
        "np.random.seed(run_num_5)\r\n",
        "surrogate_winner_5 = dtStudentProcess(d_cov_func, nu = df)\r\n",
        "\r\n",
        "winner_5 = dGPGO_stp(surrogate_winner_5, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_5.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.55735327 0.54176063 0.56042378]. \t  0.9599610140567721 \t 0.9810564697651996\n",
            "init   \t [0.38204809 0.11909418 0.84599912]. \t  0.6913364116577386 \t 0.9810564697651996\n",
            "init   \t [0.73974161 0.93235756 0.85932801]. \t  0.9810564697651996 \t 0.9810564697651996\n",
            "init   \t [0.70810463 0.68096875 0.4086023 ]. \t  0.4017443293787304 \t 0.9810564697651996\n",
            "init   \t [0.77458462 0.9844762  0.96548486]. \t  0.4769494155906823 \t 0.9810564697651996\n",
            "1      \t [0.0655006  0.90661895 0.81035589]. \t  \u001b[92m1.403888408720423\u001b[0m \t 1.403888408720423\n",
            "2      \t [0.92779976 0.07015106 0.89872933]. \t  0.3773619776486492 \t 1.403888408720423\n",
            "3      \t [0.02311239 0.85121629 0.01672445]. \t  0.000979175972205631 \t 1.403888408720423\n",
            "4      \t [0.63734064 0.00107585 0.14661451]. \t  0.45497904023793434 \t 1.403888408720423\n",
            "5      \t [0.02649119 0.21253561 0.38567984]. \t  0.44504583667959385 \t 1.403888408720423\n",
            "6      \t [0.303553   1.         0.48788571]. \t  \u001b[92m2.001858897579644\u001b[0m \t 2.001858897579644\n",
            "7      \t [0.96120724 0.00869385 0.11090436]. \t  0.1490357072509709 \t 2.001858897579644\n",
            "8      \t [1.         1.         0.67244441]. \t  0.30380848762628 \t 2.001858897579644\n",
            "9      \t [1.         0.53543245 1.        ]. \t  1.988994656181241 \t 2.001858897579644\n",
            "10     \t [0.86119036 0.97529217 0.02050864]. \t  0.00014397567361177713 \t 2.001858897579644\n",
            "11     \t [0.01099975 0.07957465 0.07591263]. \t  0.2237707492634261 \t 2.001858897579644\n",
            "12     \t [0.14911506 0.3290728  0.95281534]. \t  1.7098328722346308 \t 2.001858897579644\n",
            "13     \t [0.73411531 0.19821982 0.00156609]. \t  0.07543112919420256 \t 2.001858897579644\n",
            "14     \t [0.56333027 0.45044711 1.        ]. \t  1.8197313065982061 \t 2.001858897579644\n",
            "15     \t [0.00679996 0.896335   0.40330367]. \t  1.4574502679611663 \t 2.001858897579644\n",
            "16     \t [1.         0.29931512 0.66107751]. \t  1.0101013513785777 \t 2.001858897579644\n",
            "17     \t [0.36392126 1.         0.16020691]. \t  0.020019655152101388 \t 2.001858897579644\n",
            "18     \t [0.02820986 0.0106109  0.6507289 ]. \t  0.16509943136682842 \t 2.001858897579644\n",
            "19     \t [0.34508868 0.74861647 1.        ]. \t  1.5144246375142514 \t 2.001858897579644\n",
            "20     \t [0.22569077 0.60853508 0.0086202 ]. \t  0.011462506495314401 \t 2.001858897579644\n",
            "21     \t [1.         0.84635122 0.13860465]. \t  0.002430240062395302 \t 2.001858897579644\n",
            "22     \t [1.         0.26883363 1.        ]. \t  0.8420664410637313 \t 2.001858897579644\n",
            "23     \t [0.90694382 0.65113192 0.00606792]. \t  0.0031562977479071106 \t 2.001858897579644\n",
            "24     \t [0.01662134 0.4744515  0.02028571]. \t  0.030905426905626237 \t 2.001858897579644\n",
            "25     \t [0.55426124 0.00722097 0.98122931]. \t  0.11851565183831886 \t 2.001858897579644\n",
            "26     \t [0.00283593 0.97434653 0.98676217]. \t  0.4547206903766693 \t 2.001858897579644\n",
            "27     \t [0.7979001  0.16795402 0.32658699]. \t  0.5070644250386597 \t 2.001858897579644\n",
            "28     \t [0.04170094 0.11775908 0.98261178]. \t  0.3379706633570333 \t 2.001858897579644\n",
            "29     \t [0.24203575 0.         0.17832335]. \t  0.655310909545025 \t 2.001858897579644\n",
            "30     \t [1. 1. 1.]. \t  0.31688362070415665 \t 2.001858897579644\n",
            "31     \t [0.         0.61032085 0.65890344]. \t  \u001b[92m2.2526674003928457\u001b[0m \t 2.2526674003928457\n",
            "32     \t [0.46667652 0.         0.49288277]. \t  0.20385332907880288 \t 2.2526674003928457\n",
            "33     \t [0.95490231 0.06856234 0.55292526]. \t  0.11572506513757812 \t 2.2526674003928457\n",
            "34     \t [0.11806983 0.70829225 0.34284806]. \t  0.6176442885662776 \t 2.2526674003928457\n",
            "35     \t [0.18799866 0.03033965 0.00439332]. \t  0.1057295301615257 \t 2.2526674003928457\n",
            "36     \t [0.39227894 1.         0.79580806]. \t  0.7992800999452567 \t 2.2526674003928457\n",
            "37     \t [0.78682034 0.322846   0.83361269]. \t  \u001b[92m2.3392976577586966\u001b[0m \t 2.3392976577586966\n",
            "38     \t [0.01851557 0.73179328 0.92267116]. \t  \u001b[92m2.541439500198345\u001b[0m \t 2.541439500198345\n",
            "39     \t [0.92148327 1.         0.29079351]. \t  0.04385231181000684 \t 2.541439500198345\n",
            "40     \t [0.02429968 0.38355441 0.81071282]. \t  \u001b[92m2.8581714577383166\u001b[0m \t 2.8581714577383166\n",
            "41     \t [0.00350406 0.96832658 0.07400942]. \t  0.0027425566612047517 \t 2.8581714577383166\n",
            "42     \t [0.99699999 0.3544959  0.18498762]. \t  0.1424897363055434 \t 2.8581714577383166\n",
            "43     \t [0.98531469 0.72350328 0.64731712]. \t  0.9066551768412172 \t 2.8581714577383166\n",
            "44     \t [9.84085145e-01 3.33994979e-01 6.01048689e-04]. \t  0.023753938854586935 \t 2.8581714577383166\n",
            "45     \t [0.02938599 0.56065939 0.97880502]. \t  2.444614676629394 \t 2.8581714577383166\n",
            "46     \t [0.81843138 0.61896335 0.92959729]. \t  \u001b[92m3.1220804486666545\u001b[0m \t 3.1220804486666545\n",
            "47     \t [1.39772935e-08 1.28843251e-08 1.57592460e-08]. \t  0.06797413792274837 \t 3.1220804486666545\n",
            "48     \t [0.48087506 0.37552946 0.09681172]. \t  0.20670135686767452 \t 3.1220804486666545\n",
            "49     \t [1.         0.75592037 0.92768349]. \t  2.2034072065342287 \t 3.1220804486666545\n",
            "50     \t [0.75433967 0.77601905 0.99902975]. \t  1.3526835917367122 \t 3.1220804486666545\n",
            "51     \t [0.9663284  0.52952608 0.85277496]. \t  \u001b[92m3.6725407378377124\u001b[0m \t 3.6725407378377124\n",
            "52     \t [0.90034421 0.6655254  0.88489927]. \t  3.238930099383878 \t 3.6725407378377124\n",
            "53     \t [0.2169261  0.47773111 0.80986923]. \t  3.5220369831844267 \t 3.6725407378377124\n",
            "54     \t [0.88272374 0.61000611 0.85637307]. \t  3.601537386603247 \t 3.6725407378377124\n",
            "55     \t [0.74647783 0.5573931  0.88686418]. \t  \u001b[92m3.6802412635433215\u001b[0m \t 3.6802412635433215\n",
            "56     \t [1.         0.53769226 0.84733775]. \t  3.6584399516162365 \t 3.6802412635433215\n",
            "57     \t [0.20357473 0.55628301 0.86840925]. \t  \u001b[92m3.8319440247652423\u001b[0m \t 3.8319440247652423\n",
            "58     \t [0.15703456 0.4857899  0.81248711]. \t  3.567036659334843 \t 3.8319440247652423\n",
            "59     \t [0.37762602 0.67144825 0.80435728]. \t  3.217213017499165 \t 3.8319440247652423\n",
            "60     \t [0.8650895  0.53371127 0.85200708]. \t  3.720289991324008 \t 3.8319440247652423\n",
            "61     \t [0.14852082 0.68274042 0.75546108]. \t  2.877345582711322 \t 3.8319440247652423\n",
            "62     \t [0.82490751 0.51898101 0.8382968 ]. \t  3.685576947844261 \t 3.8319440247652423\n",
            "63     \t [0.73339043 0.56129041 0.80457228]. \t  3.5140754674532166 \t 3.8319440247652423\n",
            "64     \t [0.7051219  0.54793919 0.89625321]. \t  3.6212829907243425 \t 3.8319440247652423\n",
            "65     \t [0.40202721 0.56810384 0.83665049]. \t  3.8188313812124632 \t 3.8319440247652423\n",
            "66     \t [0.66656231 0.56625224 0.8542286 ]. \t  3.7931286149379186 \t 3.8319440247652423\n",
            "67     \t [0.87411204 0.52743669 0.8615124 ]. \t  3.7060706710986158 \t 3.8319440247652423\n",
            "68     \t [0.81281979 0.54964774 0.86170046]. \t  3.749353961047554 \t 3.8319440247652423\n",
            "69     \t [0.99021145 0.57173257 0.85201384]. \t  3.656089023378125 \t 3.8319440247652423\n",
            "70     \t [0.55157576 0.60543023 0.78574757]. \t  3.2982995037987797 \t 3.8319440247652423\n",
            "71     \t [0.48620865 0.45773753 0.84542893]. \t  3.5366628135246736 \t 3.8319440247652423\n",
            "72     \t [0.56181414 0.51656437 0.92246025]. \t  3.3117731737407983 \t 3.8319440247652423\n",
            "73     \t [0.24029324 0.52956788 0.84033948]. \t  3.824858416393607 \t 3.8319440247652423\n",
            "74     \t [0.35867473 0.60216656 0.89857087]. \t  3.592519950004152 \t 3.8319440247652423\n",
            "75     \t [0.64609821 0.55085018 0.84020802]. \t  3.782805303888799 \t 3.8319440247652423\n",
            "76     \t [0.50379806 0.54691122 0.87808006]. \t  3.7818999942935965 \t 3.8319440247652423\n",
            "77     \t [0.46132008 0.5934778  0.82171707]. \t  3.6842572286371547 \t 3.8319440247652423\n",
            "78     \t [0.57075999 0.6008048  0.83675889]. \t  3.704333813282498 \t 3.8319440247652423\n",
            "79     \t [0.5994625  0.57744742 0.83831724]. \t  3.763884979437176 \t 3.8319440247652423\n",
            "80     \t [0.38553756 0.58379017 0.90188919]. \t  3.6038980689696194 \t 3.8319440247652423\n",
            "81     \t [0.69942706 0.5680039  0.86485202]. \t  3.775253875561175 \t 3.8319440247652423\n",
            "82     \t [0.34150973 0.62080716 0.87161714]. \t  3.683000097322105 \t 3.8319440247652423\n",
            "83     \t [0.51757383 0.53352718 0.83084139]. \t  3.773780998894142 \t 3.8319440247652423\n",
            "84     \t [0.70430162 0.56449687 0.83744248]. \t  3.7453379035534673 \t 3.8319440247652423\n",
            "85     \t [0.84274566 0.61162806 0.83588888]. \t  3.5537760451154186 \t 3.8319440247652423\n",
            "86     \t [0.62299063 0.6285273  0.822756  ]. \t  3.492418605923472 \t 3.8319440247652423\n",
            "87     \t [0.48502903 0.58257667 0.83946846]. \t  3.788522128464938 \t 3.8319440247652423\n",
            "88     \t [0.25779046 0.47917257 0.87196063]. \t  3.6161392604750966 \t 3.8319440247652423\n",
            "89     \t [0.564573   0.53740841 0.91181012]. \t  3.4865105367888414 \t 3.8319440247652423\n",
            "90     \t [0.35597132 0.55087124 0.85829999]. \t  \u001b[92m3.8567738243990277\u001b[0m \t 3.8567738243990277\n",
            "91     \t [0.08581459 0.49253703 0.8946318 ]. \t  3.510223274279399 \t 3.8567738243990277\n",
            "92     \t [0.69508139 0.54319334 0.89031536]. \t  3.6657824900213933 \t 3.8567738243990277\n",
            "93     \t [0.7395561  0.57672761 0.83125079]. \t  3.684552598203736 \t 3.8567738243990277\n",
            "94     \t [0.5110653  0.4925732  0.82948548]. \t  3.668783324154873 \t 3.8567738243990277\n",
            "95     \t [0.58230778 0.54530749 0.88060184]. \t  3.754196744163601 \t 3.8567738243990277\n",
            "96     \t [0.65458974 0.57688882 0.87168833]. \t  3.7606725368931455 \t 3.8567738243990277\n",
            "97     \t [0.81409674 0.59396796 0.82395496]. \t  3.563915285825249 \t 3.8567738243990277\n",
            "98     \t [0.47681252 0.54065749 0.90970614]. \t  3.5252683205257 \t 3.8567738243990277\n",
            "99     \t [0.50912542 0.54761832 0.88511591]. \t  3.7423605177697414 \t 3.8567738243990277\n",
            "100    \t [0.60736367 0.57922544 0.83456141]. \t  3.742917527458677 \t 3.8567738243990277\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cO3I_9cbuvY2",
        "outputId": "25be5415-d30a-4d06-a522-2f40ce3c2779"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 6 \r\n",
        "\r\n",
        "np.random.seed(run_num_6)\r\n",
        "surrogate_winner_6 = dtStudentProcess(d_cov_func, nu = df)\r\n",
        "\r\n",
        "winner_6 = dGPGO_stp(surrogate_winner_6, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_6.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.89286015 0.33197981 0.82122912]. \t  2.3879127825536575 \t 2.5106636917702634\n",
            "init   \t [0.04169663 0.10765668 0.59505206]. \t  0.2321934630074273 \t 2.5106636917702634\n",
            "init   \t [0.52981736 0.41880743 0.33540785]. \t  0.37989557867087576 \t 2.5106636917702634\n",
            "init   \t [0.62251943 0.43814143 0.73588211]. \t  2.5106636917702634 \t 2.5106636917702634\n",
            "init   \t [0.51803641 0.5788586  0.6453551 ]. \t  1.7289203948034264 \t 2.5106636917702634\n",
            "1      \t [0.40499445 0.29165478 1.        ]. \t  0.993029390677449 \t 2.5106636917702634\n",
            "2      \t [1. 1. 1.]. \t  0.3168836207042244 \t 2.5106636917702634\n",
            "3      \t [0.96791463 0.11639543 0.35057408]. \t  0.2788071324534709 \t 2.5106636917702634\n",
            "4      \t [0.9915895  0.97629604 0.1207269 ]. \t  0.0010193129366017407 \t 2.5106636917702634\n",
            "5      \t [0.13151319 0.00658334 0.01207752]. \t  0.10591386851134277 \t 2.5106636917702634\n",
            "6      \t [0.82977251 0.09534006 0.88002476]. \t  0.5124375149244097 \t 2.5106636917702634\n",
            "7      \t [1.         0.72502307 0.5963915 ]. \t  0.5829890753749627 \t 2.5106636917702634\n",
            "8      \t [0.04117291 0.77388797 0.        ]. \t  0.0013981351084605524 \t 2.5106636917702634\n",
            "9      \t [0.43021369 0.99135273 1.        ]. \t  0.35845344961671827 \t 2.5106636917702634\n",
            "10     \t [0.01482046 0.83904657 0.87282687]. \t  1.8356572915728449 \t 2.5106636917702634\n",
            "11     \t [0.         0.31216391 0.        ]. \t  0.053268982513046584 \t 2.5106636917702634\n",
            "12     \t [0.99782173 0.33221121 0.00122272]. \t  0.022968470952239806 \t 2.5106636917702634\n",
            "13     \t [0.06768473 0.98835432 0.43637529]. \t  1.7201092789376264 \t 2.5106636917702634\n",
            "14     \t [0.75828276 0.6794144  0.99116716]. \t  1.9574048222276157 \t 2.5106636917702634\n",
            "15     \t [0.53497861 0.01629817 0.02246142]. \t  0.1377202054779859 \t 2.5106636917702634\n",
            "16     \t [0.55255455 0.84055299 0.01949036]. \t  0.0010675249431229448 \t 2.5106636917702634\n",
            "17     \t [0.00242388 0.23760947 0.887182  ]. \t  1.4103925449753358 \t 2.5106636917702634\n",
            "18     \t [0.         0.50247569 0.65573905]. \t  1.8757381407112779 \t 2.5106636917702634\n",
            "19     \t [0.82681248 0.97096696 0.45524768]. \t  0.4368822308269638 \t 2.5106636917702634\n",
            "20     \t [1.         0.55677724 1.        ]. \t  2.0066744901104614 \t 2.5106636917702634\n",
            "21     \t [0.24329676 0.5590133  1.        ]. \t  2.0829524481980792 \t 2.5106636917702634\n",
            "22     \t [0.57680215 0.0207608  0.64610695]. \t  0.17894747321547871 \t 2.5106636917702634\n",
            "23     \t [0. 0. 0.]. \t  0.06797411659013229 \t 2.5106636917702634\n",
            "24     \t [0.36783484 0.92493951 0.12891912]. \t  0.01120564312033409 \t 2.5106636917702634\n",
            "25     \t [1.         1.         0.76107057]. \t  0.4148283513273915 \t 2.5106636917702634\n",
            "26     \t [0.09902868 0.95307833 0.0080633 ]. \t  0.0004583990106005402 \t 2.5106636917702634\n",
            "27     \t [0.48317771 0.32584769 0.        ]. \t  0.07289771681569239 \t 2.5106636917702634\n",
            "28     \t [0.99856359 0.01723737 0.18973095]. \t  0.23005607136237888 \t 2.5106636917702634\n",
            "29     \t [0.22705775 0.         0.96713288]. \t  0.12390438683391371 \t 2.5106636917702634\n",
            "30     \t [0.16338974 0.95367156 0.74506483]. \t  1.412315249261912 \t 2.5106636917702634\n",
            "31     \t [7.75889841e-09 1.05571543e-08 2.77368177e-01]. \t  0.5782105508564478 \t 2.5106636917702634\n",
            "32     \t [0.99999998 0.18528758 0.99999999]. \t  0.48461428171601806 \t 2.5106636917702634\n",
            "33     \t [0.99891849 0.02382642 0.5105529 ]. \t  0.07808028864771989 \t 2.5106636917702634\n",
            "34     \t [0.8011336  1.         0.12200157]. \t  0.002332462501699834 \t 2.5106636917702634\n",
            "35     \t [0.01161388 0.66515042 0.99551219]. \t  1.9498416039361437 \t 2.5106636917702634\n",
            "36     \t [0.18204172 1.         1.        ]. \t  0.333802204948967 \t 2.5106636917702634\n",
            "37     \t [0.         0.         0.75808152]. \t  0.2390071441929555 \t 2.5106636917702634\n",
            "38     \t [0.7538223  0.66974844 0.1634601 ]. \t  0.027634664269832084 \t 2.5106636917702634\n",
            "39     \t [0.         0.76066974 0.32291872]. \t  0.5193752998933108 \t 2.5106636917702634\n",
            "40     \t [0.         0.99999999 0.        ]. \t  0.00027353677936475763 \t 2.5106636917702634\n",
            "41     \t [1.         1.         0.41091026]. \t  0.1282363331965388 \t 2.5106636917702634\n",
            "42     \t [0.90414179 0.11986663 0.01241238]. \t  0.06029171228202095 \t 2.5106636917702634\n",
            "43     \t [0.20788636 0.5178986  0.72637604]. \t  \u001b[92m2.7561780720324176\u001b[0m \t 2.7561780720324176\n",
            "44     \t [0.02429092 0.03217424 0.94618094]. \t  0.20149989838031493 \t 2.7561780720324176\n",
            "45     \t [0.8294516  0.41309935 1.        ]. \t  1.620897553741421 \t 2.7561780720324176\n",
            "46     \t [0.846167   0.65811644 0.        ]. \t  0.0032034939152910447 \t 2.7561780720324176\n",
            "47     \t [1.         0.36935161 0.57160501]. \t  0.47877278603843093 \t 2.7561780720324176\n",
            "48     \t [0.24713302 0.40899958 0.84727641]. \t  \u001b[92m3.183172674770348\u001b[0m \t 3.183172674770348\n",
            "49     \t [0.71921216 0.9802155  0.92768946]. \t  0.6012607717972372 \t 3.183172674770348\n",
            "50     \t [0.30810967 0.00030826 0.29092138]. \t  0.849029560868149 \t 3.183172674770348\n",
            "51     \t [0.43571619 0.5584063  0.85472462]. \t  \u001b[92m3.8511041711989336\u001b[0m \t 3.8511041711989336\n",
            "52     \t [0.43864306 0.64938556 0.83370414]. \t  3.4976691510650046 \t 3.8511041711989336\n",
            "53     \t [0.39697469 0.57339444 0.84547956]. \t  3.8354428238968894 \t 3.8511041711989336\n",
            "54     \t [0.44166976 0.56954253 0.88638824]. \t  3.7404553487367203 \t 3.8511041711989336\n",
            "55     \t [0.33814282 0.53888113 0.87698888]. \t  3.7927451676328214 \t 3.8511041711989336\n",
            "56     \t [0.50671614 0.59326421 0.87247371]. \t  3.7574224876176476 \t 3.8511041711989336\n",
            "57     \t [0.3010227  0.39673964 0.81760034]. \t  3.033062224603587 \t 3.8511041711989336\n",
            "58     \t [0.48007484 0.60469049 0.84473703]. \t  3.7403571285822976 \t 3.8511041711989336\n",
            "59     \t [0.45281498 0.53674927 0.80896081]. \t  3.6562317814385388 \t 3.8511041711989336\n",
            "60     \t [0.38276567 0.63276709 0.83612114]. \t  3.6134141289082122 \t 3.8511041711989336\n",
            "61     \t [0.44914004 0.58174251 0.84384169]. \t  3.80949057755287 \t 3.8511041711989336\n",
            "62     \t [0.28152554 0.52961078 0.80649112]. \t  3.654308429983928 \t 3.8511041711989336\n",
            "63     \t [0.43990698 0.51585291 0.82394644]. \t  3.7259390969896207 \t 3.8511041711989336\n",
            "64     \t [0.48409497 0.56947054 0.87169188]. \t  3.8073690584112034 \t 3.8511041711989336\n",
            "65     \t [0.54506429 0.51898738 0.90608446]. \t  3.5126839783928183 \t 3.8511041711989336\n",
            "66     \t [0.62614854 0.47336079 0.86149128]. \t  3.591142457212372 \t 3.8511041711989336\n",
            "67     \t [0.3801684  0.61373906 0.8667593 ]. \t  3.722747463665948 \t 3.8511041711989336\n",
            "68     \t [0.33329216 0.54664915 0.83139219]. \t  3.814578193053731 \t 3.8511041711989336\n",
            "69     \t [0.23597736 0.7127114  0.86464772]. \t  3.0771017170708928 \t 3.8511041711989336\n",
            "70     \t [0.60734781 0.66247601 0.86074059]. \t  3.4123505131269445 \t 3.8511041711989336\n",
            "71     \t [0.36592983 0.638043   0.88703633]. \t  3.527669234103462 \t 3.8511041711989336\n",
            "72     \t [0.48898651 0.48716105 0.86181623]. \t  3.682035980692091 \t 3.8511041711989336\n",
            "73     \t [0.47256406 0.56630301 0.81099718]. \t  3.660929440011475 \t 3.8511041711989336\n",
            "74     \t [0.30925331 0.62125624 0.85847677]. \t  3.708164969709453 \t 3.8511041711989336\n",
            "75     \t [0.16852748 0.57308344 0.17810079]. \t  0.10435740141067217 \t 3.8511041711989336\n",
            "76     \t [0.38994468 0.530838   0.86894034]. \t  3.811626926351229 \t 3.8511041711989336\n",
            "77     \t [0.5277165  0.65386089 0.88897478]. \t  3.4045380771848577 \t 3.8511041711989336\n",
            "78     \t [0.48597105 0.41126834 0.86066449]. \t  3.180481713049912 \t 3.8511041711989336\n",
            "79     \t [0.29173637 0.626423   0.8733764 ]. \t  3.6530812681731972 \t 3.8511041711989336\n",
            "80     \t [0.2190721  0.59122499 0.87634319]. \t  3.7626500762580988 \t 3.8511041711989336\n",
            "81     \t [0.43352151 0.61542801 0.82891886]. \t  3.6536927036649147 \t 3.8511041711989336\n",
            "82     \t [0.66961586 0.60840068 0.89917508]. \t  3.52241315194838 \t 3.8511041711989336\n",
            "83     \t [0.228383   0.68863865 0.86735976]. \t  3.271653596111253 \t 3.8511041711989336\n",
            "84     \t [0.39202579 0.67783823 0.91272571]. \t  3.0792661344611245 \t 3.8511041711989336\n",
            "85     \t [0.50307403 0.49125393 0.86759852]. \t  3.68305323578096 \t 3.8511041711989336\n",
            "86     \t [0.52796841 0.57529316 0.83388998]. \t  3.771274839369898 \t 3.8511041711989336\n",
            "87     \t [0.41836076 0.48792281 0.85995252]. \t  3.6961721248298502 \t 3.8511041711989336\n",
            "88     \t [0.73663032 0.50880545 0.86011786]. \t  3.7128051081470823 \t 3.8511041711989336\n",
            "89     \t [0.11460349 0.60128732 0.80969925]. \t  3.6126172658391527 \t 3.8511041711989336\n",
            "90     \t [0.24957718 0.50803927 0.78131922]. \t  3.3839384794292044 \t 3.8511041711989336\n",
            "91     \t [0.3433286  0.65308069 0.92654945]. \t  3.0817486329020642 \t 3.8511041711989336\n",
            "92     \t [0.36784534 0.65946029 0.86313014]. \t  3.484896294654157 \t 3.8511041711989336\n",
            "93     \t [0.4892291  0.56872635 0.83990144]. \t  3.812130597036334 \t 3.8511041711989336\n",
            "94     \t [0.65954701 0.48516216 0.84579156]. \t  3.6481421946514487 \t 3.8511041711989336\n",
            "95     \t [0.00883835 0.6013543  0.83189179]. \t  3.7006348348705727 \t 3.8511041711989336\n",
            "96     \t [0.60226808 0.56527437 0.90002825]. \t  3.6142165519154896 \t 3.8511041711989336\n",
            "97     \t [0.28897352 0.46675652 0.88104347]. \t  3.5019043779108125 \t 3.8511041711989336\n",
            "98     \t [0.26720572 0.53391107 0.82250967]. \t  3.764969429936246 \t 3.8511041711989336\n",
            "99     \t [0.76586006 0.55485107 0.85431554]. \t  3.7679018648531404 \t 3.8511041711989336\n",
            "100    \t [0.3778065  0.51867278 0.8502434 ]. \t  3.8140234881610064 \t 3.8511041711989336\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8rQbLZD8uvbI",
        "outputId": "920a47f8-8ce9-4c62-b2c4-b16858e43559"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 7 \r\n",
        "\r\n",
        "np.random.seed(run_num_7)\r\n",
        "surrogate_winner_7 = dtStudentProcess(d_cov_func, nu = df)\r\n",
        "\r\n",
        "winner_7 = dGPGO_stp(surrogate_winner_7, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_7.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.15266373 0.30235661 0.06203641]. \t  0.1742288077468587 \t 1.6237282255098657\n",
            "init   \t [0.45986034 0.83525338 0.92699705]. \t  1.6237282255098657 \t 1.6237282255098657\n",
            "init   \t [0.72698898 0.76849622 0.26920507]. \t  0.08405715787489784 \t 1.6237282255098657\n",
            "init   \t [0.64402929 0.09337326 0.07968589]. \t  0.2756481743251193 \t 1.6237282255098657\n",
            "init   \t [0.58961375 0.34334054 0.98887615]. \t  1.4020548914621052 \t 1.6237282255098657\n",
            "1      \t [0.0095305  0.06847696 0.97789675]. \t  0.22422993094430546 \t 1.6237282255098657\n",
            "2      \t [1. 1. 1.]. \t  0.31688362070415665 \t 1.6237282255098657\n",
            "3      \t [0.08306623 0.99668859 0.81981745]. \t  0.8193325731144557 \t 1.6237282255098657\n",
            "4      \t [0.22595933 0.95348785 0.12294433]. \t  0.010520248665944764 \t 1.6237282255098657\n",
            "5      \t [0.96465563 0.05659614 0.70853251]. \t  0.3326255074889067 \t 1.6237282255098657\n",
            "6      \t [0.3714494  0.05992124 0.47930973]. \t  0.2780879042129226 \t 1.6237282255098657\n",
            "7      \t [0.08978758 0.58671321 0.7206821 ]. \t  \u001b[92m2.7673013179295816\u001b[0m \t 2.7673013179295816\n",
            "8      \t [0.99064128 0.92226591 0.02931425]. \t  0.00015963016353151416 \t 2.7673013179295816\n",
            "9      \t [1.         0.40752958 0.24679289]. \t  0.13056381789607957 \t 2.7673013179295816\n",
            "10     \t [1.         0.51467805 1.        ]. \t  1.955138912888252 \t 2.7673013179295816\n",
            "11     \t [0.03795813 0.72045593 0.98192351]. \t  1.8979577221551795 \t 2.7673013179295816\n",
            "12     \t [1.         1.         0.68872136]. \t  0.3167106233330268 \t 2.7673013179295816\n",
            "13     \t [0.0027086  0.0528942  0.39439461]. \t  0.40042173991843166 \t 2.7673013179295816\n",
            "14     \t [0.02542653 0.66596513 0.47121083]. \t  1.617833738762171 \t 2.7673013179295816\n",
            "15     \t [0.94869112 0.00191215 0.22766439]. \t  0.304834859442122 \t 2.7673013179295816\n",
            "16     \t [1.         0.18157409 0.97971891]. \t  0.5646398282130499 \t 2.7673013179295816\n",
            "17     \t [0.58179329 0.50851522 0.        ]. \t  0.02213726814373627 \t 2.7673013179295816\n",
            "18     \t [0.00154781 0.36016031 0.55355155]. \t  0.5827700486594837 \t 2.7673013179295816\n",
            "19     \t [0. 0. 0.]. \t  0.06797411659013229 \t 2.7673013179295816\n",
            "20     \t [0.29999954 1.         0.48565109]. \t  1.9915837660042053 \t 2.7673013179295816\n",
            "21     \t [0.97818401 0.1403663  0.00690951]. \t  0.042713816408527346 \t 2.7673013179295816\n",
            "22     \t [0.34436512 0.60743132 0.60082685]. \t  1.8019653850753192 \t 2.7673013179295816\n",
            "23     \t [0.28536015 0.27917042 0.98155848]. \t  1.0866066208551546 \t 2.7673013179295816\n",
            "24     \t [0.01185606 0.71123816 0.0170026 ]. \t  0.0034322459069532647 \t 2.7673013179295816\n",
            "25     \t [0.44767277 0.00332131 0.04095836]. \t  0.18548934830724537 \t 2.7673013179295816\n",
            "26     \t [0.01629024 0.99339774 0.13210786]. \t  0.012107530095055512 \t 2.7673013179295816\n",
            "27     \t [0.46066    0.00143244 0.77506984]. \t  0.253147305406061 \t 2.7673013179295816\n",
            "28     \t [0.9534433  1.         0.31998681]. \t  0.057863353981463646 \t 2.7673013179295816\n",
            "29     \t [0.68237394 1.         0.63948524]. \t  0.8675097707687726 \t 2.7673013179295816\n",
            "30     \t [0.83066415 0.72134499 1.        ]. \t  1.6199887423476755 \t 2.7673013179295816\n",
            "31     \t [0.         0.00366356 0.64820643]. \t  0.15208190280604045 \t 2.7673013179295816\n",
            "32     \t [0.01207604 0.73282316 0.77466541]. \t  2.6732228492880026 \t 2.7673013179295816\n",
            "33     \t [0.94285061 0.45976222 0.7643861 ]. \t  \u001b[92m2.8387508006497235\u001b[0m \t 2.8387508006497235\n",
            "34     \t [0.00381478 0.55861788 0.97447272]. \t  2.5168138985712205 \t 2.8387508006497235\n",
            "35     \t [0.58449823 0.93793704 0.04387004]. \t  0.0008118301152897224 \t 2.8387508006497235\n",
            "36     \t [1.         0.79373496 0.4834786 ]. \t  0.26065930711850743 \t 2.8387508006497235\n",
            "37     \t [0.88769433 0.00724591 0.94307456]. \t  0.15796149391657166 \t 2.8387508006497235\n",
            "38     \t [0.58635066 0.58266936 0.82843298]. \t  \u001b[92m3.7124315085053086\u001b[0m \t 3.7124315085053086\n",
            "39     \t [0.71788578 0.58268795 0.77385733]. \t  3.1325881330984444 \t 3.7124315085053086\n",
            "40     \t [0.26441856 0.57213871 0.93573931]. \t  3.198845400899454 \t 3.7124315085053086\n",
            "41     \t [0.99298603 0.65119981 0.8163711 ]. \t  3.1402437236663596 \t 3.7124315085053086\n",
            "42     \t [0.         0.24948784 0.        ]. \t  0.06540334208524073 \t 3.7124315085053086\n",
            "43     \t [0.48015679 0.61186884 0.83331497]. \t  3.675836194252298 \t 3.7124315085053086\n",
            "44     \t [0.56982141 0.53070458 0.82710465]. \t  \u001b[92m3.7396547417311625\u001b[0m \t 3.7396547417311625\n",
            "45     \t [0.63402741 0.59231929 0.83010831]. \t  3.679288089023922 \t 3.7396547417311625\n",
            "46     \t [0.45390084 0.41880429 0.80389041]. \t  3.12823298300722 \t 3.7396547417311625\n",
            "47     \t [0.58004588 0.55917011 0.8548782 ]. \t  \u001b[92m3.8228080064413916\u001b[0m \t 3.8228080064413916\n",
            "48     \t [0.57073715 0.54414738 0.88520533]. \t  3.7289206976847 \t 3.8228080064413916\n",
            "49     \t [0.97791232 0.67406689 0.05313839]. \t  0.00383867481721726 \t 3.8228080064413916\n",
            "50     \t [0.76815558 0.53788179 0.87587183]. \t  3.720401005832236 \t 3.8228080064413916\n",
            "51     \t [0.81221335 0.52554409 0.85463609]. \t  3.7301796861155547 \t 3.8228080064413916\n",
            "52     \t [0.36152506 0.5948774  0.83405002]. \t  3.762311525016517 \t 3.8228080064413916\n",
            "53     \t [0.64794944 0.53548958 0.86938323]. \t  3.775541113671644 \t 3.8228080064413916\n",
            "54     \t [0.0153418  0.98521494 0.55009119]. \t  2.6031381026148774 \t 3.8228080064413916\n",
            "55     \t [0.58564498 0.62356618 0.89427531]. \t  3.5200609019202362 \t 3.8228080064413916\n",
            "56     \t [0.6267193  0.55360057 0.83376892]. \t  3.764310915547127 \t 3.8228080064413916\n",
            "57     \t [0.37452323 0.5849925  0.85413836]. \t  \u001b[92m3.8252663490570997\u001b[0m \t 3.8252663490570997\n",
            "58     \t [0.5967447  0.53109382 0.86230594]. \t  3.7980351191562343 \t 3.8252663490570997\n",
            "59     \t [0.5340796  0.55102156 0.85451563]. \t  \u001b[92m3.8347681843444104\u001b[0m \t 3.8347681843444104\n",
            "60     \t [0.78158552 0.53728635 0.86812877]. \t  3.7412741788220742 \t 3.8347681843444104\n",
            "61     \t [0.44585106 0.55329484 0.84947868]. \t  \u001b[92m3.8483988419515702\u001b[0m \t 3.8483988419515702\n",
            "62     \t [0.60491594 0.58294968 0.86498112]. \t  3.778500704681911 \t 3.8483988419515702\n",
            "63     \t [0.61877967 0.5174786  0.86258424]. \t  3.7647211772738722 \t 3.8483988419515702\n",
            "64     \t [0.46673913 0.55405029 0.87877963]. \t  3.786409990901845 \t 3.8483988419515702\n",
            "65     \t [0.44939898 0.59139935 0.85047147]. \t  3.797423708564322 \t 3.8483988419515702\n",
            "66     \t [0.48948333 0.5813279  0.84392529]. \t  3.8022142177965983 \t 3.8483988419515702\n",
            "67     \t [0.47180862 0.57275259 0.86202696]. \t  3.828555663463025 \t 3.8483988419515702\n",
            "68     \t [0.52869124 0.65044722 0.85710984]. \t  3.5148521049836847 \t 3.8483988419515702\n",
            "69     \t [0.4625014  0.56451064 0.87590134]. \t  3.7981351938991805 \t 3.8483988419515702\n",
            "70     \t [0.61999861 0.53238463 0.81222299]. \t  3.629868987753484 \t 3.8483988419515702\n",
            "71     \t [0.45253329 0.55771361 0.85795012]. \t  3.8476287580585917 \t 3.8483988419515702\n",
            "72     \t [0.39487297 0.55449026 0.87567485]. \t  3.8079475494327877 \t 3.8483988419515702\n",
            "73     \t [0.47901054 0.54631613 0.89461361]. \t  3.6763913305604694 \t 3.8483988419515702\n",
            "74     \t [0.42191183 0.61897629 0.82616775]. \t  3.6277972224522346 \t 3.8483988419515702\n",
            "75     \t [0.48879501 0.57598934 0.84264785]. \t  3.809254374834968 \t 3.8483988419515702\n",
            "76     \t [0.68214319 0.59852197 0.86765104]. \t  3.7111796149576 \t 3.8483988419515702\n",
            "77     \t [0.4964353  0.60225286 0.84282552]. \t  3.7403559814566902 \t 3.8483988419515702\n",
            "78     \t [0.50979907 0.51869447 0.89015176]. \t  3.6589329920250018 \t 3.8483988419515702\n",
            "79     \t [0.56118056 0.6100441  0.82694539]. \t  3.6261514858160178 \t 3.8483988419515702\n",
            "80     \t [0.72837843 0.49911905 0.84815961]. \t  3.685979712154158 \t 3.8483988419515702\n",
            "81     \t [0.44280206 0.63711349 0.86091196]. \t  3.611710810406915 \t 3.8483988419515702\n",
            "82     \t [0.66214361 0.55067432 0.84426226]. \t  3.788862426553371 \t 3.8483988419515702\n",
            "83     \t [0.6592047  0.56907599 0.87603691]. \t  3.7543171007440113 \t 3.8483988419515702\n",
            "84     \t [0.47920911 0.58039331 0.84708004]. \t  3.8125155432983235 \t 3.8483988419515702\n",
            "85     \t [0.50249448 0.57940034 0.85356964]. \t  3.816391624167448 \t 3.8483988419515702\n",
            "86     \t [0.46471271 0.59335939 0.80714464]. \t  3.578788747387582 \t 3.8483988419515702\n",
            "87     \t [0.51265354 0.51873578 0.88156774]. \t  3.7154952412918973 \t 3.8483988419515702\n",
            "88     \t [0.3552446  0.59906634 0.85134439]. \t  3.789204011969339 \t 3.8483988419515702\n",
            "89     \t [0.46398422 0.5790151  0.82800553]. \t  3.75352454776188 \t 3.8483988419515702\n",
            "90     \t [0.53620657 0.53620648 0.8850293 ]. \t  3.726769016072927 \t 3.8483988419515702\n",
            "91     \t [0.32000099 0.64126481 0.84543524]. \t  3.599962133750771 \t 3.8483988419515702\n",
            "92     \t [0.66885226 0.57050478 0.88276053]. \t  3.7187650958225165 \t 3.8483988419515702\n",
            "93     \t [0.38698081 0.57047309 0.8756528 ]. \t  3.801882408391694 \t 3.8483988419515702\n",
            "94     \t [0.45396972 0.48853604 0.85343555]. \t  3.7034168638321554 \t 3.8483988419515702\n",
            "95     \t [0.59695882 0.58469545 0.85959824]. \t  3.7827333171135717 \t 3.8483988419515702\n",
            "96     \t [0.4857485  0.58319366 0.8449725 ]. \t  3.801414963234894 \t 3.8483988419515702\n",
            "97     \t [0.43423529 0.58528538 0.84085921]. \t  3.797255931155305 \t 3.8483988419515702\n",
            "98     \t [0.54328246 0.56499754 0.85429851]. \t  3.8274544179937373 \t 3.8483988419515702\n",
            "99     \t [0.49537259 0.54959436 0.84770556]. \t  3.837603334842705 \t 3.8483988419515702\n",
            "100    \t [0.4875058  0.63157789 0.86083305]. \t  3.632446408694735 \t 3.8483988419515702\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6YRio_skuvd2",
        "outputId": "1eda1d13-fa71-4f84-9d9e-999175ca1843"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 8 \r\n",
        "\r\n",
        "np.random.seed(run_num_8)\r\n",
        "surrogate_winner_8 = dtStudentProcess(d_cov_func, nu = df)\r\n",
        "\r\n",
        "winner_8 = dGPGO_stp(surrogate_winner_8, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_8.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.10949987 0.04221501 0.59969258]. \t  0.15732379460832624 \t 0.8830091449513892\n",
            "init   \t [0.29987071 0.79661178 0.36619613]. \t  0.8830091449513892 \t 0.8830091449513892\n",
            "init   \t [0.88060834 0.29784163 0.32910326]. \t  0.2992873424905509 \t 0.8830091449513892\n",
            "init   \t [0.56871692 0.74392742 0.05385289]. \t  0.0051668955191694075 \t 0.8830091449513892\n",
            "init   \t [0.60207437 0.42931858 0.20547361]. \t  0.2908249205255991 \t 0.8830091449513892\n",
            "1      \t [1. 1. 1.]. \t  0.31688362070415665 \t 0.8830091449513892\n",
            "2      \t [0.09250474 0.65862611 0.98455533]. \t  \u001b[92m2.16574947683527\u001b[0m \t 2.16574947683527\n",
            "3      \t [0.72632131 0.2025871  0.98295773]. \t  0.6563119890328053 \t 2.16574947683527\n",
            "4      \t [0.68307375 0.03550334 0.03657775]. \t  0.1409270525778632 \t 2.16574947683527\n",
            "5      \t [0.29485996 1.         1.        ]. \t  0.33431749399514926 \t 2.16574947683527\n",
            "6      \t [0.05244061 0.63266862 0.24798141]. \t  0.15047521725959812 \t 2.16574947683527\n",
            "7      \t [1.         1.         0.45235394]. \t  0.17567782340974092 \t 2.16574947683527\n",
            "8      \t [0.2956596  0.13234635 1.        ]. \t  0.3302317180358857 \t 2.16574947683527\n",
            "9      \t [0.58116038 0.63936556 1.        ]. \t  1.976608114382244 \t 2.16574947683527\n",
            "10     \t [1.         0.58411045 1.        ]. \t  2.0032229831515265 \t 2.16574947683527\n",
            "11     \t [0.97518476 0.65973401 0.07392202]. \t  0.005878681794498589 \t 2.16574947683527\n",
            "12     \t [0.16686051 0.23260515 0.00305989]. \t  0.09529913329363152 \t 2.16574947683527\n",
            "13     \t [0.01668434 0.9442099  0.15398436]. \t  0.02299214800467075 \t 2.16574947683527\n",
            "14     \t [0.03128145 0.99827395 0.80806223]. \t  0.8431384396630579 \t 2.16574947683527\n",
            "15     \t [0.0127381  0.01117767 0.81181977]. \t  0.27446790509124414 \t 2.16574947683527\n",
            "16     \t [1.         0.69778618 0.70633157]. \t  1.554126935911118 \t 2.16574947683527\n",
            "17     \t [0.23434159 0.40567641 0.77142126]. \t  \u001b[92m2.7914013635223323\u001b[0m \t 2.7914013635223323\n",
            "18     \t [0.66940442 1.         0.6211031 ]. \t  0.9491569325150981 \t 2.7914013635223323\n",
            "19     \t [0.959752   0.04545155 0.97906076]. \t  0.17439157360892166 \t 2.7914013635223323\n",
            "20     \t [0.97722773 0.20378811 0.07686598]. \t  0.10295887245899682 \t 2.7914013635223323\n",
            "21     \t [0.00761835 0.5482665  0.78027156]. \t  \u001b[92m3.3916277796587684\u001b[0m \t 3.3916277796587684\n",
            "22     \t [2.03236863e-12 2.03236863e-12 2.03236863e-12]. \t  0.06797411659297696 \t 3.3916277796587684\n",
            "23     \t [0.00126691 0.45695769 0.96438921]. \t  2.4043558056937187 \t 3.3916277796587684\n",
            "24     \t [0.94718417 0.98957096 0.11959339]. \t  0.0011820305154726612 \t 3.3916277796587684\n",
            "25     \t [6.46444840e-01 6.30288883e-24 7.32364108e-01]. \t  0.227741627400203 \t 3.3916277796587684\n",
            "26     \t [0.         0.54891267 0.        ]. \t  0.012166756725847529 \t 3.3916277796587684\n",
            "27     \t [0.29522242 0.79054201 0.79956563]. \t  2.3048576366162505 \t 3.3916277796587684\n",
            "28     \t [0.56734818 1.         0.18625225]. \t  0.023474016920006245 \t 3.3916277796587684\n",
            "29     \t [0.95838431 0.23203177 0.80730027]. \t  1.459649933437048 \t 3.3916277796587684\n",
            "30     \t [0.88106936 0.00710255 0.26141287]. \t  0.4030898873056635 \t 3.3916277796587684\n",
            "31     \t [0.18793934 0.89505991 0.01131871]. \t  0.0007150642139074791 \t 3.3916277796587684\n",
            "32     \t [0.         0.34751745 0.40666017]. \t  0.33224047521996314 \t 3.3916277796587684\n",
            "33     \t [0.58490335 0.63312493 0.6402179 ]. \t  1.630777533373499 \t 3.3916277796587684\n",
            "34     \t [0.48817834 0.01869146 0.19693549]. \t  0.7498980566641861 \t 3.3916277796587684\n",
            "35     \t [6.46219100e-12 7.85941782e-01 5.98523762e-01]. \t  2.81423377740978 \t 3.3916277796587684\n",
            "36     \t [3.49468826e-01 1.42521577e-10 1.20593547e-09]. \t  0.10213087450094736 \t 3.3916277796587684\n",
            "37     \t [0.77112165 1.         1.        ]. \t  0.3254247209737588 \t 3.3916277796587684\n",
            "38     \t [0.28936897 1.         0.51112734]. \t  2.200387404834472 \t 3.3916277796587684\n",
            "39     \t [0.         0.         0.35770416]. \t  0.45499103168144933 \t 3.3916277796587684\n",
            "40     \t [0.80970667 0.41273325 0.00569662]. \t  0.029888666229329456 \t 3.3916277796587684\n",
            "41     \t [0.40768653 0.42248855 1.        ]. \t  1.6993400195323056 \t 3.3916277796587684\n",
            "42     \t [0.17445962 0.55786284 0.67751218]. \t  2.29637607978634 \t 3.3916277796587684\n",
            "43     \t [0.50216465 0.2369035  0.57127404]. \t  0.38923638053889364 \t 3.3916277796587684\n",
            "44     \t [0.98251531 0.81180718 0.99961378]. \t  1.1209271692643834 \t 3.3916277796587684\n",
            "45     \t [0.82752367 0.60365746 0.83718088]. \t  \u001b[92m3.598538242633402\u001b[0m \t 3.598538242633402\n",
            "46     \t [0.01221449 0.97073374 0.46677356]. \t  2.10916961503389 \t 3.598538242633402\n",
            "47     \t [0.90781037 0.49614286 0.86892498]. \t  3.590794830466022 \t 3.598538242633402\n",
            "48     \t [0.85088118 0.55542594 0.7986607 ]. \t  3.4096849028416814 \t 3.598538242633402\n",
            "49     \t [0.9999993  0.9999997  0.81836813]. \t  0.5060341472658998 \t 3.598538242633402\n",
            "50     \t [0.01657442 0.57323956 0.83092352]. \t  \u001b[92m3.7610675896649437\u001b[0m \t 3.7610675896649437\n",
            "51     \t [0.79072689 0.53472045 0.82261634]. \t  3.6431216559377653 \t 3.7610675896649437\n",
            "52     \t [0.83604362 0.53783967 0.86892628]. \t  3.720415708511183 \t 3.7610675896649437\n",
            "53     \t [0.04442027 0.60469981 0.81101668]. \t  3.593356903986762 \t 3.7610675896649437\n",
            "54     \t [0.03737851 0.38523799 0.90407873]. \t  2.6668503773826724 \t 3.7610675896649437\n",
            "55     \t [0.86226839 0.4300732  0.90634609]. \t  2.9781302343692815 \t 3.7610675896649437\n",
            "56     \t [0.51866554 0.54593437 0.83646703]. \t  \u001b[92m3.804544492488019\u001b[0m \t 3.804544492488019\n",
            "57     \t [0.55480757 0.49092273 0.88741354]. \t  3.5729706183375276 \t 3.804544492488019\n",
            "58     \t [0.76190761 0.54619213 0.8231814 ]. \t  3.661181998614543 \t 3.804544492488019\n",
            "59     \t [0.47152945 0.4554675  0.7974626 ]. \t  3.3066040131125862 \t 3.804544492488019\n",
            "60     \t [0.65692267 0.53372162 0.86305068]. \t  3.785661514119558 \t 3.804544492488019\n",
            "61     \t [0.68151439 0.50633035 0.88049045]. \t  3.651251126763999 \t 3.804544492488019\n",
            "62     \t [0.60644569 0.54153909 0.83780564]. \t  3.7847066966754617 \t 3.804544492488019\n",
            "63     \t [0.68552068 0.5081574  0.86522457]. \t  3.7161468720186184 \t 3.804544492488019\n",
            "64     \t [0.67271374 0.52706591 0.8801307 ]. \t  3.7110615764052812 \t 3.804544492488019\n",
            "65     \t [0.47653011 0.509539   0.81471931]. \t  3.648869132945113 \t 3.804544492488019\n",
            "66     \t [0.70795246 0.50344077 0.89195132]. \t  3.559178957618382 \t 3.804544492488019\n",
            "67     \t [0.69520701 0.57043091 0.91089763]. \t  3.4832714008264176 \t 3.804544492488019\n",
            "68     \t [0.62897938 0.53659524 0.90159203]. \t  3.579778548121942 \t 3.804544492488019\n",
            "69     \t [0.4373941  0.41916115 0.8831606 ]. \t  3.1502431278128324 \t 3.804544492488019\n",
            "70     \t [0.66268546 0.5672586  0.84360789]. \t  3.7764243370296495 \t 3.804544492488019\n",
            "71     \t [0.60514424 0.53925528 0.88601803]. \t  3.711862893568423 \t 3.804544492488019\n",
            "72     \t [0.66366648 0.55738433 0.85583354]. \t  3.800889496246228 \t 3.804544492488019\n",
            "73     \t [0.68570385 0.50642145 0.87716307]. \t  3.667572010690516 \t 3.804544492488019\n",
            "74     \t [0.60621669 0.5904454  0.85893399]. \t  3.766492224521927 \t 3.804544492488019\n",
            "75     \t [0.37212223 0.55754695 0.87933418]. \t  3.7917278305402933 \t 3.804544492488019\n",
            "76     \t [0.71365371 0.51285206 0.79398458]. \t  3.4065401916676956 \t 3.804544492488019\n",
            "77     \t [0.66972882 0.53960987 0.82457948]. \t  3.7023682288058404 \t 3.804544492488019\n",
            "78     \t [0.57044013 0.56964088 0.84896225]. \t  \u001b[92m3.811418063501874\u001b[0m \t 3.811418063501874\n",
            "79     \t [0.84809733 0.49557563 0.87075107]. \t  3.6049696011310655 \t 3.811418063501874\n",
            "80     \t [0.48449423 0.50569442 0.87314253]. \t  3.721354955441691 \t 3.811418063501874\n",
            "81     \t [0.52216561 0.4227574  0.86365161]. \t  3.262685438861785 \t 3.811418063501874\n",
            "82     \t [0.55192292 0.56761719 0.84011956]. \t  3.7988345065459823 \t 3.811418063501874\n",
            "83     \t [0.51974593 0.64623547 0.87244042]. \t  3.523296715764937 \t 3.811418063501874\n",
            "84     \t [0.04448756 0.61272441 0.80329361]. \t  3.513420856741252 \t 3.811418063501874\n",
            "85     \t [0.45967317 0.6175627  0.85270473]. \t  3.7053529475503715 \t 3.811418063501874\n",
            "86     \t [0.59680219 0.53601942 0.84828104]. \t  3.8071913955202095 \t 3.811418063501874\n",
            "87     \t [0.48076186 0.52726056 0.81550275]. \t  3.6877873156221948 \t 3.811418063501874\n",
            "88     \t [0.37318809 0.54726857 0.81816116]. \t  3.7418744815514593 \t 3.811418063501874\n",
            "89     \t [4.64225955e-04 5.69806590e-01 8.92431337e-01]. \t  3.652285298058547 \t 3.811418063501874\n",
            "90     \t [0.58363425 0.57794965 0.86802625]. \t  3.78703398126902 \t 3.811418063501874\n",
            "91     \t [0.88725372 0.52333557 0.87242449]. \t  3.6675459725420763 \t 3.811418063501874\n",
            "92     \t [0.55725151 0.58058974 0.85676568]. \t  3.802317926095977 \t 3.811418063501874\n",
            "93     \t [0.20705367 0.51730533 0.83333556]. \t  3.777964591936127 \t 3.811418063501874\n",
            "94     \t [0.72649696 0.52685863 0.82643375]. \t  3.682162250096872 \t 3.811418063501874\n",
            "95     \t [0.60000511 0.62303263 0.85857588]. \t  3.647139217681411 \t 3.811418063501874\n",
            "96     \t [0.51977421 0.54549783 0.85931561]. \t  \u001b[92m3.833424251615309\u001b[0m \t 3.833424251615309\n",
            "97     \t [0.47973779 0.57938055 0.86304097]. \t  3.815579980133811 \t 3.833424251615309\n",
            "98     \t [0.80083111 0.54804395 0.84607887]. \t  3.7449148705735764 \t 3.833424251615309\n",
            "99     \t [0.17548675 0.56077158 0.84874857]. \t  \u001b[92m3.8509526525165247\u001b[0m \t 3.8509526525165247\n",
            "100    \t [0.35475004 0.48793654 0.89468674]. \t  3.518026696576358 \t 3.8509526525165247\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejw6v-Ihuvf_",
        "outputId": "bc475d94-f296-4966-b33d-9447d64d7ff8"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 9 \r\n",
        "\r\n",
        "np.random.seed(run_num_9)\r\n",
        "surrogate_winner_9 = dtStudentProcess(d_cov_func, nu = df)\r\n",
        "\r\n",
        "winner_9 = dGPGO_stp(surrogate_winner_9, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_9.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.80342804 0.5275223  0.11911147]. \t  0.05516938924925502 \t 1.6536488994056173\n",
            "init   \t [0.63968144 0.09092526 0.33222568]. \t  0.7039339223433662 \t 1.6536488994056173\n",
            "init   \t [0.42738095 0.55438581 0.62812652]. \t  1.6536488994056173 \t 1.6536488994056173\n",
            "init   \t [0.69739294 0.78994969 0.13189035]. \t  0.009151170523361311 \t 1.6536488994056173\n",
            "init   \t [0.34277045 0.20155961 0.70732423]. \t  0.9342632521680911 \t 1.6536488994056173\n",
            "1      \t [1. 1. 1.]. \t  0.31688362070415665 \t 1.6536488994056173\n",
            "2      \t [0.00973073 0.9812874  0.7227327 ]. \t  1.417101643457911 \t 1.6536488994056173\n",
            "3      \t [0.09877624 0.37453955 0.03083124]. \t  0.07739494328334615 \t 1.6536488994056173\n",
            "4      \t [1.         0.37240569 1.        ]. \t  1.3856570816896538 \t 1.6536488994056173\n",
            "5      \t [0.33278837 1.         1.        ]. \t  0.3342149693811571 \t 1.6536488994056173\n",
            "6      \t [0.0079943  0.90302316 0.14483824]. \t  0.019570537618900184 \t 1.6536488994056173\n",
            "7      \t [1.         1.         0.46192582]. \t  0.18637911752933817 \t 1.6536488994056173\n",
            "8      \t [0.93258745 0.00546985 0.00254712]. \t  0.041567262261380016 \t 1.6536488994056173\n",
            "9      \t [0.06523399 0.00399447 0.02882022]. \t  0.12117535187702808 \t 1.6536488994056173\n",
            "10     \t [0.01584731 0.25439709 0.76432473]. \t  1.5398697125625622 \t 1.6536488994056173\n",
            "11     \t [0.14055046 0.38467975 1.        ]. \t  1.496496494874774 \t 1.6536488994056173\n",
            "12     \t [1.         0.49261731 0.55604388]. \t  0.45614314305464854 \t 1.6536488994056173\n",
            "13     \t [0.68952016 0.09162729 0.95184262]. \t  0.34353704708490557 \t 1.6536488994056173\n",
            "14     \t [0.01037906 0.00365069 0.85635071]. \t  0.2352643773848746 \t 1.6536488994056173\n",
            "15     \t [0.99564482 0.00311421 0.62720294]. \t  0.12359545809977787 \t 1.6536488994056173\n",
            "16     \t [0.3592241 1.        0.467367 ]. \t  \u001b[92m1.6889622350748552\u001b[0m \t 1.6889622350748552\n",
            "17     \t [0.99930022 1.         0.05929626]. \t  0.00018446305568471357 \t 1.6889622350748552\n",
            "18     \t [0.00962202 0.60269637 0.36149633]. \t  0.5115520088763187 \t 1.6889622350748552\n",
            "19     \t [0.77582558 0.65727165 1.        ]. \t  \u001b[92m1.8981742771003227\u001b[0m \t 1.8981742771003227\n",
            "20     \t [0.73740677 1.         0.69568471]. \t  0.6285456696470686 \t 1.8981742771003227\n",
            "21     \t [7.33234909e-12 0.00000000e+00 2.93739612e-01]. \t  0.5680350395879253 \t 1.8981742771003227\n",
            "22     \t [0.03040856 0.83686008 0.90963832]. \t  1.7088421293439717 \t 1.8981742771003227\n",
            "23     \t [0.54136956 0.56028844 0.        ]. \t  0.015092204489553562 \t 1.8981742771003227\n",
            "24     \t [1.         0.76092506 0.85437523]. \t  \u001b[92m2.4057942049116816\u001b[0m \t 2.4057942049116816\n",
            "25     \t [0.28469191 0.00488916 0.18349493]. \t  0.6993367459535516 \t 2.4057942049116816\n",
            "26     \t [0.25127963 0.83413476 0.75188082]. \t  2.00862409437705 \t 2.4057942049116816\n",
            "27     \t [0.41585995 0.         0.63468366]. \t  0.142307498401913 \t 2.4057942049116816\n",
            "28     \t [6.59078757e-01 3.80860439e-03 6.44766479e-04]. \t  0.08095323217040934 \t 2.4057942049116816\n",
            "29     \t [0.27457079 1.         0.12750133]. \t  0.010082691803794088 \t 2.4057942049116816\n",
            "30     \t [0.76272725 0.32342961 0.84871066]. \t  2.342050991976238 \t 2.4057942049116816\n",
            "31     \t [0.93963986 0.00650534 0.9622758 ]. \t  0.1347656818355067 \t 2.4057942049116816\n",
            "32     \t [0.01915287 0.17266497 0.97881822]. \t  0.5442181350186048 \t 2.4057942049116816\n",
            "33     \t [0.96096302 0.7661763  0.05021103]. \t  0.00140265706489451 \t 2.4057942049116816\n",
            "34     \t [2.10172943e-02 7.00662958e-01 2.12945862e-09]. \t  0.00291683892209998 \t 2.4057942049116816\n",
            "35     \t [0.25794544 0.66241689 0.19036087]. \t  0.07477367508079387 \t 2.4057942049116816\n",
            "36     \t [0.38481696 0.6021064  0.97659464]. \t  \u001b[92m2.4844056373200503\u001b[0m \t 2.4844056373200503\n",
            "37     \t [0.96190666 0.19325392 0.19544538]. \t  0.28147556179293765 \t 2.4844056373200503\n",
            "38     \t [0.         0.50973276 0.87645778]. \t  \u001b[92m3.676217737012094\u001b[0m \t 3.676217737012094\n",
            "39     \t [1.         0.69079924 1.        ]. \t  1.7288476554587584 \t 3.676217737012094\n",
            "40     \t [0.01065771 0.73520962 0.76817124]. \t  2.6322561438356162 \t 3.676217737012094\n",
            "41     \t [0.95258013 0.19113959 0.01519287]. \t  0.05060304548537241 \t 3.676217737012094\n",
            "42     \t [0.56763334 0.54234871 0.83549079]. \t  \u001b[92m3.7874673473141627\u001b[0m \t 3.7874673473141627\n",
            "43     \t [0.63076409 0.61125324 0.82541162]. \t  3.5861513316018145 \t 3.7874673473141627\n",
            "44     \t [0.64528082 0.51585356 0.84055571]. \t  3.7452582446767098 \t 3.7874673473141627\n",
            "45     \t [0.51966418 0.55820673 0.88536866]. \t  3.742231314141668 \t 3.7874673473141627\n",
            "46     \t [0.55454301 0.51250605 0.86097169]. \t  3.76779440911581 \t 3.7874673473141627\n",
            "47     \t [0.57406171 0.47316199 0.94831467]. \t  2.7664485155941687 \t 3.7874673473141627\n",
            "48     \t [0.50089427 0.64150674 0.89480178]. \t  3.4435745930909945 \t 3.7874673473141627\n",
            "49     \t [0.48692082 0.57303023 0.86060754]. \t  \u001b[92m3.827366881985974\u001b[0m \t 3.827366881985974\n",
            "50     \t [0.5669288  0.6228567  0.83091909]. \t  3.588765738605402 \t 3.827366881985974\n",
            "51     \t [0.62067773 0.51979374 0.88390804]. \t  3.685329821242024 \t 3.827366881985974\n",
            "52     \t [0.53235951 0.53786933 0.88572749]. \t  3.72523350433609 \t 3.827366881985974\n",
            "53     \t [0.60501983 0.54432409 0.83561547]. \t  3.7784448345758834 \t 3.827366881985974\n",
            "54     \t [0.55814247 0.71899318 0.8615313 ]. \t  2.968752194471857 \t 3.827366881985974\n",
            "55     \t [0.61055351 0.51568342 0.86643183]. \t  3.7533830239496266 \t 3.827366881985974\n",
            "56     \t [0.65516907 0.52473401 0.91122754]. \t  3.452670424805778 \t 3.827366881985974\n",
            "57     \t [0.35476817 0.53757878 0.89700249]. \t  3.653306116069156 \t 3.827366881985974\n",
            "58     \t [0.68024399 0.54778372 0.87141905]. \t  3.771289353662274 \t 3.827366881985974\n",
            "59     \t [0.89529899 0.56673396 0.88180609]. \t  3.6515287667483265 \t 3.827366881985974\n",
            "60     \t [0.57322954 0.56719823 0.84812219]. \t  3.8123376322713005 \t 3.827366881985974\n",
            "61     \t [0.01782557 0.53437813 0.86811361]. \t  3.775576945121209 \t 3.827366881985974\n",
            "62     \t [0.57748927 0.56254793 0.87383757]. \t  3.78770766850986 \t 3.827366881985974\n",
            "63     \t [0.45011019 0.53875748 0.8531737 ]. \t  \u001b[92m3.8422486832636986\u001b[0m \t 3.8422486832636986\n",
            "64     \t [0.40809841 0.54717425 0.84633082]. \t  \u001b[92m3.848622236285921\u001b[0m \t 3.848622236285921\n",
            "65     \t [0.60811414 0.52692634 0.84306247]. \t  3.7830746353569484 \t 3.848622236285921\n",
            "66     \t [0.15275301 0.47217989 0.86456024]. \t  3.592658145257397 \t 3.848622236285921\n",
            "67     \t [0.60379201 0.56420326 0.87805298]. \t  3.763430416303496 \t 3.848622236285921\n",
            "68     \t [0.69682538 0.53809998 0.86131914]. \t  3.782047412453876 \t 3.848622236285921\n",
            "69     \t [0.24073489 0.57266908 0.87987757]. \t  3.778785946744242 \t 3.848622236285921\n",
            "70     \t [0.54462024 0.46430495 0.86094246]. \t  3.5564099097377246 \t 3.848622236285921\n",
            "71     \t [0.46213435 0.54134527 0.83763275]. \t  3.8176643013007174 \t 3.848622236285921\n",
            "72     \t [0.60685686 0.4927932  0.86897999]. \t  3.6675990837326307 \t 3.848622236285921\n",
            "73     \t [0.54082296 0.54313489 0.84176033]. \t  3.8141662664626663 \t 3.848622236285921\n",
            "74     \t [0.54656201 0.52596766 0.86569312]. \t  3.7923402567563986 \t 3.848622236285921\n",
            "75     \t [0.54192368 0.52932808 0.86400872]. \t  3.803313863961039 \t 3.848622236285921\n",
            "76     \t [0.51779303 0.53255526 0.86905824]. \t  3.799725431766803 \t 3.848622236285921\n",
            "77     \t [0.41214987 0.48253182 0.82417807]. \t  3.6193345196029623 \t 3.848622236285921\n",
            "78     \t [0.45234434 0.60690458 0.86054117]. \t  3.7486912296488972 \t 3.848622236285921\n",
            "79     \t [0.57259163 0.5298153  0.84825626]. \t  3.805045056631082 \t 3.848622236285921\n",
            "80     \t [0.40718696 0.58642965 0.85938264]. \t  3.816473906185088 \t 3.848622236285921\n",
            "81     \t [0.28816376 0.00187843 0.93172583]. \t  0.16477699201301774 \t 3.848622236285921\n",
            "82     \t [0.44269471 0.5368585  0.85716813]. \t  3.8396596857068537 \t 3.848622236285921\n",
            "83     \t [0.4535151  0.59026174 0.81209408]. \t  3.629675245722869 \t 3.848622236285921\n",
            "84     \t [0.52519541 0.55210147 0.84112395]. \t  3.818188439198822 \t 3.848622236285921\n",
            "85     \t [0.24852428 0.60340005 0.87849574]. \t  3.7213078897644998 \t 3.848622236285921\n",
            "86     \t [0.00413096 0.61373504 0.91443515]. \t  3.3537922843571217 \t 3.848622236285921\n",
            "87     \t [0.5861789  0.54291317 0.88577   ]. \t  3.7212625965034314 \t 3.848622236285921\n",
            "88     \t [0.18602024 0.51670094 0.85427679]. \t  3.8019962320548233 \t 3.848622236285921\n",
            "89     \t [0.6791164  0.55313926 0.8965976 ]. \t  3.627618240345153 \t 3.848622236285921\n",
            "90     \t [0.98851419 0.60186819 0.86857917]. \t  3.580030279929449 \t 3.848622236285921\n",
            "91     \t [0.38596203 0.50732901 0.85856722]. \t  3.7765433382918947 \t 3.848622236285921\n",
            "92     \t [1.         0.49372274 0.87632054]. \t  3.5152273026205068 \t 3.848622236285921\n",
            "93     \t [0.63285117 0.52768837 0.86532623]. \t  3.777797295100659 \t 3.848622236285921\n",
            "94     \t [0.04015519 0.54908146 0.83851143]. \t  3.8047207557914886 \t 3.848622236285921\n",
            "95     \t [0.47325517 0.5603506  0.88133013]. \t  3.772008497056393 \t 3.848622236285921\n",
            "96     \t [0.43853178 0.53946511 0.83248586]. \t  3.8020319845636346 \t 3.848622236285921\n",
            "97     \t [0.53745075 0.57340279 0.83004544]. \t  3.754192540063274 \t 3.848622236285921\n",
            "98     \t [0.22450823 0.49617358 0.86953587]. \t  3.7026067191293333 \t 3.848622236285921\n",
            "99     \t [0.09972958 0.50747609 0.88941925]. \t  3.6139053201087137 \t 3.848622236285921\n",
            "100    \t [0.28732023 0.57561773 0.86646062]. \t  3.830056509283872 \t 3.848622236285921\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N4xG5dbuuvig",
        "outputId": "16e59647-996c-4434-d225-82dd02ed3a28"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 10 \r\n",
        "\r\n",
        "np.random.seed(run_num_10)\r\n",
        "surrogate_winner_10 = dtStudentProcess(d_cov_func, nu = df)\r\n",
        "\r\n",
        "winner_10 = dGPGO_stp(surrogate_winner_10, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_10.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.65358959 0.11500694 0.95028286]. \t  0.42730315147591735 \t 1.1029187088185965\n",
            "init   \t [0.4821914  0.87247454 0.21233268]. \t  0.06161964400032635 \t 1.1029187088185965\n",
            "init   \t [0.04070962 0.39719446 0.2331322 ]. \t  0.33269334660262956 \t 1.1029187088185965\n",
            "init   \t [0.84174072 0.20708234 0.74246953]. \t  1.1029187088185965 \t 1.1029187088185965\n",
            "init   \t [0.39215413 0.18225652 0.74353941]. \t  0.9779763535009853 \t 1.1029187088185965\n",
            "1      \t [0.65784615 0.04582828 0.06191159]. \t  0.20874301192282538 \t 1.1029187088185965\n",
            "2      \t [1. 1. 1.]. \t  0.31688362070415727 \t 1.1029187088185965\n",
            "3      \t [0.03722426 0.76271424 0.89011521]. \t  \u001b[92m2.5039656647591935\u001b[0m \t 2.5039656647591935\n",
            "4      \t [0.98139485 0.81725946 0.12404836]. \t  0.0024384188819132563 \t 2.5039656647591935\n",
            "5      \t [0.24499042 1.         1.        ]. \t  0.33424539832790867 \t 2.5039656647591935\n",
            "6      \t [0.01006359 0.94218117 0.66418144]. \t  2.1650460243161502 \t 2.5039656647591935\n",
            "7      \t [0.05955692 0.03232917 0.89086014]. \t  0.27726435960872364 \t 2.5039656647591935\n",
            "8      \t [1.         0.53030056 1.        ]. \t  1.9821252541911025 \t 2.5039656647591935\n",
            "9      \t [0.14799361 0.95085635 0.08267139]. \t  0.0038124364044775723 \t 2.5039656647591935\n",
            "10     \t [1.         0.33470203 0.26323645]. \t  0.18986542036210483 \t 2.5039656647591935\n",
            "11     \t [1.         1.         0.53241812]. \t  0.24949339780596352 \t 2.5039656647591935\n",
            "12     \t [0.60406472 0.66419621 1.        ]. \t  1.8950397654216244 \t 2.5039656647591935\n",
            "13     \t [0.10523955 0.06561795 0.21425022]. \t  0.7267500148453335 \t 2.5039656647591935\n",
            "14     \t [0.90148283 0.02039454 0.19066419]. \t  0.3261426709075436 \t 2.5039656647591935\n",
            "15     \t [0.00599313 0.04703605 0.00487161]. \t  0.08125937194666821 \t 2.5039656647591935\n",
            "16     \t [0.19930947 0.31226002 1.        ]. \t  1.1008500492351494 \t 2.5039656647591935\n",
            "17     \t [0.         0.17720722 0.71755473]. \t  0.8370608473372727 \t 2.5039656647591935\n",
            "18     \t [0.24870273 0.77016896 0.68781028]. \t  2.422846544310602 \t 2.5039656647591935\n",
            "19     \t [0.97833857 0.10894465 0.99129352]. \t  0.2833740854676733 \t 2.5039656647591935\n",
            "20     \t [1.         0.70342591 0.68356904]. \t  1.2790134421329058 \t 2.5039656647591935\n",
            "21     \t [0.46284468 0.45960559 0.00413455]. \t  0.03774755755057044 \t 2.5039656647591935\n",
            "22     \t [0.93481963 0.15852344 0.01178922]. \t  0.05304559981970046 \t 2.5039656647591935\n",
            "23     \t [0.65185195 1.         0.76337762]. \t  0.6514756670388009 \t 2.5039656647591935\n",
            "24     \t [0.66766112 0.47469891 0.46535827]. \t  0.3347530429851217 \t 2.5039656647591935\n",
            "25     \t [0.48364817 0.99964114 0.05718473]. \t  0.0011057365553234408 \t 2.5039656647591935\n",
            "26     \t [0.01343505 0.66208911 0.58057557]. \t  2.223979358527388 \t 2.5039656647591935\n",
            "27     \t [1.05517420e-26 1.05517420e-26 5.15286924e-01]. \t  0.12016298434692176 \t 2.5039656647591935\n",
            "28     \t [3.47096756e-01 3.58263415e-02 5.70043121e-10]. \t  0.10961353496067727 \t 2.5039656647591935\n",
            "29     \t [0.27883024 1.         0.43958162]. \t  1.5769201608011865 \t 2.5039656647591935\n",
            "30     \t [0.53681413 0.01790835 0.68716226]. \t  0.22284470710325321 \t 2.5039656647591935\n",
            "31     \t [0.         0.50547242 0.        ]. \t  0.017307754731893145 \t 2.5039656647591935\n",
            "32     \t [0.24178709 0.79214494 0.        ]. \t  0.001431779296227261 \t 2.5039656647591935\n",
            "33     \t [0.84013164 0.50893123 0.01259639]. \t  0.015811894498021145 \t 2.5039656647591935\n",
            "34     \t [0.97679747 0.92554258 0.06420638]. \t  0.0003624208559662594 \t 2.5039656647591935\n",
            "35     \t [0.92713448 0.79767364 1.        ]. \t  1.2015067516849158 \t 2.5039656647591935\n",
            "36     \t [0.03449885 0.50749964 0.89817488]. \t  \u001b[92m3.526066341700382\u001b[0m \t 3.526066341700382\n",
            "37     \t [9.74587251e-01 8.38699379e-04 8.11299910e-01]. \t  0.24523984604176646 \t 3.526066341700382\n",
            "38     \t [0.19615224 0.60942068 0.81015975]. \t  \u001b[92m3.5992144308170673\u001b[0m \t 3.5992144308170673\n",
            "39     \t [0.78935195 0.55423522 0.8627827 ]. \t  \u001b[92m3.756624733280659\u001b[0m \t 3.756624733280659\n",
            "40     \t [0.8033506  0.54224228 0.91184873]. \t  3.437720972651075 \t 3.756624733280659\n",
            "41     \t [0.58516412 0.60582745 0.82647674]. \t  3.631288983967031 \t 3.756624733280659\n",
            "42     \t [0.15076231 0.57503785 0.80178503]. \t  3.6142648401837913 \t 3.756624733280659\n",
            "43     \t [0.67318156 0.53451694 0.83916263]. \t  \u001b[92m3.7638466587465005\u001b[0m \t 3.7638466587465005\n",
            "44     \t [0.66276256 0.51532299 0.84926046]. \t  3.7542428401095798 \t 3.7638466587465005\n",
            "45     \t [0.53689816 0.52165651 0.82600002]. \t  3.728772667080434 \t 3.7638466587465005\n",
            "46     \t [0.04079719 0.53175486 0.83336977]. \t  \u001b[92m3.7736879999617043\u001b[0m \t 3.7736879999617043\n",
            "47     \t [0.1288282  0.52863163 0.79327494]. \t  3.5331095483692367 \t 3.7736879999617043\n",
            "48     \t [0.10079985 0.52911067 0.8136013 ]. \t  3.6876816293593784 \t 3.7736879999617043\n",
            "49     \t [0.77510839 0.51894138 0.87072712]. \t  3.703690775291159 \t 3.7736879999617043\n",
            "50     \t [0.6816073  0.52628092 0.84835372]. \t  3.770500100899562 \t 3.7736879999617043\n",
            "51     \t [0.5783509  0.49008829 0.81913902]. \t  3.5934784878492065 \t 3.7736879999617043\n",
            "52     \t [0.69725584 0.5295871  0.84164218]. \t  3.7576441408623653 \t 3.7736879999617043\n",
            "53     \t [0.79635105 0.50655038 0.90666044]. \t  3.414109828864203 \t 3.7736879999617043\n",
            "54     \t [0.02052924 0.42236748 0.9151631 ]. \t  2.8610371358799878 \t 3.7736879999617043\n",
            "55     \t [0.5188402  0.57083201 0.85833947]. \t  \u001b[92m3.825950046619553\u001b[0m \t 3.825950046619553\n",
            "56     \t [0.16672977 0.60022757 0.81929626]. \t  3.682934237244009 \t 3.825950046619553\n",
            "57     \t [0.01655061 0.52046714 0.82017626]. \t  3.687509227943081 \t 3.825950046619553\n",
            "58     \t [0.22846112 0.58820135 0.82332901]. \t  3.739914084363119 \t 3.825950046619553\n",
            "59     \t [0.5593614  0.61021027 0.87489433]. \t  3.683853551342828 \t 3.825950046619553\n",
            "60     \t [0.35245662 0.57133179 0.85406789]. \t  \u001b[92m3.8498531050229103\u001b[0m \t 3.8498531050229103\n",
            "61     \t [0.64710286 0.50404069 0.83054531]. \t  3.6794069750942553 \t 3.8498531050229103\n",
            "62     \t [0.44388872 0.58453215 0.89441782]. \t  3.6629151417590777 \t 3.8498531050229103\n",
            "63     \t [0.45929237 0.58821551 0.83125875]. \t  3.7498614078639596 \t 3.8498531050229103\n",
            "64     \t [0.78975585 0.55101328 0.8395171 ]. \t  3.730511720325424 \t 3.8498531050229103\n",
            "65     \t [0.77183222 0.51562668 0.84182963]. \t  3.7081223903210168 \t 3.8498531050229103\n",
            "66     \t [0.35862142 0.61100529 0.83064731]. \t  3.6949871089712416 \t 3.8498531050229103\n",
            "67     \t [0.52201569 0.56108116 0.83557713]. \t  3.797191764551868 \t 3.8498531050229103\n",
            "68     \t [0.7218625  0.54150924 0.87530942]. \t  3.740829326737398 \t 3.8498531050229103\n",
            "69     \t [0.31550215 0.57189083 0.83286274]. \t  3.8101857268687374 \t 3.8498531050229103\n",
            "70     \t [0.38675238 0.61018679 0.86775722]. \t  3.733552173990296 \t 3.8498531050229103\n",
            "71     \t [0.36689723 0.46891745 0.80815362]. \t  3.4681345143155995 \t 3.8498531050229103\n",
            "72     \t [0.25649221 0.66677602 0.84244604]. \t  3.4399721496451727 \t 3.8498531050229103\n",
            "73     \t [0.32078452 0.5653035  0.91972557]. \t  3.4249850246036213 \t 3.8498531050229103\n",
            "74     \t [0.66574237 0.53602666 0.8791568 ]. \t  3.7327605971935744 \t 3.8498531050229103\n",
            "75     \t [0.51330809 0.5434217  0.83184181]. \t  3.786759704672072 \t 3.8498531050229103\n",
            "76     \t [0.44284024 0.58573078 0.90199665]. \t  3.595832416936627 \t 3.8498531050229103\n",
            "77     \t [0.72723184 0.50857393 0.89271942]. \t  3.565895019089002 \t 3.8498531050229103\n",
            "78     \t [0.37233743 0.60712296 0.88700563]. \t  3.6620385309785286 \t 3.8498531050229103\n",
            "79     \t [0.03512121 0.56339619 0.77719585]. \t  3.3698741363400537 \t 3.8498531050229103\n",
            "80     \t [0.19079315 0.54928146 0.85276276]. \t  \u001b[92m3.854017651179421\u001b[0m \t 3.854017651179421\n",
            "81     \t [0.10102554 0.58336054 0.79946854]. \t  3.574586006643063 \t 3.854017651179421\n",
            "82     \t [0.43622448 0.53391477 0.88017159]. \t  3.7642921002387695 \t 3.854017651179421\n",
            "83     \t [0.50955181 0.53577727 0.85506922]. \t  3.8292629479514866 \t 3.854017651179421\n",
            "84     \t [0.43701925 0.58615304 0.8409104 ]. \t  3.794873738333761 \t 3.854017651179421\n",
            "85     \t [0.03127939 0.60986704 0.8462831 ]. \t  3.7158909199603967 \t 3.854017651179421\n",
            "86     \t [0.7473456  0.99999993 0.18065435]. \t  0.011530113355800032 \t 3.854017651179421\n",
            "87     \t [0.26918183 0.53514156 0.85578561]. \t  3.846089044797511 \t 3.854017651179421\n",
            "88     \t [0.69229456 0.54462075 0.80891688]. \t  3.58123174430963 \t 3.854017651179421\n",
            "89     \t [0.52443718 0.54372326 0.79887095]. \t  3.5512242427520704 \t 3.854017651179421\n",
            "90     \t [0.73039605 0.53077196 0.8307066 ]. \t  3.7073383438093246 \t 3.854017651179421\n",
            "91     \t [0.3414756  0.01354679 0.9713006 ]. \t  0.13860610252655584 \t 3.854017651179421\n",
            "92     \t [0.24782861 0.472361   0.83995492]. \t  3.6218761795245475 \t 3.854017651179421\n",
            "93     \t [0.52211052 0.55120252 0.846784  ]. \t  3.8309921428730616 \t 3.854017651179421\n",
            "94     \t [0.54612984 0.53991923 0.88095058]. \t  3.7539664535334647 \t 3.854017651179421\n",
            "95     \t [0.2326055  0.52617439 0.8524771 ]. \t  3.8300230674427915 \t 3.854017651179421\n",
            "96     \t [0.49802636 0.58181442 0.84650913]. \t  3.8046314950211317 \t 3.854017651179421\n",
            "97     \t [0.91728291 0.54052123 0.84522166]. \t  3.6933970076875626 \t 3.854017651179421\n",
            "98     \t [0.23227003 0.62734706 0.81933632]. \t  3.583827231467855 \t 3.854017651179421\n",
            "99     \t [0.24580154 0.4780867  0.83406884]. \t  3.638437935343501 \t 3.854017651179421\n",
            "100    \t [0.50498268 0.54356777 0.90116607]. \t  3.6130074335879523 \t 3.854017651179421\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pSj_CQIAuvk3",
        "outputId": "b4479efc-cbcc-441b-b386-bf60dc2f57de"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 11 \r\n",
        "\r\n",
        "np.random.seed(run_num_11)\r\n",
        "surrogate_winner_11 = dtStudentProcess(d_cov_func, nu = df)\r\n",
        "\r\n",
        "winner_11 = dGPGO_stp(surrogate_winner_11, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_11.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.98103566 0.82124785 0.64589361]. \t  0.6570240879762975 \t 0.687459437576373\n",
            "init   \t [0.42368801 0.20231098 0.49190677]. \t  0.29901296656621285 \t 0.687459437576373\n",
            "init   \t [0.13855833 0.45252104 0.11373463]. \t  0.1378633538075966 \t 0.687459437576373\n",
            "init   \t [0.00292449 0.52342617 0.18997116]. \t  0.12299763462541902 \t 0.687459437576373\n",
            "init   \t [0.14171979 0.33586637 0.58369219]. \t  0.687459437576373 \t 0.687459437576373\n",
            "1      \t [0.30140596 1.         1.        ]. \t  0.3343092983548445 \t 0.687459437576373\n",
            "2      \t [0.95983675 0.98162579 0.03051357]. \t  0.00011698065361110391 \t 0.687459437576373\n",
            "3      \t [1.         0.22824488 1.        ]. \t  0.6547058073753838 \t 0.687459437576373\n",
            "4      \t [0.96635878 0.05065822 0.14510777]. \t  0.2095493914518875 \t 0.687459437576373\n",
            "5      \t [0.03585299 0.07198281 0.0945006 ]. \t  0.28685286173585883 \t 0.687459437576373\n",
            "6      \t [1. 1. 1.]. \t  0.31688362070415665 \t 0.687459437576373\n",
            "7      \t [0.28848165 0.05439596 0.89452173]. \t  0.34082352259076326 \t 0.687459437576373\n",
            "8      \t [0.24346909 1.         0.29077624]. \t  0.2998132613015416 \t 0.687459437576373\n",
            "9      \t [0.52922698 0.59173356 1.        ]. \t  \u001b[92m2.0713768937246058\u001b[0m \t 2.0713768937246058\n",
            "10     \t [0.99622896 0.50984357 0.00987988]. \t  0.008998496185923171 \t 2.0713768937246058\n",
            "11     \t [0.00147484 0.93964119 0.00205779]. \t  0.0003838484044950603 \t 2.0713768937246058\n",
            "12     \t [0.01103206 0.38516658 0.95715505]. \t  2.064382960818198 \t 2.0713768937246058\n",
            "13     \t [0.00130088 0.72237187 0.94516519]. \t  \u001b[92m2.36742295985413\u001b[0m \t 2.36742295985413\n",
            "14     \t [1.         0.62203781 1.        ]. \t  1.9504394281415787 \t 2.36742295985413\n",
            "15     \t [0.53602989 0.02602734 0.12525392]. \t  0.46215351735479604 \t 2.36742295985413\n",
            "16     \t [0.704128   0.70093908 0.11821882]. \t  0.01464995047488491 \t 2.36742295985413\n",
            "17     \t [4.94860658e-04 9.90237744e-01 8.69827454e-01]. \t  0.7172608470354785 \t 2.36742295985413\n",
            "18     \t [0.6550934  1.         0.68826397]. \t  0.7949083780545517 \t 2.36742295985413\n",
            "19     \t [0.01371829 0.00437134 0.46741252]. \t  0.19453886002096168 \t 2.36742295985413\n",
            "20     \t [0.87310995 0.00328482 0.56684991]. \t  0.08861991975427909 \t 2.36742295985413\n",
            "21     \t [1.         0.33503192 0.67685023]. \t  1.286777364396289 \t 2.36742295985413\n",
            "22     \t [0.32342891 0.80047835 0.71151665]. \t  2.1819502208945303 \t 2.36742295985413\n",
            "23     \t [0.67382926 0.34939661 0.        ]. \t  0.0516964835527557 \t 2.36742295985413\n",
            "24     \t [0.49642754 0.91122944 0.0068758 ]. \t  0.0004586937058660191 \t 2.36742295985413\n",
            "25     \t [0.80416118 0.54955304 0.80589372]. \t  \u001b[92m3.506566712857393\u001b[0m \t 3.506566712857393\n",
            "26     \t [2.94143112e-04 7.03829535e-02 8.83032592e-01]. \t  0.4072846438789818 \t 3.506566712857393\n",
            "27     \t [0.77179565 0.31646623 0.96631208]. \t  1.4604093157303226 \t 3.506566712857393\n",
            "28     \t [0.58843471 0.         0.        ]. \t  0.08848215890538311 \t 3.506566712857393\n",
            "29     \t [0.78152702 0.78998911 0.96321422]. \t  1.6492557409985094 \t 3.506566712857393\n",
            "30     \t [1.11369147e-10 1.11369147e-10 1.11369147e-10]. \t  0.06797411674601414 \t 3.506566712857393\n",
            "31     \t [0.72035363 0.57599463 0.51902066]. \t  0.5926680539149762 \t 3.506566712857393\n",
            "32     \t [0.01213219 0.99473636 0.46684949]. \t  2.0008127807912492 \t 3.506566712857393\n",
            "33     \t [0.         0.36004233 0.        ]. \t  0.04319786629507342 \t 3.506566712857393\n",
            "34     \t [0.25908408 0.62659055 0.9582113 ]. \t  2.7356070503270544 \t 3.506566712857393\n",
            "35     \t [0.47464485 0.54858574 0.83945509]. \t  \u001b[92m3.8240191513291695\u001b[0m \t 3.8240191513291695\n",
            "36     \t [0.59501759 0.58722439 0.8223761 ]. \t  3.6622452809387007 \t 3.8240191513291695\n",
            "37     \t [0.53650775 0.56504184 0.84025321]. \t  3.8060383596826872 \t 3.8240191513291695\n",
            "38     \t [0.53392401 0.52366573 0.85609441]. \t  3.8044979048666656 \t 3.8240191513291695\n",
            "39     \t [0.99814007 0.61889995 0.85536633]. \t  3.51014670199804 \t 3.8240191513291695\n",
            "40     \t [2.35933885e-01 2.96055728e-08 3.68135885e-08]. \t  0.0969648645347796 \t 3.8240191513291695\n",
            "41     \t [9.64108070e-01 4.31068632e-04 9.50987587e-01]. \t  0.1375891401438709 \t 3.8240191513291695\n",
            "42     \t [0.00086147 0.6113122  0.81335031]. \t  3.570947679500471 \t 3.8240191513291695\n",
            "43     \t [0.59042362 0.47739178 0.85134538]. \t  3.6304022665755395 \t 3.8240191513291695\n",
            "44     \t [0.4951418  0.58218932 0.879917  ]. \t  3.753615982393099 \t 3.8240191513291695\n",
            "45     \t [0.61237707 0.57878111 0.8590689 ]. \t  3.790903145009177 \t 3.8240191513291695\n",
            "46     \t [0.56253081 0.6229285  0.86368221]. \t  3.6549268920170466 \t 3.8240191513291695\n",
            "47     \t [0.68686303 0.5423051  0.83638924]. \t  3.7552442193208644 \t 3.8240191513291695\n",
            "48     \t [1.        0.5660804 0.8582858]. \t  3.663448208534131 \t 3.8240191513291695\n",
            "49     \t [0.66798825 0.         0.74520105]. \t  0.23576644360691348 \t 3.8240191513291695\n",
            "50     \t [0.68919421 0.55539972 0.83461432]. \t  3.746531429936774 \t 3.8240191513291695\n",
            "51     \t [0.57279206 0.52530739 0.83008798]. \t  3.7458147895524174 \t 3.8240191513291695\n",
            "52     \t [0.67339362 0.53357051 0.84624574]. \t  3.780126297594286 \t 3.8240191513291695\n",
            "53     \t [0.48075247 0.4868558  0.84329412]. \t  3.6882293140094626 \t 3.8240191513291695\n",
            "54     \t [0.59559117 0.53108787 0.85284905]. \t  3.804759288438708 \t 3.8240191513291695\n",
            "55     \t [0.64277334 0.63418898 0.85585456]. \t  3.575963854561796 \t 3.8240191513291695\n",
            "56     \t [0.62283883 0.5793228  0.87298203]. \t  3.7609426822187464 \t 3.8240191513291695\n",
            "57     \t [0.74162047 0.56069339 0.84366437]. \t  3.7561750157619422 \t 3.8240191513291695\n",
            "58     \t [0.61148856 0.5419913  0.83251847]. \t  3.7629281236108185 \t 3.8240191513291695\n",
            "59     \t [0.36060133 0.57182717 0.81018529]. \t  3.6746579538437913 \t 3.8240191513291695\n",
            "60     \t [0.60422443 0.56862892 0.83022691]. \t  3.7418460678877175 \t 3.8240191513291695\n",
            "61     \t [0.8757561  0.         0.00839321]. \t  0.054010304725915045 \t 3.8240191513291695\n",
            "62     \t [0.61725085 0.51430447 0.87369413]. \t  3.723605539629802 \t 3.8240191513291695\n",
            "63     \t [0.68558831 0.53446758 0.82808875]. \t  3.7132248452986794 \t 3.8240191513291695\n",
            "64     \t [0.57824836 0.57191239 0.88275547]. \t  3.738750594556996 \t 3.8240191513291695\n",
            "65     \t [0.60966291 0.57860843 0.87222256]. \t  3.767952357979145 \t 3.8240191513291695\n",
            "66     \t [0.38311807 0.57716251 0.84585113]. \t  \u001b[92m3.832003610417766\u001b[0m \t 3.832003610417766\n",
            "67     \t [0.42543808 0.51666062 0.79332895]. \t  3.498873847195137 \t 3.832003610417766\n",
            "68     \t [0.46543487 0.56629145 0.82146156]. \t  3.7357473186401746 \t 3.832003610417766\n",
            "69     \t [0.44287795 0.56525754 0.81561841]. \t  3.704077081131519 \t 3.832003610417766\n",
            "70     \t [0.367879   0.59033283 0.80778642]. \t  3.619522117636804 \t 3.832003610417766\n",
            "71     \t [0.68338404 0.53759698 0.86747179]. \t  3.773848293262308 \t 3.832003610417766\n",
            "72     \t [0.67477505 0.5265029  0.83515692]. \t  3.7394932996546917 \t 3.832003610417766\n",
            "73     \t [0.63452504 0.49949897 0.85508417]. \t  3.7146414413011213 \t 3.832003610417766\n",
            "74     \t [0.75503119 0.59985963 0.84681049]. \t  3.675256751045525 \t 3.832003610417766\n",
            "75     \t [0.41443338 0.60581294 0.863799  ]. \t  3.7542537274948486 \t 3.832003610417766\n",
            "76     \t [0.65781763 0.570529   0.86210917]. \t  3.7885335495364783 \t 3.832003610417766\n",
            "77     \t [0.92425243 0.57137913 0.86031544]. \t  3.691069408366933 \t 3.832003610417766\n",
            "78     \t [0.62391326 0.53747248 0.86608167]. \t  3.792757109597063 \t 3.832003610417766\n",
            "79     \t [0.44004829 0.57021048 0.83858703]. \t  3.8162505296339466 \t 3.832003610417766\n",
            "80     \t [0.58527478 0.60636899 0.81945489]. \t  3.5847143204483807 \t 3.832003610417766\n",
            "81     \t [0.59098796 0.49395803 0.83742698]. \t  3.6843413595625014 \t 3.832003610417766\n",
            "82     \t [0.3768818  0.58418398 0.82541667]. \t  3.747688861624937 \t 3.832003610417766\n",
            "83     \t [0.47610048 0.54370322 0.85028981]. \t  \u001b[92m3.8412959730727603\u001b[0m \t 3.8412959730727603\n",
            "84     \t [0.99999977 0.99999996 0.61502671]. \t  0.2797785531720632 \t 3.8412959730727603\n",
            "85     \t [0.2196997  0.59317631 0.78930095]. \t  3.4776286596331603 \t 3.8412959730727603\n",
            "86     \t [0.3508875  0.60968053 0.8876112 ]. \t  3.6503589841860644 \t 3.8412959730727603\n",
            "87     \t [0.99999998 0.99999989 0.82851064]. \t  0.5181184208992482 \t 3.8412959730727603\n",
            "88     \t [0.75942035 0.52718378 0.85730657]. \t  3.7509613322068147 \t 3.8412959730727603\n",
            "89     \t [0.50940053 0.57562184 0.85740423]. \t  3.821596768146967 \t 3.8412959730727603\n",
            "90     \t [3.36437397e-04 5.92900497e-01 8.76078865e-01]. \t  3.714162490553864 \t 3.8412959730727603\n",
            "91     \t [0.50664272 0.49178535 0.86479097]. \t  3.6930998760399145 \t 3.8412959730727603\n",
            "92     \t [0.41503592 0.54752483 0.84909805]. \t  \u001b[92m3.8511165079641128\u001b[0m \t 3.8511165079641128\n",
            "93     \t [0.53755182 0.57473184 0.82558755]. \t  3.72820576314579 \t 3.8511165079641128\n",
            "94     \t [0.5646148  0.55060909 0.85849711]. \t  3.826774523245273 \t 3.8511165079641128\n",
            "95     \t [0.2134507  0.74797844 0.01777065]. \t  0.003115655917609856 \t 3.8511165079641128\n",
            "96     \t [0.         0.70411404 0.67424477]. \t  2.4803899913376712 \t 3.8511165079641128\n",
            "97     \t [0.19024557 0.57825055 0.79120828]. \t  3.5220760988493867 \t 3.8511165079641128\n",
            "98     \t [0.39260134 0.5181469  0.78806525]. \t  3.4566962423159744 \t 3.8511165079641128\n",
            "99     \t [0.80700014 0.5839537  0.86000212]. \t  3.7158208265381116 \t 3.8511165079641128\n",
            "100    \t [0.546645   0.60719743 0.81005009]. \t  3.526398648731125 \t 3.8511165079641128\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l98Nt7Tguvna",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "717f478e-a727-4357-fc10-6e8d38b19db8"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 12\r\n",
        "\r\n",
        "np.random.seed(run_num_12)\r\n",
        "surrogate_winner_12 = dtStudentProcess(d_cov_func, nu = df)\r\n",
        "\r\n",
        "winner_12 = dGPGO_stp(surrogate_winner_12, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_12.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.19151945 0.62210877 0.43772774]. \t  1.1006281843679786 \t 1.6482992955272024\n",
            "init   \t [0.78535858 0.77997581 0.27259261]. \t  0.07153771680480671 \t 1.6482992955272024\n",
            "init   \t [0.27646426 0.80187218 0.95813935]. \t  1.6482992955272024 \t 1.6482992955272024\n",
            "init   \t [0.87593263 0.35781727 0.50099513]. \t  0.2282556248207173 \t 1.6482992955272024\n",
            "init   \t [0.68346294 0.71270203 0.37025075]. \t  0.33032494760932407 \t 1.6482992955272024\n",
            "1      \t [0.00218943 0.04196524 0.14178738]. \t  0.3936517086962368 \t 1.6482992955272024\n",
            "2      \t [1. 1. 1.]. \t  0.31688362070415665 \t 1.6482992955272024\n",
            "3      \t [0.03494356 0.11089546 0.89536938]. \t  0.5535687532914756 \t 1.6482992955272024\n",
            "4      \t [0.13413814 0.98228353 0.15387565]. \t  0.021772348581531112 \t 1.6482992955272024\n",
            "5      \t [0.93873479 0.11407852 1.        ]. \t  0.27423096946867415 \t 1.6482992955272024\n",
            "6      \t [0.78571851 0.14497732 0.08748815]. \t  0.22335685662819138 \t 1.6482992955272024\n",
            "7      \t [0.54474169 0.00881528 0.61970314]. \t  0.13911391521212932 \t 1.6482992955272024\n",
            "8      \t [0.11420061 1.         0.62576796]. \t  \u001b[92m2.276838743264109\u001b[0m \t 2.276838743264109\n",
            "9      \t [1.         1.         0.54235385]. \t  0.2553289385901763 \t 2.276838743264109\n",
            "10     \t [0.10667908 1.         0.95758056]. \t  0.456787271254165 \t 2.276838743264109\n",
            "11     \t [0.52161503 0.35890866 1.        ]. \t  1.361838439668055 \t 2.276838743264109\n",
            "12     \t [0.97819982 0.01314559 0.20132326]. \t  0.2586848822969663 \t 2.276838743264109\n",
            "13     \t [1.         0.58595661 1.        ]. \t  2.0019309573451456 \t 2.276838743264109\n",
            "14     \t [0.62603858 1.         1.        ]. \t  0.329606738462941 \t 2.276838743264109\n",
            "15     \t [0.02935112 0.70100715 0.8213863 ]. \t  \u001b[92m3.105400376498487\u001b[0m \t 3.105400376498487\n",
            "16     \t [0.         0.49004472 0.        ]. \t  0.019448243637276985 \t 3.105400376498487\n",
            "17     \t [0.12754176 0.         0.        ]. \t  0.08585216882457655 \t 3.105400376498487\n",
            "18     \t [0.99234468 0.52237081 0.09386033]. \t  0.024579500544421547 \t 3.105400376498487\n",
            "19     \t [0.01383622 0.78326225 0.33981593]. \t  0.6819022931981269 \t 3.105400376498487\n",
            "20     \t [0.60037952 0.96523351 0.05612067]. \t  0.0009388251525651218 \t 3.105400376498487\n",
            "21     \t [0.21071453 0.40300262 0.72499041]. \t  2.2866131044631803 \t 3.105400376498487\n",
            "22     \t [5.17292505e-01 8.32045096e-05 9.67356488e-01]. \t  0.12400915396724171 \t 3.105400376498487\n",
            "23     \t [0.9743797  0.97277054 0.10385308]. \t  0.0007447166859934018 \t 3.105400376498487\n",
            "24     \t [0.         0.         0.61883191]. \t  0.12204578324432823 \t 3.105400376498487\n",
            "25     \t [0.78266494 0.73514362 0.92809802]. \t  2.4288406486482264 \t 3.105400376498487\n",
            "26     \t [0.45533985 0.44564233 0.        ]. \t  0.03896445780445815 \t 3.105400376498487\n",
            "27     \t [0.02534578 0.36195495 0.68089661]. \t  1.5709283732912893 \t 3.105400376498487\n",
            "28     \t [0.32966312 0.04658589 0.20537526]. \t  0.8443868079031063 \t 3.105400376498487\n",
            "29     \t [0.0926595 0.8442467 0.       ]. \t  0.0007619493786926339 \t 3.105400376498487\n",
            "30     \t [0.54451308 0.90029928 0.66199411]. \t  1.5045734502944648 \t 3.105400376498487\n",
            "31     \t [1.         0.81815515 0.63056082]. \t  0.5817507129622483 \t 3.105400376498487\n",
            "32     \t [0.26379035 0.32317051 1.        ]. \t  1.1632851347975706 \t 3.105400376498487\n",
            "33     \t [0.00578961 0.89176266 0.7354263 ]. \t  1.7754667399494544 \t 3.105400376498487\n",
            "34     \t [0.03557381 0.55943661 0.97036237]. \t  2.5962273165187235 \t 3.105400376498487\n",
            "35     \t [0.74742011 0.         0.        ]. \t  0.0665234684824122 \t 3.105400376498487\n",
            "36     \t [0.86960669 0.78612373 0.03906799]. \t  0.0012911302984273654 \t 3.105400376498487\n",
            "37     \t [0.97989587 0.06248791 0.67797507]. \t  0.2943884536749346 \t 3.105400376498487\n",
            "38     \t [0.97957568 0.0948278  0.04640728]. \t  0.07520828112349966 \t 3.105400376498487\n",
            "39     \t [0.90017851 0.51696857 0.78507368]. \t  \u001b[92m3.2371998850188994\u001b[0m \t 3.2371998850188994\n",
            "40     \t [0.4543223  0.56843091 0.81722508]. \t  \u001b[92m3.708636929766368\u001b[0m \t 3.708636929766368\n",
            "41     \t [0.21167486 0.55358147 0.90230115]. \t  3.6121755970973832 \t 3.708636929766368\n",
            "42     \t [0.27571469 0.63460619 0.84335632]. \t  3.63570150434712 \t 3.708636929766368\n",
            "43     \t [0.26411983 0.60018639 0.85699569]. \t  \u001b[92m3.7893837274114697\u001b[0m \t 3.7893837274114697\n",
            "44     \t [0.71657588 0.54044507 0.84356729]. \t  3.766811372455482 \t 3.7893837274114697\n",
            "45     \t [0.41927593 0.5618017  0.80926967]. \t  3.6659324408038163 \t 3.7893837274114697\n",
            "46     \t [0.65639962 0.56702326 0.83436864]. \t  3.746229755088729 \t 3.7893837274114697\n",
            "47     \t [0.98401112 0.49027819 0.8769639 ]. \t  3.505160734314603 \t 3.7893837274114697\n",
            "48     \t [0.83686764 0.52107609 0.8521479 ]. \t  3.711849811433367 \t 3.7893837274114697\n",
            "49     \t [0.6820785  0.45724139 0.88294847]. \t  3.3963369422552905 \t 3.7893837274114697\n",
            "50     \t [0.22757238 0.66268978 0.79502668]. \t  3.247533282256793 \t 3.7893837274114697\n",
            "51     \t [0.81217067 0.5180341  0.83646616]. \t  3.6822952875365496 \t 3.7893837274114697\n",
            "52     \t [0.67304503 0.59071561 0.80632821]. \t  3.4950156418584104 \t 3.7893837274114697\n",
            "53     \t [0.74501288 0.55848434 0.87399338]. \t  3.7437377440946737 \t 3.7893837274114697\n",
            "54     \t [0.91604483 0.52663076 0.85613388]. \t  3.6911475571034194 \t 3.7893837274114697\n",
            "55     \t [0.72694808 0.55746752 0.89870272]. \t  3.597948576454113 \t 3.7893837274114697\n",
            "56     \t [0.88654769 0.48973426 0.83779634]. \t  3.5764442262168297 \t 3.7893837274114697\n",
            "57     \t [0.63408849 0.58620252 0.87297418]. \t  3.7443911029165595 \t 3.7893837274114697\n",
            "58     \t [0.30028592 0.82482565 0.13310265]. \t  0.01652387503590396 \t 3.7893837274114697\n",
            "59     \t [0.51689053 0.5690095  0.839371  ]. \t  \u001b[92m3.803731999016607\u001b[0m \t 3.803731999016607\n",
            "60     \t [0.60324661 0.52437432 0.89685261]. \t  3.604416165798052 \t 3.803731999016607\n",
            "61     \t [0.32066155 0.59586571 0.83741362]. \t  3.775592218694796 \t 3.803731999016607\n",
            "62     \t [0.15119391 0.56925814 0.79511403]. \t  3.5644692837944616 \t 3.803731999016607\n",
            "63     \t [0.67525418 0.54281694 0.88667282]. \t  3.6952596217340234 \t 3.803731999016607\n",
            "64     \t [0.35601953 0.60182617 0.83915746]. \t  3.75871121505775 \t 3.803731999016607\n",
            "65     \t [0.27283473 0.57237248 0.86144059]. \t  \u001b[92m3.844234204541263\u001b[0m \t 3.844234204541263\n",
            "66     \t [0.57598667 0.5759874  0.81201361]. \t  3.6197096227302104 \t 3.844234204541263\n",
            "67     \t [0.81969283 0.57490996 0.83490177]. \t  3.673844522416728 \t 3.844234204541263\n",
            "68     \t [0.48223375 0.59607287 0.8925643 ]. \t  3.6470615637289394 \t 3.844234204541263\n",
            "69     \t [0.79569401 0.99999992 0.8037275 ]. \t  0.5547443234180887 \t 3.844234204541263\n",
            "70     \t [0.3173907  0.6049759  0.79598172]. \t  3.4925804576197246 \t 3.844234204541263\n",
            "71     \t [0.41081436 0.61911121 0.86954641]. \t  3.690219890705846 \t 3.844234204541263\n",
            "72     \t [0.43690465 0.54231763 0.86322439]. \t  3.8377951175229255 \t 3.844234204541263\n",
            "73     \t [0.30911371 0.61021159 0.84350478]. \t  3.744171172856882 \t 3.844234204541263\n",
            "74     \t [0.8435284  0.52420099 0.83799086]. \t  3.686411563113091 \t 3.844234204541263\n",
            "75     \t [0.54120045 0.60391473 0.81668154]. \t  3.5911152021843384 \t 3.844234204541263\n",
            "76     \t [0.01071897 0.54582501 0.89325607]. \t  3.6465121555213167 \t 3.844234204541263\n",
            "77     \t [0.79601336 0.53598043 0.84987784]. \t  3.7469806915840733 \t 3.844234204541263\n",
            "78     \t [0.38289288 0.53742998 0.81764235]. \t  3.731833190479916 \t 3.844234204541263\n",
            "79     \t [0.34813871 0.99160308 0.73888399]. \t  1.1218258941450596 \t 3.844234204541263\n",
            "80     \t [0.60704222 0.55128533 0.85456202]. \t  3.8176047853224455 \t 3.844234204541263\n",
            "81     \t [0.49361559 0.58808875 0.87942569]. \t  3.74431835895859 \t 3.844234204541263\n",
            "82     \t [0.00424535 0.55607527 0.78737257]. \t  3.4619395440050607 \t 3.844234204541263\n",
            "83     \t [0.74549632 0.48758222 0.83934489]. \t  3.622144384359814 \t 3.844234204541263\n",
            "84     \t [0.93027027 0.49454114 0.82860211]. \t  3.540869823603064 \t 3.844234204541263\n",
            "85     \t [0.4446356  0.64950865 0.87285246]. \t  3.5165983988849527 \t 3.844234204541263\n",
            "86     \t [0.60830839 0.55698757 0.81872536]. \t  3.68345203194561 \t 3.844234204541263\n",
            "87     \t [0.60335257 0.58063906 0.87756155]. \t  3.7460036296230035 \t 3.844234204541263\n",
            "88     \t [0.43448546 0.6082282  0.85670849]. \t  3.7488352336689683 \t 3.844234204541263\n",
            "89     \t [0.27834205 0.59177129 0.82157517]. \t  3.720808784222422 \t 3.844234204541263\n",
            "90     \t [0.61990749 0.58096279 0.79408936]. \t  3.424223581232038 \t 3.844234204541263\n",
            "91     \t [0.66546094 0.55096428 0.89342967]. \t  3.6555229757209715 \t 3.844234204541263\n",
            "92     \t [0.40862899 0.60480958 0.86182773]. \t  3.761316992801402 \t 3.844234204541263\n",
            "93     \t [0.59580853 0.54706989 0.8954845 ]. \t  3.6520416857870073 \t 3.844234204541263\n",
            "94     \t [0.36016516 0.56044254 0.87076451]. \t  3.828786664636247 \t 3.844234204541263\n",
            "95     \t [0.47835214 0.54081799 0.88617191]. \t  3.7333962259439204 \t 3.844234204541263\n",
            "96     \t [0.12726301 0.52402271 0.88224719]. \t  3.716830777144037 \t 3.844234204541263\n",
            "97     \t [0.59128881 0.52879558 0.87987536]. \t  3.7343188168112302 \t 3.844234204541263\n",
            "98     \t [0.33474354 0.58250051 0.84267298]. \t  3.8209213797986923 \t 3.844234204541263\n",
            "99     \t [0.59395172 0.55538932 0.86863363]. \t  3.80300551713736 \t 3.844234204541263\n",
            "100    \t [0.39414903 0.57429471 0.81947017]. \t  3.728661995275446 \t 3.844234204541263\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Bpn-kmNuvqC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "585a0673-f2f1-4e27-f875-5329c73f227c"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 13 \r\n",
        "\r\n",
        "np.random.seed(run_num_13)\r\n",
        "surrogate_winner_13 = dtStudentProcess(d_cov_func, nu = df)\r\n",
        "\r\n",
        "winner_13 = dGPGO_stp(surrogate_winner_13, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_13.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.66064431 0.41360065 0.66810256]. \t  1.5457680899590245 \t 2.6697919207500047\n",
            "init   \t [0.22994342 0.80767834 0.63681846]. \t  2.6697919207500047 \t 2.6697919207500047\n",
            "init   \t [0.17219385 0.26038587 0.91531999]. \t  1.4685599870430508 \t 2.6697919207500047\n",
            "init   \t [0.46281551 0.12019095 0.88088551]. \t  0.639297028201682 \t 2.6697919207500047\n",
            "init   \t [0.22621895 0.81144033 0.44587892]. \t  1.960293029067079 \t 2.6697919207500047\n",
            "1      \t [1. 1. 1.]. \t  0.31688362070416154 \t 2.6697919207500047\n",
            "2      \t [0.91418486 0.13498533 0.09072509]. \t  0.16031997849670143 \t 2.6697919207500047\n",
            "3      \t [0.04920409 0.08370037 0.06705598]. \t  0.21857769543557976 \t 2.6697919207500047\n",
            "4      \t [0.26673576 0.79626328 0.97650777]. \t  1.5012278885964654 \t 2.6697919207500047\n",
            "5      \t [1.         1.         0.45476098]. \t  0.17839337002987452 \t 2.6697919207500047\n",
            "6      \t [0.00286271 0.76945409 0.49467831]. \t  2.3928287410314106 \t 2.6697919207500047\n",
            "7      \t [0.01638524 0.07150708 0.45513128]. \t  0.2535329679860764 \t 2.6697919207500047\n",
            "8      \t [1.         0.50372287 0.97605451]. \t  2.3397990714189474 \t 2.6697919207500047\n",
            "9      \t [1.         0.23343377 0.95371656]. \t  0.9797031197000384 \t 2.6697919207500047\n",
            "10     \t [0.01146743 0.95931049 0.74566827]. \t  1.3558188388370775 \t 2.6697919207500047\n",
            "11     \t [0.96250345 0.41632771 0.01852631]. \t  0.022161989222513164 \t 2.6697919207500047\n",
            "12     \t [0.4347071  0.         0.13332379]. \t  0.5023995950622405 \t 2.6697919207500047\n",
            "13     \t [0.96980854 0.93198336 0.06567144]. \t  0.000370894274647498 \t 2.6697919207500047\n",
            "14     \t [0.66264199 0.52155578 1.        ]. \t  2.028825457118635 \t 2.6697919207500047\n",
            "15     \t [0.0181113  0.04327127 0.97080259]. \t  0.18622584787616364 \t 2.6697919207500047\n",
            "16     \t [0.00581101 0.79626665 0.        ]. \t  0.001050821649966478 \t 2.6697919207500047\n",
            "17     \t [0.24716338 0.4011864  0.        ]. \t  0.05003866762588539 \t 2.6697919207500047\n",
            "18     \t [9.35848662e-01 5.44902705e-04 5.75766976e-01]. \t  0.08531791242138119 \t 2.6697919207500047\n",
            "19     \t [0.01258657 0.53505322 0.83509179]. \t  \u001b[92m3.774748202317712\u001b[0m \t 3.774748202317712\n",
            "20     \t [0.67173804 1.         0.61154089]. \t  0.9633089589022253 \t 3.774748202317712\n",
            "21     \t [0.28741398 0.97418176 0.02885626]. \t  0.0007295725322032098 \t 3.774748202317712\n",
            "22     \t [0.97299327 0.74983482 0.76088323]. \t  1.8676354092960927 \t 3.774748202317712\n",
            "23     \t [0.03636463 0.68862757 0.91354292]. \t  2.967448458701654 \t 3.774748202317712\n",
            "24     \t [0.7806164  0.71447996 0.32158849]. \t  0.13461650744348783 \t 3.774748202317712\n",
            "25     \t [0.68362276 0.         0.        ]. \t  0.07596221512599906 \t 3.774748202317712\n",
            "26     \t [0.05245123 0.46400841 0.66700558]. \t  1.8691967742641107 \t 3.774748202317712\n",
            "27     \t [0. 0. 0.]. \t  0.06797411659013229 \t 3.774748202317712\n",
            "28     \t [0.99203055 0.49616274 0.4476672 ]. \t  0.13037698403876274 \t 3.774748202317712\n",
            "29     \t [0.72780054 0.1692164  0.24517512]. \t  0.6520284118335742 \t 3.774748202317712\n",
            "30     \t [5.01818449e-04 5.07514438e-01 9.62903781e-01]. \t  2.633819372649354 \t 3.774748202317712\n",
            "31     \t [0.40725375 0.67604811 0.85966797]. \t  3.365147640024362 \t 3.774748202317712\n",
            "32     \t [0.08896701 0.92033931 0.30151844]. \t  0.4296549820205614 \t 3.774748202317712\n",
            "33     \t [0.66751083 0.87696694 0.01775905]. \t  0.0005729467891736366 \t 3.774748202317712\n",
            "34     \t [0.73595889 0.83299804 1.        ]. \t  1.0297080169233825 \t 3.774748202317712\n",
            "35     \t [0.         0.28704463 0.        ]. \t  0.05838270442729277 \t 3.774748202317712\n",
            "36     \t [0.00600578 0.55598257 0.20217194]. \t  0.11191056204297312 \t 3.774748202317712\n",
            "37     \t [0.66289263 0.99305929 0.29529588]. \t  0.1379391800063483 \t 3.774748202317712\n",
            "38     \t [0.90859722 0.03558028 0.98872683]. \t  0.1453575804886937 \t 3.774748202317712\n",
            "39     \t [0.17028316 0.60496627 0.80241325]. \t  3.556550547115171 \t 3.774748202317712\n",
            "40     \t [0.24455283 0.00639588 0.60187062]. \t  0.12700581745709566 \t 3.774748202317712\n",
            "41     \t [0.05347341 0.7149222  0.75597926]. \t  2.7141459403220387 \t 3.774748202317712\n",
            "42     \t [0.3960652  0.65859829 0.73875875]. \t  2.714711796064626 \t 3.774748202317712\n",
            "43     \t [0.59211669 0.30523862 0.        ]. \t  0.07084848724720504 \t 3.774748202317712\n",
            "44     \t [0.04116564 0.04266125 0.77403746]. \t  0.36413007721554713 \t 3.774748202317712\n",
            "45     \t [0.25316754 0.61660229 0.94177334]. \t  3.025452969052979 \t 3.774748202317712\n",
            "46     \t [0.00537949 0.99486981 0.01127333]. \t  0.0003981770330706773 \t 3.774748202317712\n",
            "47     \t [0.73781128 0.00921606 0.75693385]. \t  0.26278342050016146 \t 3.774748202317712\n",
            "48     \t [0.28790865 0.         0.        ]. \t  0.10025421292582978 \t 3.774748202317712\n",
            "49     \t [1.         0.65232327 1.        ]. \t  1.870888291935871 \t 3.774748202317712\n",
            "50     \t [0.96370462 0.0349654  0.03511535]. \t  0.06418638003838145 \t 3.774748202317712\n",
            "51     \t [0.4005134  0.03090525 0.9519055 ]. \t  0.19363202951038105 \t 3.774748202317712\n",
            "52     \t [0.41686811 0.98678575 0.92920228]. \t  0.589449036600237 \t 3.774748202317712\n",
            "53     \t [0.00353274 0.5490535  0.84900383]. \t  \u001b[92m3.809578189645349\u001b[0m \t 3.809578189645349\n",
            "54     \t [0.99999956 0.99999893 0.7856246 ]. \t  0.45648058399762775 \t 3.809578189645349\n",
            "55     \t [0.04611301 0.68998684 0.83244092]. \t  3.2278033500470196 \t 3.809578189645349\n",
            "56     \t [0.11487117 0.58061845 0.88393902]. \t  3.7278915301314166 \t 3.809578189645349\n",
            "57     \t [0.18364302 0.63777823 0.79766296]. \t  3.3960732145741246 \t 3.809578189645349\n",
            "58     \t [0.2498018  0.57092626 0.80597727]. \t  3.656329765594794 \t 3.809578189645349\n",
            "59     \t [0.0329684  0.98787527 0.9805881 ]. \t  0.42872411477246064 \t 3.809578189645349\n",
            "60     \t [0.16707947 0.55603188 0.8881064 ]. \t  3.7264663047580604 \t 3.809578189645349\n",
            "61     \t [0.13795492 0.57666434 0.81168307]. \t  3.6837020637456277 \t 3.809578189645349\n",
            "62     \t [0.05034588 0.65261895 0.7356394 ]. \t  2.8213550163149406 \t 3.809578189645349\n",
            "63     \t [0.34468669 0.44392764 0.97683978]. \t  2.1713115246051355 \t 3.809578189645349\n",
            "64     \t [0.75726854 0.54330057 0.8982174 ]. \t  3.5864410275999807 \t 3.809578189645349\n",
            "65     \t [0.79408368 0.00444109 0.2971376 ]. \t  0.49890659849038493 \t 3.809578189645349\n",
            "66     \t [0.41249524 0.65940901 0.78237002]. \t  3.100719944538893 \t 3.809578189645349\n",
            "67     \t [0.74413912 0.50011567 0.8698954 ]. \t  3.6589205029457275 \t 3.809578189645349\n",
            "68     \t [0.30692547 0.66337827 0.84157176]. \t  3.4573564301884763 \t 3.809578189645349\n",
            "69     \t [0.09933303 0.64506797 0.78876379]. \t  3.2877163515496375 \t 3.809578189645349\n",
            "70     \t [0.73999124 0.51658602 0.89231816]. \t  3.589599614094743 \t 3.809578189645349\n",
            "71     \t [0.17103348 0.5454918  0.85119782]. \t  \u001b[92m3.8492373843924264\u001b[0m \t 3.8492373843924264\n",
            "72     \t [0.16870621 0.64238349 0.84030578]. \t  3.58490381151352 \t 3.8492373843924264\n",
            "73     \t [0.75470843 0.57341589 0.84398547]. \t  3.7361961841410523 \t 3.8492373843924264\n",
            "74     \t [0.88888785 0.41874685 0.84717689]. \t  3.1732293252457318 \t 3.8492373843924264\n",
            "75     \t [0.74581085 0.50856939 0.90342896]. \t  3.4672261811070184 \t 3.8492373843924264\n",
            "76     \t [0.19548623 0.48442209 0.8887376 ]. \t  3.5380471855003353 \t 3.8492373843924264\n",
            "77     \t [0.63119619 0.5441101  0.88932923]. \t  3.6891436614998856 \t 3.8492373843924264\n",
            "78     \t [0.75167395 0.65688837 0.85529144]. \t  3.397965930403134 \t 3.8492373843924264\n",
            "79     \t [0.05189627 0.59171723 0.84187337]. \t  3.7704697731419277 \t 3.8492373843924264\n",
            "80     \t [0.68939258 0.52375207 0.88165166]. \t  3.691592425297216 \t 3.8492373843924264\n",
            "81     \t [0.77188784 0.5886227  0.8980359 ]. \t  3.557399275447801 \t 3.8492373843924264\n",
            "82     \t [0.62950032 0.54667603 0.86157553]. \t  3.8067574590252153 \t 3.8492373843924264\n",
            "83     \t [0.13386779 0.68185195 0.85863729]. \t  3.3304148897363786 \t 3.8492373843924264\n",
            "84     \t [0.80984916 0.55964952 0.79882561]. \t  3.425194588510967 \t 3.8492373843924264\n",
            "85     \t [0.18627296 0.55046063 0.83011921]. \t  3.808191794064511 \t 3.8492373843924264\n",
            "86     \t [0.71285005 0.55597427 0.89392576]. \t  3.6407233661718355 \t 3.8492373843924264\n",
            "87     \t [0.73349456 0.60263321 0.87273615]. \t  3.6694401985501015 \t 3.8492373843924264\n",
            "88     \t [0.66240496 0.60063145 0.854831  ]. \t  3.7181474150796885 \t 3.8492373843924264\n",
            "89     \t [0.83707872 0.54399345 0.8888279 ]. \t  3.6329000849748074 \t 3.8492373843924264\n",
            "90     \t [0.80233167 0.48814811 0.90335908]. \t  3.3723543906826103 \t 3.8492373843924264\n",
            "91     \t [0.74233119 0.56940815 0.90649837]. \t  3.5171736543730097 \t 3.8492373843924264\n",
            "92     \t [0.25199372 0.68268871 0.79032272]. \t  3.093612468627537 \t 3.8492373843924264\n",
            "93     \t [0.61153397 0.54534242 0.85233782]. \t  3.8141240646128285 \t 3.8492373843924264\n",
            "94     \t [0.34849882 0.59629712 0.89851465]. \t  3.6091776235873017 \t 3.8492373843924264\n",
            "95     \t [0.48904417 0.57651288 0.85369525]. \t  3.8242154868625153 \t 3.8492373843924264\n",
            "96     \t [0.64835443 0.59681207 0.84264735]. \t  3.712611342931939 \t 3.8492373843924264\n",
            "97     \t [0.65685007 0.62741438 0.88488034]. \t  3.542806741698026 \t 3.8492373843924264\n",
            "98     \t [0.26398215 0.54654532 0.89585018]. \t  3.671333423870368 \t 3.8492373843924264\n",
            "99     \t [0.79144612 0.56616938 0.89651198]. \t  3.594204634488944 \t 3.8492373843924264\n",
            "100    \t [0.3639437  0.49296533 0.84844836]. \t  3.7294242837998492 \t 3.8492373843924264\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NdFRXtPuvsP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0205f545-d95d-4d0d-9420-239e73d2f349"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 14 \r\n",
        "\r\n",
        "np.random.seed(run_num_14)\r\n",
        "surrogate_winner_14 = dtStudentProcess(d_cov_func, nu = df)\r\n",
        "\r\n",
        "winner_14 = dGPGO_stp(surrogate_winner_14, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_14.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.64755105 0.50714969 0.52834138]. \t  0.5963212059988954 \t 2.610000357863649\n",
            "init   \t [0.8962852  0.69999119 0.7142971 ]. \t  1.7197848290620104 \t 2.610000357863649\n",
            "init   \t [0.71733838 0.22281946 0.17515452]. \t  0.48166052848103497 \t 2.610000357863649\n",
            "init   \t [0.45684149 0.92873843 0.00988589]. \t  0.0004588015757462679 \t 2.610000357863649\n",
            "init   \t [0.08992219 0.85020027 0.48562106]. \t  2.610000357863649 \t 2.610000357863649\n",
            "1      \t [0.07426471 0.99752623 0.88021278]. \t  0.6630933245090485 \t 2.610000357863649\n",
            "2      \t [0. 0. 0.]. \t  0.06797411659013229 \t 2.610000357863649\n",
            "3      \t [1.         0.31649499 1.        ]. \t  1.086634164623213 \t 2.610000357863649\n",
            "4      \t [1. 1. 1.]. \t  0.31688362070415665 \t 2.610000357863649\n",
            "5      \t [1.         1.         0.27726818]. \t  0.02361051388630304 \t 2.610000357863649\n",
            "6      \t [0.9941856  0.02190666 0.2793216 ]. \t  0.28159434907205527 \t 2.610000357863649\n",
            "7      \t [0.05053044 0.99023432 0.07082292]. \t  0.00241787352453403 \t 2.610000357863649\n",
            "8      \t [0.07336514 0.45917719 0.88990025]. \t  \u001b[92m3.3656194798194723\u001b[0m \t 3.3656194798194723\n",
            "9      \t [2.35263929e-05 6.06004502e-01 4.57960580e-01]. \t  1.1443508708499905 \t 3.3656194798194723\n",
            "10     \t [0.08960719 0.07019454 0.86353831]. \t  0.43610424654224383 \t 3.3656194798194723\n",
            "11     \t [0.82661661 0.02338278 0.01499173]. \t  0.07237182666742638 \t 3.3656194798194723\n",
            "12     \t [0.98159559 0.54955259 0.12326486]. \t  0.02719791087230826 \t 3.3656194798194723\n",
            "13     \t [0.01522586 0.57289097 0.99981868]. \t  2.0631098819241993 \t 3.3656194798194723\n",
            "14     \t [0.4356414  0.98254487 0.53442387]. \t  1.9309924703785986 \t 3.3656194798194723\n",
            "15     \t [0.22189961 0.39492571 0.        ]. \t  0.05078434108037827 \t 3.3656194798194723\n",
            "16     \t [0.22869761 0.02263875 0.25741239]. \t  0.8600906920758487 \t 3.3656194798194723\n",
            "17     \t [0.54893717 0.50880728 1.        ]. \t  2.0128709579341395 \t 3.3656194798194723\n",
            "18     \t [1.         0.67859776 1.        ]. \t  1.7782923435612439 \t 3.3656194798194723\n",
            "19     \t [0.26524749 0.67324492 0.83511947]. \t  \u001b[92m3.378044355460206\u001b[0m \t 3.378044355460206\n",
            "20     \t [0.22840538 0.45446563 0.97420147]. \t  2.262297859390493 \t 3.378044355460206\n",
            "21     \t [0.78394426 0.01876647 0.62251561]. \t  0.14461112962206243 \t 3.378044355460206\n",
            "22     \t [3.20964094e-01 2.08180485e-11 1.11464359e-02]. \t  0.12096893516998673 \t 3.378044355460206\n",
            "23     \t [1.         0.38528667 0.6067059 ]. \t  0.7472012281559484 \t 3.378044355460206\n",
            "24     \t [1.00859194e-10 3.02348948e-11 6.47052277e-01]. \t  0.14617326426120097 \t 3.378044355460206\n",
            "25     \t [0.45603265 0.05969466 0.94833449]. \t  0.26385370950022324 \t 3.378044355460206\n",
            "26     \t [0.80765679 0.99081809 0.03598249]. \t  0.0002430780006518369 \t 3.378044355460206\n",
            "27     \t [0.11651878 0.54230003 0.73626352]. \t  2.917103437290247 \t 3.378044355460206\n",
            "28     \t [0.02792452 0.82824971 0.76822915]. \t  2.0354650299203105 \t 3.378044355460206\n",
            "29     \t [0.49615547 0.92823628 0.91237351]. \t  0.9675345107523711 \t 3.378044355460206\n",
            "30     \t [0.18057401 0.97258615 0.61033212]. \t  2.523519266913563 \t 3.378044355460206\n",
            "31     \t [0.9571637  0.11605647 0.83529533]. \t  0.6654223593849512 \t 3.378044355460206\n",
            "32     \t [0.83290042 0.01398428 0.99121239]. \t  0.11416467971894746 \t 3.378044355460206\n",
            "33     \t [1.        1.        0.8218254]. \t  0.510377798721477 \t 3.378044355460206\n",
            "34     \t [0.76371428 0.35838957 0.92441319]. \t  2.233771951690372 \t 3.378044355460206\n",
            "35     \t [0.22240643 0.93726879 0.30841977]. \t  0.45026611968163477 \t 3.378044355460206\n",
            "36     \t [0.16709094 0.67716607 0.00116897]. \t  0.004791694951147966 \t 3.378044355460206\n",
            "37     \t [0.70159187 0.3833708  0.00801613]. \t  0.046973528518229744 \t 3.378044355460206\n",
            "38     \t [0.03082142 0.31415528 0.94365559]. \t  1.6730633302165474 \t 3.378044355460206\n",
            "39     \t [0.26205238 0.16156101 0.71547612]. \t  0.7641118324346099 \t 3.378044355460206\n",
            "40     \t [0.98015806 0.09088488 0.0678592 ]. \t  0.09817317170746036 \t 3.378044355460206\n",
            "41     \t [0.71801744 0.89009859 0.32369832]. \t  0.2003205600592563 \t 3.378044355460206\n",
            "42     \t [0.18866103 0.68260534 0.70271293]. \t  2.585432511617517 \t 3.378044355460206\n",
            "43     \t [0.78071839 1.         0.72654527]. \t  0.5411396690436381 \t 3.378044355460206\n",
            "44     \t [0.0200457  0.17763069 0.27724005]. \t  0.6696643037018387 \t 3.378044355460206\n",
            "45     \t [0.01683176 0.42487174 0.06116513]. \t  0.07494951923773191 \t 3.378044355460206\n",
            "46     \t [0.18547869 0.79887441 0.95079228]. \t  1.738761728445261 \t 3.378044355460206\n",
            "47     \t [0.02525811 0.93984928 0.61182345]. \t  2.6616950517647218 \t 3.378044355460206\n",
            "48     \t [0.02143173 0.4658913  0.81452756]. \t  \u001b[92m3.4547356782675562\u001b[0m \t 3.4547356782675562\n",
            "49     \t [0.4667266  0.68096284 0.80702163]. \t  3.131564938892078 \t 3.4547356782675562\n",
            "50     \t [0.3925083  0.51981974 0.87377861]. \t  \u001b[92m3.769213337580452\u001b[0m \t 3.769213337580452\n",
            "51     \t [0.23584444 0.5518094  0.83685653]. \t  \u001b[92m3.835906379723688\u001b[0m \t 3.835906379723688\n",
            "52     \t [0.99677021 0.73494779 0.02466057]. \t  0.0012015530512083103 \t 3.835906379723688\n",
            "53     \t [0.4557037  0.4557037  0.87236446]. \t  3.476036048264304 \t 3.835906379723688\n",
            "54     \t [0.38986325 0.52899815 0.81242779]. \t  3.6863645329490278 \t 3.835906379723688\n",
            "55     \t [0.35526094 0.63390383 0.87391617]. \t  3.611831772767787 \t 3.835906379723688\n",
            "56     \t [0.95040613 0.51448629 0.8819655 ]. \t  3.578182938524542 \t 3.835906379723688\n",
            "57     \t [0.45722529 0.61733784 0.82283267]. \t  3.606682167011595 \t 3.835906379723688\n",
            "58     \t [0.44660655 0.59127984 0.7972218 ]. \t  3.503562591746211 \t 3.835906379723688\n",
            "59     \t [0.2782426  0.47921656 0.818648  ]. \t  3.583402728726905 \t 3.835906379723688\n",
            "60     \t [0.10003217 0.54068166 0.88585606]. \t  3.7183622705898465 \t 3.835906379723688\n",
            "61     \t [0.0190663  0.97310444 0.51434577]. \t  2.5392640430866167 \t 3.835906379723688\n",
            "62     \t [0.30874421 0.59841267 0.80209753]. \t  3.5650235237437338 \t 3.835906379723688\n",
            "63     \t [0.99673217 0.59663525 0.85441138]. \t  3.5965665351648743 \t 3.835906379723688\n",
            "64     \t [0.31201982 0.50949507 0.8663638 ]. \t  3.7678497235772834 \t 3.835906379723688\n",
            "65     \t [0.4435435  0.53892811 0.83840775]. \t  3.821230515130119 \t 3.835906379723688\n",
            "66     \t [0.38213321 0.48764375 0.89252568]. \t  3.534245739669886 \t 3.835906379723688\n",
            "67     \t [0.42446203 0.53614107 0.8977553 ]. \t  3.6419825890128554 \t 3.835906379723688\n",
            "68     \t [0.44682052 0.5411825  0.84125833]. \t  3.8301258251170287 \t 3.835906379723688\n",
            "69     \t [0.24907094 0.6030287  0.88781508]. \t  3.6696272680487736 \t 3.835906379723688\n",
            "70     \t [0.48902206 0.4903042  0.89092523]. \t  3.5526703166813927 \t 3.835906379723688\n",
            "71     \t [0.38388103 0.44650792 0.84355201]. \t  3.471666888811298 \t 3.835906379723688\n",
            "72     \t [0.3849367  0.54552983 0.82933314]. \t  3.800233009071777 \t 3.835906379723688\n",
            "73     \t [0.46173262 0.55438079 0.89072718]. \t  3.7129220208125266 \t 3.835906379723688\n",
            "74     \t [0.97001835 0.57796597 0.81862362]. \t  3.497982963943334 \t 3.835906379723688\n",
            "75     \t [0.45506756 0.49871733 0.86237334]. \t  3.733421628631016 \t 3.835906379723688\n",
            "76     \t [0.28631071 0.60016317 0.80280076]. \t  3.5692540465893314 \t 3.835906379723688\n",
            "77     \t [0.53598977 0.56323884 0.81449689]. \t  3.671562065765736 \t 3.835906379723688\n",
            "78     \t [0.49014831 0.57832432 0.85437286]. \t  3.821107327450391 \t 3.835906379723688\n",
            "79     \t [0.23331484 0.61833606 0.83279703]. \t  3.6842036350353298 \t 3.835906379723688\n",
            "80     \t [0.97528777 0.61101149 0.88199524]. \t  3.5132640977745324 \t 3.835906379723688\n",
            "81     \t [0.20895322 0.59125103 0.78541625]. \t  3.445957014962695 \t 3.835906379723688\n",
            "82     \t [0.99807668 0.53227242 0.89848652]. \t  3.481955888934822 \t 3.835906379723688\n",
            "83     \t [1.         0.5499033  0.90930497]. \t  3.3993821663402035 \t 3.835906379723688\n",
            "84     \t [0.43407273 0.58086642 0.81391244]. \t  3.670344490433213 \t 3.835906379723688\n",
            "85     \t [0.98273638 0.5275688  0.85091083]. \t  3.660753933371679 \t 3.835906379723688\n",
            "86     \t [1.         0.60575328 0.89598454]. \t  3.4408839459540257 \t 3.835906379723688\n",
            "87     \t [0.2491679  0.56829007 0.86205731]. \t  \u001b[92m3.8463719539915955\u001b[0m \t 3.8463719539915955\n",
            "88     \t [0.27822008 0.59208072 0.85911039]. \t  3.810705800303571 \t 3.8463719539915955\n",
            "89     \t [0.50004767 0.53829109 0.90581813]. \t  3.5610795405479028 \t 3.8463719539915955\n",
            "90     \t [0.14941398 0.52339642 0.84834944]. \t  3.813041249913049 \t 3.8463719539915955\n",
            "91     \t [0.42561974 0.54511646 0.81687077]. \t  3.723901195505996 \t 3.8463719539915955\n",
            "92     \t [0.34821101 0.51673253 0.86659301]. \t  3.7887754165588263 \t 3.8463719539915955\n",
            "93     \t [0.37763898 0.52393856 0.83822749]. \t  3.807486437969807 \t 3.8463719539915955\n",
            "94     \t [0.53823233 0.58285809 0.78559635]. \t  3.366631498563113 \t 3.8463719539915955\n",
            "95     \t [0.59996432 0.4988854  0.8278061 ]. \t  3.6640349961844114 \t 3.8463719539915955\n",
            "96     \t [0.44439264 0.5545907  0.12665798]. \t  0.08321267819503755 \t 3.8463719539915955\n",
            "97     \t [0.32523804 0.52599822 0.84823202]. \t  3.831315180540739 \t 3.8463719539915955\n",
            "98     \t [0.63888429 0.4023675  0.83454219]. \t  3.0966548854503104 \t 3.8463719539915955\n",
            "99     \t [0.40503574 0.53205612 0.87584673]. \t  3.7854502773340077 \t 3.8463719539915955\n",
            "100    \t [0.6578584  0.50215836 0.8316448 ]. \t  3.6752572849028975 \t 3.8463719539915955\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d86panpOuvum",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96ffc261-ccd9-443a-d3b3-df2f8da2c9f9"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 15 \r\n",
        "\r\n",
        "np.random.seed(run_num_15)\r\n",
        "surrogate_winner_15 = dtStudentProcess(d_cov_func, nu = df)\r\n",
        "\r\n",
        "winner_15 = dGPGO_stp(surrogate_winner_15, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_15.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.57788186 0.25550133 0.25252687]. \t  0.7217338409961529 \t 1.540625560354162\n",
            "init   \t [0.70990435 0.44755236 0.22694296]. \t  0.23199690080875204 \t 1.540625560354162\n",
            "init   \t [0.40170957 0.88225774 0.43721347]. \t  1.540625560354162 \t 1.540625560354162\n",
            "init   \t [0.87842518 0.78052775 0.53421429]. \t  0.5653855252190516 \t 1.540625560354162\n",
            "init   \t [0.01173301 0.26575648 0.3311941 ]. \t  0.5019682819961726 \t 1.540625560354162\n",
            "1      \t [0.29508373 0.36182513 0.99034168]. \t  1.50082405810891 \t 1.540625560354162\n",
            "2      \t [0.48429745 1.         1.        ]. \t  0.332588794531681 \t 1.540625560354162\n",
            "3      \t [0.94092746 0.08463947 0.97697611]. \t  0.25936799881533984 \t 1.540625560354162\n",
            "4      \t [0.29282856 0.01990927 0.89800727]. \t  0.24000383198261988 \t 1.540625560354162\n",
            "5      \t [0.07041611 0.81699998 0.93065622]. \t  \u001b[92m1.7514423583450274\u001b[0m \t 1.7514423583450274\n",
            "6      \t [0.98867055 0.06642364 0.0331871 ]. \t  0.059475571583973874 \t 1.7514423583450274\n",
            "7      \t [0.10920232 0.62316109 0.00506846]. \t  0.008194335621292969 \t 1.7514423583450274\n",
            "8      \t [0.03746919 0.97101815 0.13363633]. \t  0.013436689364015326 \t 1.7514423583450274\n",
            "9      \t [0.74251385 0.98471366 0.13923277]. \t  0.0046978053716866955 \t 1.7514423583450274\n",
            "10     \t [1. 1. 1.]. \t  0.31688362070415665 \t 1.7514423583450274\n",
            "11     \t [0.50816107 0.00995969 0.01074378]. \t  0.11678871706697912 \t 1.7514423583450274\n",
            "12     \t [1.         0.54977338 1.        ]. \t  \u001b[92m2.0028276387328865\u001b[0m \t 2.0028276387328865\n",
            "13     \t [0.99727511 0.14552814 0.61351442]. \t  0.31315946960942775 \t 2.0028276387328865\n",
            "14     \t [0.37789372 0.56995133 0.75925371]. \t  \u001b[92m3.1537251428351327\u001b[0m \t 3.1537251428351327\n",
            "15     \t [0.00831018 0.98856256 0.46664083]. \t  2.023703096659543 \t 3.1537251428351327\n",
            "16     \t [0.05538163 0.38623824 0.7325246 ]. \t  2.255741501955727 \t 3.1537251428351327\n",
            "17     \t [0.7021323  0.63797413 1.        ]. \t  1.967464418951561 \t 3.1537251428351327\n",
            "18     \t [0.08061025 0.66478924 0.44399255]. \t  1.3891409987312584 \t 3.1537251428351327\n",
            "19     \t [9.83553380e-01 6.07497839e-01 3.85365705e-04]. \t  0.003440078028613454 \t 3.1537251428351327\n",
            "20     \t [0.54394508 0.36483248 0.79933144]. \t  2.665930281511788 \t 3.1537251428351327\n",
            "21     \t [0.48889934 0.81892467 0.        ]. \t  0.0009990597119208609 \t 3.1537251428351327\n",
            "22     \t [3.87816277e-12 1.82534114e-01 4.78635731e-12]. \t  0.07467161098930794 \t 3.1537251428351327\n",
            "23     \t [0.01642777 0.00405711 0.44101499]. \t  0.25310566707261706 \t 3.1537251428351327\n",
            "24     \t [0.60850927 0.00374245 0.50371498]. \t  0.16268702647807504 \t 3.1537251428351327\n",
            "25     \t [0.26948188 1.         0.68104026]. \t  1.6344396170017303 \t 3.1537251428351327\n",
            "26     \t [1.         1.         0.28809785]. \t  0.028155586455540844 \t 3.1537251428351327\n",
            "27     \t [0.29635015 0.23003391 0.6104208 ]. \t  0.5404189493248927 \t 3.1537251428351327\n",
            "28     \t [0.70459245 0.99558532 0.77685916]. \t  0.6172441009686533 \t 3.1537251428351327\n",
            "29     \t [0.03248896 0.18988597 0.9467068 ]. \t  0.7824338678004278 \t 3.1537251428351327\n",
            "30     \t [5.99691030e-01 3.36053727e-01 1.38020532e-22]. \t  0.06185337692078329 \t 3.1537251428351327\n",
            "31     \t [3.12437968e-08 5.41005664e-01 6.59220207e-01]. \t  2.042476260533502 \t 3.1537251428351327\n",
            "32     \t [1.         0.6698458  0.79855254]. \t  2.8465780183041316 \t 3.1537251428351327\n",
            "33     \t [0. 0. 0.]. \t  0.06797411659013229 \t 3.1537251428351327\n",
            "34     \t [0.06371844 0.99466952 0.83724412]. \t  0.7781606597375126 \t 3.1537251428351327\n",
            "35     \t [0.00731695 0.51280769 0.96959525]. \t  2.5351019914388533 \t 3.1537251428351327\n",
            "36     \t [0.20834562 0.14765098 0.0858899 ]. \t  0.341654264804854 \t 3.1537251428351327\n",
            "37     \t [0.9992372  0.45624679 0.7884592 ]. \t  3.067049334752515 \t 3.1537251428351327\n",
            "38     \t [0.92468711 0.92890149 0.04146907]. \t  0.00026382466787973666 \t 3.1537251428351327\n",
            "39     \t [0.99220854 0.35254015 0.22482363]. \t  0.17038768577067015 \t 3.1537251428351327\n",
            "40     \t [0.99999999 0.99999999 0.77347727]. \t  0.4359270125252406 \t 3.1537251428351327\n",
            "41     \t [0.83436141 0.4823103  0.6892522 ]. \t  1.8889162074341688 \t 3.1537251428351327\n",
            "42     \t [0.51449107 0.68189106 0.85100922]. \t  \u001b[92m3.2922723386027046\u001b[0m \t 3.2922723386027046\n",
            "43     \t [0.75808483 0.03311965 0.81603081]. \t  0.3388867391649292 \t 3.2922723386027046\n",
            "44     \t [0.47326959 0.5596032  0.86884322]. \t  \u001b[92m3.8243881526601493\u001b[0m \t 3.8243881526601493\n",
            "45     \t [0.         0.         0.75385621]. \t  0.23704790862169367 \t 3.8243881526601493\n",
            "46     \t [0.48043352 0.55873158 0.87096495]. \t  3.816977298272902 \t 3.8243881526601493\n",
            "47     \t [0.93889726 0.03262728 0.19213743]. \t  0.29661407402815904 \t 3.8243881526601493\n",
            "48     \t [0.30392452 0.9983419  0.0484963 ]. \t  0.0011492115108926349 \t 3.8243881526601493\n",
            "49     \t [0.28138743 0.6552006  0.89104069]. \t  3.4064847398336093 \t 3.8243881526601493\n",
            "50     \t [0.39808412 0.58473835 0.87071721]. \t  3.7980494961619837 \t 3.8243881526601493\n",
            "51     \t [0.30097717 0.5812723  0.79625432]. \t  3.5552849259419275 \t 3.8243881526601493\n",
            "52     \t [0.50972609 0.60775544 0.82770761]. \t  3.6564571968976285 \t 3.8243881526601493\n",
            "53     \t [0.32389444 0.71950331 0.81986556]. \t  2.9576632442535753 \t 3.8243881526601493\n",
            "54     \t [0.46418646 0.62250434 0.80768893]. \t  3.4776510528964155 \t 3.8243881526601493\n",
            "55     \t [0.6475716  0.56123357 0.8738434 ]. \t  3.771593754565109 \t 3.8243881526601493\n",
            "56     \t [0.44606946 0.63068813 0.83542034]. \t  3.6071589337852163 \t 3.8243881526601493\n",
            "57     \t [0.54134615 0.55111158 0.84553353]. \t  \u001b[92m3.8245307409040867\u001b[0m \t 3.8245307409040867\n",
            "58     \t [0.52616007 0.60948191 0.86340847]. \t  3.7213517015917157 \t 3.8245307409040867\n",
            "59     \t [0.5232524  0.6152683  0.82184139]. \t  3.587742706287848 \t 3.8245307409040867\n",
            "60     \t [0.56049188 0.56049204 0.85279029]. \t  \u001b[92m3.82613386645848\u001b[0m \t 3.82613386645848\n",
            "61     \t [0.41540288 0.63861922 0.82726008]. \t  3.5384214355758825 \t 3.82613386645848\n",
            "62     \t [0.32141074 0.484453   0.80997018]. \t  3.5533313132146196 \t 3.82613386645848\n",
            "63     \t [0.46917195 0.57148458 0.83713463]. \t  3.8039983523264755 \t 3.82613386645848\n",
            "64     \t [0.44371029 0.56619029 0.89048124]. \t  3.7145947163112294 \t 3.82613386645848\n",
            "65     \t [0.50210644 0.60926051 0.86352891]. \t  3.7272114888041994 \t 3.82613386645848\n",
            "66     \t [0.38838664 0.55691085 0.88003711]. \t  3.787222936144267 \t 3.82613386645848\n",
            "67     \t [0.4776729  0.63141189 0.83429465]. \t  3.590361715172583 \t 3.82613386645848\n",
            "68     \t [0.55810902 0.54560413 0.8975286 ]. \t  3.6403090654070125 \t 3.82613386645848\n",
            "69     \t [0.57665216 0.5568635  0.88157792]. \t  3.754171181933457 \t 3.82613386645848\n",
            "70     \t [0.35078638 0.54314855 0.9055879 ]. \t  3.5783607602555554 \t 3.82613386645848\n",
            "71     \t [0.26092271 1.         0.32168871]. \t  0.46805988131762283 \t 3.82613386645848\n",
            "72     \t [0.29494063 0.61369802 0.87519607]. \t  3.700815701671898 \t 3.82613386645848\n",
            "73     \t [0.3947326  0.56175769 0.8552919 ]. \t  \u001b[92m3.854389083216549\u001b[0m \t 3.854389083216549\n",
            "74     \t [0.55960485 0.55834863 0.8929861 ]. \t  3.6825674879770913 \t 3.854389083216549\n",
            "75     \t [0.48035311 0.57727131 0.83360385]. \t  3.778750352333577 \t 3.854389083216549\n",
            "76     \t [0.78033333 0.56435585 0.91676593]. \t  3.3977313459139586 \t 3.854389083216549\n",
            "77     \t [0.52564813 0.56898776 0.83700737]. \t  3.793585996248126 \t 3.854389083216549\n",
            "78     \t [0.43510979 0.59766281 0.81358024]. \t  3.625140580088143 \t 3.854389083216549\n",
            "79     \t [0.23971755 0.57924837 0.8705885 ]. \t  3.810420017853351 \t 3.854389083216549\n",
            "80     \t [0.36969825 0.62193064 0.88236617]. \t  3.631351212969145 \t 3.854389083216549\n",
            "81     \t [0.00224749 0.62101478 0.8430322 ]. \t  3.65662011575285 \t 3.854389083216549\n",
            "82     \t [0.49517317 0.60303153 0.90831178]. \t  3.4883051725373724 \t 3.854389083216549\n",
            "83     \t [0.49644787 0.54991272 0.85532653]. \t  3.841839767843276 \t 3.854389083216549\n",
            "84     \t [0.45554389 0.56453606 0.87103392]. \t  3.817638523035912 \t 3.854389083216549\n",
            "85     \t [0.39572226 0.55028303 0.88635013]. \t  3.747278953113572 \t 3.854389083216549\n",
            "86     \t [0.56965328 0.53517017 0.84099421]. \t  3.7989218302072594 \t 3.854389083216549\n",
            "87     \t [0.54219151 0.57182538 0.82481149]. \t  3.7271150611425483 \t 3.854389083216549\n",
            "88     \t [0.19333861 0.52943629 0.84441475]. \t  3.82755004937123 \t 3.854389083216549\n",
            "89     \t [0.22523989 0.46307685 0.86479607]. \t  3.5485143101141867 \t 3.854389083216549\n",
            "90     \t [0.317162   0.62590689 0.86344756]. \t  3.680570344511672 \t 3.854389083216549\n",
            "91     \t [0.45882101 0.5525949  0.89545   ]. \t  3.675464812520233 \t 3.854389083216549\n",
            "92     \t [0.5510216  0.59712952 0.85586622]. \t  3.7621018577213063 \t 3.854389083216549\n",
            "93     \t [0.52695897 0.5571352  0.88297578]. \t  3.755455132990629 \t 3.854389083216549\n",
            "94     \t [0.45974299 0.47522424 0.81592469]. \t  3.5366529419686863 \t 3.854389083216549\n",
            "95     \t [0.49519851 0.5715585  0.88832895]. \t  3.7194618274105085 \t 3.854389083216549\n",
            "96     \t [0.32540865 0.60378966 0.84057755]. \t  3.759292399176378 \t 3.854389083216549\n",
            "97     \t [0.45523221 0.60972132 0.8095719 ]. \t  3.5478793881310753 \t 3.854389083216549\n",
            "98     \t [0.44372046 0.56459384 0.82699429]. \t  3.773736779493473 \t 3.854389083216549\n",
            "99     \t [0.6198808  0.50287142 0.85537688]. \t  3.7297942672936446 \t 3.854389083216549\n",
            "100    \t [0.40093235 0.60162257 0.79584646]. \t  3.4779139835261295 \t 3.854389083216549\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "any0xrgYuvxA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7440f490-0df8-49a1-9446-2787d9b91148"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 16 \r\n",
        "\r\n",
        "np.random.seed(run_num_16)\r\n",
        "surrogate_winner_16 = dtStudentProcess(d_cov_func, nu = df)\r\n",
        "\r\n",
        "winner_16 = dGPGO_stp(surrogate_winner_16, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_16.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.86394833 0.316366   0.67215078]. \t  1.1947633822631252 \t 3.8084053754826726\n",
            "init   \t [0.50791167 0.78166043 0.28368168]. \t  0.20052765717741963 \t 3.8084053754826726\n",
            "init   \t [0.23370878 0.56379969 0.87502436]. \t  3.8084053754826726 \t 3.8084053754826726\n",
            "init   \t [0.71894292 0.18213174 0.24380041]. \t  0.6532826728302027 \t 3.8084053754826726\n",
            "init   \t [0.08605673 0.53424539 0.83969965]. \t  3.8072565344663847 \t 3.8084053754826726\n",
            "1      \t [0.12378065 0.3168353  0.9881258 ]. \t  1.2451880833929392 \t 3.8084053754826726\n",
            "2      \t [0.43751288 0.99716585 0.9184722 ]. \t  0.5658696288532565 \t 3.8084053754826726\n",
            "3      \t [0.00078822 0.37238738 0.0555749 ]. \t  0.09052847708814837 \t 3.8084053754826726\n",
            "4      \t [0.04295809 0.77648209 0.67327646]. \t  2.5320700396767513 \t 3.8084053754826726\n",
            "5      \t [0.17874    0.26983844 0.47944919]. \t  0.3268459847263784 \t 3.8084053754826726\n",
            "6      \t [1.        0.7423503 1.       ]. \t  1.4857206442344493 \t 3.8084053754826726\n",
            "7      \t [0.9420988  0.81466355 0.01029373]. \t  0.0004455515635959126 \t 3.8084053754826726\n",
            "8      \t [0.00628562 0.62733695 0.98683809]. \t  2.216178855902818 \t 3.8084053754826726\n",
            "9      \t [0.97684885 0.16625467 0.10343961]. \t  0.14391422782213323 \t 3.8084053754826726\n",
            "10     \t [1.         1.         0.59844886]. \t  0.27573991880442383 \t 3.8084053754826726\n",
            "11     \t [0.46229365 0.54823253 0.74162981]. \t  2.896214384613141 \t 3.8084053754826726\n",
            "12     \t [0.97751905 0.26823441 1.        ]. \t  0.8413983724147325 \t 3.8084053754826726\n",
            "13     \t [0.05790357 0.97831053 0.0286539 ]. \t  0.0007424991674578819 \t 3.8084053754826726\n",
            "14     \t [1. 1. 1.]. \t  0.31688362072106735 \t 3.8084053754826726\n",
            "15     \t [0.14193253 0.         0.        ]. \t  0.08760565264993837 \t 3.8084053754826726\n",
            "16     \t [0.87041744 0.00477875 0.62333708]. \t  0.12625234887657366 \t 3.8084053754826726\n",
            "17     \t [0.41426528 0.39613038 0.        ]. \t  0.05348449747343011 \t 3.8084053754826726\n",
            "18     \t [0.49285801 0.57379258 0.98619909]. \t  2.3362410882084697 \t 3.8084053754826726\n",
            "19     \t [0.12095067 0.5523865  0.7986036 ]. \t  3.5970589852668997 \t 3.8084053754826726\n",
            "20     \t [0.93146679 0.99576218 0.0816992 ]. \t  0.0004738601906585057 \t 3.8084053754826726\n",
            "21     \t [0.         0.         0.21416324]. \t  0.5327198129358311 \t 3.8084053754826726\n",
            "22     \t [0.59568835 0.03957295 0.96014734]. \t  0.19773227987468825 \t 3.8084053754826726\n",
            "23     \t [0.22920365 0.02010065 0.70654537]. \t  0.24886131082936253 \t 3.8084053754826726\n",
            "24     \t [0.64805698 1.         0.41563373]. \t  0.5997475085385635 \t 3.8084053754826726\n",
            "25     \t [0.05690765 0.94216998 0.98567173]. \t  0.5884703782923711 \t 3.8084053754826726\n",
            "26     \t [0.37818408 0.86223545 0.00816775]. \t  0.0008259099666383707 \t 3.8084053754826726\n",
            "27     \t [0. 0. 0.]. \t  0.06797411659013229 \t 3.8084053754826726\n",
            "28     \t [0.96891308 0.00211221 0.03632779]. \t  0.06005568719616629 \t 3.8084053754826726\n",
            "29     \t [0.22098724 0.715023   0.80480041]. \t  2.955140641514817 \t 3.8084053754826726\n",
            "30     \t [0.95894486 0.65412763 0.41162985]. \t  0.1412051974033965 \t 3.8084053754826726\n",
            "31     \t [4.92550734e-01 5.95453361e-11 2.99116710e-02]. \t  0.15360767794529828 \t 3.8084053754826726\n",
            "32     \t [0.00184198 0.78623206 0.02105978]. \t  0.0017521802432675408 \t 3.8084053754826726\n",
            "33     \t [0.43675068 0.34782499 0.910072  ]. \t  2.3048717198793227 \t 3.8084053754826726\n",
            "34     \t [0.81402476 0.8137549  0.82034563]. \t  1.863330090749562 \t 3.8084053754826726\n",
            "35     \t [0.00617673 0.36546393 0.71982755]. \t  1.9881326686835707 \t 3.8084053754826726\n",
            "36     \t [0.72023732 0.97251784 0.00446747]. \t  0.0001583279247718986 \t 3.8084053754826726\n",
            "37     \t [0.06709828 0.95579775 0.5772558 ]. \t  2.8030572011833668 \t 3.8084053754826726\n",
            "38     \t [0.14617975 0.59194485 0.82341198]. \t  3.7247501239631142 \t 3.8084053754826726\n",
            "39     \t [0.16192308 0.98204201 0.23979483]. \t  0.135396854253552 \t 3.8084053754826726\n",
            "40     \t [0.09516882 0.97245262 0.71107802]. \t  1.6017819001869786 \t 3.8084053754826726\n",
            "41     \t [0.96016717 0.05431443 0.88604207]. \t  0.34300095655585683 \t 3.8084053754826726\n",
            "42     \t [0.46981763 0.01579792 0.37435033]. \t  0.6228508055961295 \t 3.8084053754826726\n",
            "43     \t [0.25375673 0.61632526 0.9228542 ]. \t  3.287629818020309 \t 3.8084053754826726\n",
            "44     \t [0.7174323  0.60195623 0.03526326]. \t  0.013292644189248578 \t 3.8084053754826726\n",
            "45     \t [0.97808322 0.54554817 0.87388363]. \t  3.6528419214094825 \t 3.8084053754826726\n",
            "46     \t [0.9993436  0.24771774 0.62284063]. \t  0.5825462855479026 \t 3.8084053754826726\n",
            "47     \t [0.00166962 0.79791474 0.41341338]. \t  1.515325912266853 \t 3.8084053754826726\n",
            "48     \t [0.98892569 0.58008369 0.81652021]. \t  3.4670538600108336 \t 3.8084053754826726\n",
            "49     \t [0.14134409 0.48080159 0.80153125]. \t  3.4677754223296535 \t 3.8084053754826726\n",
            "50     \t [0.94067832 0.00282782 0.17395275]. \t  0.2534627126008512 \t 3.8084053754826726\n",
            "51     \t [0.25170708 0.59320396 0.96519115]. \t  2.69604246732515 \t 3.8084053754826726\n",
            "52     \t [0.04345529 0.65940503 0.84965698]. \t  3.4702595428248726 \t 3.8084053754826726\n",
            "53     \t [0.05848135 0.47135776 0.81994148]. \t  3.5213504013127426 \t 3.8084053754826726\n",
            "54     \t [0.02910611 0.09839349 0.98657991]. \t  0.274741909637238 \t 3.8084053754826726\n",
            "55     \t [1.         0.54228724 0.82959525]. \t  3.595819440824884 \t 3.8084053754826726\n",
            "56     \t [0.9786107  0.49351691 0.90315046]. \t  3.3363133765424218 \t 3.8084053754826726\n",
            "57     \t [0.22919795 0.64189721 0.84653794]. \t  3.601704518251694 \t 3.8084053754826726\n",
            "58     \t [0.01422735 0.98429237 0.88575216]. \t  0.7145882760974416 \t 3.8084053754826726\n",
            "59     \t [0.19675509 0.58147403 0.70131044]. \t  2.5666130957907916 \t 3.8084053754826726\n",
            "60     \t [0.99999781 0.63076583 0.85976974]. \t  3.4546157626678786 \t 3.8084053754826726\n",
            "61     \t [0.11280495 0.53382277 0.80833149]. \t  3.6605056120633757 \t 3.8084053754826726\n",
            "62     \t [0.13718202 0.52044274 0.91206784]. \t  3.4452616045125963 \t 3.8084053754826726\n",
            "63     \t [0.19397515 0.68182541 0.82359872]. \t  3.2823330317923674 \t 3.8084053754826726\n",
            "64     \t [0.86040545 0.50888626 0.84674788]. \t  3.6685721117124075 \t 3.8084053754826726\n",
            "65     \t [0.31829734 0.51272667 0.83250801]. \t  3.7671693141782705 \t 3.8084053754826726\n",
            "66     \t [1.         0.46193721 0.84185034]. \t  3.4110077562585994 \t 3.8084053754826726\n",
            "67     \t [0.09001315 0.49888268 0.77834536]. \t  3.312947326511715 \t 3.8084053754826726\n",
            "68     \t [0.0796643  0.536108   0.88064967]. \t  3.7395446042988794 \t 3.8084053754826726\n",
            "69     \t [0.08688444 0.6271243  0.92520369]. \t  3.196413594926658 \t 3.8084053754826726\n",
            "70     \t [0.23796614 0.56620689 0.84659931]. \t  \u001b[92m3.851497310705611\u001b[0m \t 3.851497310705611\n",
            "71     \t [0.98525595 0.53054274 0.88672034]. \t  3.5708692783185745 \t 3.851497310705611\n",
            "72     \t [0.25025865 0.49817968 0.80930379]. \t  3.6007228933938826 \t 3.851497310705611\n",
            "73     \t [0.96569506 0.57588097 0.82438042]. \t  3.546293974535231 \t 3.851497310705611\n",
            "74     \t [0.93052666 0.52983008 0.85783044]. \t  3.689428041307031 \t 3.851497310705611\n",
            "75     \t [0.08934167 0.48178952 0.84929771]. \t  3.6533060392850523 \t 3.851497310705611\n",
            "76     \t [0.6114282 0.5402622 0.8958659]. \t  3.6391336711884668 \t 3.851497310705611\n",
            "77     \t [0.88721719 0.58590453 0.81744956]. \t  3.5074431251266347 \t 3.851497310705611\n",
            "78     \t [0.82560583 0.54713437 0.85355961]. \t  3.745703130198269 \t 3.851497310705611\n",
            "79     \t [0.49254834 0.57721608 0.81827002]. \t  3.69147813877718 \t 3.851497310705611\n",
            "80     \t [0.08592806 0.64802937 0.85836576]. \t  3.549999912819504 \t 3.851497310705611\n",
            "81     \t [0.76056384 0.47371236 0.90622988]. \t  3.2845786547016074 \t 3.851497310705611\n",
            "82     \t [0.45538391 0.54676818 0.85461998]. \t  3.8474941743476574 \t 3.851497310705611\n",
            "83     \t [0.66839054 0.55267888 0.84459152]. \t  3.787407413649402 \t 3.851497310705611\n",
            "84     \t [0.65851214 0.43161574 0.8406884 ]. \t  3.3284953464353464 \t 3.851497310705611\n",
            "85     \t [0.25982411 0.59176759 0.86619733]. \t  3.7989029926220277 \t 3.851497310705611\n",
            "86     \t [0.73550378 0.57306883 0.88608319]. \t  3.678165752837509 \t 3.851497310705611\n",
            "87     \t [0.32014186 0.4927334  0.85450152]. \t  3.7278381942572056 \t 3.851497310705611\n",
            "88     \t [0.13044132 0.58809795 0.87017856]. \t  3.781360571917484 \t 3.851497310705611\n",
            "89     \t [0.67914396 0.51234    0.85483802]. \t  3.7437628265271012 \t 3.851497310705611\n",
            "90     \t [0.59647757 0.46444458 0.85153307]. \t  3.5597149620177264 \t 3.851497310705611\n",
            "91     \t [0.31402964 0.46534822 0.84569073]. \t  3.5916796952222874 \t 3.851497310705611\n",
            "92     \t [0.40161724 0.51766173 0.84997923]. \t  3.809641282046565 \t 3.851497310705611\n",
            "93     \t [7.74416749e-05 9.66650758e-01 3.58764972e-01]. \t  0.839334923478042 \t 3.851497310705611\n",
            "94     \t [0.67264784 0.5393051  0.82625384]. \t  3.7108646280754174 \t 3.851497310705611\n",
            "95     \t [0.44233886 0.55878348 0.83074907]. \t  3.7964141245474363 \t 3.851497310705611\n",
            "96     \t [0.51590941 0.57751381 0.87627384]. \t  3.774622438927911 \t 3.851497310705611\n",
            "97     \t [0.25198225 0.5943798  0.79597747]. \t  3.53193393321682 \t 3.851497310705611\n",
            "98     \t [0.44522401 0.51569338 0.86437356]. \t  3.7859368164780833 \t 3.851497310705611\n",
            "99     \t [0.0110953  0.61978764 0.79919724]. \t  3.445618403341059 \t 3.851497310705611\n",
            "100    \t [0.35997784 0.54650445 0.84849974]. \t  \u001b[92m3.8552853235563407\u001b[0m \t 3.8552853235563407\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "reLyKt6Quvzx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac92abec-4ce8-482c-8f25-d8372bd9e4ec"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 17 \r\n",
        "\r\n",
        "np.random.seed(run_num_17)\r\n",
        "surrogate_winner_17 = dtStudentProcess(d_cov_func, nu = df)\r\n",
        "\r\n",
        "winner_17 = dGPGO_stp(surrogate_winner_17, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_17.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.65182403 0.73051962 0.19336736]. \t  0.03562358530655929 \t 3.1179188940604616\n",
            "init   \t [0.51967724 0.67638327 0.80444487]. \t  3.1179188940604616 \t 3.1179188940604616\n",
            "init   \t [0.53351286 0.90803418 0.44348113]. \t  1.1984486017622078 \t 3.1179188940604616\n",
            "init   \t [0.25597221 0.27866502 0.04332186]. \t  0.1645724004847893 \t 3.1179188940604616\n",
            "init   \t [0.46897499 0.9416138  0.60040624]. \t  1.9192522196289552 \t 3.1179188940604616\n",
            "1      \t [0.40487194 0.12946702 0.94080138]. \t  0.5171352955947353 \t 3.1179188940604616\n",
            "2      \t [1.         0.95654156 1.        ]. \t  0.45092739852965463 \t 3.1179188940604616\n",
            "3      \t [0.0434631  0.65910314 0.84197486]. \t  \u001b[92m3.46354892023729\u001b[0m \t 3.46354892023729\n",
            "4      \t [0.9769569  0.06267365 0.80209202]. \t  0.435305807242099 \t 3.46354892023729\n",
            "5      \t [0.34345257 0.81839366 1.        ]. \t  1.1268850704279032 \t 3.46354892023729\n",
            "6      \t [0.01072212 0.33442544 0.47524514]. \t  0.3330139157455057 \t 3.46354892023729\n",
            "7      \t [0.83093373 0.00874913 0.12872981]. \t  0.26351314636660267 \t 3.46354892023729\n",
            "8      \t [0.00759575 0.88890306 0.30465969]. \t  0.446543740840736 \t 3.46354892023729\n",
            "9      \t [1.         0.69564332 0.61402546]. \t  0.7191400640443628 \t 3.46354892023729\n",
            "10     \t [0.30937877 0.56516556 0.65202933]. \t  2.0160791458777085 \t 3.46354892023729\n",
            "11     \t [1.         0.50103441 1.        ]. \t  1.9243335640453942 \t 3.46354892023729\n",
            "12     \t [0. 0. 0.]. \t  0.06797411659013229 \t 3.46354892023729\n",
            "13     \t [0.00240499 0.42898317 0.90288537]. \t  3.0298527751570026 \t 3.46354892023729\n",
            "14     \t [0.00477979 0.92638886 0.82972184]. \t  1.20510554368015 \t 3.46354892023729\n",
            "15     \t [0.         0.43597424 0.00166635]. \t  0.028983920874505076 \t 3.46354892023729\n",
            "16     \t [0.9904124  0.99246671 0.18700461]. \t  0.004506449944843551 \t 3.46354892023729\n",
            "17     \t [0.71693573 0.33150088 1.        ]. \t  1.198309073352385 \t 3.46354892023729\n",
            "18     \t [0.52811365 0.         0.        ]. \t  0.09475935034397429 \t 3.46354892023729\n",
            "19     \t [0.45863232 0.97830654 0.06516793]. \t  0.0015770696232439625 \t 3.46354892023729\n",
            "20     \t [0.9107796  0.24775313 0.01336517]. \t  0.05047258758526262 \t 3.46354892023729\n",
            "21     \t [0.16554566 0.         0.56217642]. \t  0.11388827115785259 \t 3.46354892023729\n",
            "22     \t [0.04513087 0.11224568 0.91598636]. \t  0.5096257946939037 \t 3.46354892023729\n",
            "23     \t [1.         1.         0.70422125]. \t  0.33234172747211627 \t 3.46354892023729\n",
            "24     \t [0.96368261 0.85904114 0.02595699]. \t  0.0003210902948301794 \t 3.46354892023729\n",
            "25     \t [0.67828159 0.1563101  0.45865462]. \t  0.2817096830348212 \t 3.46354892023729\n",
            "26     \t [0.71543223 1.         1.        ]. \t  0.32715023841575464 \t 3.46354892023729\n",
            "27     \t [0.88059456 0.00713882 0.93810163]. \t  0.16345574466800683 \t 3.46354892023729\n",
            "28     \t [0.63974411 0.         0.74319308]. \t  0.23490645609706962 \t 3.46354892023729\n",
            "29     \t [0.80359425 0.7419072  0.99297146]. \t  1.6108165961436156 \t 3.46354892023729\n",
            "30     \t [0.         0.70933835 0.        ]. \t  0.0025503351430716877 \t 3.46354892023729\n",
            "31     \t [0.25396113 0.         0.21716763]. \t  0.7773610976586508 \t 3.46354892023729\n",
            "32     \t [1.         0.46521653 0.17191626]. \t  0.0691973453008514 \t 3.46354892023729\n",
            "33     \t [0.01972314 0.73771699 0.92289111]. \t  2.49059089092362 \t 3.46354892023729\n",
            "34     \t [0.21530384 0.53454545 0.89002161]. \t  \u001b[92m3.6985899205203587\u001b[0m \t 3.6985899205203587\n",
            "35     \t [0.14344129 0.47150639 0.92624163]. \t  3.0709307191037363 \t 3.6985899205203587\n",
            "36     \t [0.91386447 0.51391653 0.79009551]. \t  3.2843215618842665 \t 3.6985899205203587\n",
            "37     \t [0.76810327 1.         0.42084054]. \t  0.40717298526822127 \t 3.6985899205203587\n",
            "38     \t [0.06639408 0.99444496 0.04231428]. \t  0.0010534060639869145 \t 3.6985899205203587\n",
            "39     \t [1.18814013e-10 1.18814013e-10 4.57661624e-01]. \t  0.2059038463432127 \t 3.6985899205203587\n",
            "40     \t [0.0988583  0.66452426 0.75403167]. \t  2.9366180685825687 \t 3.6985899205203587\n",
            "41     \t [0.83081155 0.99851534 0.02280783]. \t  0.00014776172059906273 \t 3.6985899205203587\n",
            "42     \t [1.         0.50817646 0.8668273 ]. \t  3.598288705955429 \t 3.6985899205203587\n",
            "43     \t [0.81263707 0.35833496 0.85200986]. \t  2.6650622708009113 \t 3.6985899205203587\n",
            "44     \t [0.0971357  0.99999989 0.99999996]. \t  0.33255839299709133 \t 3.6985899205203587\n",
            "45     \t [0.12217275 0.59884134 0.8208518 ]. \t  3.6890968342714765 \t 3.6985899205203587\n",
            "46     \t [0.94787482 0.68304759 0.8554681 ]. \t  3.126593039199851 \t 3.6985899205203587\n",
            "47     \t [0.49538727 0.49819113 0.89388965]. \t  3.5621639998713692 \t 3.6985899205203587\n",
            "48     \t [0.66339085 0.52939676 0.87904133]. \t  \u001b[92m3.7234266661802726\u001b[0m \t 3.7234266661802726\n",
            "49     \t [0.63440597 0.59892129 0.85253506]. \t  \u001b[92m3.730697533134973\u001b[0m \t 3.730697533134973\n",
            "50     \t [0.12984761 0.50374317 0.8308734 ]. \t  3.719537631954169 \t 3.730697533134973\n",
            "51     \t [0.51371129 0.55397474 0.89328106]. \t  3.686998567090521 \t 3.730697533134973\n",
            "52     \t [0.41507281 0.55091735 0.88014048]. \t  \u001b[92m3.783569335182621\u001b[0m \t 3.783569335182621\n",
            "53     \t [0.36590345 0.56171795 0.87078636]. \t  \u001b[92m3.828002041088139\u001b[0m \t 3.828002041088139\n",
            "54     \t [0.35816374 0.58616059 0.85245892]. \t  3.8238266245703754 \t 3.828002041088139\n",
            "55     \t [0.53031189 0.60924887 0.8548249 ]. \t  3.7247574055062733 \t 3.828002041088139\n",
            "56     \t [0.22511526 0.99363634 0.80360218]. \t  0.8792084482270819 \t 3.828002041088139\n",
            "57     \t [0.3265543  0.55591276 0.89680522]. \t  3.670737319159672 \t 3.828002041088139\n",
            "58     \t [0.54744388 0.55823515 0.84070929]. \t  3.809796950132483 \t 3.828002041088139\n",
            "59     \t [0.59707675 0.59393188 0.84162835]. \t  3.735237862721261 \t 3.828002041088139\n",
            "60     \t [0.6648872  0.5536998  0.86451136]. \t  3.794063104244056 \t 3.828002041088139\n",
            "61     \t [0.53195762 0.54615331 0.90617161]. \t  3.562649268424366 \t 3.828002041088139\n",
            "62     \t [0.51528532 0.54967247 0.8925555 ]. \t  3.690598063759129 \t 3.828002041088139\n",
            "63     \t [0.49314145 0.61935806 0.839552  ]. \t  3.66353018695358 \t 3.828002041088139\n",
            "64     \t [0.51722785 0.47111095 0.89008316]. \t  3.458360983998543 \t 3.828002041088139\n",
            "65     \t [0.46721171 0.59151239 0.85269089]. \t  3.795580221951946 \t 3.828002041088139\n",
            "66     \t [0.0033798  0.61768491 0.81983961]. \t  3.5859620561054033 \t 3.828002041088139\n",
            "67     \t [0.53609523 0.5405908  0.88713619]. \t  3.718772730763557 \t 3.828002041088139\n",
            "68     \t [0.98857006 0.56639069 0.86017701]. \t  3.6682257939179728 \t 3.828002041088139\n",
            "69     \t [0.09242895 0.58401918 0.81684372]. \t  3.695469783619871 \t 3.828002041088139\n",
            "70     \t [0.03043172 0.55365923 0.84809971]. \t  3.818930249261756 \t 3.828002041088139\n",
            "71     \t [0.46399829 0.58976428 0.85470486]. \t  3.801524850706744 \t 3.828002041088139\n",
            "72     \t [0.5256138  0.51955875 0.90957835]. \t  3.479438788880762 \t 3.828002041088139\n",
            "73     \t [0.38462782 0.53247591 0.89438087]. \t  3.666722542966756 \t 3.828002041088139\n",
            "74     \t [0.44428898 0.52166384 0.90090655]. \t  3.5817793548688615 \t 3.828002041088139\n",
            "75     \t [0.45439196 0.56175538 0.86796119]. \t  \u001b[92m3.828632191246126\u001b[0m \t 3.828632191246126\n",
            "76     \t [0.67555303 0.50589002 0.82602431]. \t  3.6549416298917574 \t 3.828632191246126\n",
            "77     \t [0.59412286 0.56519176 0.87235384]. \t  3.7875961715604523 \t 3.828632191246126\n",
            "78     \t [0.39232599 0.54277683 0.81525315]. \t  3.718236112738308 \t 3.828632191246126\n",
            "79     \t [0.30038612 0.5079817  0.82422004]. \t  3.7197659357078 \t 3.828632191246126\n",
            "80     \t [0.48771166 0.61044346 0.81837942]. \t  3.5974065473158654 \t 3.828632191246126\n",
            "81     \t [0.02771174 0.56478037 0.79316068]. \t  3.5214114902814204 \t 3.828632191246126\n",
            "82     \t [0.50692743 0.58792979 0.84713893]. \t  3.789798583967441 \t 3.828632191246126\n",
            "83     \t [0.45598187 0.53557457 0.85969201]. \t  \u001b[92m3.8337733735230812\u001b[0m \t 3.8337733735230812\n",
            "84     \t [0.52091802 0.57257791 0.82913152]. \t  3.7556247448087428 \t 3.8337733735230812\n",
            "85     \t [0.49589897 0.554726   0.83660579]. \t  3.8103212466492273 \t 3.8337733735230812\n",
            "86     \t [0.66655077 0.55489654 0.87672864]. \t  3.7566109702799113 \t 3.8337733735230812\n",
            "87     \t [0.43563433 0.57535865 0.83388057]. \t  3.7929148549190908 \t 3.8337733735230812\n",
            "88     \t [0.4790739  0.56722436 0.8558572 ]. \t  \u001b[92m3.838433899948719\u001b[0m \t 3.838433899948719\n",
            "89     \t [0.25596617 0.54701253 0.84138026]. \t  \u001b[92m3.8466903804095542\u001b[0m \t 3.8466903804095542\n",
            "90     \t [0.46895826 0.55730683 0.86723655]. \t  3.8299698207979755 \t 3.8466903804095542\n",
            "91     \t [0.40658933 0.50425381 0.87391844]. \t  3.7197932609753064 \t 3.8466903804095542\n",
            "92     \t [0.4826169  0.5357556  0.86057313]. \t  3.8290930342568137 \t 3.8466903804095542\n",
            "93     \t [0.14313963 0.55366968 0.80960787]. \t  3.686967149955889 \t 3.8466903804095542\n",
            "94     \t [0.48830809 0.55671599 0.86052375]. \t  3.8398011054601002 \t 3.8466903804095542\n",
            "95     \t [0.50145009 0.54098404 0.88460443]. \t  3.7407143219977375 \t 3.8466903804095542\n",
            "96     \t [0.48247671 0.58233484 0.87544955]. \t  3.7753049497885702 \t 3.8466903804095542\n",
            "97     \t [0.47883583 0.58282232 0.83995692]. \t  3.790909579063083 \t 3.8466903804095542\n",
            "98     \t [0.08318396 0.59995006 0.85911047]. \t  3.7647966351440827 \t 3.8466903804095542\n",
            "99     \t [0.40764217 0.53441665 0.86914852]. \t  3.815606522423459 \t 3.8466903804095542\n",
            "100    \t [0.2113483  0.51846748 0.87067963]. \t  3.7731669929122353 \t 3.8466903804095542\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cI3FtfYSuv2u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fab0e29-da3a-437b-a1ea-d7542e2c8528"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 18 \r\n",
        "\r\n",
        "np.random.seed(run_num_18)\r\n",
        "surrogate_winner_18 = dtStudentProcess(d_cov_func, nu = df)\r\n",
        "\r\n",
        "winner_18 = dGPGO_stp(surrogate_winner_18, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_18.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.8734294  0.96854066 0.86919454]. \t  0.722531189286755 \t 1.1210522139432408\n",
            "init   \t [0.53085569 0.23272833 0.0113988 ]. \t  0.1133646794700679 \t 1.1210522139432408\n",
            "init   \t [0.43046882 0.40235136 0.52267467]. \t  0.5232646985053151 \t 1.1210522139432408\n",
            "init   \t [0.4783918  0.55535647 0.54338602]. \t  1.0390748854353227 \t 1.1210522139432408\n",
            "init   \t [0.76089558 0.71237457 0.6196821 ]. \t  1.1210522139432408 \t 1.1210522139432408\n",
            "1      \t [0.28982083 0.95201409 0.18404427]. \t  0.04220734680078784 \t 1.1210522139432408\n",
            "2      \t [0.00482474 0.64977529 0.94137026]. \t  \u001b[92m2.8634726533487465\u001b[0m \t 2.8634726533487465\n",
            "3      \t [0.99659594 0.17559011 0.91257217]. \t  0.8330954963238189 \t 2.8634726533487465\n",
            "4      \t [0.12883542 0.98258307 0.90180597]. \t  0.6910738837771888 \t 2.8634726533487465\n",
            "5      \t [0.34732565 0.41739735 1.        ]. \t  1.6744289894629165 \t 2.8634726533487465\n",
            "6      \t [0.92481221 0.89538898 0.01033975]. \t  0.0001834476676030118 \t 2.8634726533487465\n",
            "7      \t [0.03882967 0.52573124 0.0876224 ]. \t  0.052781048696316224 \t 2.8634726533487465\n",
            "8      \t [0.99294203 0.18530791 0.09475188]. \t  0.12146947227861624 \t 2.8634726533487465\n",
            "9      \t [1.         0.63046904 0.989953  ]. \t  2.0994194253212894 \t 2.8634726533487465\n",
            "10     \t [0.00586118 0.08551021 0.64362908]. \t  0.285320959152505 \t 2.8634726533487465\n",
            "11     \t [0.06716145 0.05276566 0.98151703]. \t  0.18726883925691168 \t 2.8634726533487465\n",
            "12     \t [1.         1.         0.44237824]. \t  0.16429788935963272 \t 2.8634726533487465\n",
            "13     \t [0.038904   0.0078251  0.03940271]. \t  0.13479574553623774 \t 2.8634726533487465\n",
            "14     \t [0.10195839 0.9221158  0.5776728 ]. \t  \u001b[92m2.9610300008493953\u001b[0m \t 2.9610300008493953\n",
            "15     \t [0.89657108 0.         0.04793644]. \t  0.08929496065904569 \t 2.9610300008493953\n",
            "16     \t [0.01217753 0.87871288 0.55243032]. \t  \u001b[92m2.9879392691557105\u001b[0m \t 2.9879392691557105\n",
            "17     \t [0.03989821 0.65650288 0.68551679]. \t  2.506075517798532 \t 2.9879392691557105\n",
            "18     \t [0.03591706 0.95476209 0.07665491]. \t  0.0031251970395808457 \t 2.9879392691557105\n",
            "19     \t [0.49380672 0.00776333 0.47869001]. \t  0.2380474198071523 \t 2.9879392691557105\n",
            "20     \t [0.49420074 0.03999441 0.99297449]. \t  0.14975319318469735 \t 2.9879392691557105\n",
            "21     \t [0.53693559 1.         0.66322087]. \t  1.172637699964568 \t 2.9879392691557105\n",
            "22     \t [0.98292282 0.08349082 0.60550662]. \t  0.19391777510028924 \t 2.9879392691557105\n",
            "23     \t [0.66436302 0.87692373 1.        ]. \t  0.807241073528929 \t 2.9879392691557105\n",
            "24     \t [0.99382574 0.67291054 0.15853588]. \t  0.011618423499300682 \t 2.9879392691557105\n",
            "25     \t [0.72959906 0.33476437 0.91119127]. \t  2.149137944712573 \t 2.9879392691557105\n",
            "26     \t [0.77490973 0.22891543 0.28438986]. \t  0.5351944374332035 \t 2.9879392691557105\n",
            "27     \t [0.02531226 0.36418907 0.9594858 ]. \t  1.8848202498768818 \t 2.9879392691557105\n",
            "28     \t [1. 1. 1.]. \t  0.31688362070415665 \t 2.9879392691557105\n",
            "29     \t [0.26956599 0.74201131 0.        ]. \t  0.00251845256395572 \t 2.9879392691557105\n",
            "30     \t [0.9710131  0.40958601 0.66480424]. \t  1.389960559506917 \t 2.9879392691557105\n",
            "31     \t [0.88196417 0.41364233 0.        ]. \t  0.022084453048844967 \t 2.9879392691557105\n",
            "32     \t [2.82560374e-10 2.62603731e-10 4.31586614e-01]. \t  0.2642422508061413 \t 2.9879392691557105\n",
            "33     \t [0.23011156 0.         0.01914985]. \t  0.1297650753286031 \t 2.9879392691557105\n",
            "34     \t [0.3243808  0.8077228  0.80596369]. \t  2.1568018364813533 \t 2.9879392691557105\n",
            "35     \t [0.86809143 0.99668771 0.19038462]. \t  0.008706474371485117 \t 2.9879392691557105\n",
            "36     \t [0.00387957 0.83216769 0.25641182]. \t  0.20182835126681017 \t 2.9879392691557105\n",
            "37     \t [0.02570944 0.86615546 0.78070906]. \t  1.7523196431795618 \t 2.9879392691557105\n",
            "38     \t [0.00370626 0.26897926 0.29244568]. \t  0.532237546025301 \t 2.9879392691557105\n",
            "39     \t [0.56477047 0.94388927 0.02404304]. \t  0.0004901317378073801 \t 2.9879392691557105\n",
            "40     \t [0.87771962 0.01703303 0.99357617]. \t  0.11478469271626598 \t 2.9879392691557105\n",
            "41     \t [6.07772424e-03 1.67995072e-01 1.77099896e-08]. \t  0.07696705759893144 \t 2.9879392691557105\n",
            "42     \t [0.03274463 0.95717049 0.42241135]. \t  1.6278943810016173 \t 2.9879392691557105\n",
            "43     \t [0.86486976 0.56907113 1.        ]. \t  2.0389376700300312 \t 2.9879392691557105\n",
            "44     \t [0.65685674 0.01558919 0.10498057]. \t  0.3191734230902305 \t 2.9879392691557105\n",
            "45     \t [0.79347375 0.23425538 0.99259316]. \t  0.743886622712009 \t 2.9879392691557105\n",
            "46     \t [0.7465437  0.01715215 0.78256611]. \t  0.2926860258567445 \t 2.9879392691557105\n",
            "47     \t [0.12966607 0.75805239 0.99433987]. \t  1.5259990041136422 \t 2.9879392691557105\n",
            "48     \t [0.19582437 0.10134251 0.24315849]. \t  0.8964775237805079 \t 2.9879392691557105\n",
            "49     \t [1.         0.80455654 0.90954791]. \t  1.8949895347257413 \t 2.9879392691557105\n",
            "50     \t [0.20254255 0.17019535 0.83374762]. \t  1.0200908995687672 \t 2.9879392691557105\n",
            "51     \t [0.53833069 0.63253275 0.20891002]. \t  0.08487207653334662 \t 2.9879392691557105\n",
            "52     \t [0.71227967 0.58880257 0.89027136]. \t  \u001b[92m3.631826054202018\u001b[0m \t 3.631826054202018\n",
            "53     \t [0.6428899  0.60635408 0.87097137]. \t  \u001b[92m3.6888917940820773\u001b[0m \t 3.6888917940820773\n",
            "54     \t [0.64100178 0.56377553 0.96026903]. \t  2.785215145248432 \t 3.6888917940820773\n",
            "55     \t [0.96215922 0.56824606 0.84582984]. \t  3.662195394620562 \t 3.6888917940820773\n",
            "56     \t [0.97708053 0.52863296 0.8521332 ]. \t  3.66595794756135 \t 3.6888917940820773\n",
            "57     \t [0.96311908 0.9919408  0.05516091]. \t  0.00020640532600665827 \t 3.6888917940820773\n",
            "58     \t [0.8717017  0.00921603 0.23991026]. \t  0.4077854596129545 \t 3.6888917940820773\n",
            "59     \t [0.82504715 0.527561   0.86227197]. \t  \u001b[92m3.7239660672819688\u001b[0m \t 3.7239660672819688\n",
            "60     \t [0.11285453 0.96732101 0.58399234]. \t  2.7341824453420935 \t 3.7239660672819688\n",
            "61     \t [0.72331989 0.59059321 0.80312785]. \t  3.442107041266852 \t 3.7239660672819688\n",
            "62     \t [0.68016436 0.61242595 0.8649708 ]. \t  3.665759385255173 \t 3.7239660672819688\n",
            "63     \t [0.72160458 0.52892569 0.86315764]. \t  \u001b[92m3.7596745948788\u001b[0m \t 3.7596745948788\n",
            "64     \t [0.85375635 0.49745369 0.88060003]. \t  3.566673581952663 \t 3.7596745948788\n",
            "65     \t [0.80936673 0.53441392 0.89402149]. \t  3.5935046307953273 \t 3.7596745948788\n",
            "66     \t [0.75134088 0.53914173 0.88936487]. \t  3.65232269438816 \t 3.7596745948788\n",
            "67     \t [0.5558998  0.62882464 0.83772693]. \t  3.5925269532246933 \t 3.7596745948788\n",
            "68     \t [0.9431117  0.54033861 0.87971116]. \t  3.6403833124337925 \t 3.7596745948788\n",
            "69     \t [0.94186476 0.56623633 0.88201331]. \t  3.6316620318308956 \t 3.7596745948788\n",
            "70     \t [0.99308145 0.57565679 0.83873414]. \t  3.611284638590225 \t 3.7596745948788\n",
            "71     \t [0.92334626 0.53659879 0.88851098]. \t  3.594521856926326 \t 3.7596745948788\n",
            "72     \t [0.89527686 0.60240419 0.89135168]. \t  3.526294285569052 \t 3.7596745948788\n",
            "73     \t [0.6205178  0.46815578 0.85552278]. \t  3.5731676201237246 \t 3.7596745948788\n",
            "74     \t [0.97046484 0.55285825 0.81112387]. \t  3.4759938427992942 \t 3.7596745948788\n",
            "75     \t [0.91762603 0.50124187 0.85517118]. \t  3.6278916234837326 \t 3.7596745948788\n",
            "76     \t [0.86489694 0.5362604  0.77844099]. \t  3.1808731703896673 \t 3.7596745948788\n",
            "77     \t [0.75549216 0.56114479 0.83231062]. \t  3.707301791200532 \t 3.7596745948788\n",
            "78     \t [0.87955525 0.57845209 0.83717959]. \t  3.6512368936545903 \t 3.7596745948788\n",
            "79     \t [0.71753895 0.49921687 0.8597226 ]. \t  3.687282746634007 \t 3.7596745948788\n",
            "80     \t [0.66812179 0.59476719 0.90294194]. \t  3.5301277030992115 \t 3.7596745948788\n",
            "81     \t [0.56670814 0.60372073 0.85400501]. \t  3.735358150554029 \t 3.7596745948788\n",
            "82     \t [0.64376249 0.56879222 0.86740661]. \t  \u001b[92m3.785717688858303\u001b[0m \t 3.785717688858303\n",
            "83     \t [0.67004475 0.52844909 0.9129091 ]. \t  3.438667218681339 \t 3.785717688858303\n",
            "84     \t [0.64498084 0.49968926 0.80807826]. \t  3.5295335451598078 \t 3.785717688858303\n",
            "85     \t [0.30039219 0.64792294 0.84477494]. \t  3.5620866037255947 \t 3.785717688858303\n",
            "86     \t [0.51702805 0.58423275 0.88063021]. \t  3.7426404072212036 \t 3.785717688858303\n",
            "87     \t [0.59523926 0.57884717 0.86531655]. \t  \u001b[92m3.788276701906005\u001b[0m \t 3.788276701906005\n",
            "88     \t [0.76471785 0.59733086 0.82755844]. \t  3.5971915147537916 \t 3.788276701906005\n",
            "89     \t [1.         0.51063706 0.86201389]. \t  3.614698528468624 \t 3.788276701906005\n",
            "90     \t [0.7372215  0.52906953 0.90639465]. \t  3.494062314264882 \t 3.788276701906005\n",
            "91     \t [0.03252796 0.50918515 0.8327253 ]. \t  3.718776648243307 \t 3.788276701906005\n",
            "92     \t [0.64129252 0.57869757 0.83317665]. \t  3.7263898464353007 \t 3.788276701906005\n",
            "93     \t [0.61354061 0.5430218  0.90778575]. \t  3.5293744617154736 \t 3.788276701906005\n",
            "94     \t [0.52897407 0.54925181 0.88068636]. \t  3.7659958974425827 \t 3.788276701906005\n",
            "95     \t [0.8499885  0.49942563 0.89155455]. \t  3.505154897399043 \t 3.788276701906005\n",
            "96     \t [0.54937703 0.60000052 0.86708325]. \t  3.743325362323267 \t 3.788276701906005\n",
            "97     \t [0.63767919 0.54165669 0.86454649]. \t  \u001b[92m3.7966748587892676\u001b[0m \t 3.7966748587892676\n",
            "98     \t [0.93319406 0.45946903 0.81203623]. \t  3.3013424352872613 \t 3.7966748587892676\n",
            "99     \t [0.72205969 0.52573576 0.86186979]. \t  3.755952239151492 \t 3.7966748587892676\n",
            "100    \t [0.64616009 0.58847105 0.82368867]. \t  3.6482891573542124 \t 3.7966748587892676\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9YT-CgKvuv4q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d352bf97-c204-401f-bd87-d68f02ed05db"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 19 \r\n",
        "\r\n",
        "np.random.seed(run_num_19)\r\n",
        "surrogate_winner_19 = dtStudentProcess(d_cov_func, nu = df)\r\n",
        "\r\n",
        "winner_19 = dGPGO_stp(surrogate_winner_19, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_19.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.55944558 0.76149199 0.24453155]. \t  0.09433080659504385 \t 2.524990008735946\n",
            "init   \t [0.77168947 0.35447208 0.7966974 ]. \t  2.524990008735946 \t 2.524990008735946\n",
            "init   \t [0.34600002 0.44387444 0.30330359]. \t  0.3914194506273641 \t 2.524990008735946\n",
            "init   \t [0.44211392 0.57320936 0.06566402]. \t  0.036928944981920925 \t 2.524990008735946\n",
            "init   \t [0.02408889 0.82062263 0.36513765]. \t  0.9772047942100802 \t 2.524990008735946\n",
            "1      \t [1. 1. 1.]. \t  0.31688362070415665 \t 2.524990008735946\n",
            "2      \t [0.70791378 0.08665984 0.96063659]. \t  0.307474938582594 \t 2.524990008735946\n",
            "3      \t [0.92432215 0.00224485 0.20852539]. \t  0.31324730242838555 \t 2.524990008735946\n",
            "4      \t [0.10387501 0.61669768 0.98356585]. \t  2.3139736100945267 \t 2.524990008735946\n",
            "5      \t [1.         0.47715465 0.6008699 ]. \t  0.7751199086027457 \t 2.524990008735946\n",
            "6      \t [0.47441475 0.79361848 1.        ]. \t  1.2631956022408146 \t 2.524990008735946\n",
            "7      \t [0. 0. 0.]. \t  0.06797411659013229 \t 2.524990008735946\n",
            "8      \t [0.99899004 0.54982638 0.99309416]. \t  2.1242468394510916 \t 2.524990008735946\n",
            "9      \t [0.0902038  0.09563088 0.95420816]. \t  0.3496523574073788 \t 2.524990008735946\n",
            "10     \t [0.1798799  0.96426485 0.08322449]. \t  0.0037026666845450117 \t 2.524990008735946\n",
            "11     \t [0.14901123 0.97432239 0.96978333]. \t  0.5202183544325574 \t 2.524990008735946\n",
            "12     \t [0.86890678 0.97256538 0.01290941]. \t  0.00011693650628807542 \t 2.524990008735946\n",
            "13     \t [0.44325407 0.         0.        ]. \t  0.10056477254430622 \t 2.524990008735946\n",
            "14     \t [0.0302724  0.36474832 0.69934002]. \t  1.7764667917357269 \t 2.524990008735946\n",
            "15     \t [0.         0.56544211 0.        ]. \t  0.010543400739731899 \t 2.524990008735946\n",
            "16     \t [0.83652111 0.14906396 0.01169926]. \t  0.07235033655995021 \t 2.524990008735946\n",
            "17     \t [0.         0.         0.33915498]. \t  0.49742442565535117 \t 2.524990008735946\n",
            "18     \t [1.         1.         0.57213349]. \t  0.26830912511829247 \t 2.524990008735946\n",
            "19     \t [0.97338538 0.07208306 0.86962537]. \t  0.42682325839938634 \t 2.524990008735946\n",
            "20     \t [0.23836574 0.70105811 0.71478604]. \t  \u001b[92m2.574348977553737\u001b[0m \t 2.574348977553737\n",
            "21     \t [0.47010942 1.         0.65544193]. \t  1.3981210082522395 \t 2.574348977553737\n",
            "22     \t [0.75861514 0.76097186 0.79889124]. \t  2.2482438966591287 \t 2.574348977553737\n",
            "23     \t [0.62175285 0.05370151 0.64502976]. \t  0.2304986921229368 \t 2.574348977553737\n",
            "24     \t [0.22048804 0.34158703 0.85281509]. \t  2.544877529029513 \t 2.574348977553737\n",
            "25     \t [0.04295749 0.99960944 0.58061125]. \t  2.513757704129864 \t 2.574348977553737\n",
            "26     \t [0.98744754 0.71990944 0.25229799]. \t  0.02350129308764298 \t 2.574348977553737\n",
            "27     \t [0.01381987 0.98972179 0.81693901]. \t  0.8543229768816982 \t 2.574348977553737\n",
            "28     \t [0.67984619 0.25895367 0.22257342]. \t  0.5770416699531011 \t 2.574348977553737\n",
            "29     \t [0.44647614 0.48159862 0.99299918]. \t  2.062024395459586 \t 2.574348977553737\n",
            "30     \t [0.         0.24914797 0.09453755]. \t  0.22812831017873078 \t 2.574348977553737\n",
            "31     \t [0.01620228 0.34106485 0.97177369]. \t  1.5725493390366632 \t 2.574348977553737\n",
            "32     \t [0.98986012 0.68494222 0.00835794]. \t  0.001698265150241373 \t 2.574348977553737\n",
            "33     \t [2.08314151e-01 4.68389599e-08 2.25070597e-01]. \t  0.7651639454503508 \t 2.574348977553737\n",
            "34     \t [1.92850793e-01 1.94381545e-01 6.66287209e-13]. \t  0.10062956681518152 \t 2.574348977553737\n",
            "35     \t [1.         0.27353365 0.98562051]. \t  0.9819765766219773 \t 2.574348977553737\n",
            "36     \t [1.         0.77705204 0.86468692]. \t  2.26117677047732 \t 2.574348977553737\n",
            "37     \t [0.501136   0.5304004  0.75391179]. \t  \u001b[92m3.0314650308187727\u001b[0m \t 3.0314650308187727\n",
            "38     \t [1.         1.         0.21878743]. \t  0.00811825885989651 \t 3.0314650308187727\n",
            "39     \t [1.62964831e-01 4.73876704e-08 6.62700940e-01]. \t  0.16495135256798066 \t 3.0314650308187727\n",
            "40     \t [0.02405399 0.86555359 0.00787775]. \t  0.0007154904118034696 \t 3.0314650308187727\n",
            "41     \t [0.99767452 0.26448272 0.08286627]. \t  0.08857038368700751 \t 3.0314650308187727\n",
            "42     \t [0.73043305 1.         0.2438358 ]. \t  0.04446300287503116 \t 3.0314650308187727\n",
            "43     \t [0.02589191 0.99574598 0.14845411]. \t  0.018095871420458362 \t 3.0314650308187727\n",
            "44     \t [0.85048046 0.5817326  1.        ]. \t  2.037639566943825 \t 3.0314650308187727\n",
            "45     \t [0.67532694 1.         1.        ]. \t  0.3283019333219134 \t 3.0314650308187727\n",
            "46     \t [0.43413946 0.33488148 0.72738601]. \t  1.887526463766913 \t 3.0314650308187727\n",
            "47     \t [0.09207321 0.65656586 0.80399048]. \t  \u001b[92m3.3337282366068828\u001b[0m \t 3.3337282366068828\n",
            "48     \t [0.18261119 0.63595611 0.85624401]. \t  \u001b[92m3.6347498860097005\u001b[0m \t 3.6347498860097005\n",
            "49     \t [0.4080678  0.00529694 0.9864664 ]. \t  0.1108751489933052 \t 3.6347498860097005\n",
            "50     \t [0.77970275 0.5368608  0.80199449]. \t  3.482264203963946 \t 3.6347498860097005\n",
            "51     \t [0.82804576 0.5729535  0.79860799]. \t  3.392106723073161 \t 3.6347498860097005\n",
            "52     \t [0.82251281 0.53677887 0.79292161]. \t  3.3704760924852444 \t 3.6347498860097005\n",
            "53     \t [0.17668749 0.48760178 0.84184563]. \t  \u001b[92m3.6916506482565152\u001b[0m \t 3.6916506482565152\n",
            "54     \t [0.36020224 0.5523089  0.79994731]. \t  3.60523782686228 \t 3.6916506482565152\n",
            "55     \t [0.33125429 0.53246644 0.8815315 ]. \t  \u001b[92m3.758587882028354\u001b[0m \t 3.758587882028354\n",
            "56     \t [0.89142335 0.57172806 0.84792071]. \t  3.6936660369956993 \t 3.758587882028354\n",
            "57     \t [0.14332728 0.47180483 0.87583763]. \t  3.543727332777782 \t 3.758587882028354\n",
            "58     \t [0.49881339 0.59610827 0.88432055]. \t  3.6981130447501274 \t 3.758587882028354\n",
            "59     \t [0.31415282 0.52555704 0.88290316]. \t  3.736257897221189 \t 3.758587882028354\n",
            "60     \t [0.37518555 0.49890336 0.84469778]. \t  3.749210403103338 \t 3.758587882028354\n",
            "61     \t [0.31674381 0.64629924 0.82252106]. \t  3.494794467000477 \t 3.758587882028354\n",
            "62     \t [0.24334848 0.99545733 0.82374142]. \t  0.8025258615797011 \t 3.758587882028354\n",
            "63     \t [0.34692819 0.54475494 0.87531735]. \t  \u001b[92m3.8067498302643545\u001b[0m \t 3.8067498302643545\n",
            "64     \t [0.26345614 0.45403693 0.83321725]. \t  3.503332877541431 \t 3.8067498302643545\n",
            "65     \t [0.80961728 0.57623428 0.88725616]. \t  3.642949614513468 \t 3.8067498302643545\n",
            "66     \t [0.16889282 0.53371454 0.83922726]. \t  \u001b[92m3.821543429737374\u001b[0m \t 3.821543429737374\n",
            "67     \t [0.29810788 0.60562419 0.8582124 ]. \t  3.7706871322924136 \t 3.821543429737374\n",
            "68     \t [0.26600018 0.48821383 0.84169133]. \t  3.7025629905159336 \t 3.821543429737374\n",
            "69     \t [0.2457234  0.59892131 0.81010012]. \t  3.631085954451675 \t 3.821543429737374\n",
            "70     \t [0.81234142 0.50320693 0.91620711]. \t  3.2928939393449728 \t 3.821543429737374\n",
            "71     \t [0.20431485 0.58351757 0.88645975]. \t  3.721369373892509 \t 3.821543429737374\n",
            "72     \t [0.80196084 0.56701248 0.82649649]. \t  3.6494041511912005 \t 3.821543429737374\n",
            "73     \t [0.36063581 0.54424985 0.88223465]. \t  3.7700355262267764 \t 3.821543429737374\n",
            "74     \t [0.21702301 0.61749763 0.84623491]. \t  3.720459160095694 \t 3.821543429737374\n",
            "75     \t [0.18527394 0.65549735 0.85118754]. \t  3.5219368293740776 \t 3.821543429737374\n",
            "76     \t [0.88330985 0.60418485 0.88224539]. \t  3.5750583947689125 \t 3.821543429737374\n",
            "77     \t [0.99326946 0.49052809 0.81243661]. \t  3.403718196721376 \t 3.821543429737374\n",
            "78     \t [0.7008356  0.51246841 0.86856123]. \t  3.7158695519962706 \t 3.821543429737374\n",
            "79     \t [0.85528196 0.50431313 0.89483737]. \t  3.495700388030346 \t 3.821543429737374\n",
            "80     \t [0.         0.         0.81492402]. \t  0.24519395323422855 \t 3.821543429737374\n",
            "81     \t [0.34560092 0.64553284 0.82517679]. \t  3.5065818234127466 \t 3.821543429737374\n",
            "82     \t [0.32382834 0.52370133 0.87469632]. \t  3.776145753696981 \t 3.821543429737374\n",
            "83     \t [0.31143515 0.57466099 0.85726005]. \t  \u001b[92m3.8462367467611593\u001b[0m \t 3.8462367467611593\n",
            "84     \t [0.22552914 0.49016516 0.88695273]. \t  3.581809532434937 \t 3.8462367467611593\n",
            "85     \t [0.94784307 0.53638287 0.85081535]. \t  3.6867046497586404 \t 3.8462367467611593\n",
            "86     \t [0.71990979 0.55496913 0.88775217]. \t  3.6824571440872957 \t 3.8462367467611593\n",
            "87     \t [0.50990504 0.50798049 0.84600198]. \t  3.7654845659765326 \t 3.8462367467611593\n",
            "88     \t [0.30556145 0.53717841 0.84178561]. \t  3.8400452777459906 \t 3.8462367467611593\n",
            "89     \t [0.41380225 0.5742827  0.87717747]. \t  3.7887579944956236 \t 3.8462367467611593\n",
            "90     \t [0.65301349 0.55285517 0.80905556]. \t  3.594863726551615 \t 3.8462367467611593\n",
            "91     \t [0.86922586 0.43972522 0.85382503]. \t  3.329190797485915 \t 3.8462367467611593\n",
            "92     \t [0.81014129 0.60786318 0.86603949]. \t  3.637166809322262 \t 3.8462367467611593\n",
            "93     \t [0.21988096 0.62856883 0.82143445]. \t  3.5894336221577006 \t 3.8462367467611593\n",
            "94     \t [0.84202391 0.43925574 0.86159375]. \t  3.3225102790163 \t 3.8462367467611593\n",
            "95     \t [0.33872206 0.57980693 0.75188529]. \t  3.0738371386936403 \t 3.8462367467611593\n",
            "96     \t [0.21353801 0.48592161 0.89846447]. \t  3.467051440081773 \t 3.8462367467611593\n",
            "97     \t [0.49480555 0.53785574 0.89628012]. \t  3.6510908353348053 \t 3.8462367467611593\n",
            "98     \t [0.32692265 0.63074226 0.84188908]. \t  3.6484987898276646 \t 3.8462367467611593\n",
            "99     \t [0.9023587  0.54834923 0.91047516]. \t  3.424869542794152 \t 3.8462367467611593\n",
            "100    \t [0.96760946 0.02885497 0.01304281]. \t  0.04539037434328082 \t 3.8462367467611593\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHz_Jg2_uv7E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d97e433e-0e6e-4ba0-930d-2a91e46c1a4d"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 20 \r\n",
        "\r\n",
        "np.random.seed(run_num_20)\r\n",
        "surrogate_winner_20 = dtStudentProcess(d_cov_func, nu = df)\r\n",
        "\r\n",
        "winner_20 = dGPGO_stp(surrogate_winner_20, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_20.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.57051729 0.56452876 0.48844183]. \t  0.675391399411646 \t 0.675391399411646\n",
            "init   \t [0.33647775 0.37586818 0.53203587]. \t  0.5331349538596052 \t 0.675391399411646\n",
            "init   \t [0.06810629 0.58452906 0.23789776]. \t  0.14747335095307315 \t 0.675391399411646\n",
            "init   \t [0.16075658 0.15211915 0.12706922]. \t  0.48089725804912437 \t 0.675391399411646\n",
            "init   \t [0.32744117 0.69415387 0.35896647]. \t  0.6289408047543804 \t 0.675391399411646\n",
            "1      \t [1. 1. 1.]. \t  0.31688362070415665 \t 0.675391399411646\n",
            "2      \t [0.91932094 0.0125958  0.17618825]. \t  0.2816999755052942 \t 0.675391399411646\n",
            "3      \t [1.         0.90207465 0.13294524]. \t  0.0016950024553579913 \t 0.675391399411646\n",
            "4      \t [0.18395127 1.         1.        ]. \t  0.33382158510028465 \t 0.675391399411646\n",
            "5      \t [0.83573293 0.19882234 1.        ]. \t  0.5441525145872365 \t 0.675391399411646\n",
            "6      \t [0.00657248 0.04994934 0.95603609]. \t  0.22346365116669764 \t 0.675391399411646\n",
            "7      \t [0.0140605  0.99189634 0.09256457]. \t  0.0043141598608878945 \t 0.675391399411646\n",
            "8      \t [0.33624944 0.53714166 0.02942954]. \t  0.03145734984516174 \t 0.675391399411646\n",
            "9      \t [0.99334667 0.13255172 0.66027862]. \t  0.4327537094869812 \t 0.675391399411646\n",
            "10     \t [0.11101871 0.54032108 1.        ]. \t  \u001b[92m2.0579860620065245\u001b[0m \t 2.0579860620065245\n",
            "11     \t [1.         0.54649181 1.        ]. \t  2.0003654928539554 \t 2.0579860620065245\n",
            "12     \t [0.68643943 0.96115223 0.00220553]. \t  0.0001798512637650066 \t 2.0579860620065245\n",
            "13     \t [0.0027654  0.73359186 0.85439411]. \t  \u001b[92m2.869345701145049\u001b[0m \t 2.869345701145049\n",
            "14     \t [0.93533982 0.32156202 0.00920076]. \t  0.03406779030379178 \t 2.869345701145049\n",
            "15     \t [1.         1.         0.58993496]. \t  0.2735855587227074 \t 2.869345701145049\n",
            "16     \t [0.03402977 0.98853375 0.5817744 ]. \t  2.572039231210569 \t 2.869345701145049\n",
            "17     \t [0.47191704 0.79344098 0.89911795]. \t  2.165359000842128 \t 2.869345701145049\n",
            "18     \t [ 5.25856441e-01 -2.77555756e-17  0.00000000e+00]. \t  0.09496244608241783 \t 2.869345701145049\n",
            "19     \t [0.05039964 0.         0.        ]. \t  0.07541913566712308 \t 2.869345701145049\n",
            "20     \t [0.         0.         0.60597164]. \t  0.11324687913792308 \t 2.869345701145049\n",
            "21     \t [0.39130425 0.         0.70396687]. \t  0.20608253113062203 \t 2.869345701145049\n",
            "22     \t [0.50299034 1.         0.71325474]. \t  0.9945789296511351 \t 2.869345701145049\n",
            "23     \t [1.         0.70501959 0.66619337]. \t  1.1032882687088823 \t 2.869345701145049\n",
            "24     \t [0.99805935 0.11252197 0.98164866]. \t  0.31804997638114696 \t 2.869345701145049\n",
            "25     \t [0.80058562 0.82708484 1.        ]. \t  1.0558998953280903 \t 2.869345701145049\n",
            "26     \t [1.         0.44672532 0.27753619]. \t  0.10677225512962407 \t 2.869345701145049\n",
            "27     \t [0.         0.52373974 0.        ]. \t  0.014987832247749891 \t 2.869345701145049\n",
            "28     \t [4.77992661e-01 1.29674595e-18 3.55039182e-01]. \t  0.6690628699572132 \t 2.869345701145049\n",
            "29     \t [0.         0.24698544 0.80867973]. \t  1.6111805329028273 \t 2.869345701145049\n",
            "30     \t [0.61397131 0.00122625 0.97200229]. \t  0.12030954693295912 \t 2.869345701145049\n",
            "31     \t [0.29197868 1.         0.21705167]. \t  0.07594718718186709 \t 2.869345701145049\n",
            "32     \t [0.29885182 0.19346893 0.98082492]. \t  0.6325810363650595 \t 2.869345701145049\n",
            "33     \t [0.15358665 0.77724284 0.76637752]. \t  2.3941433030300727 \t 2.869345701145049\n",
            "34     \t [0.03070277 0.97766133 0.83798594]. \t  0.8650295960225168 \t 2.869345701145049\n",
            "35     \t [0.62735386 0.25377617 0.08543575]. \t  0.25169062325423086 \t 2.869345701145049\n",
            "36     \t [0.77720952 0.55767728 0.        ]. \t  0.010226166880070641 \t 2.869345701145049\n",
            "37     \t [0.72382182 0.94035117 0.29470754]. \t  0.12232069562148853 \t 2.869345701145049\n",
            "38     \t [0.59208095 1.         1.        ]. \t  0.3304303085631051 \t 2.869345701145049\n",
            "39     \t [0.01605992 0.7412219  0.65390472]. \t  2.557621954562743 \t 2.869345701145049\n",
            "40     \t [1.41571093e-08 1.90745098e-01 9.93808690e-09]. \t  0.07382284498670126 \t 2.869345701145049\n",
            "41     \t [9.35385537e-09 1.14208751e-08 3.39261052e-01]. \t  0.497199284765606 \t 2.869345701145049\n",
            "42     \t [0.56362262 0.49762483 1.        ]. \t  1.9837020864881632 \t 2.869345701145049\n",
            "43     \t [0.954865   0.46832658 0.78976726]. \t  \u001b[92m3.1469856401841847\u001b[0m \t 3.1469856401841847\n",
            "44     \t [0.80291915 0.51939269 0.88860366]. \t  \u001b[92m3.604912010359722\u001b[0m \t 3.604912010359722\n",
            "45     \t [0.00306021 0.87832358 0.28710983]. \t  0.3405218589775217 \t 3.604912010359722\n",
            "46     \t [0.85459918 0.44793103 0.89199808]. \t  3.2283604440720195 \t 3.604912010359722\n",
            "47     \t [0.96651503 0.51409959 0.85969293]. \t  \u001b[92m3.64163216933071\u001b[0m \t 3.64163216933071\n",
            "48     \t [0.99233226 0.45367329 0.79391048]. \t  3.1089273019412076 \t 3.64163216933071\n",
            "49     \t [0.06488216 0.58938257 0.7337644 ]. \t  2.8971518973871637 \t 3.64163216933071\n",
            "50     \t [0.99459633 0.4612212  0.83072778]. \t  3.3804388897282562 \t 3.64163216933071\n",
            "51     \t [0.01998241 0.56530053 0.85130153]. \t  \u001b[92m3.8139458455457116\u001b[0m \t 3.8139458455457116\n",
            "52     \t [0.01891692 0.57377491 0.82736302]. \t  3.7461455599363624 \t 3.8139458455457116\n",
            "53     \t [0.51310314 0.43547505 0.85482873]. \t  3.3800463124933473 \t 3.8139458455457116\n",
            "54     \t [0.96210366 0.97400315 0.02389618]. \t  0.00010318941332441756 \t 3.8139458455457116\n",
            "55     \t [0.99217256 0.09838577 0.02702041]. \t  0.054973930313249486 \t 3.8139458455457116\n",
            "56     \t [0.03362637 0.53816055 0.8645663 ]. \t  3.795788458419731 \t 3.8139458455457116\n",
            "57     \t [0.72299423 0.53572496 0.87699229]. \t  3.7268508669849796 \t 3.8139458455457116\n",
            "58     \t [0.74181229 0.56463367 0.87632501]. \t  3.732746580080355 \t 3.8139458455457116\n",
            "59     \t [0.33565558 0.57680593 0.8481784 ]. \t  \u001b[92m3.8401137258740046\u001b[0m \t 3.8401137258740046\n",
            "60     \t [0.02384854 0.55888663 0.86491059]. \t  3.8036298266311954 \t 3.8401137258740046\n",
            "61     \t [0.61846635 0.56681723 0.90696944]. \t  3.543797858752272 \t 3.8401137258740046\n",
            "62     \t [0.0230226  0.55759794 0.85307311]. \t  3.8182839366351597 \t 3.8401137258740046\n",
            "63     \t [0.27621591 0.64601164 0.83408706]. \t  3.5491310954671973 \t 3.8401137258740046\n",
            "64     \t [0.37752254 0.59637732 0.82143553]. \t  3.694338205626849 \t 3.8401137258740046\n",
            "65     \t [0.34129467 0.65034085 0.81731469]. \t  3.4388971867111247 \t 3.8401137258740046\n",
            "66     \t [0.46478352 0.58701088 0.87815202]. \t  3.7568647135909967 \t 3.8401137258740046\n",
            "67     \t [0.00253456 0.55658122 0.83125083]. \t  3.7695940431892083 \t 3.8401137258740046\n",
            "68     \t [0.51513396 0.52631571 0.86819144]. \t  3.791619228715035 \t 3.8401137258740046\n",
            "69     \t [0.63821709 0.55635012 0.86257742]. \t  3.804088258153546 \t 3.8401137258740046\n",
            "70     \t [0.4676842  0.62491978 0.80722015]. \t  3.4617420262299925 \t 3.8401137258740046\n",
            "71     \t [0.88547272 0.590213   0.88291759]. \t  3.6115287063533854 \t 3.8401137258740046\n",
            "72     \t [0.01734796 0.58852205 0.85598983]. \t  3.7784774542189874 \t 3.8401137258740046\n",
            "73     \t [0.06558108 0.60683762 0.81033467]. \t  3.588492304919198 \t 3.8401137258740046\n",
            "74     \t [0.08042055 0.5185713  0.83791382]. \t  3.771973244357548 \t 3.8401137258740046\n",
            "75     \t [0.45373321 0.58623462 0.86229639]. \t  3.8072079760902846 \t 3.8401137258740046\n",
            "76     \t [0.81772461 0.49901898 0.86488292]. \t  3.6454770646355463 \t 3.8401137258740046\n",
            "77     \t [0.05520639 0.47095986 0.84563662]. \t  3.588226924871891 \t 3.8401137258740046\n",
            "78     \t [0.69092781 0.62125038 0.80550243]. \t  3.3616619121193767 \t 3.8401137258740046\n",
            "79     \t [0.39725423 0.5261547  0.77963125]. \t  3.3783721916373786 \t 3.8401137258740046\n",
            "80     \t [0.24144565 0.51696176 0.83242375]. \t  3.776690395212916 \t 3.8401137258740046\n",
            "81     \t [0.48804984 0.57829786 0.84127233]. \t  3.8017074550582746 \t 3.8401137258740046\n",
            "82     \t [0.26108863 0.58082551 0.81772159]. \t  3.7229212451773153 \t 3.8401137258740046\n",
            "83     \t [0.53236811 0.59647274 0.87525663]. \t  3.733906031019176 \t 3.8401137258740046\n",
            "84     \t [0.42804805 0.57731867 0.85816354]. \t  3.832333311735268 \t 3.8401137258740046\n",
            "85     \t [0.05664048 0.55932333 0.84186255]. \t  3.8174045836538544 \t 3.8401137258740046\n",
            "86     \t [0.7107255  0.58804065 0.8692481 ]. \t  3.7280408667933864 \t 3.8401137258740046\n",
            "87     \t [0.06503026 0.47875643 0.8355565 ]. \t  3.6164129654774864 \t 3.8401137258740046\n",
            "88     \t [0.87890796 0.43847679 0.82573615]. \t  3.274541209119217 \t 3.8401137258740046\n",
            "89     \t [0.28248767 0.4988328  0.79023621]. \t  3.4455982212587832 \t 3.8401137258740046\n",
            "90     \t [0.35168548 0.47925509 0.82501363]. \t  3.6130299278002527 \t 3.8401137258740046\n",
            "91     \t [0.21729022 0.55748228 0.84399702]. \t  \u001b[92m3.8508123402671313\u001b[0m \t 3.8508123402671313\n",
            "92     \t [0.51925059 0.51297145 0.85710698]. \t  3.7803182730057396 \t 3.8508123402671313\n",
            "93     \t [0.36468334 0.56833044 0.90326664]. \t  3.6087152880381055 \t 3.8508123402671313\n",
            "94     \t [0.62680443 0.51748389 0.93239055]. \t  3.1687961217894176 \t 3.8508123402671313\n",
            "95     \t [0.20372773 0.51560341 0.80428992]. \t  3.6105815437954325 \t 3.8508123402671313\n",
            "96     \t [0.99503464 0.64439556 0.03865552]. \t  0.004039348555144956 \t 3.8508123402671313\n",
            "97     \t [0.51885461 0.58299471 0.83483196]. \t  3.762424693670776 \t 3.8508123402671313\n",
            "98     \t [0.28094583 0.56704615 0.85348783]. \t  \u001b[92m3.8562383580084028\u001b[0m \t 3.8562383580084028\n",
            "99     \t [0.46091812 0.52450903 0.86811454]. \t  3.7955667357398264 \t 3.8562383580084028\n",
            "100    \t [0.37655251 0.57883059 0.80215131]. \t  3.5958031791352534 \t 3.8562383580084028\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUnhsKpCuv9o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94c749f4-c136-4566-bc6c-5483aa7ad517"
      },
      "source": [
        "end_win = time.time()\r\n",
        "end_win\r\n",
        "\r\n",
        "time_win = end_win - start_win\r\n",
        "time_win"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "23300.253478765488"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJG0SLpwuwAL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "166a7cb2-043a-4e4f-ebfb-6a84318a8f51"
      },
      "source": [
        "### Training regret minimization: run number = 1\r\n",
        "\r\n",
        "loser_output_1 = np.append(np.max(loser_1.GP.y[0:n_init]),loser_1.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_1 = np.append(np.max(winner_1.GP.y[0:n_init]),winner_1.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_1 = np.log(y_global_orig - loser_output_1)\r\n",
        "regret_winner_1 = np.log(y_global_orig - winner_output_1)\r\n",
        "\r\n",
        "train_regret_loser_1 = min_max_array(regret_loser_1)\r\n",
        "train_regret_winner_1 = min_max_array(regret_winner_1)\r\n",
        "\r\n",
        "min_train_regret_loser_1 = min(train_regret_loser_1)\r\n",
        "min_train_regret_winner_1 = min(train_regret_winner_1)\r\n",
        "\r\n",
        "min_train_regret_loser_1, min_train_regret_winner_1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-3.282774727971788, -5.152839084157031)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lA9eZf0uwCx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0518a4a8-3e9a-4618-9ce0-5cb4c99c939d"
      },
      "source": [
        "### Training regret minimization: run number = 2\r\n",
        "\r\n",
        "loser_output_2 = np.append(np.max(loser_2.GP.y[0:n_init]),loser_2.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_2 = np.append(np.max(winner_2.GP.y[0:n_init]),winner_2.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_2 = np.log(y_global_orig - loser_output_2)\r\n",
        "regret_winner_2 = np.log(y_global_orig - winner_output_2)\r\n",
        "\r\n",
        "train_regret_loser_2 = min_max_array(regret_loser_2)\r\n",
        "train_regret_winner_2 = min_max_array(regret_winner_2)\r\n",
        "\r\n",
        "min_train_regret_loser_2 = min(train_regret_loser_2)\r\n",
        "min_train_regret_winner_2 = min(train_regret_winner_2)\r\n",
        "\r\n",
        "min_train_regret_loser_2, min_train_regret_winner_2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-1.8749236392611095, -4.856329994320982)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glrTGcpAuwFa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4d001f2-7d03-4b68-e981-c4e33402e196"
      },
      "source": [
        "### Training regret minimization: run number = 3\r\n",
        "\r\n",
        "loser_output_3 = np.append(np.max(loser_3.GP.y[0:n_init]),loser_3.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_3 = np.append(np.max(winner_3.GP.y[0:n_init]),winner_3.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_3 = np.log(y_global_orig - loser_output_3)\r\n",
        "regret_winner_3 = np.log(y_global_orig - winner_output_3)\r\n",
        "\r\n",
        "train_regret_loser_3 = min_max_array(regret_loser_3)\r\n",
        "train_regret_winner_3 = min_max_array(regret_winner_3)\r\n",
        "\r\n",
        "min_train_regret_loser_3 = min(train_regret_loser_3)\r\n",
        "min_train_regret_winner_3 = min(train_regret_winner_3)\r\n",
        "\r\n",
        "min_train_regret_loser_3, min_train_regret_winner_3"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-2.6160962144747533, -5.139611548054701)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LaRwfbxeuwIR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40dade44-8cdd-4b7f-fae8-62190c79cf68"
      },
      "source": [
        "### Training regret minimization: run number = 4\r\n",
        "\r\n",
        "loser_output_4 = np.append(np.max(loser_4.GP.y[0:n_init]),loser_4.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_4 = np.append(np.max(winner_4.GP.y[0:n_init]),winner_4.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_4 = np.log(y_global_orig - loser_output_4)\r\n",
        "regret_winner_4 = np.log(y_global_orig - winner_output_4)\r\n",
        "\r\n",
        "train_regret_loser_4 = min_max_array(regret_loser_4)\r\n",
        "train_regret_winner_4 = min_max_array(regret_winner_4)\r\n",
        "\r\n",
        "min_train_regret_loser_4 = min(train_regret_loser_4)\r\n",
        "min_train_regret_winner_4 = min(train_regret_winner_4)\r\n",
        "\r\n",
        "min_train_regret_loser_4, min_train_regret_winner_4"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-2.854269340274048, -4.528867319093919)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Nb5NkfyuwKp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7b14d53-e7a2-46ca-e324-04242c0537a7"
      },
      "source": [
        "### Training regret minimization: run number = 5\r\n",
        "\r\n",
        "loser_output_5 = np.append(np.max(loser_5.GP.y[0:n_init]),loser_5.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_5 = np.append(np.max(winner_5.GP.y[0:n_init]),winner_5.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_5 = np.log(y_global_orig - loser_output_5)\r\n",
        "regret_winner_5 = np.log(y_global_orig - winner_output_5)\r\n",
        "\r\n",
        "train_regret_loser_5 = min_max_array(regret_loser_5)\r\n",
        "train_regret_winner_5 = min_max_array(regret_winner_5)\r\n",
        "\r\n",
        "min_train_regret_loser_5 = min(train_regret_loser_5)\r\n",
        "min_train_regret_winner_5 = min(train_regret_winner_5)\r\n",
        "\r\n",
        "min_train_regret_loser_5, min_train_regret_winner_5"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-2.9864402258065152, -5.114967072257306)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0Q-WfXbuwNg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78a72a60-98a7-4d34-ddc5-0603c1c580e6"
      },
      "source": [
        "### Training regret minimization: run number = 6\r\n",
        "\r\n",
        "loser_output_6 = np.append(np.max(loser_6.GP.y[0:n_init]),loser_6.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_6 = np.append(np.max(winner_6.GP.y[0:n_init]),winner_6.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_6 = np.log(y_global_orig - loser_output_6)\r\n",
        "regret_winner_6 = np.log(y_global_orig - winner_output_6)\r\n",
        "\r\n",
        "train_regret_loser_6 = min_max_array(regret_loser_6)\r\n",
        "train_regret_winner_6 = min_max_array(regret_winner_6)\r\n",
        "\r\n",
        "min_train_regret_loser_6 = min(train_regret_loser_6)\r\n",
        "min_train_regret_winner_6 = min(train_regret_winner_6)\r\n",
        "\r\n",
        "min_train_regret_loser_6, min_train_regret_winner_6"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-2.652528982265508, -4.450234488562107)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqqS7VLcuwPy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a11bbb7c-5c10-4836-e44a-9fcdec4ae9eb"
      },
      "source": [
        "### Training regret minimization: run number = 7\r\n",
        "\r\n",
        "loser_output_7 = np.append(np.max(loser_7.GP.y[0:n_init]),loser_7.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_7 = np.append(np.max(winner_7.GP.y[0:n_init]),winner_7.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_7 = np.log(y_global_orig - loser_output_7)\r\n",
        "regret_winner_7 = np.log(y_global_orig - winner_output_7)\r\n",
        "\r\n",
        "train_regret_loser_7 = min_max_array(regret_loser_7)\r\n",
        "train_regret_winner_7 = min_max_array(regret_winner_7)\r\n",
        "\r\n",
        "min_train_regret_loser_7 = min(train_regret_loser_7)\r\n",
        "min_train_regret_winner_7 = min(train_regret_winner_7)\r\n",
        "\r\n",
        "min_train_regret_loser_7, min_train_regret_winner_7"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-2.427527665591056, -4.241836398052084)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOi5iX8guwSS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f58ad7e-4155-4600-8ef1-150a4122ebc7"
      },
      "source": [
        "### Training regret minimization: run number = 8\r\n",
        "\r\n",
        "loser_output_8 = np.append(np.max(loser_8.GP.y[0:n_init]),loser_8.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_8 = np.append(np.max(winner_8.GP.y[0:n_init]),winner_8.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_8 = np.log(y_global_orig - loser_output_8)\r\n",
        "regret_winner_8 = np.log(y_global_orig - winner_output_8)\r\n",
        "\r\n",
        "train_regret_loser_8 = min_max_array(regret_loser_8)\r\n",
        "train_regret_winner_8 = min_max_array(regret_winner_8)\r\n",
        "\r\n",
        "min_train_regret_loser_8 = min(train_regret_loser_8)\r\n",
        "min_train_regret_winner_8 = min(train_regret_winner_8)\r\n",
        "\r\n",
        "min_train_regret_loser_8, min_train_regret_winner_8"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-2.405697629363659, -4.437340845619253)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFVApeazuwU5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25de2944-545e-4600-d522-92e72802bf16"
      },
      "source": [
        "### Training regret minimization: run number = 9\r\n",
        "\r\n",
        "loser_output_9 = np.append(np.max(loser_9.GP.y[0:n_init]),loser_9.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_9 = np.append(np.max(winner_9.GP.y[0:n_init]),winner_9.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_9 = np.log(y_global_orig - loser_output_9)\r\n",
        "regret_winner_9 = np.log(y_global_orig - winner_output_9)\r\n",
        "\r\n",
        "train_regret_loser_9 = min_max_array(regret_loser_9)\r\n",
        "train_regret_winner_9 = min_max_array(regret_winner_9)\r\n",
        "\r\n",
        "min_train_regret_loser_9 = min(train_regret_loser_9)\r\n",
        "min_train_regret_winner_9 = min(train_regret_winner_9)\r\n",
        "\r\n",
        "min_train_regret_loser_9, min_train_regret_winner_9"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-0.9276396500365369, -4.257492132985464)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g92jk9WJuwXb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8abcd236-1dc2-487c-857a-6049597210d8"
      },
      "source": [
        "### Training regret minimization: run number = 10\r\n",
        "\r\n",
        "loser_output_10 = np.append(np.max(loser_10.GP.y[0:n_init]),loser_10.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_10 = np.append(np.max(winner_10.GP.y[0:n_init]),winner_10.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_10 = np.log(y_global_orig - loser_output_10)\r\n",
        "regret_winner_10 = np.log(y_global_orig - winner_output_10)\r\n",
        "\r\n",
        "train_regret_loser_10 = min_max_array(regret_loser_10)\r\n",
        "train_regret_winner_10 = min_max_array(regret_winner_10)\r\n",
        "\r\n",
        "min_train_regret_loser_10 = min(train_regret_loser_10)\r\n",
        "min_train_regret_winner_10 = min(train_regret_winner_10)\r\n",
        "\r\n",
        "min_train_regret_loser_10, min_train_regret_winner_10"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-3.301444590661275, -4.737291279771231)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmcF1x-NuwZz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c3d403c-0d69-44e3-fa64-27554349ccae"
      },
      "source": [
        "### Training regret minimization: run number = 11\r\n",
        "\r\n",
        "loser_output_11 = np.append(np.max(loser_11.GP.y[0:n_init]),loser_11.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_11 = np.append(np.max(winner_11.GP.y[0:n_init]),winner_11.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_11 = np.log(y_global_orig - loser_output_11)\r\n",
        "regret_winner_11 = np.log(y_global_orig - winner_output_11)\r\n",
        "\r\n",
        "train_regret_loser_11 = min_max_array(regret_loser_11)\r\n",
        "train_regret_winner_11 = min_max_array(regret_winner_11)\r\n",
        "\r\n",
        "min_train_regret_loser_11 = min(train_regret_loser_11)\r\n",
        "min_train_regret_winner_11 = min(train_regret_winner_11)\r\n",
        "\r\n",
        "min_train_regret_loser_11, min_train_regret_winner_11"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-4.451291654399481, -4.451291654399481)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8axVhb6uwcc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47862900-9a8d-4104-d973-72a64df92d3a"
      },
      "source": [
        "### Training regret minimization: run number = 12\r\n",
        "\r\n",
        "loser_output_12 = np.append(np.max(loser_12.GP.y[0:n_init]),loser_12.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_12 = np.append(np.max(winner_12.GP.y[0:n_init]),winner_12.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_12 = np.log(y_global_orig - loser_output_12)\r\n",
        "regret_winner_12 = np.log(y_global_orig - winner_output_12)\r\n",
        "\r\n",
        "train_regret_loser_12 = min_max_array(regret_loser_12)\r\n",
        "train_regret_winner_12 = min_max_array(regret_winner_12)\r\n",
        "\r\n",
        "min_train_regret_loser_12 = min(train_regret_loser_12)\r\n",
        "min_train_regret_winner_12 = min(train_regret_winner_12)\r\n",
        "\r\n",
        "min_train_regret_loser_12, min_train_regret_winner_12"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-3.1007531149503116, -3.9875121755263097)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzlrL8XFuwfB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1e3302f-6ebb-4fdf-df27-034a690bb17d"
      },
      "source": [
        "### Training regret minimization: run number = 13\r\n",
        "\r\n",
        "loser_output_13 = np.append(np.max(loser_13.GP.y[0:n_init]),loser_13.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_13 = np.append(np.max(winner_13.GP.y[0:n_init]),winner_13.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_13 = np.log(y_global_orig - loser_output_13)\r\n",
        "regret_winner_13 = np.log(y_global_orig - winner_output_13)\r\n",
        "\r\n",
        "train_regret_loser_13 = min_max_array(regret_loser_13)\r\n",
        "train_regret_winner_13 = min_max_array(regret_winner_13)\r\n",
        "\r\n",
        "min_train_regret_loser_13 = min(train_regret_loser_13)\r\n",
        "min_train_regret_winner_13 = min(train_regret_winner_13)\r\n",
        "\r\n",
        "min_train_regret_loser_13, min_train_regret_winner_13"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-1.9578012996071412, -4.301913853819329)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3uZlLJ1quwh6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17924e20-7704-4f45-9e4c-bf9881d41eef"
      },
      "source": [
        "### Training regret minimization: run number = 14\r\n",
        "\r\n",
        "loser_output_14 = np.append(np.max(loser_14.GP.y[0:n_init]),loser_14.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_14 = np.append(np.max(winner_14.GP.y[0:n_init]),winner_14.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_14 = np.log(y_global_orig - loser_output_14)\r\n",
        "regret_winner_14 = np.log(y_global_orig - winner_output_14)\r\n",
        "\r\n",
        "train_regret_loser_14 = min_max_array(regret_loser_14)\r\n",
        "train_regret_winner_14 = min_max_array(regret_winner_14)\r\n",
        "\r\n",
        "min_train_regret_loser_14 = min(train_regret_loser_14)\r\n",
        "min_train_regret_winner_14 = min(train_regret_winner_14)\r\n",
        "\r\n",
        "min_train_regret_loser_14, min_train_regret_winner_14"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-1.4423529446398398, -4.1099834541932925)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVBcHRiMuwjx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a42c27e8-5865-4be5-a0e5-720719c59e95"
      },
      "source": [
        "### Training regret minimization: run number = 15\r\n",
        "\r\n",
        "loser_output_15 = np.append(np.max(loser_15.GP.y[0:n_init]),loser_15.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_15 = np.append(np.max(winner_15.GP.y[0:n_init]),winner_15.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_15 = np.log(y_global_orig - loser_output_15)\r\n",
        "regret_winner_15 = np.log(y_global_orig - winner_output_15)\r\n",
        "\r\n",
        "train_regret_loser_15 = min_max_array(regret_loser_15)\r\n",
        "train_regret_winner_15 = min_max_array(regret_winner_15)\r\n",
        "\r\n",
        "min_train_regret_loser_15 = min(train_regret_loser_15)\r\n",
        "min_train_regret_winner_15 = min(train_regret_winner_15)\r\n",
        "\r\n",
        "min_train_regret_loser_15, min_train_regret_winner_15"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-2.431852117724113, -4.780605493501167)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8nZ_DrKuwnA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "054f859a-f6cc-4736-9498-ed53858d8df4"
      },
      "source": [
        "### Training regret minimization: run number = 16\r\n",
        "\r\n",
        "loser_output_16 = np.append(np.max(loser_16.GP.y[0:n_init]),loser_16.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_16 = np.append(np.max(winner_16.GP.y[0:n_init]),winner_16.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_16 = np.log(y_global_orig - loser_output_16)\r\n",
        "regret_winner_16 = np.log(y_global_orig - winner_output_16)\r\n",
        "\r\n",
        "train_regret_loser_16 = min_max_array(regret_loser_16)\r\n",
        "train_regret_winner_16 = min_max_array(regret_winner_16)\r\n",
        "\r\n",
        "min_train_regret_loser_16 = min(train_regret_loser_16)\r\n",
        "min_train_regret_winner_16 = min(train_regret_winner_16)\r\n",
        "\r\n",
        "min_train_regret_loser_16, min_train_regret_winner_16"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-3.2310411317387597, -4.8935623179846015)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eg6qzzQFuwpU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6dc9aa0-3393-4fdf-bd81-7f1b58d8a302"
      },
      "source": [
        "### Training regret minimization: run number = 17\r\n",
        "\r\n",
        "loser_output_17 = np.append(np.max(loser_17.GP.y[0:n_init]),loser_17.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_17 = np.append(np.max(winner_17.GP.y[0:n_init]),winner_17.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_17 = np.log(y_global_orig - loser_output_17)\r\n",
        "regret_winner_17 = np.log(y_global_orig - winner_output_17)\r\n",
        "\r\n",
        "train_regret_loser_17 = min_max_array(regret_loser_17)\r\n",
        "train_regret_winner_17 = min_max_array(regret_winner_17)\r\n",
        "\r\n",
        "min_train_regret_loser_17 = min(train_regret_loser_17)\r\n",
        "min_train_regret_winner_17 = min(train_regret_winner_17)\r\n",
        "\r\n",
        "min_train_regret_loser_17, min_train_regret_winner_17"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-2.02432998517116, -4.129580960864817)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrRtDLMFuwsB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9220dd09-bb86-4661-9210-0c096f623bbb"
      },
      "source": [
        "### Training regret minimization: run number = 18\r\n",
        "\r\n",
        "loser_output_18 = np.append(np.max(loser_18.GP.y[0:n_init]),loser_18.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_18 = np.append(np.max(winner_18.GP.y[0:n_init]),winner_18.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_18 = np.log(y_global_orig - loser_output_18)\r\n",
        "regret_winner_18 = np.log(y_global_orig - winner_output_18)\r\n",
        "\r\n",
        "train_regret_loser_18 = min_max_array(regret_loser_18)\r\n",
        "train_regret_winner_18 = min_max_array(regret_winner_18)\r\n",
        "\r\n",
        "min_train_regret_loser_18 = min(train_regret_loser_18)\r\n",
        "min_train_regret_winner_18 = min(train_regret_winner_18)\r\n",
        "\r\n",
        "min_train_regret_loser_18, min_train_regret_winner_18"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-2.3352725279655404, -2.716508755864242)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkmSu4CUuwuh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93653cb1-e10c-440b-d85c-5645fff01419"
      },
      "source": [
        "### Training regret minimization: run number = 19\r\n",
        "\r\n",
        "loser_output_19 = np.append(np.max(loser_19.GP.y[0:n_init]),loser_19.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_19 = np.append(np.max(winner_19.GP.y[0:n_init]),winner_19.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_19 = np.log(y_global_orig - loser_output_19)\r\n",
        "regret_winner_19 = np.log(y_global_orig - winner_output_19)\r\n",
        "\r\n",
        "train_regret_loser_19 = min_max_array(regret_loser_19)\r\n",
        "train_regret_winner_19 = min_max_array(regret_winner_19)\r\n",
        "\r\n",
        "min_train_regret_loser_19 = min(train_regret_loser_19)\r\n",
        "min_train_regret_winner_19 = min(train_regret_winner_19)\r\n",
        "\r\n",
        "min_train_regret_loser_19, min_train_regret_winner_19"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-1.2665005804060472, -4.10177691955926)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eymecjwkuwxb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d7a229f-6531-44fd-8d6e-f9afaa758d64"
      },
      "source": [
        "### Training regret minimization: run number = 20\r\n",
        "\r\n",
        "loser_output_20 = np.append(np.max(loser_20.GP.y[0:n_init]),loser_20.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_20 = np.append(np.max(winner_20.GP.y[0:n_init]),winner_20.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_20 = np.log(y_global_orig - loser_output_20)\r\n",
        "regret_winner_20 = np.log(y_global_orig - winner_output_20)\r\n",
        "\r\n",
        "train_regret_loser_20 = min_max_array(regret_loser_20)\r\n",
        "train_regret_winner_20 = min_max_array(regret_winner_20)\r\n",
        "\r\n",
        "min_train_regret_loser_20 = min(train_regret_loser_20)\r\n",
        "min_train_regret_winner_20 = min(train_regret_winner_20)\r\n",
        "\r\n",
        "min_train_regret_loser_20, min_train_regret_winner_20"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-2.167359607010987, -5.029567075974041)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5P6kq2Kuw0d"
      },
      "source": [
        "# Iteration1 :\r\n",
        "\r\n",
        "slice1 = 0\r\n",
        "\r\n",
        "loser1 = [train_regret_loser_1[slice1],\r\n",
        "       train_regret_loser_2[slice1],\r\n",
        "       train_regret_loser_3[slice1],\r\n",
        "       train_regret_loser_4[slice1],\r\n",
        "       train_regret_loser_5[slice1],\r\n",
        "       train_regret_loser_6[slice1],\r\n",
        "       train_regret_loser_7[slice1],\r\n",
        "       train_regret_loser_8[slice1],\r\n",
        "       train_regret_loser_9[slice1],\r\n",
        "       train_regret_loser_10[slice1],\r\n",
        "       train_regret_loser_11[slice1],\r\n",
        "       train_regret_loser_12[slice1],\r\n",
        "       train_regret_loser_13[slice1],\r\n",
        "       train_regret_loser_14[slice1],\r\n",
        "       train_regret_loser_15[slice1],\r\n",
        "       train_regret_loser_16[slice1],\r\n",
        "       train_regret_loser_17[slice1],\r\n",
        "       train_regret_loser_18[slice1],\r\n",
        "       train_regret_loser_19[slice1],\r\n",
        "       train_regret_loser_20[slice1]]\r\n",
        "\r\n",
        "winner1 = [train_regret_winner_1[slice1],\r\n",
        "       train_regret_winner_2[slice1],\r\n",
        "       train_regret_winner_3[slice1],\r\n",
        "       train_regret_winner_4[slice1],\r\n",
        "       train_regret_winner_5[slice1],\r\n",
        "       train_regret_winner_6[slice1],\r\n",
        "       train_regret_winner_7[slice1],\r\n",
        "       train_regret_winner_8[slice1],\r\n",
        "       train_regret_winner_9[slice1],\r\n",
        "       train_regret_winner_10[slice1],\r\n",
        "       train_regret_winner_11[slice1],\r\n",
        "       train_regret_winner_12[slice1],\r\n",
        "       train_regret_winner_13[slice1],\r\n",
        "       train_regret_winner_14[slice1],\r\n",
        "       train_regret_winner_15[slice1],\r\n",
        "       train_regret_winner_16[slice1],\r\n",
        "       train_regret_winner_17[slice1],\r\n",
        "       train_regret_winner_18[slice1],\r\n",
        "       train_regret_winner_19[slice1],\r\n",
        "       train_regret_winner_20[slice1]]\r\n",
        "\r\n",
        "loser1_results = pd.DataFrame(loser1).sort_values(by=[0], ascending=False)\r\n",
        "winner1_results = pd.DataFrame(winner1).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser1 = np.asarray(loser1_results[4:5][0])[0]\r\n",
        "median_loser1 = np.asarray(loser1_results[9:10][0])[0]\r\n",
        "upper_loser1 = np.asarray(loser1_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner1 = np.asarray(winner1_results[4:5][0])[0]\r\n",
        "median_winner1 = np.asarray(winner1_results[9:10][0])[0]\r\n",
        "upper_winner1 = np.asarray(winner1_results[14:15][0])[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXcnj2pNuw3X"
      },
      "source": [
        "# Iteration11 :\r\n",
        "\r\n",
        "slice11 = 10\r\n",
        "\r\n",
        "loser11 = [train_regret_loser_1[slice11],\r\n",
        "       train_regret_loser_2[slice11],\r\n",
        "       train_regret_loser_3[slice11],\r\n",
        "       train_regret_loser_4[slice11],\r\n",
        "       train_regret_loser_5[slice11],\r\n",
        "       train_regret_loser_6[slice11],\r\n",
        "       train_regret_loser_7[slice11],\r\n",
        "       train_regret_loser_8[slice11],\r\n",
        "       train_regret_loser_9[slice11],\r\n",
        "       train_regret_loser_10[slice11],\r\n",
        "       train_regret_loser_11[slice11],\r\n",
        "       train_regret_loser_12[slice11],\r\n",
        "       train_regret_loser_13[slice11],\r\n",
        "       train_regret_loser_14[slice11],\r\n",
        "       train_regret_loser_15[slice11],\r\n",
        "       train_regret_loser_16[slice11],\r\n",
        "       train_regret_loser_17[slice11],\r\n",
        "       train_regret_loser_18[slice11],\r\n",
        "       train_regret_loser_19[slice11],\r\n",
        "       train_regret_loser_20[slice11]]\r\n",
        "\r\n",
        "winner11 = [train_regret_winner_1[slice11],\r\n",
        "       train_regret_winner_2[slice11],\r\n",
        "       train_regret_winner_3[slice11],\r\n",
        "       train_regret_winner_4[slice11],\r\n",
        "       train_regret_winner_5[slice11],\r\n",
        "       train_regret_winner_6[slice11],\r\n",
        "       train_regret_winner_7[slice11],\r\n",
        "       train_regret_winner_8[slice11],\r\n",
        "       train_regret_winner_9[slice11],\r\n",
        "       train_regret_winner_10[slice11],\r\n",
        "       train_regret_winner_11[slice11],\r\n",
        "       train_regret_winner_12[slice11],\r\n",
        "       train_regret_winner_13[slice11],\r\n",
        "       train_regret_winner_14[slice11],\r\n",
        "       train_regret_winner_15[slice11],\r\n",
        "       train_regret_winner_16[slice11],\r\n",
        "       train_regret_winner_17[slice11],\r\n",
        "       train_regret_winner_18[slice11],\r\n",
        "       train_regret_winner_19[slice11],\r\n",
        "       train_regret_winner_20[slice11]]\r\n",
        "\r\n",
        "loser11_results = pd.DataFrame(loser11).sort_values(by=[0], ascending=False)\r\n",
        "winner11_results = pd.DataFrame(winner11).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser11 = np.asarray(loser11_results[4:5][0])[0]\r\n",
        "median_loser11 = np.asarray(loser11_results[9:10][0])[0]\r\n",
        "upper_loser11 = np.asarray(loser11_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner11 = np.asarray(winner11_results[4:5][0])[0]\r\n",
        "median_winner11 = np.asarray(winner11_results[9:10][0])[0]\r\n",
        "upper_winner11 = np.asarray(winner11_results[14:15][0])[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxQa6BAVuw6L"
      },
      "source": [
        "# Iteration21 :\r\n",
        "\r\n",
        "slice21 = 20\r\n",
        "\r\n",
        "loser21 = [train_regret_loser_1[slice21],\r\n",
        "       train_regret_loser_2[slice21],\r\n",
        "       train_regret_loser_3[slice21],\r\n",
        "       train_regret_loser_4[slice21],\r\n",
        "       train_regret_loser_5[slice21],\r\n",
        "       train_regret_loser_6[slice21],\r\n",
        "       train_regret_loser_7[slice21],\r\n",
        "       train_regret_loser_8[slice21],\r\n",
        "       train_regret_loser_9[slice21],\r\n",
        "       train_regret_loser_10[slice21],\r\n",
        "       train_regret_loser_11[slice21],\r\n",
        "       train_regret_loser_12[slice21],\r\n",
        "       train_regret_loser_13[slice21],\r\n",
        "       train_regret_loser_14[slice21],\r\n",
        "       train_regret_loser_15[slice21],\r\n",
        "       train_regret_loser_16[slice21],\r\n",
        "       train_regret_loser_17[slice21],\r\n",
        "       train_regret_loser_18[slice21],\r\n",
        "       train_regret_loser_19[slice21],\r\n",
        "       train_regret_loser_20[slice21]]\r\n",
        "\r\n",
        "winner21 = [train_regret_winner_1[slice21],\r\n",
        "       train_regret_winner_2[slice21],\r\n",
        "       train_regret_winner_3[slice21],\r\n",
        "       train_regret_winner_4[slice21],\r\n",
        "       train_regret_winner_5[slice21],\r\n",
        "       train_regret_winner_6[slice21],\r\n",
        "       train_regret_winner_7[slice21],\r\n",
        "       train_regret_winner_8[slice21],\r\n",
        "       train_regret_winner_9[slice21],\r\n",
        "       train_regret_winner_10[slice21],\r\n",
        "       train_regret_winner_11[slice21],\r\n",
        "       train_regret_winner_12[slice21],\r\n",
        "       train_regret_winner_13[slice21],\r\n",
        "       train_regret_winner_14[slice21],\r\n",
        "       train_regret_winner_15[slice21],\r\n",
        "       train_regret_winner_16[slice21],\r\n",
        "       train_regret_winner_17[slice21],\r\n",
        "       train_regret_winner_18[slice21],\r\n",
        "       train_regret_winner_19[slice21],\r\n",
        "       train_regret_winner_20[slice21]]\r\n",
        "\r\n",
        "loser21_results = pd.DataFrame(loser21).sort_values(by=[0], ascending=False)\r\n",
        "winner21_results = pd.DataFrame(winner21).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser21 = np.asarray(loser21_results[4:5][0])[0]\r\n",
        "median_loser21 = np.asarray(loser21_results[9:10][0])[0]\r\n",
        "upper_loser21 = np.asarray(loser21_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner21 = np.asarray(winner21_results[4:5][0])[0]\r\n",
        "median_winner21 = np.asarray(winner21_results[9:10][0])[0]\r\n",
        "upper_winner21 = np.asarray(winner21_results[14:15][0])[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wdJdIsWuw8r"
      },
      "source": [
        "# Iteration31 :\r\n",
        "\r\n",
        "slice31 = 30\r\n",
        "\r\n",
        "loser31 = [train_regret_loser_1[slice31],\r\n",
        "       train_regret_loser_2[slice31],\r\n",
        "       train_regret_loser_3[slice31],\r\n",
        "       train_regret_loser_4[slice31],\r\n",
        "       train_regret_loser_5[slice31],\r\n",
        "       train_regret_loser_6[slice31],\r\n",
        "       train_regret_loser_7[slice31],\r\n",
        "       train_regret_loser_8[slice31],\r\n",
        "       train_regret_loser_9[slice31],\r\n",
        "       train_regret_loser_10[slice31],\r\n",
        "       train_regret_loser_11[slice31],\r\n",
        "       train_regret_loser_12[slice31],\r\n",
        "       train_regret_loser_13[slice31],\r\n",
        "       train_regret_loser_14[slice31],\r\n",
        "       train_regret_loser_15[slice31],\r\n",
        "       train_regret_loser_16[slice31],\r\n",
        "       train_regret_loser_17[slice31],\r\n",
        "       train_regret_loser_18[slice31],\r\n",
        "       train_regret_loser_19[slice31],\r\n",
        "       train_regret_loser_20[slice31]]\r\n",
        "\r\n",
        "winner31 = [train_regret_winner_1[slice31],\r\n",
        "       train_regret_winner_2[slice31],\r\n",
        "       train_regret_winner_3[slice31],\r\n",
        "       train_regret_winner_4[slice31],\r\n",
        "       train_regret_winner_5[slice31],\r\n",
        "       train_regret_winner_6[slice31],\r\n",
        "       train_regret_winner_7[slice31],\r\n",
        "       train_regret_winner_8[slice31],\r\n",
        "       train_regret_winner_9[slice31],\r\n",
        "       train_regret_winner_10[slice31],\r\n",
        "       train_regret_winner_11[slice31],\r\n",
        "       train_regret_winner_12[slice31],\r\n",
        "       train_regret_winner_13[slice31],\r\n",
        "       train_regret_winner_14[slice31],\r\n",
        "       train_regret_winner_15[slice31],\r\n",
        "       train_regret_winner_16[slice31],\r\n",
        "       train_regret_winner_17[slice31],\r\n",
        "       train_regret_winner_18[slice31],\r\n",
        "       train_regret_winner_19[slice31],\r\n",
        "       train_regret_winner_20[slice31]]\r\n",
        "\r\n",
        "loser31_results = pd.DataFrame(loser31).sort_values(by=[0], ascending=False)\r\n",
        "winner31_results = pd.DataFrame(winner31).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser31 = np.asarray(loser31_results[4:5][0])[0]\r\n",
        "median_loser31 = np.asarray(loser31_results[9:10][0])[0]\r\n",
        "upper_loser31 = np.asarray(loser31_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner31 = np.asarray(winner31_results[4:5][0])[0]\r\n",
        "median_winner31 = np.asarray(winner31_results[9:10][0])[0]\r\n",
        "upper_winner31 = np.asarray(winner31_results[14:15][0])[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vWZBDufuw_Z"
      },
      "source": [
        "# Iteration41 :\r\n",
        "\r\n",
        "slice41 = 40\r\n",
        "\r\n",
        "loser41 = [train_regret_loser_1[slice41],\r\n",
        "       train_regret_loser_2[slice41],\r\n",
        "       train_regret_loser_3[slice41],\r\n",
        "       train_regret_loser_4[slice41],\r\n",
        "       train_regret_loser_5[slice41],\r\n",
        "       train_regret_loser_6[slice41],\r\n",
        "       train_regret_loser_7[slice41],\r\n",
        "       train_regret_loser_8[slice41],\r\n",
        "       train_regret_loser_9[slice41],\r\n",
        "       train_regret_loser_10[slice41],\r\n",
        "       train_regret_loser_11[slice41],\r\n",
        "       train_regret_loser_12[slice41],\r\n",
        "       train_regret_loser_13[slice41],\r\n",
        "       train_regret_loser_14[slice41],\r\n",
        "       train_regret_loser_15[slice41],\r\n",
        "       train_regret_loser_16[slice41],\r\n",
        "       train_regret_loser_17[slice41],\r\n",
        "       train_regret_loser_18[slice41],\r\n",
        "       train_regret_loser_19[slice41],\r\n",
        "       train_regret_loser_20[slice41]]\r\n",
        "\r\n",
        "winner41 = [train_regret_winner_1[slice41],\r\n",
        "       train_regret_winner_2[slice41],\r\n",
        "       train_regret_winner_3[slice41],\r\n",
        "       train_regret_winner_4[slice41],\r\n",
        "       train_regret_winner_5[slice41],\r\n",
        "       train_regret_winner_6[slice41],\r\n",
        "       train_regret_winner_7[slice41],\r\n",
        "       train_regret_winner_8[slice41],\r\n",
        "       train_regret_winner_9[slice41],\r\n",
        "       train_regret_winner_10[slice41],\r\n",
        "       train_regret_winner_11[slice41],\r\n",
        "       train_regret_winner_12[slice41],\r\n",
        "       train_regret_winner_13[slice41],\r\n",
        "       train_regret_winner_14[slice41],\r\n",
        "       train_regret_winner_15[slice41],\r\n",
        "       train_regret_winner_16[slice41],\r\n",
        "       train_regret_winner_17[slice41],\r\n",
        "       train_regret_winner_18[slice41],\r\n",
        "       train_regret_winner_19[slice41],\r\n",
        "       train_regret_winner_20[slice41]]\r\n",
        "\r\n",
        "loser41_results = pd.DataFrame(loser41).sort_values(by=[0], ascending=False)\r\n",
        "winner41_results = pd.DataFrame(winner41).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser41 = np.asarray(loser41_results[4:5][0])[0]\r\n",
        "median_loser41 = np.asarray(loser41_results[9:10][0])[0]\r\n",
        "upper_loser41 = np.asarray(loser41_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner41 = np.asarray(winner41_results[4:5][0])[0]\r\n",
        "median_winner41 = np.asarray(winner41_results[9:10][0])[0]\r\n",
        "upper_winner41 = np.asarray(winner41_results[14:15][0])[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwi_GzjiuxB6"
      },
      "source": [
        "# Iteration51 :\r\n",
        "\r\n",
        "slice51 = 50\r\n",
        "\r\n",
        "loser51 = [train_regret_loser_1[slice51],\r\n",
        "       train_regret_loser_2[slice51],\r\n",
        "       train_regret_loser_3[slice51],\r\n",
        "       train_regret_loser_4[slice51],\r\n",
        "       train_regret_loser_5[slice51],\r\n",
        "       train_regret_loser_6[slice51],\r\n",
        "       train_regret_loser_7[slice51],\r\n",
        "       train_regret_loser_8[slice51],\r\n",
        "       train_regret_loser_9[slice51],\r\n",
        "       train_regret_loser_10[slice51],\r\n",
        "       train_regret_loser_11[slice51],\r\n",
        "       train_regret_loser_12[slice51],\r\n",
        "       train_regret_loser_13[slice51],\r\n",
        "       train_regret_loser_14[slice51],\r\n",
        "       train_regret_loser_15[slice51],\r\n",
        "       train_regret_loser_16[slice51],\r\n",
        "       train_regret_loser_17[slice51],\r\n",
        "       train_regret_loser_18[slice51],\r\n",
        "       train_regret_loser_19[slice51],\r\n",
        "       train_regret_loser_20[slice51]]\r\n",
        "\r\n",
        "winner51 = [train_regret_winner_1[slice51],\r\n",
        "       train_regret_winner_2[slice51],\r\n",
        "       train_regret_winner_3[slice51],\r\n",
        "       train_regret_winner_4[slice51],\r\n",
        "       train_regret_winner_5[slice51],\r\n",
        "       train_regret_winner_6[slice51],\r\n",
        "       train_regret_winner_7[slice51],\r\n",
        "       train_regret_winner_8[slice51],\r\n",
        "       train_regret_winner_9[slice51],\r\n",
        "       train_regret_winner_10[slice51],\r\n",
        "       train_regret_winner_11[slice51],\r\n",
        "       train_regret_winner_12[slice51],\r\n",
        "       train_regret_winner_13[slice51],\r\n",
        "       train_regret_winner_14[slice51],\r\n",
        "       train_regret_winner_15[slice51],\r\n",
        "       train_regret_winner_16[slice51],\r\n",
        "       train_regret_winner_17[slice51],\r\n",
        "       train_regret_winner_18[slice51],\r\n",
        "       train_regret_winner_19[slice51],\r\n",
        "       train_regret_winner_20[slice51]]\r\n",
        "\r\n",
        "loser51_results = pd.DataFrame(loser51).sort_values(by=[0], ascending=False)\r\n",
        "winner51_results = pd.DataFrame(winner51).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser51 = np.asarray(loser51_results[4:5][0])[0]\r\n",
        "median_loser51 = np.asarray(loser51_results[9:10][0])[0]\r\n",
        "upper_loser51 = np.asarray(loser51_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner51 = np.asarray(winner51_results[4:5][0])[0]\r\n",
        "median_winner51 = np.asarray(winner51_results[9:10][0])[0]\r\n",
        "upper_winner51 = np.asarray(winner51_results[14:15][0])[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wk9nlS6puxFZ"
      },
      "source": [
        "# Iteration61 :\r\n",
        "\r\n",
        "slice61 = 60\r\n",
        "\r\n",
        "loser61 = [train_regret_loser_1[slice61],\r\n",
        "       train_regret_loser_2[slice61],\r\n",
        "       train_regret_loser_3[slice61],\r\n",
        "       train_regret_loser_4[slice61],\r\n",
        "       train_regret_loser_5[slice61],\r\n",
        "       train_regret_loser_6[slice61],\r\n",
        "       train_regret_loser_7[slice61],\r\n",
        "       train_regret_loser_8[slice61],\r\n",
        "       train_regret_loser_9[slice61],\r\n",
        "       train_regret_loser_10[slice61],\r\n",
        "       train_regret_loser_11[slice61],\r\n",
        "       train_regret_loser_12[slice61],\r\n",
        "       train_regret_loser_13[slice61],\r\n",
        "       train_regret_loser_14[slice61],\r\n",
        "       train_regret_loser_15[slice61],\r\n",
        "       train_regret_loser_16[slice61],\r\n",
        "       train_regret_loser_17[slice61],\r\n",
        "       train_regret_loser_18[slice61],\r\n",
        "       train_regret_loser_19[slice61],\r\n",
        "       train_regret_loser_20[slice61]]\r\n",
        "\r\n",
        "winner61 = [train_regret_winner_1[slice61],\r\n",
        "       train_regret_winner_2[slice61],\r\n",
        "       train_regret_winner_3[slice61],\r\n",
        "       train_regret_winner_4[slice61],\r\n",
        "       train_regret_winner_5[slice61],\r\n",
        "       train_regret_winner_6[slice61],\r\n",
        "       train_regret_winner_7[slice61],\r\n",
        "       train_regret_winner_8[slice61],\r\n",
        "       train_regret_winner_9[slice61],\r\n",
        "       train_regret_winner_10[slice61],\r\n",
        "       train_regret_winner_11[slice61],\r\n",
        "       train_regret_winner_12[slice61],\r\n",
        "       train_regret_winner_13[slice61],\r\n",
        "       train_regret_winner_14[slice61],\r\n",
        "       train_regret_winner_15[slice61],\r\n",
        "       train_regret_winner_16[slice61],\r\n",
        "       train_regret_winner_17[slice61],\r\n",
        "       train_regret_winner_18[slice61],\r\n",
        "       train_regret_winner_19[slice61],\r\n",
        "       train_regret_winner_20[slice61]]\r\n",
        "\r\n",
        "loser61_results = pd.DataFrame(loser61).sort_values(by=[0], ascending=False)\r\n",
        "winner61_results = pd.DataFrame(winner61).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser61 = np.asarray(loser61_results[4:5][0])[0]\r\n",
        "median_loser61 = np.asarray(loser61_results[9:10][0])[0]\r\n",
        "upper_loser61 = np.asarray(loser61_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner61 = np.asarray(winner61_results[4:5][0])[0]\r\n",
        "median_winner61 = np.asarray(winner61_results[9:10][0])[0]\r\n",
        "upper_winner61 = np.asarray(winner61_results[14:15][0])[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMus9vDpuxHN"
      },
      "source": [
        "# Iteration71 :\r\n",
        "\r\n",
        "slice71 = 70\r\n",
        "\r\n",
        "loser71 = [train_regret_loser_1[slice71],\r\n",
        "       train_regret_loser_2[slice71],\r\n",
        "       train_regret_loser_3[slice71],\r\n",
        "       train_regret_loser_4[slice71],\r\n",
        "       train_regret_loser_5[slice71],\r\n",
        "       train_regret_loser_6[slice71],\r\n",
        "       train_regret_loser_7[slice71],\r\n",
        "       train_regret_loser_8[slice71],\r\n",
        "       train_regret_loser_9[slice71],\r\n",
        "       train_regret_loser_10[slice71],\r\n",
        "       train_regret_loser_11[slice71],\r\n",
        "       train_regret_loser_12[slice71],\r\n",
        "       train_regret_loser_13[slice71],\r\n",
        "       train_regret_loser_14[slice71],\r\n",
        "       train_regret_loser_15[slice71],\r\n",
        "       train_regret_loser_16[slice71],\r\n",
        "       train_regret_loser_17[slice71],\r\n",
        "       train_regret_loser_18[slice71],\r\n",
        "       train_regret_loser_19[slice71],\r\n",
        "       train_regret_loser_20[slice71]]\r\n",
        "\r\n",
        "winner71 = [train_regret_winner_1[slice71],\r\n",
        "       train_regret_winner_2[slice71],\r\n",
        "       train_regret_winner_3[slice71],\r\n",
        "       train_regret_winner_4[slice71],\r\n",
        "       train_regret_winner_5[slice71],\r\n",
        "       train_regret_winner_6[slice71],\r\n",
        "       train_regret_winner_7[slice71],\r\n",
        "       train_regret_winner_8[slice71],\r\n",
        "       train_regret_winner_9[slice71],\r\n",
        "       train_regret_winner_10[slice71],\r\n",
        "       train_regret_winner_11[slice71],\r\n",
        "       train_regret_winner_12[slice71],\r\n",
        "       train_regret_winner_13[slice71],\r\n",
        "       train_regret_winner_14[slice71],\r\n",
        "       train_regret_winner_15[slice71],\r\n",
        "       train_regret_winner_16[slice71],\r\n",
        "       train_regret_winner_17[slice71],\r\n",
        "       train_regret_winner_18[slice71],\r\n",
        "       train_regret_winner_19[slice71],\r\n",
        "       train_regret_winner_20[slice71]]\r\n",
        "\r\n",
        "loser71_results = pd.DataFrame(loser71).sort_values(by=[0], ascending=False)\r\n",
        "winner71_results = pd.DataFrame(winner71).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser71 = np.asarray(loser71_results[4:5][0])[0]\r\n",
        "median_loser71 = np.asarray(loser71_results[9:10][0])[0]\r\n",
        "upper_loser71 = np.asarray(loser71_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner71 = np.asarray(winner71_results[4:5][0])[0]\r\n",
        "median_winner71 = np.asarray(winner71_results[9:10][0])[0]\r\n",
        "upper_winner71 = np.asarray(winner71_results[14:15][0])[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6Mg0x9vuxL4"
      },
      "source": [
        "# Iteration81 :\r\n",
        "\r\n",
        "slice81 = 80\r\n",
        "\r\n",
        "loser81 = [train_regret_loser_1[slice81],\r\n",
        "       train_regret_loser_2[slice81],\r\n",
        "       train_regret_loser_3[slice81],\r\n",
        "       train_regret_loser_4[slice81],\r\n",
        "       train_regret_loser_5[slice81],\r\n",
        "       train_regret_loser_6[slice81],\r\n",
        "       train_regret_loser_7[slice81],\r\n",
        "       train_regret_loser_8[slice81],\r\n",
        "       train_regret_loser_9[slice81],\r\n",
        "       train_regret_loser_10[slice81],\r\n",
        "       train_regret_loser_11[slice81],\r\n",
        "       train_regret_loser_12[slice81],\r\n",
        "       train_regret_loser_13[slice81],\r\n",
        "       train_regret_loser_14[slice81],\r\n",
        "       train_regret_loser_15[slice81],\r\n",
        "       train_regret_loser_16[slice81],\r\n",
        "       train_regret_loser_17[slice81],\r\n",
        "       train_regret_loser_18[slice81],\r\n",
        "       train_regret_loser_19[slice81],\r\n",
        "       train_regret_loser_20[slice81]]\r\n",
        "\r\n",
        "winner81 = [train_regret_winner_1[slice81],\r\n",
        "       train_regret_winner_2[slice81],\r\n",
        "       train_regret_winner_3[slice81],\r\n",
        "       train_regret_winner_4[slice81],\r\n",
        "       train_regret_winner_5[slice81],\r\n",
        "       train_regret_winner_6[slice81],\r\n",
        "       train_regret_winner_7[slice81],\r\n",
        "       train_regret_winner_8[slice81],\r\n",
        "       train_regret_winner_9[slice81],\r\n",
        "       train_regret_winner_10[slice81],\r\n",
        "       train_regret_winner_11[slice81],\r\n",
        "       train_regret_winner_12[slice81],\r\n",
        "       train_regret_winner_13[slice81],\r\n",
        "       train_regret_winner_14[slice81],\r\n",
        "       train_regret_winner_15[slice81],\r\n",
        "       train_regret_winner_16[slice81],\r\n",
        "       train_regret_winner_17[slice81],\r\n",
        "       train_regret_winner_18[slice81],\r\n",
        "       train_regret_winner_19[slice81],\r\n",
        "       train_regret_winner_20[slice81]]\r\n",
        "\r\n",
        "loser81_results = pd.DataFrame(loser81).sort_values(by=[0], ascending=False)\r\n",
        "winner81_results = pd.DataFrame(winner81).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser81 = np.asarray(loser81_results[4:5][0])[0]\r\n",
        "median_loser81 = np.asarray(loser81_results[9:10][0])[0]\r\n",
        "upper_loser81 = np.asarray(loser81_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner81 = np.asarray(winner81_results[4:5][0])[0]\r\n",
        "median_winner81 = np.asarray(winner81_results[9:10][0])[0]\r\n",
        "upper_winner81 = np.asarray(winner81_results[14:15][0])[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Gu9K8XlteV4"
      },
      "source": [
        "# Iteration91 :\r\n",
        "\r\n",
        "slice91 = 90\r\n",
        "\r\n",
        "loser91 = [train_regret_loser_1[slice91],\r\n",
        "       train_regret_loser_2[slice91],\r\n",
        "       train_regret_loser_3[slice91],\r\n",
        "       train_regret_loser_4[slice91],\r\n",
        "       train_regret_loser_5[slice91],\r\n",
        "       train_regret_loser_6[slice91],\r\n",
        "       train_regret_loser_7[slice91],\r\n",
        "       train_regret_loser_8[slice91],\r\n",
        "       train_regret_loser_9[slice91],\r\n",
        "       train_regret_loser_10[slice91],\r\n",
        "       train_regret_loser_11[slice91],\r\n",
        "       train_regret_loser_12[slice91],\r\n",
        "       train_regret_loser_13[slice91],\r\n",
        "       train_regret_loser_14[slice91],\r\n",
        "       train_regret_loser_15[slice91],\r\n",
        "       train_regret_loser_16[slice91],\r\n",
        "       train_regret_loser_17[slice91],\r\n",
        "       train_regret_loser_18[slice91],\r\n",
        "       train_regret_loser_19[slice91],\r\n",
        "       train_regret_loser_20[slice91]]\r\n",
        "\r\n",
        "winner91 = [train_regret_winner_1[slice91],\r\n",
        "       train_regret_winner_2[slice91],\r\n",
        "       train_regret_winner_3[slice91],\r\n",
        "       train_regret_winner_4[slice91],\r\n",
        "       train_regret_winner_5[slice91],\r\n",
        "       train_regret_winner_6[slice91],\r\n",
        "       train_regret_winner_7[slice91],\r\n",
        "       train_regret_winner_8[slice91],\r\n",
        "       train_regret_winner_9[slice91],\r\n",
        "       train_regret_winner_10[slice91],\r\n",
        "       train_regret_winner_11[slice91],\r\n",
        "       train_regret_winner_12[slice91],\r\n",
        "       train_regret_winner_13[slice91],\r\n",
        "       train_regret_winner_14[slice91],\r\n",
        "       train_regret_winner_15[slice91],\r\n",
        "       train_regret_winner_16[slice91],\r\n",
        "       train_regret_winner_17[slice91],\r\n",
        "       train_regret_winner_18[slice91],\r\n",
        "       train_regret_winner_19[slice91],\r\n",
        "       train_regret_winner_20[slice91]]\r\n",
        "\r\n",
        "loser91_results = pd.DataFrame(loser91).sort_values(by=[0], ascending=False)\r\n",
        "winner91_results = pd.DataFrame(winner91).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser91 = np.asarray(loser91_results[4:5][0])[0]\r\n",
        "median_loser91 = np.asarray(loser91_results[9:10][0])[0]\r\n",
        "upper_loser91 = np.asarray(loser91_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner91 = np.asarray(winner91_results[4:5][0])[0]\r\n",
        "median_winner91 = np.asarray(winner91_results[9:10][0])[0]\r\n",
        "upper_winner91 = np.asarray(winner91_results[14:15][0])[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVVeft6steXv"
      },
      "source": [
        "# Iteration101 :\r\n",
        "\r\n",
        "slice101 = 100\r\n",
        "\r\n",
        "loser101 = [train_regret_loser_1[slice101],\r\n",
        "       train_regret_loser_2[slice101],\r\n",
        "       train_regret_loser_3[slice101],\r\n",
        "       train_regret_loser_4[slice101],\r\n",
        "       train_regret_loser_5[slice101],\r\n",
        "       train_regret_loser_6[slice101],\r\n",
        "       train_regret_loser_7[slice101],\r\n",
        "       train_regret_loser_8[slice101],\r\n",
        "       train_regret_loser_9[slice101],\r\n",
        "       train_regret_loser_10[slice101],\r\n",
        "       train_regret_loser_11[slice101],\r\n",
        "       train_regret_loser_12[slice101],\r\n",
        "       train_regret_loser_13[slice101],\r\n",
        "       train_regret_loser_14[slice101],\r\n",
        "       train_regret_loser_15[slice101],\r\n",
        "       train_regret_loser_16[slice101],\r\n",
        "       train_regret_loser_17[slice101],\r\n",
        "       train_regret_loser_18[slice101],\r\n",
        "       train_regret_loser_19[slice101],\r\n",
        "       train_regret_loser_20[slice101]]\r\n",
        "\r\n",
        "winner101 = [train_regret_winner_1[slice101],\r\n",
        "       train_regret_winner_2[slice101],\r\n",
        "       train_regret_winner_3[slice101],\r\n",
        "       train_regret_winner_4[slice101],\r\n",
        "       train_regret_winner_5[slice101],\r\n",
        "       train_regret_winner_6[slice101],\r\n",
        "       train_regret_winner_7[slice101],\r\n",
        "       train_regret_winner_8[slice101],\r\n",
        "       train_regret_winner_9[slice101],\r\n",
        "       train_regret_winner_10[slice101],\r\n",
        "       train_regret_winner_11[slice101],\r\n",
        "       train_regret_winner_12[slice101],\r\n",
        "       train_regret_winner_13[slice101],\r\n",
        "       train_regret_winner_14[slice101],\r\n",
        "       train_regret_winner_15[slice101],\r\n",
        "       train_regret_winner_16[slice101],\r\n",
        "       train_regret_winner_17[slice101],\r\n",
        "       train_regret_winner_18[slice101],\r\n",
        "       train_regret_winner_19[slice101],\r\n",
        "       train_regret_winner_20[slice101]]\r\n",
        "\r\n",
        "loser101_results = pd.DataFrame(loser101).sort_values(by=[0], ascending=False)\r\n",
        "winner101_results = pd.DataFrame(winner101).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser101 = np.asarray(loser101_results[4:5][0])[0]\r\n",
        "median_loser101 = np.asarray(loser101_results[9:10][0])[0]\r\n",
        "upper_loser101 = np.asarray(loser101_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner101 = np.asarray(winner101_results[4:5][0])[0]\r\n",
        "median_winner101 = np.asarray(winner101_results[9:10][0])[0]\r\n",
        "upper_winner101 = np.asarray(winner101_results[14:15][0])[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "maqbtTXXtebT"
      },
      "source": [
        "# Iteration2 :\r\n",
        "\r\n",
        "slice2 = 1\r\n",
        "\r\n",
        "loser2 = [train_regret_loser_1[slice2],\r\n",
        "       train_regret_loser_2[slice2],\r\n",
        "       train_regret_loser_3[slice2],\r\n",
        "       train_regret_loser_4[slice2],\r\n",
        "       train_regret_loser_5[slice2],\r\n",
        "       train_regret_loser_6[slice2],\r\n",
        "       train_regret_loser_7[slice2],\r\n",
        "       train_regret_loser_8[slice2],\r\n",
        "       train_regret_loser_9[slice2],\r\n",
        "       train_regret_loser_10[slice2],\r\n",
        "       train_regret_loser_11[slice2],\r\n",
        "       train_regret_loser_12[slice2],\r\n",
        "       train_regret_loser_13[slice2],\r\n",
        "       train_regret_loser_14[slice2],\r\n",
        "       train_regret_loser_15[slice2],\r\n",
        "       train_regret_loser_16[slice2],\r\n",
        "       train_regret_loser_17[slice2],\r\n",
        "       train_regret_loser_18[slice2],\r\n",
        "       train_regret_loser_19[slice2],\r\n",
        "       train_regret_loser_20[slice2]]\r\n",
        "\r\n",
        "winner2 = [train_regret_winner_1[slice2],\r\n",
        "       train_regret_winner_2[slice2],\r\n",
        "       train_regret_winner_3[slice2],\r\n",
        "       train_regret_winner_4[slice2],\r\n",
        "       train_regret_winner_5[slice2],\r\n",
        "       train_regret_winner_6[slice2],\r\n",
        "       train_regret_winner_7[slice2],\r\n",
        "       train_regret_winner_8[slice2],\r\n",
        "       train_regret_winner_9[slice2],\r\n",
        "       train_regret_winner_10[slice2],\r\n",
        "       train_regret_winner_11[slice2],\r\n",
        "       train_regret_winner_12[slice2],\r\n",
        "       train_regret_winner_13[slice2],\r\n",
        "       train_regret_winner_14[slice2],\r\n",
        "       train_regret_winner_15[slice2],\r\n",
        "       train_regret_winner_16[slice2],\r\n",
        "       train_regret_winner_17[slice2],\r\n",
        "       train_regret_winner_18[slice2],\r\n",
        "       train_regret_winner_19[slice2],\r\n",
        "       train_regret_winner_20[slice2]]\r\n",
        "\r\n",
        "loser2_results = pd.DataFrame(loser2).sort_values(by=[0], ascending=False)\r\n",
        "winner2_results = pd.DataFrame(winner2).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser2 = np.asarray(loser2_results[4:5][0])[0]\r\n",
        "median_loser2 = np.asarray(loser2_results[9:10][0])[0]\r\n",
        "upper_loser2 = np.asarray(loser2_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner2 = np.asarray(winner2_results[4:5][0])[0]\r\n",
        "median_winner2 = np.asarray(winner2_results[9:10][0])[0]\r\n",
        "upper_winner2 = np.asarray(winner2_results[14:15][0])[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWIBh0dQw-ZM"
      },
      "source": [
        "# Iteration12 :\r\n",
        "\r\n",
        "slice12 = 11\r\n",
        "\r\n",
        "loser12 = [train_regret_loser_1[slice12],\r\n",
        "       train_regret_loser_2[slice12],\r\n",
        "       train_regret_loser_3[slice12],\r\n",
        "       train_regret_loser_4[slice12],\r\n",
        "       train_regret_loser_5[slice12],\r\n",
        "       train_regret_loser_6[slice12],\r\n",
        "       train_regret_loser_7[slice12],\r\n",
        "       train_regret_loser_8[slice12],\r\n",
        "       train_regret_loser_9[slice12],\r\n",
        "       train_regret_loser_10[slice12],\r\n",
        "       train_regret_loser_11[slice12],\r\n",
        "       train_regret_loser_12[slice12],\r\n",
        "       train_regret_loser_13[slice12],\r\n",
        "       train_regret_loser_14[slice12],\r\n",
        "       train_regret_loser_15[slice12],\r\n",
        "       train_regret_loser_16[slice12],\r\n",
        "       train_regret_loser_17[slice12],\r\n",
        "       train_regret_loser_18[slice12],\r\n",
        "       train_regret_loser_19[slice12],\r\n",
        "       train_regret_loser_20[slice12]]\r\n",
        "\r\n",
        "winner12 = [train_regret_winner_1[slice12],\r\n",
        "       train_regret_winner_2[slice12],\r\n",
        "       train_regret_winner_3[slice12],\r\n",
        "       train_regret_winner_4[slice12],\r\n",
        "       train_regret_winner_5[slice12],\r\n",
        "       train_regret_winner_6[slice12],\r\n",
        "       train_regret_winner_7[slice12],\r\n",
        "       train_regret_winner_8[slice12],\r\n",
        "       train_regret_winner_9[slice12],\r\n",
        "       train_regret_winner_10[slice12],\r\n",
        "       train_regret_winner_11[slice12],\r\n",
        "       train_regret_winner_12[slice12],\r\n",
        "       train_regret_winner_13[slice12],\r\n",
        "       train_regret_winner_14[slice12],\r\n",
        "       train_regret_winner_15[slice12],\r\n",
        "       train_regret_winner_16[slice12],\r\n",
        "       train_regret_winner_17[slice12],\r\n",
        "       train_regret_winner_18[slice12],\r\n",
        "       train_regret_winner_19[slice12],\r\n",
        "       train_regret_winner_20[slice12]]\r\n",
        "\r\n",
        "loser12_results = pd.DataFrame(loser12).sort_values(by=[0], ascending=False)\r\n",
        "winner12_results = pd.DataFrame(winner12).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser12 = np.asarray(loser12_results[4:5][0])[0]\r\n",
        "median_loser12 = np.asarray(loser12_results[9:10][0])[0]\r\n",
        "upper_loser12 = np.asarray(loser12_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner12 = np.asarray(winner12_results[4:5][0])[0]\r\n",
        "median_winner12 = np.asarray(winner12_results[9:10][0])[0]\r\n",
        "upper_winner12 = np.asarray(winner12_results[14:15][0])[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjIhsei3w-cV"
      },
      "source": [
        "# Iteration22 :\r\n",
        "\r\n",
        "slice22 = 21\r\n",
        "\r\n",
        "loser22 = [train_regret_loser_1[slice22],\r\n",
        "       train_regret_loser_2[slice22],\r\n",
        "       train_regret_loser_3[slice22],\r\n",
        "       train_regret_loser_4[slice22],\r\n",
        "       train_regret_loser_5[slice22],\r\n",
        "       train_regret_loser_6[slice22],\r\n",
        "       train_regret_loser_7[slice22],\r\n",
        "       train_regret_loser_8[slice22],\r\n",
        "       train_regret_loser_9[slice22],\r\n",
        "       train_regret_loser_10[slice22],\r\n",
        "       train_regret_loser_11[slice22],\r\n",
        "       train_regret_loser_12[slice22],\r\n",
        "       train_regret_loser_13[slice22],\r\n",
        "       train_regret_loser_14[slice22],\r\n",
        "       train_regret_loser_15[slice22],\r\n",
        "       train_regret_loser_16[slice22],\r\n",
        "       train_regret_loser_17[slice22],\r\n",
        "       train_regret_loser_18[slice22],\r\n",
        "       train_regret_loser_19[slice22],\r\n",
        "       train_regret_loser_20[slice22]]\r\n",
        "\r\n",
        "winner22 = [train_regret_winner_1[slice22],\r\n",
        "       train_regret_winner_2[slice22],\r\n",
        "       train_regret_winner_3[slice22],\r\n",
        "       train_regret_winner_4[slice22],\r\n",
        "       train_regret_winner_5[slice22],\r\n",
        "       train_regret_winner_6[slice22],\r\n",
        "       train_regret_winner_7[slice22],\r\n",
        "       train_regret_winner_8[slice22],\r\n",
        "       train_regret_winner_9[slice22],\r\n",
        "       train_regret_winner_10[slice22],\r\n",
        "       train_regret_winner_11[slice22],\r\n",
        "       train_regret_winner_12[slice22],\r\n",
        "       train_regret_winner_13[slice22],\r\n",
        "       train_regret_winner_14[slice22],\r\n",
        "       train_regret_winner_15[slice22],\r\n",
        "       train_regret_winner_16[slice22],\r\n",
        "       train_regret_winner_17[slice22],\r\n",
        "       train_regret_winner_18[slice22],\r\n",
        "       train_regret_winner_19[slice22],\r\n",
        "       train_regret_winner_20[slice22]]\r\n",
        "\r\n",
        "loser22_results = pd.DataFrame(loser22).sort_values(by=[0], ascending=False)\r\n",
        "winner22_results = pd.DataFrame(winner22).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser22 = np.asarray(loser22_results[4:5][0])[0]\r\n",
        "median_loser22 = np.asarray(loser22_results[9:10][0])[0]\r\n",
        "upper_loser22 = np.asarray(loser22_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner22 = np.asarray(winner22_results[4:5][0])[0]\r\n",
        "median_winner22 = np.asarray(winner22_results[9:10][0])[0]\r\n",
        "upper_winner22 = np.asarray(winner22_results[14:15][0])[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ur9p9GkHw-fq"
      },
      "source": [
        "# Iteration32 :\r\n",
        "\r\n",
        "slice32 = 31\r\n",
        "\r\n",
        "loser32 = [train_regret_loser_1[slice32],\r\n",
        "       train_regret_loser_2[slice32],\r\n",
        "       train_regret_loser_3[slice32],\r\n",
        "       train_regret_loser_4[slice32],\r\n",
        "       train_regret_loser_5[slice32],\r\n",
        "       train_regret_loser_6[slice32],\r\n",
        "       train_regret_loser_7[slice32],\r\n",
        "       train_regret_loser_8[slice32],\r\n",
        "       train_regret_loser_9[slice32],\r\n",
        "       train_regret_loser_10[slice32],\r\n",
        "       train_regret_loser_11[slice32],\r\n",
        "       train_regret_loser_12[slice32],\r\n",
        "       train_regret_loser_13[slice32],\r\n",
        "       train_regret_loser_14[slice32],\r\n",
        "       train_regret_loser_15[slice32],\r\n",
        "       train_regret_loser_16[slice32],\r\n",
        "       train_regret_loser_17[slice32],\r\n",
        "       train_regret_loser_18[slice32],\r\n",
        "       train_regret_loser_19[slice32],\r\n",
        "       train_regret_loser_20[slice32]]\r\n",
        "\r\n",
        "winner32 = [train_regret_winner_1[slice32],\r\n",
        "       train_regret_winner_2[slice32],\r\n",
        "       train_regret_winner_3[slice32],\r\n",
        "       train_regret_winner_4[slice32],\r\n",
        "       train_regret_winner_5[slice32],\r\n",
        "       train_regret_winner_6[slice32],\r\n",
        "       train_regret_winner_7[slice32],\r\n",
        "       train_regret_winner_8[slice32],\r\n",
        "       train_regret_winner_9[slice32],\r\n",
        "       train_regret_winner_10[slice32],\r\n",
        "       train_regret_winner_11[slice32],\r\n",
        "       train_regret_winner_12[slice32],\r\n",
        "       train_regret_winner_13[slice32],\r\n",
        "       train_regret_winner_14[slice32],\r\n",
        "       train_regret_winner_15[slice32],\r\n",
        "       train_regret_winner_16[slice32],\r\n",
        "       train_regret_winner_17[slice32],\r\n",
        "       train_regret_winner_18[slice32],\r\n",
        "       train_regret_winner_19[slice32],\r\n",
        "       train_regret_winner_20[slice32]]\r\n",
        "\r\n",
        "loser32_results = pd.DataFrame(loser32).sort_values(by=[0], ascending=False)\r\n",
        "winner32_results = pd.DataFrame(winner32).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser32 = np.asarray(loser32_results[4:5][0])[0]\r\n",
        "median_loser32 = np.asarray(loser32_results[9:10][0])[0]\r\n",
        "upper_loser32 = np.asarray(loser32_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner32 = np.asarray(winner32_results[4:5][0])[0]\r\n",
        "median_winner32 = np.asarray(winner32_results[9:10][0])[0]\r\n",
        "upper_winner32 = np.asarray(winner32_results[14:15][0])[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jd_rzHB8w-hy"
      },
      "source": [
        "# Iteration42 :\r\n",
        "\r\n",
        "slice42 = 41\r\n",
        "\r\n",
        "loser42 = [train_regret_loser_1[slice42],\r\n",
        "       train_regret_loser_2[slice42],\r\n",
        "       train_regret_loser_3[slice42],\r\n",
        "       train_regret_loser_4[slice42],\r\n",
        "       train_regret_loser_5[slice42],\r\n",
        "       train_regret_loser_6[slice42],\r\n",
        "       train_regret_loser_7[slice42],\r\n",
        "       train_regret_loser_8[slice42],\r\n",
        "       train_regret_loser_9[slice42],\r\n",
        "       train_regret_loser_10[slice42],\r\n",
        "       train_regret_loser_11[slice42],\r\n",
        "       train_regret_loser_12[slice42],\r\n",
        "       train_regret_loser_13[slice42],\r\n",
        "       train_regret_loser_14[slice42],\r\n",
        "       train_regret_loser_15[slice42],\r\n",
        "       train_regret_loser_16[slice42],\r\n",
        "       train_regret_loser_17[slice42],\r\n",
        "       train_regret_loser_18[slice42],\r\n",
        "       train_regret_loser_19[slice42],\r\n",
        "       train_regret_loser_20[slice42]]\r\n",
        "\r\n",
        "winner42 = [train_regret_winner_1[slice42],\r\n",
        "       train_regret_winner_2[slice42],\r\n",
        "       train_regret_winner_3[slice42],\r\n",
        "       train_regret_winner_4[slice42],\r\n",
        "       train_regret_winner_5[slice42],\r\n",
        "       train_regret_winner_6[slice42],\r\n",
        "       train_regret_winner_7[slice42],\r\n",
        "       train_regret_winner_8[slice42],\r\n",
        "       train_regret_winner_9[slice42],\r\n",
        "       train_regret_winner_10[slice42],\r\n",
        "       train_regret_winner_11[slice42],\r\n",
        "       train_regret_winner_12[slice42],\r\n",
        "       train_regret_winner_13[slice42],\r\n",
        "       train_regret_winner_14[slice42],\r\n",
        "       train_regret_winner_15[slice42],\r\n",
        "       train_regret_winner_16[slice42],\r\n",
        "       train_regret_winner_17[slice42],\r\n",
        "       train_regret_winner_18[slice42],\r\n",
        "       train_regret_winner_19[slice42],\r\n",
        "       train_regret_winner_20[slice42]]\r\n",
        "\r\n",
        "loser42_results = pd.DataFrame(loser42).sort_values(by=[0], ascending=False)\r\n",
        "winner42_results = pd.DataFrame(winner42).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser42 = np.asarray(loser42_results[4:5][0])[0]\r\n",
        "median_loser42 = np.asarray(loser42_results[9:10][0])[0]\r\n",
        "upper_loser42 = np.asarray(loser42_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner42 = np.asarray(winner42_results[4:5][0])[0]\r\n",
        "median_winner42 = np.asarray(winner42_results[9:10][0])[0]\r\n",
        "upper_winner42 = np.asarray(winner42_results[14:15][0])[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKUUG6vAw-kz"
      },
      "source": [
        "# Iteration52 :\r\n",
        "\r\n",
        "slice52 = 51\r\n",
        "\r\n",
        "loser52 = [train_regret_loser_1[slice52],\r\n",
        "       train_regret_loser_2[slice52],\r\n",
        "       train_regret_loser_3[slice52],\r\n",
        "       train_regret_loser_4[slice52],\r\n",
        "       train_regret_loser_5[slice52],\r\n",
        "       train_regret_loser_6[slice52],\r\n",
        "       train_regret_loser_7[slice52],\r\n",
        "       train_regret_loser_8[slice52],\r\n",
        "       train_regret_loser_9[slice52],\r\n",
        "       train_regret_loser_10[slice52],\r\n",
        "       train_regret_loser_11[slice52],\r\n",
        "       train_regret_loser_12[slice52],\r\n",
        "       train_regret_loser_13[slice52],\r\n",
        "       train_regret_loser_14[slice52],\r\n",
        "       train_regret_loser_15[slice52],\r\n",
        "       train_regret_loser_16[slice52],\r\n",
        "       train_regret_loser_17[slice52],\r\n",
        "       train_regret_loser_18[slice52],\r\n",
        "       train_regret_loser_19[slice52],\r\n",
        "       train_regret_loser_20[slice52]]\r\n",
        "\r\n",
        "winner52 = [train_regret_winner_1[slice52],\r\n",
        "       train_regret_winner_2[slice52],\r\n",
        "       train_regret_winner_3[slice52],\r\n",
        "       train_regret_winner_4[slice52],\r\n",
        "       train_regret_winner_5[slice52],\r\n",
        "       train_regret_winner_6[slice52],\r\n",
        "       train_regret_winner_7[slice52],\r\n",
        "       train_regret_winner_8[slice52],\r\n",
        "       train_regret_winner_9[slice52],\r\n",
        "       train_regret_winner_10[slice52],\r\n",
        "       train_regret_winner_11[slice52],\r\n",
        "       train_regret_winner_12[slice52],\r\n",
        "       train_regret_winner_13[slice52],\r\n",
        "       train_regret_winner_14[slice52],\r\n",
        "       train_regret_winner_15[slice52],\r\n",
        "       train_regret_winner_16[slice52],\r\n",
        "       train_regret_winner_17[slice52],\r\n",
        "       train_regret_winner_18[slice52],\r\n",
        "       train_regret_winner_19[slice52],\r\n",
        "       train_regret_winner_20[slice52]]\r\n",
        "\r\n",
        "loser52_results = pd.DataFrame(loser52).sort_values(by=[0], ascending=False)\r\n",
        "winner52_results = pd.DataFrame(winner52).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser52 = np.asarray(loser52_results[4:5][0])[0]\r\n",
        "median_loser52 = np.asarray(loser52_results[9:10][0])[0]\r\n",
        "upper_loser52 = np.asarray(loser52_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner52 = np.asarray(winner52_results[4:5][0])[0]\r\n",
        "median_winner52 = np.asarray(winner52_results[9:10][0])[0]\r\n",
        "upper_winner52 = np.asarray(winner52_results[14:15][0])[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-9cJNkew-nY"
      },
      "source": [
        "# Iteration62 :\r\n",
        "\r\n",
        "slice62 = 61\r\n",
        "\r\n",
        "loser62 = [train_regret_loser_1[slice62],\r\n",
        "       train_regret_loser_2[slice62],\r\n",
        "       train_regret_loser_3[slice62],\r\n",
        "       train_regret_loser_4[slice62],\r\n",
        "       train_regret_loser_5[slice62],\r\n",
        "       train_regret_loser_6[slice62],\r\n",
        "       train_regret_loser_7[slice62],\r\n",
        "       train_regret_loser_8[slice62],\r\n",
        "       train_regret_loser_9[slice62],\r\n",
        "       train_regret_loser_10[slice62],\r\n",
        "       train_regret_loser_11[slice62],\r\n",
        "       train_regret_loser_12[slice62],\r\n",
        "       train_regret_loser_13[slice62],\r\n",
        "       train_regret_loser_14[slice62],\r\n",
        "       train_regret_loser_15[slice62],\r\n",
        "       train_regret_loser_16[slice62],\r\n",
        "       train_regret_loser_17[slice62],\r\n",
        "       train_regret_loser_18[slice62],\r\n",
        "       train_regret_loser_19[slice62],\r\n",
        "       train_regret_loser_20[slice62]]\r\n",
        "\r\n",
        "winner62 = [train_regret_winner_1[slice62],\r\n",
        "       train_regret_winner_2[slice62],\r\n",
        "       train_regret_winner_3[slice62],\r\n",
        "       train_regret_winner_4[slice62],\r\n",
        "       train_regret_winner_5[slice62],\r\n",
        "       train_regret_winner_6[slice62],\r\n",
        "       train_regret_winner_7[slice62],\r\n",
        "       train_regret_winner_8[slice62],\r\n",
        "       train_regret_winner_9[slice62],\r\n",
        "       train_regret_winner_10[slice62],\r\n",
        "       train_regret_winner_11[slice62],\r\n",
        "       train_regret_winner_12[slice62],\r\n",
        "       train_regret_winner_13[slice62],\r\n",
        "       train_regret_winner_14[slice62],\r\n",
        "       train_regret_winner_15[slice62],\r\n",
        "       train_regret_winner_16[slice62],\r\n",
        "       train_regret_winner_17[slice62],\r\n",
        "       train_regret_winner_18[slice62],\r\n",
        "       train_regret_winner_19[slice62],\r\n",
        "       train_regret_winner_20[slice62]]\r\n",
        "\r\n",
        "loser62_results = pd.DataFrame(loser62).sort_values(by=[0], ascending=False)\r\n",
        "winner62_results = pd.DataFrame(winner62).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser62 = np.asarray(loser62_results[4:5][0])[0]\r\n",
        "median_loser62 = np.asarray(loser62_results[9:10][0])[0]\r\n",
        "upper_loser62 = np.asarray(loser62_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner62 = np.asarray(winner62_results[4:5][0])[0]\r\n",
        "median_winner62 = np.asarray(winner62_results[9:10][0])[0]\r\n",
        "upper_winner62 = np.asarray(winner62_results[14:15][0])[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DZ8kHHmw-qK"
      },
      "source": [
        "# Iteration72 :\r\n",
        "\r\n",
        "slice72 = 71\r\n",
        "\r\n",
        "loser72 = [train_regret_loser_1[slice72],\r\n",
        "       train_regret_loser_2[slice72],\r\n",
        "       train_regret_loser_3[slice72],\r\n",
        "       train_regret_loser_4[slice72],\r\n",
        "       train_regret_loser_5[slice72],\r\n",
        "       train_regret_loser_6[slice72],\r\n",
        "       train_regret_loser_7[slice72],\r\n",
        "       train_regret_loser_8[slice72],\r\n",
        "       train_regret_loser_9[slice72],\r\n",
        "       train_regret_loser_10[slice72],\r\n",
        "       train_regret_loser_11[slice72],\r\n",
        "       train_regret_loser_12[slice72],\r\n",
        "       train_regret_loser_13[slice72],\r\n",
        "       train_regret_loser_14[slice72],\r\n",
        "       train_regret_loser_15[slice72],\r\n",
        "       train_regret_loser_16[slice72],\r\n",
        "       train_regret_loser_17[slice72],\r\n",
        "       train_regret_loser_18[slice72],\r\n",
        "       train_regret_loser_19[slice72],\r\n",
        "       train_regret_loser_20[slice72]]\r\n",
        "\r\n",
        "winner72 = [train_regret_winner_1[slice72],\r\n",
        "       train_regret_winner_2[slice72],\r\n",
        "       train_regret_winner_3[slice72],\r\n",
        "       train_regret_winner_4[slice72],\r\n",
        "       train_regret_winner_5[slice72],\r\n",
        "       train_regret_winner_6[slice72],\r\n",
        "       train_regret_winner_7[slice72],\r\n",
        "       train_regret_winner_8[slice72],\r\n",
        "       train_regret_winner_9[slice72],\r\n",
        "       train_regret_winner_10[slice72],\r\n",
        "       train_regret_winner_11[slice72],\r\n",
        "       train_regret_winner_12[slice72],\r\n",
        "       train_regret_winner_13[slice72],\r\n",
        "       train_regret_winner_14[slice72],\r\n",
        "       train_regret_winner_15[slice72],\r\n",
        "       train_regret_winner_16[slice72],\r\n",
        "       train_regret_winner_17[slice72],\r\n",
        "       train_regret_winner_18[slice72],\r\n",
        "       train_regret_winner_19[slice72],\r\n",
        "       train_regret_winner_20[slice72]]\r\n",
        "\r\n",
        "loser72_results = pd.DataFrame(loser72).sort_values(by=[0], ascending=False)\r\n",
        "winner72_results = pd.DataFrame(winner72).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser72 = np.asarray(loser72_results[4:5][0])[0]\r\n",
        "median_loser72 = np.asarray(loser72_results[9:10][0])[0]\r\n",
        "upper_loser72 = np.asarray(loser72_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner72 = np.asarray(winner72_results[4:5][0])[0]\r\n",
        "median_winner72 = np.asarray(winner72_results[9:10][0])[0]\r\n",
        "upper_winner72 = np.asarray(winner72_results[14:15][0])[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBzBvnVyw-sz"
      },
      "source": [
        "# Iteration82 :\r\n",
        "\r\n",
        "slice82 = 81\r\n",
        "\r\n",
        "loser82 = [train_regret_loser_1[slice82],\r\n",
        "       train_regret_loser_2[slice82],\r\n",
        "       train_regret_loser_3[slice82],\r\n",
        "       train_regret_loser_4[slice82],\r\n",
        "       train_regret_loser_5[slice82],\r\n",
        "       train_regret_loser_6[slice82],\r\n",
        "       train_regret_loser_7[slice82],\r\n",
        "       train_regret_loser_8[slice82],\r\n",
        "       train_regret_loser_9[slice82],\r\n",
        "       train_regret_loser_10[slice82],\r\n",
        "       train_regret_loser_11[slice82],\r\n",
        "       train_regret_loser_12[slice82],\r\n",
        "       train_regret_loser_13[slice82],\r\n",
        "       train_regret_loser_14[slice82],\r\n",
        "       train_regret_loser_15[slice82],\r\n",
        "       train_regret_loser_16[slice82],\r\n",
        "       train_regret_loser_17[slice82],\r\n",
        "       train_regret_loser_18[slice82],\r\n",
        "       train_regret_loser_19[slice82],\r\n",
        "       train_regret_loser_20[slice82]]\r\n",
        "\r\n",
        "winner82 = [train_regret_winner_1[slice82],\r\n",
        "       train_regret_winner_2[slice82],\r\n",
        "       train_regret_winner_3[slice82],\r\n",
        "       train_regret_winner_4[slice82],\r\n",
        "       train_regret_winner_5[slice82],\r\n",
        "       train_regret_winner_6[slice82],\r\n",
        "       train_regret_winner_7[slice82],\r\n",
        "       train_regret_winner_8[slice82],\r\n",
        "       train_regret_winner_9[slice82],\r\n",
        "       train_regret_winner_10[slice82],\r\n",
        "       train_regret_winner_11[slice82],\r\n",
        "       train_regret_winner_12[slice82],\r\n",
        "       train_regret_winner_13[slice82],\r\n",
        "       train_regret_winner_14[slice82],\r\n",
        "       train_regret_winner_15[slice82],\r\n",
        "       train_regret_winner_16[slice82],\r\n",
        "       train_regret_winner_17[slice82],\r\n",
        "       train_regret_winner_18[slice82],\r\n",
        "       train_regret_winner_19[slice82],\r\n",
        "       train_regret_winner_20[slice82]]\r\n",
        "\r\n",
        "loser82_results = pd.DataFrame(loser82).sort_values(by=[0], ascending=False)\r\n",
        "winner82_results = pd.DataFrame(winner82).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser82 = np.asarray(loser82_results[4:5][0])[0]\r\n",
        "median_loser82 = np.asarray(loser82_results[9:10][0])[0]\r\n",
        "upper_loser82 = np.asarray(loser82_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner82 = np.asarray(winner82_results[4:5][0])[0]\r\n",
        "median_winner82 = np.asarray(winner82_results[9:10][0])[0]\r\n",
        "upper_winner82 = np.asarray(winner82_results[14:15][0])[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Qj--n6Bw-y2"
      },
      "source": [
        "# Iteration92 :\r\n",
        "\r\n",
        "slice92 = 91\r\n",
        "\r\n",
        "loser92 = [train_regret_loser_1[slice92],\r\n",
        "       train_regret_loser_2[slice92],\r\n",
        "       train_regret_loser_3[slice92],\r\n",
        "       train_regret_loser_4[slice92],\r\n",
        "       train_regret_loser_5[slice92],\r\n",
        "       train_regret_loser_6[slice92],\r\n",
        "       train_regret_loser_7[slice92],\r\n",
        "       train_regret_loser_8[slice92],\r\n",
        "       train_regret_loser_9[slice92],\r\n",
        "       train_regret_loser_10[slice92],\r\n",
        "       train_regret_loser_11[slice92],\r\n",
        "       train_regret_loser_12[slice92],\r\n",
        "       train_regret_loser_13[slice92],\r\n",
        "       train_regret_loser_14[slice92],\r\n",
        "       train_regret_loser_15[slice92],\r\n",
        "       train_regret_loser_16[slice92],\r\n",
        "       train_regret_loser_17[slice92],\r\n",
        "       train_regret_loser_18[slice92],\r\n",
        "       train_regret_loser_19[slice92],\r\n",
        "       train_regret_loser_20[slice92]]\r\n",
        "\r\n",
        "winner92 = [train_regret_winner_1[slice92],\r\n",
        "       train_regret_winner_2[slice92],\r\n",
        "       train_regret_winner_3[slice92],\r\n",
        "       train_regret_winner_4[slice92],\r\n",
        "       train_regret_winner_5[slice92],\r\n",
        "       train_regret_winner_6[slice92],\r\n",
        "       train_regret_winner_7[slice92],\r\n",
        "       train_regret_winner_8[slice92],\r\n",
        "       train_regret_winner_9[slice92],\r\n",
        "       train_regret_winner_10[slice92],\r\n",
        "       train_regret_winner_11[slice92],\r\n",
        "       train_regret_winner_12[slice92],\r\n",
        "       train_regret_winner_13[slice92],\r\n",
        "       train_regret_winner_14[slice92],\r\n",
        "       train_regret_winner_15[slice92],\r\n",
        "       train_regret_winner_16[slice92],\r\n",
        "       train_regret_winner_17[slice92],\r\n",
        "       train_regret_winner_18[slice92],\r\n",
        "       train_regret_winner_19[slice92],\r\n",
        "       train_regret_winner_20[slice92]]\r\n",
        "\r\n",
        "loser92_results = pd.DataFrame(loser92).sort_values(by=[0], ascending=False)\r\n",
        "winner92_results = pd.DataFrame(winner92).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser92 = np.asarray(loser92_results[4:5][0])[0]\r\n",
        "median_loser92 = np.asarray(loser92_results[9:10][0])[0]\r\n",
        "upper_loser92 = np.asarray(loser92_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner92 = np.asarray(winner92_results[4:5][0])[0]\r\n",
        "median_winner92 = np.asarray(winner92_results[9:10][0])[0]\r\n",
        "upper_winner92 = np.asarray(winner92_results[14:15][0])[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aaJ5KqLdw-0L"
      },
      "source": [
        "# Iteration3 :\r\n",
        "\r\n",
        "slice3 = 2\r\n",
        "\r\n",
        "loser3 = [train_regret_loser_1[slice3],\r\n",
        "       train_regret_loser_2[slice3],\r\n",
        "       train_regret_loser_3[slice3],\r\n",
        "       train_regret_loser_4[slice3],\r\n",
        "       train_regret_loser_5[slice3],\r\n",
        "       train_regret_loser_6[slice3],\r\n",
        "       train_regret_loser_7[slice3],\r\n",
        "       train_regret_loser_8[slice3],\r\n",
        "       train_regret_loser_9[slice3],\r\n",
        "       train_regret_loser_10[slice3],\r\n",
        "       train_regret_loser_11[slice3],\r\n",
        "       train_regret_loser_12[slice3],\r\n",
        "       train_regret_loser_13[slice3],\r\n",
        "       train_regret_loser_14[slice3],\r\n",
        "       train_regret_loser_15[slice3],\r\n",
        "       train_regret_loser_16[slice3],\r\n",
        "       train_regret_loser_17[slice3],\r\n",
        "       train_regret_loser_18[slice3],\r\n",
        "       train_regret_loser_19[slice3],\r\n",
        "       train_regret_loser_20[slice3]]\r\n",
        "\r\n",
        "winner3 = [train_regret_winner_1[slice3],\r\n",
        "       train_regret_winner_2[slice3],\r\n",
        "       train_regret_winner_3[slice3],\r\n",
        "       train_regret_winner_4[slice3],\r\n",
        "       train_regret_winner_5[slice3],\r\n",
        "       train_regret_winner_6[slice3],\r\n",
        "       train_regret_winner_7[slice3],\r\n",
        "       train_regret_winner_8[slice3],\r\n",
        "       train_regret_winner_9[slice3],\r\n",
        "       train_regret_winner_10[slice3],\r\n",
        "       train_regret_winner_11[slice3],\r\n",
        "       train_regret_winner_12[slice3],\r\n",
        "       train_regret_winner_13[slice3],\r\n",
        "       train_regret_winner_14[slice3],\r\n",
        "       train_regret_winner_15[slice3],\r\n",
        "       train_regret_winner_16[slice3],\r\n",
        "       train_regret_winner_17[slice3],\r\n",
        "       train_regret_winner_18[slice3],\r\n",
        "       train_regret_winner_19[slice3],\r\n",
        "       train_regret_winner_20[slice3]]\r\n",
        "\r\n",
        "loser3_results = pd.DataFrame(loser3).sort_values(by=[0], ascending=False)\r\n",
        "winner3_results = pd.DataFrame(winner3).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser3 = np.asarray(loser3_results[4:5][0])[0]\r\n",
        "median_loser3 = np.asarray(loser3_results[9:10][0])[0]\r\n",
        "upper_loser3 = np.asarray(loser3_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner3 = np.asarray(winner3_results[4:5][0])[0]\r\n",
        "median_winner3 = np.asarray(winner3_results[9:10][0])[0]\r\n",
        "upper_winner3 = np.asarray(winner3_results[14:15][0])[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkaoT1dyw-16"
      },
      "source": [
        "# Iteration13 :\r\n",
        "\r\n",
        "slice13 = 12\r\n",
        "\r\n",
        "loser13 = [train_regret_loser_1[slice13],\r\n",
        "       train_regret_loser_2[slice13],\r\n",
        "       train_regret_loser_3[slice13],\r\n",
        "       train_regret_loser_4[slice13],\r\n",
        "       train_regret_loser_5[slice13],\r\n",
        "       train_regret_loser_6[slice13],\r\n",
        "       train_regret_loser_7[slice13],\r\n",
        "       train_regret_loser_8[slice13],\r\n",
        "       train_regret_loser_9[slice13],\r\n",
        "       train_regret_loser_10[slice13],\r\n",
        "       train_regret_loser_11[slice13],\r\n",
        "       train_regret_loser_12[slice13],\r\n",
        "       train_regret_loser_13[slice13],\r\n",
        "       train_regret_loser_14[slice13],\r\n",
        "       train_regret_loser_15[slice13],\r\n",
        "       train_regret_loser_16[slice13],\r\n",
        "       train_regret_loser_17[slice13],\r\n",
        "       train_regret_loser_18[slice13],\r\n",
        "       train_regret_loser_19[slice13],\r\n",
        "       train_regret_loser_20[slice13]]\r\n",
        "\r\n",
        "winner13 = [train_regret_winner_1[slice13],\r\n",
        "       train_regret_winner_2[slice13],\r\n",
        "       train_regret_winner_3[slice13],\r\n",
        "       train_regret_winner_4[slice13],\r\n",
        "       train_regret_winner_5[slice13],\r\n",
        "       train_regret_winner_6[slice13],\r\n",
        "       train_regret_winner_7[slice13],\r\n",
        "       train_regret_winner_8[slice13],\r\n",
        "       train_regret_winner_9[slice13],\r\n",
        "       train_regret_winner_10[slice13],\r\n",
        "       train_regret_winner_11[slice13],\r\n",
        "       train_regret_winner_12[slice13],\r\n",
        "       train_regret_winner_13[slice13],\r\n",
        "       train_regret_winner_14[slice13],\r\n",
        "       train_regret_winner_15[slice13],\r\n",
        "       train_regret_winner_16[slice13],\r\n",
        "       train_regret_winner_17[slice13],\r\n",
        "       train_regret_winner_18[slice13],\r\n",
        "       train_regret_winner_19[slice13],\r\n",
        "       train_regret_winner_20[slice13]]\r\n",
        "\r\n",
        "loser13_results = pd.DataFrame(loser12).sort_values(by=[0], ascending=False)\r\n",
        "winner13_results = pd.DataFrame(winner12).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser13 = np.asarray(loser13_results[4:5][0])[0]\r\n",
        "median_loser13 = np.asarray(loser13_results[9:10][0])[0]\r\n",
        "upper_loser13 = np.asarray(loser13_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner13 = np.asarray(winner13_results[4:5][0])[0]\r\n",
        "median_winner13 = np.asarray(winner13_results[9:10][0])[0]\r\n",
        "upper_winner13 = np.asarray(winner13_results[14:15][0])[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZbVMrBaw-4B"
      },
      "source": [
        "# Iteration23 :\r\n",
        "\r\n",
        "slice23 = 22\r\n",
        "\r\n",
        "loser23 = [train_regret_loser_1[slice23],\r\n",
        "       train_regret_loser_2[slice23],\r\n",
        "       train_regret_loser_3[slice23],\r\n",
        "       train_regret_loser_4[slice23],\r\n",
        "       train_regret_loser_5[slice23],\r\n",
        "       train_regret_loser_6[slice23],\r\n",
        "       train_regret_loser_7[slice23],\r\n",
        "       train_regret_loser_8[slice23],\r\n",
        "       train_regret_loser_9[slice23],\r\n",
        "       train_regret_loser_10[slice23],\r\n",
        "       train_regret_loser_11[slice23],\r\n",
        "       train_regret_loser_12[slice23],\r\n",
        "       train_regret_loser_13[slice23],\r\n",
        "       train_regret_loser_14[slice23],\r\n",
        "       train_regret_loser_15[slice23],\r\n",
        "       train_regret_loser_16[slice23],\r\n",
        "       train_regret_loser_17[slice23],\r\n",
        "       train_regret_loser_18[slice23],\r\n",
        "       train_regret_loser_19[slice23],\r\n",
        "       train_regret_loser_20[slice23]]\r\n",
        "\r\n",
        "winner23 = [train_regret_winner_1[slice23],\r\n",
        "       train_regret_winner_2[slice23],\r\n",
        "       train_regret_winner_3[slice23],\r\n",
        "       train_regret_winner_4[slice23],\r\n",
        "       train_regret_winner_5[slice23],\r\n",
        "       train_regret_winner_6[slice23],\r\n",
        "       train_regret_winner_7[slice23],\r\n",
        "       train_regret_winner_8[slice23],\r\n",
        "       train_regret_winner_9[slice23],\r\n",
        "       train_regret_winner_10[slice23],\r\n",
        "       train_regret_winner_11[slice23],\r\n",
        "       train_regret_winner_12[slice23],\r\n",
        "       train_regret_winner_13[slice23],\r\n",
        "       train_regret_winner_14[slice23],\r\n",
        "       train_regret_winner_15[slice23],\r\n",
        "       train_regret_winner_16[slice23],\r\n",
        "       train_regret_winner_17[slice23],\r\n",
        "       train_regret_winner_18[slice23],\r\n",
        "       train_regret_winner_19[slice23],\r\n",
        "       train_regret_winner_20[slice23]]\r\n",
        "\r\n",
        "loser23_results = pd.DataFrame(loser23).sort_values(by=[0], ascending=False)\r\n",
        "winner23_results = pd.DataFrame(winner23).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser23 = np.asarray(loser23_results[4:5][0])[0]\r\n",
        "median_loser23 = np.asarray(loser23_results[9:10][0])[0]\r\n",
        "upper_loser23 = np.asarray(loser23_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner23 = np.asarray(winner23_results[4:5][0])[0]\r\n",
        "median_winner23 = np.asarray(winner23_results[9:10][0])[0]\r\n",
        "upper_winner23 = np.asarray(winner23_results[14:15][0])[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUa6HRCUw-7G"
      },
      "source": [
        "# Iteration33 :\r\n",
        "\r\n",
        "slice33 = 32\r\n",
        "\r\n",
        "loser33 = [train_regret_loser_1[slice33],\r\n",
        "       train_regret_loser_2[slice33],\r\n",
        "       train_regret_loser_3[slice33],\r\n",
        "       train_regret_loser_4[slice33],\r\n",
        "       train_regret_loser_5[slice33],\r\n",
        "       train_regret_loser_6[slice33],\r\n",
        "       train_regret_loser_7[slice33],\r\n",
        "       train_regret_loser_8[slice33],\r\n",
        "       train_regret_loser_9[slice33],\r\n",
        "       train_regret_loser_10[slice33],\r\n",
        "       train_regret_loser_11[slice33],\r\n",
        "       train_regret_loser_12[slice33],\r\n",
        "       train_regret_loser_13[slice33],\r\n",
        "       train_regret_loser_14[slice33],\r\n",
        "       train_regret_loser_15[slice33],\r\n",
        "       train_regret_loser_16[slice33],\r\n",
        "       train_regret_loser_17[slice33],\r\n",
        "       train_regret_loser_18[slice33],\r\n",
        "       train_regret_loser_19[slice33],\r\n",
        "       train_regret_loser_20[slice33]]\r\n",
        "\r\n",
        "winner33 = [train_regret_winner_1[slice33],\r\n",
        "       train_regret_winner_2[slice33],\r\n",
        "       train_regret_winner_3[slice33],\r\n",
        "       train_regret_winner_4[slice33],\r\n",
        "       train_regret_winner_5[slice33],\r\n",
        "       train_regret_winner_6[slice33],\r\n",
        "       train_regret_winner_7[slice33],\r\n",
        "       train_regret_winner_8[slice33],\r\n",
        "       train_regret_winner_9[slice33],\r\n",
        "       train_regret_winner_10[slice33],\r\n",
        "       train_regret_winner_11[slice33],\r\n",
        "       train_regret_winner_12[slice33],\r\n",
        "       train_regret_winner_13[slice33],\r\n",
        "       train_regret_winner_14[slice33],\r\n",
        "       train_regret_winner_15[slice33],\r\n",
        "       train_regret_winner_16[slice33],\r\n",
        "       train_regret_winner_17[slice33],\r\n",
        "       train_regret_winner_18[slice33],\r\n",
        "       train_regret_winner_19[slice33],\r\n",
        "       train_regret_winner_20[slice33]]\r\n",
        "\r\n",
        "loser33_results = pd.DataFrame(loser33).sort_values(by=[0], ascending=False)\r\n",
        "winner33_results = pd.DataFrame(winner33).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser33 = np.asarray(loser33_results[4:5][0])[0]\r\n",
        "median_loser33 = np.asarray(loser33_results[9:10][0])[0]\r\n",
        "upper_loser33 = np.asarray(loser33_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner33 = np.asarray(winner33_results[4:5][0])[0]\r\n",
        "median_winner33 = np.asarray(winner33_results[9:10][0])[0]\r\n",
        "upper_winner33 = np.asarray(winner33_results[14:15][0])[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4EVxQwsxw-9n"
      },
      "source": [
        "# Iteration43 :\r\n",
        "\r\n",
        "slice43 = 42\r\n",
        "\r\n",
        "loser43 = [train_regret_loser_1[slice43],\r\n",
        "       train_regret_loser_2[slice43],\r\n",
        "       train_regret_loser_3[slice43],\r\n",
        "       train_regret_loser_4[slice43],\r\n",
        "       train_regret_loser_5[slice43],\r\n",
        "       train_regret_loser_6[slice43],\r\n",
        "       train_regret_loser_7[slice43],\r\n",
        "       train_regret_loser_8[slice43],\r\n",
        "       train_regret_loser_9[slice43],\r\n",
        "       train_regret_loser_10[slice43],\r\n",
        "       train_regret_loser_11[slice43],\r\n",
        "       train_regret_loser_12[slice43],\r\n",
        "       train_regret_loser_13[slice43],\r\n",
        "       train_regret_loser_14[slice43],\r\n",
        "       train_regret_loser_15[slice43],\r\n",
        "       train_regret_loser_16[slice43],\r\n",
        "       train_regret_loser_17[slice43],\r\n",
        "       train_regret_loser_18[slice43],\r\n",
        "       train_regret_loser_19[slice43],\r\n",
        "       train_regret_loser_20[slice43]]\r\n",
        "\r\n",
        "winner43 = [train_regret_winner_1[slice43],\r\n",
        "       train_regret_winner_2[slice43],\r\n",
        "       train_regret_winner_3[slice43],\r\n",
        "       train_regret_winner_4[slice43],\r\n",
        "       train_regret_winner_5[slice43],\r\n",
        "       train_regret_winner_6[slice43],\r\n",
        "       train_regret_winner_7[slice43],\r\n",
        "       train_regret_winner_8[slice43],\r\n",
        "       train_regret_winner_9[slice43],\r\n",
        "       train_regret_winner_10[slice43],\r\n",
        "       train_regret_winner_11[slice43],\r\n",
        "       train_regret_winner_12[slice43],\r\n",
        "       train_regret_winner_13[slice43],\r\n",
        "       train_regret_winner_14[slice43],\r\n",
        "       train_regret_winner_15[slice43],\r\n",
        "       train_regret_winner_16[slice43],\r\n",
        "       train_regret_winner_17[slice43],\r\n",
        "       train_regret_winner_18[slice43],\r\n",
        "       train_regret_winner_19[slice43],\r\n",
        "       train_regret_winner_20[slice43]]\r\n",
        "\r\n",
        "loser43_results = pd.DataFrame(loser43).sort_values(by=[0], ascending=False)\r\n",
        "winner43_results = pd.DataFrame(winner43).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser43 = np.asarray(loser43_results[4:5][0])[0]\r\n",
        "median_loser43 = np.asarray(loser43_results[9:10][0])[0]\r\n",
        "upper_loser43 = np.asarray(loser43_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner43 = np.asarray(winner43_results[4:5][0])[0]\r\n",
        "median_winner43 = np.asarray(winner43_results[9:10][0])[0]\r\n",
        "upper_winner43 = np.asarray(winner43_results[14:15][0])[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQtRyMv-w_AZ"
      },
      "source": [
        "# Iteration53 :\r\n",
        "\r\n",
        "slice53 = 52\r\n",
        "\r\n",
        "loser53 = [train_regret_loser_1[slice53],\r\n",
        "       train_regret_loser_2[slice53],\r\n",
        "       train_regret_loser_3[slice53],\r\n",
        "       train_regret_loser_4[slice53],\r\n",
        "       train_regret_loser_5[slice53],\r\n",
        "       train_regret_loser_6[slice53],\r\n",
        "       train_regret_loser_7[slice53],\r\n",
        "       train_regret_loser_8[slice53],\r\n",
        "       train_regret_loser_9[slice53],\r\n",
        "       train_regret_loser_10[slice53],\r\n",
        "       train_regret_loser_11[slice53],\r\n",
        "       train_regret_loser_12[slice53],\r\n",
        "       train_regret_loser_13[slice53],\r\n",
        "       train_regret_loser_14[slice53],\r\n",
        "       train_regret_loser_15[slice53],\r\n",
        "       train_regret_loser_16[slice53],\r\n",
        "       train_regret_loser_17[slice53],\r\n",
        "       train_regret_loser_18[slice53],\r\n",
        "       train_regret_loser_19[slice53],\r\n",
        "       train_regret_loser_20[slice53]]\r\n",
        "\r\n",
        "winner53 = [train_regret_winner_1[slice53],\r\n",
        "       train_regret_winner_2[slice53],\r\n",
        "       train_regret_winner_3[slice53],\r\n",
        "       train_regret_winner_4[slice53],\r\n",
        "       train_regret_winner_5[slice53],\r\n",
        "       train_regret_winner_6[slice53],\r\n",
        "       train_regret_winner_7[slice53],\r\n",
        "       train_regret_winner_8[slice53],\r\n",
        "       train_regret_winner_9[slice53],\r\n",
        "       train_regret_winner_10[slice53],\r\n",
        "       train_regret_winner_11[slice53],\r\n",
        "       train_regret_winner_12[slice53],\r\n",
        "       train_regret_winner_13[slice53],\r\n",
        "       train_regret_winner_14[slice53],\r\n",
        "       train_regret_winner_15[slice53],\r\n",
        "       train_regret_winner_16[slice53],\r\n",
        "       train_regret_winner_17[slice53],\r\n",
        "       train_regret_winner_18[slice53],\r\n",
        "       train_regret_winner_19[slice53],\r\n",
        "       train_regret_winner_20[slice53]]\r\n",
        "\r\n",
        "loser53_results = pd.DataFrame(loser53).sort_values(by=[0], ascending=False)\r\n",
        "winner53_results = pd.DataFrame(winner53).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser53 = np.asarray(loser53_results[4:5][0])[0]\r\n",
        "median_loser53 = np.asarray(loser53_results[9:10][0])[0]\r\n",
        "upper_loser53 = np.asarray(loser53_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner53 = np.asarray(winner53_results[4:5][0])[0]\r\n",
        "median_winner53 = np.asarray(winner53_results[9:10][0])[0]\r\n",
        "upper_winner53 = np.asarray(winner53_results[14:15][0])[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1CIMEXXYw_DR"
      },
      "source": [
        "# Iteration63 :\r\n",
        "\r\n",
        "slice63 = 62\r\n",
        "\r\n",
        "loser63 = [train_regret_loser_1[slice63],\r\n",
        "       train_regret_loser_2[slice63],\r\n",
        "       train_regret_loser_3[slice63],\r\n",
        "       train_regret_loser_4[slice63],\r\n",
        "       train_regret_loser_5[slice63],\r\n",
        "       train_regret_loser_6[slice63],\r\n",
        "       train_regret_loser_7[slice63],\r\n",
        "       train_regret_loser_8[slice63],\r\n",
        "       train_regret_loser_9[slice63],\r\n",
        "       train_regret_loser_10[slice63],\r\n",
        "       train_regret_loser_11[slice63],\r\n",
        "       train_regret_loser_12[slice63],\r\n",
        "       train_regret_loser_13[slice63],\r\n",
        "       train_regret_loser_14[slice63],\r\n",
        "       train_regret_loser_15[slice63],\r\n",
        "       train_regret_loser_16[slice63],\r\n",
        "       train_regret_loser_17[slice63],\r\n",
        "       train_regret_loser_18[slice63],\r\n",
        "       train_regret_loser_19[slice63],\r\n",
        "       train_regret_loser_20[slice63]]\r\n",
        "\r\n",
        "winner63 = [train_regret_winner_1[slice63],\r\n",
        "       train_regret_winner_2[slice63],\r\n",
        "       train_regret_winner_3[slice63],\r\n",
        "       train_regret_winner_4[slice63],\r\n",
        "       train_regret_winner_5[slice63],\r\n",
        "       train_regret_winner_6[slice63],\r\n",
        "       train_regret_winner_7[slice63],\r\n",
        "       train_regret_winner_8[slice63],\r\n",
        "       train_regret_winner_9[slice63],\r\n",
        "       train_regret_winner_10[slice63],\r\n",
        "       train_regret_winner_11[slice63],\r\n",
        "       train_regret_winner_12[slice63],\r\n",
        "       train_regret_winner_13[slice63],\r\n",
        "       train_regret_winner_14[slice63],\r\n",
        "       train_regret_winner_15[slice63],\r\n",
        "       train_regret_winner_16[slice63],\r\n",
        "       train_regret_winner_17[slice63],\r\n",
        "       train_regret_winner_18[slice63],\r\n",
        "       train_regret_winner_19[slice63],\r\n",
        "       train_regret_winner_20[slice63]]\r\n",
        "\r\n",
        "loser63_results = pd.DataFrame(loser63).sort_values(by=[0], ascending=False)\r\n",
        "winner63_results = pd.DataFrame(winner63).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser63 = np.asarray(loser63_results[4:5][0])[0]\r\n",
        "median_loser63 = np.asarray(loser63_results[9:10][0])[0]\r\n",
        "upper_loser63 = np.asarray(loser63_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner63 = np.asarray(winner63_results[4:5][0])[0]\r\n",
        "median_winner63 = np.asarray(winner63_results[9:10][0])[0]\r\n",
        "upper_winner63 = np.asarray(winner63_results[14:15][0])[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xg4GfwU_w_GF"
      },
      "source": [
        "# Iteration73 :\r\n",
        "\r\n",
        "slice73 = 72\r\n",
        "\r\n",
        "loser73 = [train_regret_loser_1[slice73],\r\n",
        "       train_regret_loser_2[slice73],\r\n",
        "       train_regret_loser_3[slice73],\r\n",
        "       train_regret_loser_4[slice73],\r\n",
        "       train_regret_loser_5[slice73],\r\n",
        "       train_regret_loser_6[slice73],\r\n",
        "       train_regret_loser_7[slice73],\r\n",
        "       train_regret_loser_8[slice73],\r\n",
        "       train_regret_loser_9[slice73],\r\n",
        "       train_regret_loser_10[slice73],\r\n",
        "       train_regret_loser_11[slice73],\r\n",
        "       train_regret_loser_12[slice73],\r\n",
        "       train_regret_loser_13[slice73],\r\n",
        "       train_regret_loser_14[slice73],\r\n",
        "       train_regret_loser_15[slice73],\r\n",
        "       train_regret_loser_16[slice73],\r\n",
        "       train_regret_loser_17[slice73],\r\n",
        "       train_regret_loser_18[slice73],\r\n",
        "       train_regret_loser_19[slice73],\r\n",
        "       train_regret_loser_20[slice73]]\r\n",
        "\r\n",
        "winner73 = [train_regret_winner_1[slice73],\r\n",
        "       train_regret_winner_2[slice73],\r\n",
        "       train_regret_winner_3[slice73],\r\n",
        "       train_regret_winner_4[slice73],\r\n",
        "       train_regret_winner_5[slice73],\r\n",
        "       train_regret_winner_6[slice73],\r\n",
        "       train_regret_winner_7[slice73],\r\n",
        "       train_regret_winner_8[slice73],\r\n",
        "       train_regret_winner_9[slice73],\r\n",
        "       train_regret_winner_10[slice73],\r\n",
        "       train_regret_winner_11[slice73],\r\n",
        "       train_regret_winner_12[slice73],\r\n",
        "       train_regret_winner_13[slice73],\r\n",
        "       train_regret_winner_14[slice73],\r\n",
        "       train_regret_winner_15[slice73],\r\n",
        "       train_regret_winner_16[slice73],\r\n",
        "       train_regret_winner_17[slice73],\r\n",
        "       train_regret_winner_18[slice73],\r\n",
        "       train_regret_winner_19[slice73],\r\n",
        "       train_regret_winner_20[slice73]]\r\n",
        "\r\n",
        "loser73_results = pd.DataFrame(loser73).sort_values(by=[0], ascending=False)\r\n",
        "winner73_results = pd.DataFrame(winner73).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser73 = np.asarray(loser73_results[4:5][0])[0]\r\n",
        "median_loser73 = np.asarray(loser73_results[9:10][0])[0]\r\n",
        "upper_loser73 = np.asarray(loser73_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner73 = np.asarray(winner73_results[4:5][0])[0]\r\n",
        "median_winner73 = np.asarray(winner73_results[9:10][0])[0]\r\n",
        "upper_winner73 = np.asarray(winner73_results[14:15][0])[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WmpcEV4w_I9"
      },
      "source": [
        "# Iteration83 :\r\n",
        "\r\n",
        "slice83 = 82\r\n",
        "\r\n",
        "loser83 = [train_regret_loser_1[slice83],\r\n",
        "       train_regret_loser_2[slice83],\r\n",
        "       train_regret_loser_3[slice83],\r\n",
        "       train_regret_loser_4[slice83],\r\n",
        "       train_regret_loser_5[slice83],\r\n",
        "       train_regret_loser_6[slice83],\r\n",
        "       train_regret_loser_7[slice83],\r\n",
        "       train_regret_loser_8[slice83],\r\n",
        "       train_regret_loser_9[slice83],\r\n",
        "       train_regret_loser_10[slice83],\r\n",
        "       train_regret_loser_11[slice83],\r\n",
        "       train_regret_loser_12[slice83],\r\n",
        "       train_regret_loser_13[slice83],\r\n",
        "       train_regret_loser_14[slice83],\r\n",
        "       train_regret_loser_15[slice83],\r\n",
        "       train_regret_loser_16[slice83],\r\n",
        "       train_regret_loser_17[slice83],\r\n",
        "       train_regret_loser_18[slice83],\r\n",
        "       train_regret_loser_19[slice83],\r\n",
        "       train_regret_loser_20[slice83]]\r\n",
        "\r\n",
        "winner83 = [train_regret_winner_1[slice83],\r\n",
        "       train_regret_winner_2[slice83],\r\n",
        "       train_regret_winner_3[slice83],\r\n",
        "       train_regret_winner_4[slice83],\r\n",
        "       train_regret_winner_5[slice83],\r\n",
        "       train_regret_winner_6[slice83],\r\n",
        "       train_regret_winner_7[slice83],\r\n",
        "       train_regret_winner_8[slice83],\r\n",
        "       train_regret_winner_9[slice83],\r\n",
        "       train_regret_winner_10[slice83],\r\n",
        "       train_regret_winner_11[slice83],\r\n",
        "       train_regret_winner_12[slice83],\r\n",
        "       train_regret_winner_13[slice83],\r\n",
        "       train_regret_winner_14[slice83],\r\n",
        "       train_regret_winner_15[slice83],\r\n",
        "       train_regret_winner_16[slice83],\r\n",
        "       train_regret_winner_17[slice83],\r\n",
        "       train_regret_winner_18[slice83],\r\n",
        "       train_regret_winner_19[slice83],\r\n",
        "       train_regret_winner_20[slice83]]\r\n",
        "\r\n",
        "loser83_results = pd.DataFrame(loser83).sort_values(by=[0], ascending=False)\r\n",
        "winner83_results = pd.DataFrame(winner83).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser83 = np.asarray(loser83_results[4:5][0])[0]\r\n",
        "median_loser83 = np.asarray(loser83_results[9:10][0])[0]\r\n",
        "upper_loser83 = np.asarray(loser83_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner83 = np.asarray(winner83_results[4:5][0])[0]\r\n",
        "median_winner83 = np.asarray(winner83_results[9:10][0])[0]\r\n",
        "upper_winner83 = np.asarray(winner83_results[14:15][0])[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGPwoBZyw_MF"
      },
      "source": [
        "# Iteration93 :\r\n",
        "\r\n",
        "slice93 = 92\r\n",
        "\r\n",
        "loser93 = [train_regret_loser_1[slice93],\r\n",
        "       train_regret_loser_2[slice93],\r\n",
        "       train_regret_loser_3[slice93],\r\n",
        "       train_regret_loser_4[slice93],\r\n",
        "       train_regret_loser_5[slice93],\r\n",
        "       train_regret_loser_6[slice93],\r\n",
        "       train_regret_loser_7[slice93],\r\n",
        "       train_regret_loser_8[slice93],\r\n",
        "       train_regret_loser_9[slice93],\r\n",
        "       train_regret_loser_10[slice93],\r\n",
        "       train_regret_loser_11[slice93],\r\n",
        "       train_regret_loser_12[slice93],\r\n",
        "       train_regret_loser_13[slice93],\r\n",
        "       train_regret_loser_14[slice93],\r\n",
        "       train_regret_loser_15[slice93],\r\n",
        "       train_regret_loser_16[slice93],\r\n",
        "       train_regret_loser_17[slice93],\r\n",
        "       train_regret_loser_18[slice93],\r\n",
        "       train_regret_loser_19[slice93],\r\n",
        "       train_regret_loser_20[slice93]]\r\n",
        "\r\n",
        "winner93 = [train_regret_winner_1[slice93],\r\n",
        "       train_regret_winner_2[slice93],\r\n",
        "       train_regret_winner_3[slice93],\r\n",
        "       train_regret_winner_4[slice93],\r\n",
        "       train_regret_winner_5[slice93],\r\n",
        "       train_regret_winner_6[slice93],\r\n",
        "       train_regret_winner_7[slice93],\r\n",
        "       train_regret_winner_8[slice93],\r\n",
        "       train_regret_winner_9[slice93],\r\n",
        "       train_regret_winner_10[slice93],\r\n",
        "       train_regret_winner_11[slice93],\r\n",
        "       train_regret_winner_12[slice93],\r\n",
        "       train_regret_winner_13[slice93],\r\n",
        "       train_regret_winner_14[slice93],\r\n",
        "       train_regret_winner_15[slice93],\r\n",
        "       train_regret_winner_16[slice93],\r\n",
        "       train_regret_winner_17[slice93],\r\n",
        "       train_regret_winner_18[slice93],\r\n",
        "       train_regret_winner_19[slice93],\r\n",
        "       train_regret_winner_20[slice93]]\r\n",
        "\r\n",
        "loser93_results = pd.DataFrame(loser93).sort_values(by=[0], ascending=False)\r\n",
        "winner93_results = pd.DataFrame(winner93).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser93 = np.asarray(loser93_results[4:5][0])[0]\r\n",
        "median_loser93 = np.asarray(loser93_results[9:10][0])[0]\r\n",
        "upper_loser93 = np.asarray(loser93_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner93 = np.asarray(winner93_results[4:5][0])[0]\r\n",
        "median_winner93 = np.asarray(winner93_results[9:10][0])[0]\r\n",
        "upper_winner93 = np.asarray(winner93_results[14:15][0])[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_ghdGzPw_Oq"
      },
      "source": [
        "# Iteration4 :\r\n",
        "\r\n",
        "slice4 = 3\r\n",
        "\r\n",
        "loser4 = [train_regret_loser_1[slice4],\r\n",
        "       train_regret_loser_2[slice4],\r\n",
        "       train_regret_loser_3[slice4],\r\n",
        "       train_regret_loser_4[slice4],\r\n",
        "       train_regret_loser_5[slice4],\r\n",
        "       train_regret_loser_6[slice4],\r\n",
        "       train_regret_loser_7[slice4],\r\n",
        "       train_regret_loser_8[slice4],\r\n",
        "       train_regret_loser_9[slice4],\r\n",
        "       train_regret_loser_10[slice4],\r\n",
        "       train_regret_loser_11[slice4],\r\n",
        "       train_regret_loser_12[slice4],\r\n",
        "       train_regret_loser_13[slice4],\r\n",
        "       train_regret_loser_14[slice4],\r\n",
        "       train_regret_loser_15[slice4],\r\n",
        "       train_regret_loser_16[slice4],\r\n",
        "       train_regret_loser_17[slice4],\r\n",
        "       train_regret_loser_18[slice4],\r\n",
        "       train_regret_loser_19[slice4],\r\n",
        "       train_regret_loser_20[slice4]]\r\n",
        "\r\n",
        "winner4 = [train_regret_winner_1[slice4],\r\n",
        "       train_regret_winner_2[slice4],\r\n",
        "       train_regret_winner_3[slice4],\r\n",
        "       train_regret_winner_4[slice4],\r\n",
        "       train_regret_winner_5[slice4],\r\n",
        "       train_regret_winner_6[slice4],\r\n",
        "       train_regret_winner_7[slice4],\r\n",
        "       train_regret_winner_8[slice4],\r\n",
        "       train_regret_winner_9[slice4],\r\n",
        "       train_regret_winner_10[slice4],\r\n",
        "       train_regret_winner_11[slice4],\r\n",
        "       train_regret_winner_12[slice4],\r\n",
        "       train_regret_winner_13[slice4],\r\n",
        "       train_regret_winner_14[slice4],\r\n",
        "       train_regret_winner_15[slice4],\r\n",
        "       train_regret_winner_16[slice4],\r\n",
        "       train_regret_winner_17[slice4],\r\n",
        "       train_regret_winner_18[slice4],\r\n",
        "       train_regret_winner_19[slice4],\r\n",
        "       train_regret_winner_20[slice4]]\r\n",
        "\r\n",
        "loser4_results = pd.DataFrame(loser4).sort_values(by=[0], ascending=False)\r\n",
        "winner4_results = pd.DataFrame(winner4).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser4 = np.asarray(loser4_results[4:5][0])[0]\r\n",
        "median_loser4 = np.asarray(loser4_results[9:10][0])[0]\r\n",
        "upper_loser4 = np.asarray(loser4_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner4 = np.asarray(winner4_results[4:5][0])[0]\r\n",
        "median_winner4 = np.asarray(winner4_results[9:10][0])[0]\r\n",
        "upper_winner4 = np.asarray(winner4_results[14:15][0])[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNvDt4QMw_Rz"
      },
      "source": [
        "# Iteration14 :\r\n",
        "\r\n",
        "slice14 = 13\r\n",
        "\r\n",
        "loser14 = [train_regret_loser_1[slice14],\r\n",
        "       train_regret_loser_2[slice14],\r\n",
        "       train_regret_loser_3[slice14],\r\n",
        "       train_regret_loser_4[slice14],\r\n",
        "       train_regret_loser_5[slice14],\r\n",
        "       train_regret_loser_6[slice14],\r\n",
        "       train_regret_loser_7[slice14],\r\n",
        "       train_regret_loser_8[slice14],\r\n",
        "       train_regret_loser_9[slice14],\r\n",
        "       train_regret_loser_10[slice14],\r\n",
        "       train_regret_loser_11[slice14],\r\n",
        "       train_regret_loser_12[slice14],\r\n",
        "       train_regret_loser_13[slice14],\r\n",
        "       train_regret_loser_14[slice14],\r\n",
        "       train_regret_loser_15[slice14],\r\n",
        "       train_regret_loser_16[slice14],\r\n",
        "       train_regret_loser_17[slice14],\r\n",
        "       train_regret_loser_18[slice14],\r\n",
        "       train_regret_loser_19[slice14],\r\n",
        "       train_regret_loser_20[slice14]]\r\n",
        "\r\n",
        "winner14 = [train_regret_winner_1[slice14],\r\n",
        "       train_regret_winner_2[slice14],\r\n",
        "       train_regret_winner_3[slice14],\r\n",
        "       train_regret_winner_4[slice14],\r\n",
        "       train_regret_winner_5[slice14],\r\n",
        "       train_regret_winner_6[slice14],\r\n",
        "       train_regret_winner_7[slice14],\r\n",
        "       train_regret_winner_8[slice14],\r\n",
        "       train_regret_winner_9[slice14],\r\n",
        "       train_regret_winner_10[slice14],\r\n",
        "       train_regret_winner_11[slice14],\r\n",
        "       train_regret_winner_12[slice14],\r\n",
        "       train_regret_winner_13[slice14],\r\n",
        "       train_regret_winner_14[slice14],\r\n",
        "       train_regret_winner_15[slice14],\r\n",
        "       train_regret_winner_16[slice14],\r\n",
        "       train_regret_winner_17[slice14],\r\n",
        "       train_regret_winner_18[slice14],\r\n",
        "       train_regret_winner_19[slice14],\r\n",
        "       train_regret_winner_20[slice14]]\r\n",
        "\r\n",
        "loser14_results = pd.DataFrame(loser14).sort_values(by=[0], ascending=False)\r\n",
        "winner14_results = pd.DataFrame(winner14).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser14 = np.asarray(loser14_results[4:5][0])[0]\r\n",
        "median_loser14 = np.asarray(loser14_results[9:10][0])[0]\r\n",
        "upper_loser14 = np.asarray(loser14_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner14 = np.asarray(winner14_results[4:5][0])[0]\r\n",
        "median_winner14 = np.asarray(winner14_results[9:10][0])[0]\r\n",
        "upper_winner14 = np.asarray(winner14_results[14:15][0])[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zx3kMQV0w_Ua"
      },
      "source": [
        "# Iteration24 :\r\n",
        "\r\n",
        "slice24 = 23\r\n",
        "\r\n",
        "loser24 = [train_regret_loser_1[slice24],\r\n",
        "       train_regret_loser_2[slice24],\r\n",
        "       train_regret_loser_3[slice24],\r\n",
        "       train_regret_loser_4[slice24],\r\n",
        "       train_regret_loser_5[slice24],\r\n",
        "       train_regret_loser_6[slice24],\r\n",
        "       train_regret_loser_7[slice24],\r\n",
        "       train_regret_loser_8[slice24],\r\n",
        "       train_regret_loser_9[slice24],\r\n",
        "       train_regret_loser_10[slice24],\r\n",
        "       train_regret_loser_11[slice24],\r\n",
        "       train_regret_loser_12[slice24],\r\n",
        "       train_regret_loser_13[slice24],\r\n",
        "       train_regret_loser_14[slice24],\r\n",
        "       train_regret_loser_15[slice24],\r\n",
        "       train_regret_loser_16[slice24],\r\n",
        "       train_regret_loser_17[slice24],\r\n",
        "       train_regret_loser_18[slice24],\r\n",
        "       train_regret_loser_19[slice24],\r\n",
        "       train_regret_loser_20[slice24]]\r\n",
        "\r\n",
        "winner24 = [train_regret_winner_1[slice24],\r\n",
        "       train_regret_winner_2[slice24],\r\n",
        "       train_regret_winner_3[slice24],\r\n",
        "       train_regret_winner_4[slice24],\r\n",
        "       train_regret_winner_5[slice24],\r\n",
        "       train_regret_winner_6[slice24],\r\n",
        "       train_regret_winner_7[slice24],\r\n",
        "       train_regret_winner_8[slice24],\r\n",
        "       train_regret_winner_9[slice24],\r\n",
        "       train_regret_winner_10[slice24],\r\n",
        "       train_regret_winner_11[slice24],\r\n",
        "       train_regret_winner_12[slice24],\r\n",
        "       train_regret_winner_13[slice24],\r\n",
        "       train_regret_winner_14[slice24],\r\n",
        "       train_regret_winner_15[slice24],\r\n",
        "       train_regret_winner_16[slice24],\r\n",
        "       train_regret_winner_17[slice24],\r\n",
        "       train_regret_winner_18[slice24],\r\n",
        "       train_regret_winner_19[slice24],\r\n",
        "       train_regret_winner_20[slice24]]\r\n",
        "\r\n",
        "loser24_results = pd.DataFrame(loser24).sort_values(by=[0], ascending=False)\r\n",
        "winner24_results = pd.DataFrame(winner24).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser24 = np.asarray(loser24_results[4:5][0])[0]\r\n",
        "median_loser24 = np.asarray(loser24_results[9:10][0])[0]\r\n",
        "upper_loser24 = np.asarray(loser24_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner24 = np.asarray(winner24_results[4:5][0])[0]\r\n",
        "median_winner24 = np.asarray(winner24_results[9:10][0])[0]\r\n",
        "upper_winner24 = np.asarray(winner24_results[14:15][0])[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4ljkyMfw_Xj"
      },
      "source": [
        "# Iteration34 :\r\n",
        "\r\n",
        "slice34 = 33\r\n",
        "\r\n",
        "loser34 = [train_regret_loser_1[slice34],\r\n",
        "       train_regret_loser_2[slice34],\r\n",
        "       train_regret_loser_3[slice34],\r\n",
        "       train_regret_loser_4[slice34],\r\n",
        "       train_regret_loser_5[slice34],\r\n",
        "       train_regret_loser_6[slice34],\r\n",
        "       train_regret_loser_7[slice34],\r\n",
        "       train_regret_loser_8[slice34],\r\n",
        "       train_regret_loser_9[slice34],\r\n",
        "       train_regret_loser_10[slice34],\r\n",
        "       train_regret_loser_11[slice34],\r\n",
        "       train_regret_loser_12[slice34],\r\n",
        "       train_regret_loser_13[slice34],\r\n",
        "       train_regret_loser_14[slice34],\r\n",
        "       train_regret_loser_15[slice34],\r\n",
        "       train_regret_loser_16[slice34],\r\n",
        "       train_regret_loser_17[slice34],\r\n",
        "       train_regret_loser_18[slice34],\r\n",
        "       train_regret_loser_19[slice34],\r\n",
        "       train_regret_loser_20[slice34]]\r\n",
        "\r\n",
        "winner34 = [train_regret_winner_1[slice34],\r\n",
        "       train_regret_winner_2[slice34],\r\n",
        "       train_regret_winner_3[slice34],\r\n",
        "       train_regret_winner_4[slice34],\r\n",
        "       train_regret_winner_5[slice34],\r\n",
        "       train_regret_winner_6[slice34],\r\n",
        "       train_regret_winner_7[slice34],\r\n",
        "       train_regret_winner_8[slice34],\r\n",
        "       train_regret_winner_9[slice34],\r\n",
        "       train_regret_winner_10[slice34],\r\n",
        "       train_regret_winner_11[slice34],\r\n",
        "       train_regret_winner_12[slice34],\r\n",
        "       train_regret_winner_13[slice34],\r\n",
        "       train_regret_winner_14[slice34],\r\n",
        "       train_regret_winner_15[slice34],\r\n",
        "       train_regret_winner_16[slice34],\r\n",
        "       train_regret_winner_17[slice34],\r\n",
        "       train_regret_winner_18[slice34],\r\n",
        "       train_regret_winner_19[slice34],\r\n",
        "       train_regret_winner_20[slice34]]\r\n",
        "\r\n",
        "loser34_results = pd.DataFrame(loser34).sort_values(by=[0], ascending=False)\r\n",
        "winner34_results = pd.DataFrame(winner34).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser34 = np.asarray(loser34_results[4:5][0])[0]\r\n",
        "median_loser34 = np.asarray(loser34_results[9:10][0])[0]\r\n",
        "upper_loser34 = np.asarray(loser34_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner34 = np.asarray(winner34_results[4:5][0])[0]\r\n",
        "median_winner34 = np.asarray(winner34_results[9:10][0])[0]\r\n",
        "upper_winner34 = np.asarray(winner34_results[14:15][0])[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGKj-NR8w_ae"
      },
      "source": [
        "# Iteration44 :\r\n",
        "\r\n",
        "slice44 = 43\r\n",
        "\r\n",
        "loser44 = [train_regret_loser_1[slice44],\r\n",
        "       train_regret_loser_2[slice44],\r\n",
        "       train_regret_loser_3[slice44],\r\n",
        "       train_regret_loser_4[slice44],\r\n",
        "       train_regret_loser_5[slice44],\r\n",
        "       train_regret_loser_6[slice44],\r\n",
        "       train_regret_loser_7[slice44],\r\n",
        "       train_regret_loser_8[slice44],\r\n",
        "       train_regret_loser_9[slice44],\r\n",
        "       train_regret_loser_10[slice44],\r\n",
        "       train_regret_loser_11[slice44],\r\n",
        "       train_regret_loser_12[slice44],\r\n",
        "       train_regret_loser_13[slice44],\r\n",
        "       train_regret_loser_14[slice44],\r\n",
        "       train_regret_loser_15[slice44],\r\n",
        "       train_regret_loser_16[slice44],\r\n",
        "       train_regret_loser_17[slice44],\r\n",
        "       train_regret_loser_18[slice44],\r\n",
        "       train_regret_loser_19[slice44],\r\n",
        "       train_regret_loser_20[slice44]]\r\n",
        "\r\n",
        "winner44 = [train_regret_winner_1[slice44],\r\n",
        "       train_regret_winner_2[slice44],\r\n",
        "       train_regret_winner_3[slice44],\r\n",
        "       train_regret_winner_4[slice44],\r\n",
        "       train_regret_winner_5[slice44],\r\n",
        "       train_regret_winner_6[slice44],\r\n",
        "       train_regret_winner_7[slice44],\r\n",
        "       train_regret_winner_8[slice44],\r\n",
        "       train_regret_winner_9[slice44],\r\n",
        "       train_regret_winner_10[slice44],\r\n",
        "       train_regret_winner_11[slice44],\r\n",
        "       train_regret_winner_12[slice44],\r\n",
        "       train_regret_winner_13[slice44],\r\n",
        "       train_regret_winner_14[slice44],\r\n",
        "       train_regret_winner_15[slice44],\r\n",
        "       train_regret_winner_16[slice44],\r\n",
        "       train_regret_winner_17[slice44],\r\n",
        "       train_regret_winner_18[slice44],\r\n",
        "       train_regret_winner_19[slice44],\r\n",
        "       train_regret_winner_20[slice44]]\r\n",
        "\r\n",
        "loser44_results = pd.DataFrame(loser44).sort_values(by=[0], ascending=False)\r\n",
        "winner44_results = pd.DataFrame(winner44).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser44 = np.asarray(loser44_results[4:5][0])[0]\r\n",
        "median_loser44 = np.asarray(loser44_results[9:10][0])[0]\r\n",
        "upper_loser44 = np.asarray(loser44_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner44 = np.asarray(winner44_results[4:5][0])[0]\r\n",
        "median_winner44 = np.asarray(winner44_results[9:10][0])[0]\r\n",
        "upper_winner44 = np.asarray(winner44_results[14:15][0])[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJzx094Fw_c_"
      },
      "source": [
        "# Iteration54 :\r\n",
        "\r\n",
        "slice54 = 53\r\n",
        "\r\n",
        "loser54 = [train_regret_loser_1[slice54],\r\n",
        "       train_regret_loser_2[slice54],\r\n",
        "       train_regret_loser_3[slice54],\r\n",
        "       train_regret_loser_4[slice54],\r\n",
        "       train_regret_loser_5[slice54],\r\n",
        "       train_regret_loser_6[slice54],\r\n",
        "       train_regret_loser_7[slice54],\r\n",
        "       train_regret_loser_8[slice54],\r\n",
        "       train_regret_loser_9[slice54],\r\n",
        "       train_regret_loser_10[slice54],\r\n",
        "       train_regret_loser_11[slice54],\r\n",
        "       train_regret_loser_12[slice54],\r\n",
        "       train_regret_loser_13[slice54],\r\n",
        "       train_regret_loser_14[slice54],\r\n",
        "       train_regret_loser_15[slice54],\r\n",
        "       train_regret_loser_16[slice54],\r\n",
        "       train_regret_loser_17[slice54],\r\n",
        "       train_regret_loser_18[slice54],\r\n",
        "       train_regret_loser_19[slice54],\r\n",
        "       train_regret_loser_20[slice54]]\r\n",
        "\r\n",
        "winner54 = [train_regret_winner_1[slice54],\r\n",
        "       train_regret_winner_2[slice54],\r\n",
        "       train_regret_winner_3[slice54],\r\n",
        "       train_regret_winner_4[slice54],\r\n",
        "       train_regret_winner_5[slice54],\r\n",
        "       train_regret_winner_6[slice54],\r\n",
        "       train_regret_winner_7[slice54],\r\n",
        "       train_regret_winner_8[slice54],\r\n",
        "       train_regret_winner_9[slice54],\r\n",
        "       train_regret_winner_10[slice54],\r\n",
        "       train_regret_winner_11[slice54],\r\n",
        "       train_regret_winner_12[slice54],\r\n",
        "       train_regret_winner_13[slice54],\r\n",
        "       train_regret_winner_14[slice54],\r\n",
        "       train_regret_winner_15[slice54],\r\n",
        "       train_regret_winner_16[slice54],\r\n",
        "       train_regret_winner_17[slice54],\r\n",
        "       train_regret_winner_18[slice54],\r\n",
        "       train_regret_winner_19[slice54],\r\n",
        "       train_regret_winner_20[slice54]]\r\n",
        "\r\n",
        "loser54_results = pd.DataFrame(loser54).sort_values(by=[0], ascending=False)\r\n",
        "winner54_results = pd.DataFrame(winner54).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser54 = np.asarray(loser54_results[4:5][0])[0]\r\n",
        "median_loser54 = np.asarray(loser54_results[9:10][0])[0]\r\n",
        "upper_loser54 = np.asarray(loser54_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner54 = np.asarray(winner54_results[4:5][0])[0]\r\n",
        "median_winner54 = np.asarray(winner54_results[9:10][0])[0]\r\n",
        "upper_winner54 = np.asarray(winner54_results[14:15][0])[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEVYMqj_w_fr"
      },
      "source": [
        "# Iteration64 :\r\n",
        "\r\n",
        "slice64 = 63\r\n",
        "\r\n",
        "loser64 = [train_regret_loser_1[slice64],\r\n",
        "       train_regret_loser_2[slice64],\r\n",
        "       train_regret_loser_3[slice64],\r\n",
        "       train_regret_loser_4[slice64],\r\n",
        "       train_regret_loser_5[slice64],\r\n",
        "       train_regret_loser_6[slice64],\r\n",
        "       train_regret_loser_7[slice64],\r\n",
        "       train_regret_loser_8[slice64],\r\n",
        "       train_regret_loser_9[slice64],\r\n",
        "       train_regret_loser_10[slice64],\r\n",
        "       train_regret_loser_11[slice64],\r\n",
        "       train_regret_loser_12[slice64],\r\n",
        "       train_regret_loser_13[slice64],\r\n",
        "       train_regret_loser_14[slice64],\r\n",
        "       train_regret_loser_15[slice64],\r\n",
        "       train_regret_loser_16[slice64],\r\n",
        "       train_regret_loser_17[slice64],\r\n",
        "       train_regret_loser_18[slice64],\r\n",
        "       train_regret_loser_19[slice64],\r\n",
        "       train_regret_loser_20[slice64]]\r\n",
        "\r\n",
        "winner64 = [train_regret_winner_1[slice64],\r\n",
        "       train_regret_winner_2[slice64],\r\n",
        "       train_regret_winner_3[slice64],\r\n",
        "       train_regret_winner_4[slice64],\r\n",
        "       train_regret_winner_5[slice64],\r\n",
        "       train_regret_winner_6[slice64],\r\n",
        "       train_regret_winner_7[slice64],\r\n",
        "       train_regret_winner_8[slice64],\r\n",
        "       train_regret_winner_9[slice64],\r\n",
        "       train_regret_winner_10[slice64],\r\n",
        "       train_regret_winner_11[slice64],\r\n",
        "       train_regret_winner_12[slice64],\r\n",
        "       train_regret_winner_13[slice64],\r\n",
        "       train_regret_winner_14[slice64],\r\n",
        "       train_regret_winner_15[slice64],\r\n",
        "       train_regret_winner_16[slice64],\r\n",
        "       train_regret_winner_17[slice64],\r\n",
        "       train_regret_winner_18[slice64],\r\n",
        "       train_regret_winner_19[slice64],\r\n",
        "       train_regret_winner_20[slice64]]\r\n",
        "\r\n",
        "loser64_results = pd.DataFrame(loser64).sort_values(by=[0], ascending=False)\r\n",
        "winner64_results = pd.DataFrame(winner64).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser64 = np.asarray(loser64_results[4:5][0])[0]\r\n",
        "median_loser64 = np.asarray(loser64_results[9:10][0])[0]\r\n",
        "upper_loser64 = np.asarray(loser64_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner64 = np.asarray(winner64_results[4:5][0])[0]\r\n",
        "median_winner64 = np.asarray(winner64_results[9:10][0])[0]\r\n",
        "upper_winner64 = np.asarray(winner64_results[14:15][0])[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvr3S_p0w_ii"
      },
      "source": [
        "# Iteration74 :\r\n",
        "\r\n",
        "slice74 = 73\r\n",
        "\r\n",
        "loser74 = [train_regret_loser_1[slice74],\r\n",
        "       train_regret_loser_2[slice74],\r\n",
        "       train_regret_loser_3[slice74],\r\n",
        "       train_regret_loser_4[slice74],\r\n",
        "       train_regret_loser_5[slice74],\r\n",
        "       train_regret_loser_6[slice74],\r\n",
        "       train_regret_loser_7[slice74],\r\n",
        "       train_regret_loser_8[slice74],\r\n",
        "       train_regret_loser_9[slice74],\r\n",
        "       train_regret_loser_10[slice74],\r\n",
        "       train_regret_loser_11[slice74],\r\n",
        "       train_regret_loser_12[slice74],\r\n",
        "       train_regret_loser_13[slice74],\r\n",
        "       train_regret_loser_14[slice74],\r\n",
        "       train_regret_loser_15[slice74],\r\n",
        "       train_regret_loser_16[slice74],\r\n",
        "       train_regret_loser_17[slice74],\r\n",
        "       train_regret_loser_18[slice74],\r\n",
        "       train_regret_loser_19[slice74],\r\n",
        "       train_regret_loser_20[slice74]]\r\n",
        "\r\n",
        "winner74 = [train_regret_winner_1[slice74],\r\n",
        "       train_regret_winner_2[slice74],\r\n",
        "       train_regret_winner_3[slice74],\r\n",
        "       train_regret_winner_4[slice74],\r\n",
        "       train_regret_winner_5[slice74],\r\n",
        "       train_regret_winner_6[slice74],\r\n",
        "       train_regret_winner_7[slice74],\r\n",
        "       train_regret_winner_8[slice74],\r\n",
        "       train_regret_winner_9[slice74],\r\n",
        "       train_regret_winner_10[slice74],\r\n",
        "       train_regret_winner_11[slice74],\r\n",
        "       train_regret_winner_12[slice74],\r\n",
        "       train_regret_winner_13[slice74],\r\n",
        "       train_regret_winner_14[slice74],\r\n",
        "       train_regret_winner_15[slice74],\r\n",
        "       train_regret_winner_16[slice74],\r\n",
        "       train_regret_winner_17[slice74],\r\n",
        "       train_regret_winner_18[slice74],\r\n",
        "       train_regret_winner_19[slice74],\r\n",
        "       train_regret_winner_20[slice74]]\r\n",
        "\r\n",
        "loser74_results = pd.DataFrame(loser74).sort_values(by=[0], ascending=False)\r\n",
        "winner74_results = pd.DataFrame(winner74).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser74 = np.asarray(loser74_results[4:5][0])[0]\r\n",
        "median_loser74 = np.asarray(loser74_results[9:10][0])[0]\r\n",
        "upper_loser74 = np.asarray(loser74_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner74 = np.asarray(winner74_results[4:5][0])[0]\r\n",
        "median_winner74 = np.asarray(winner74_results[9:10][0])[0]\r\n",
        "upper_winner74 = np.asarray(winner74_results[14:15][0])[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEAvUVK7w_lO"
      },
      "source": [
        "# Iteration84 :\r\n",
        "\r\n",
        "slice84 = 83\r\n",
        "\r\n",
        "loser84 = [train_regret_loser_1[slice84],\r\n",
        "       train_regret_loser_2[slice84],\r\n",
        "       train_regret_loser_3[slice84],\r\n",
        "       train_regret_loser_4[slice84],\r\n",
        "       train_regret_loser_5[slice84],\r\n",
        "       train_regret_loser_6[slice84],\r\n",
        "       train_regret_loser_7[slice84],\r\n",
        "       train_regret_loser_8[slice84],\r\n",
        "       train_regret_loser_9[slice84],\r\n",
        "       train_regret_loser_10[slice84],\r\n",
        "       train_regret_loser_11[slice84],\r\n",
        "       train_regret_loser_12[slice84],\r\n",
        "       train_regret_loser_13[slice84],\r\n",
        "       train_regret_loser_14[slice84],\r\n",
        "       train_regret_loser_15[slice84],\r\n",
        "       train_regret_loser_16[slice84],\r\n",
        "       train_regret_loser_17[slice84],\r\n",
        "       train_regret_loser_18[slice84],\r\n",
        "       train_regret_loser_19[slice84],\r\n",
        "       train_regret_loser_20[slice84]]\r\n",
        "\r\n",
        "winner84 = [train_regret_winner_1[slice84],\r\n",
        "       train_regret_winner_2[slice84],\r\n",
        "       train_regret_winner_3[slice84],\r\n",
        "       train_regret_winner_4[slice84],\r\n",
        "       train_regret_winner_5[slice84],\r\n",
        "       train_regret_winner_6[slice84],\r\n",
        "       train_regret_winner_7[slice84],\r\n",
        "       train_regret_winner_8[slice84],\r\n",
        "       train_regret_winner_9[slice84],\r\n",
        "       train_regret_winner_10[slice84],\r\n",
        "       train_regret_winner_11[slice84],\r\n",
        "       train_regret_winner_12[slice84],\r\n",
        "       train_regret_winner_13[slice84],\r\n",
        "       train_regret_winner_14[slice84],\r\n",
        "       train_regret_winner_15[slice84],\r\n",
        "       train_regret_winner_16[slice84],\r\n",
        "       train_regret_winner_17[slice84],\r\n",
        "       train_regret_winner_18[slice84],\r\n",
        "       train_regret_winner_19[slice84],\r\n",
        "       train_regret_winner_20[slice84]]\r\n",
        "\r\n",
        "loser84_results = pd.DataFrame(loser84).sort_values(by=[0], ascending=False)\r\n",
        "winner84_results = pd.DataFrame(winner84).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser84 = np.asarray(loser84_results[4:5][0])[0]\r\n",
        "median_loser84 = np.asarray(loser84_results[9:10][0])[0]\r\n",
        "upper_loser84 = np.asarray(loser84_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner84 = np.asarray(winner84_results[4:5][0])[0]\r\n",
        "median_winner84 = np.asarray(winner84_results[9:10][0])[0]\r\n",
        "upper_winner84 = np.asarray(winner84_results[14:15][0])[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9TMZzhdGw_oC"
      },
      "source": [
        "# Iteration94 :\r\n",
        "\r\n",
        "slice94 = 93\r\n",
        "\r\n",
        "loser94 = [train_regret_loser_1[slice94],\r\n",
        "       train_regret_loser_2[slice94],\r\n",
        "       train_regret_loser_3[slice94],\r\n",
        "       train_regret_loser_4[slice94],\r\n",
        "       train_regret_loser_5[slice94],\r\n",
        "       train_regret_loser_6[slice94],\r\n",
        "       train_regret_loser_7[slice94],\r\n",
        "       train_regret_loser_8[slice94],\r\n",
        "       train_regret_loser_9[slice94],\r\n",
        "       train_regret_loser_10[slice94],\r\n",
        "       train_regret_loser_11[slice94],\r\n",
        "       train_regret_loser_12[slice94],\r\n",
        "       train_regret_loser_13[slice94],\r\n",
        "       train_regret_loser_14[slice94],\r\n",
        "       train_regret_loser_15[slice94],\r\n",
        "       train_regret_loser_16[slice94],\r\n",
        "       train_regret_loser_17[slice94],\r\n",
        "       train_regret_loser_18[slice94],\r\n",
        "       train_regret_loser_19[slice94],\r\n",
        "       train_regret_loser_20[slice94]]\r\n",
        "\r\n",
        "winner94 = [train_regret_winner_1[slice94],\r\n",
        "       train_regret_winner_2[slice94],\r\n",
        "       train_regret_winner_3[slice94],\r\n",
        "       train_regret_winner_4[slice94],\r\n",
        "       train_regret_winner_5[slice94],\r\n",
        "       train_regret_winner_6[slice94],\r\n",
        "       train_regret_winner_7[slice94],\r\n",
        "       train_regret_winner_8[slice94],\r\n",
        "       train_regret_winner_9[slice94],\r\n",
        "       train_regret_winner_10[slice94],\r\n",
        "       train_regret_winner_11[slice94],\r\n",
        "       train_regret_winner_12[slice94],\r\n",
        "       train_regret_winner_13[slice94],\r\n",
        "       train_regret_winner_14[slice94],\r\n",
        "       train_regret_winner_15[slice94],\r\n",
        "       train_regret_winner_16[slice94],\r\n",
        "       train_regret_winner_17[slice94],\r\n",
        "       train_regret_winner_18[slice94],\r\n",
        "       train_regret_winner_19[slice94],\r\n",
        "       train_regret_winner_20[slice94]]\r\n",
        "\r\n",
        "loser94_results = pd.DataFrame(loser94).sort_values(by=[0], ascending=False)\r\n",
        "winner94_results = pd.DataFrame(winner94).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser94 = np.asarray(loser94_results[4:5][0])[0]\r\n",
        "median_loser94 = np.asarray(loser94_results[9:10][0])[0]\r\n",
        "upper_loser94 = np.asarray(loser94_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner94 = np.asarray(winner94_results[4:5][0])[0]\r\n",
        "median_winner94 = np.asarray(winner94_results[9:10][0])[0]\r\n",
        "upper_winner94 = np.asarray(winner94_results[14:15][0])[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMxHQOgaw_qu"
      },
      "source": [
        "# Iteration5 :\r\n",
        "\r\n",
        "slice5 = 4\r\n",
        "\r\n",
        "loser5 = [train_regret_loser_1[slice5],\r\n",
        "       train_regret_loser_2[slice5],\r\n",
        "       train_regret_loser_3[slice5],\r\n",
        "       train_regret_loser_4[slice5],\r\n",
        "       train_regret_loser_5[slice5],\r\n",
        "       train_regret_loser_6[slice5],\r\n",
        "       train_regret_loser_7[slice5],\r\n",
        "       train_regret_loser_8[slice5],\r\n",
        "       train_regret_loser_9[slice5],\r\n",
        "       train_regret_loser_10[slice5],\r\n",
        "       train_regret_loser_11[slice5],\r\n",
        "       train_regret_loser_12[slice5],\r\n",
        "       train_regret_loser_13[slice5],\r\n",
        "       train_regret_loser_14[slice5],\r\n",
        "       train_regret_loser_15[slice5],\r\n",
        "       train_regret_loser_16[slice5],\r\n",
        "       train_regret_loser_17[slice5],\r\n",
        "       train_regret_loser_18[slice5],\r\n",
        "       train_regret_loser_19[slice5],\r\n",
        "       train_regret_loser_20[slice5]]\r\n",
        "\r\n",
        "winner5 = [train_regret_winner_1[slice5],\r\n",
        "       train_regret_winner_2[slice5],\r\n",
        "       train_regret_winner_3[slice5],\r\n",
        "       train_regret_winner_4[slice5],\r\n",
        "       train_regret_winner_5[slice5],\r\n",
        "       train_regret_winner_6[slice5],\r\n",
        "       train_regret_winner_7[slice5],\r\n",
        "       train_regret_winner_8[slice5],\r\n",
        "       train_regret_winner_9[slice5],\r\n",
        "       train_regret_winner_10[slice5],\r\n",
        "       train_regret_winner_11[slice5],\r\n",
        "       train_regret_winner_12[slice5],\r\n",
        "       train_regret_winner_13[slice5],\r\n",
        "       train_regret_winner_14[slice5],\r\n",
        "       train_regret_winner_15[slice5],\r\n",
        "       train_regret_winner_16[slice5],\r\n",
        "       train_regret_winner_17[slice5],\r\n",
        "       train_regret_winner_18[slice5],\r\n",
        "       train_regret_winner_19[slice5],\r\n",
        "       train_regret_winner_20[slice5]]\r\n",
        "\r\n",
        "loser5_results = pd.DataFrame(loser5).sort_values(by=[0], ascending=False)\r\n",
        "winner5_results = pd.DataFrame(winner5).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser5 = np.asarray(loser5_results[4:5][0])[0]\r\n",
        "median_loser5 = np.asarray(loser5_results[9:10][0])[0]\r\n",
        "upper_loser5 = np.asarray(loser5_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner5 = np.asarray(winner5_results[4:5][0])[0]\r\n",
        "median_winner5 = np.asarray(winner5_results[9:10][0])[0]\r\n",
        "upper_winner5 = np.asarray(winner5_results[14:15][0])[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPVFQTfNw_wF"
      },
      "source": [
        "# Iteration15 :\r\n",
        "\r\n",
        "slice15 = 14\r\n",
        "\r\n",
        "loser15 = [train_regret_loser_1[slice15],\r\n",
        "       train_regret_loser_2[slice15],\r\n",
        "       train_regret_loser_3[slice15],\r\n",
        "       train_regret_loser_4[slice15],\r\n",
        "       train_regret_loser_5[slice15],\r\n",
        "       train_regret_loser_6[slice15],\r\n",
        "       train_regret_loser_7[slice15],\r\n",
        "       train_regret_loser_8[slice15],\r\n",
        "       train_regret_loser_9[slice15],\r\n",
        "       train_regret_loser_10[slice15],\r\n",
        "       train_regret_loser_11[slice15],\r\n",
        "       train_regret_loser_12[slice15],\r\n",
        "       train_regret_loser_13[slice15],\r\n",
        "       train_regret_loser_14[slice15],\r\n",
        "       train_regret_loser_15[slice15],\r\n",
        "       train_regret_loser_16[slice15],\r\n",
        "       train_regret_loser_17[slice15],\r\n",
        "       train_regret_loser_18[slice15],\r\n",
        "       train_regret_loser_19[slice15],\r\n",
        "       train_regret_loser_20[slice15]]\r\n",
        "\r\n",
        "winner15 = [train_regret_winner_1[slice15],\r\n",
        "       train_regret_winner_2[slice15],\r\n",
        "       train_regret_winner_3[slice15],\r\n",
        "       train_regret_winner_4[slice15],\r\n",
        "       train_regret_winner_5[slice15],\r\n",
        "       train_regret_winner_6[slice15],\r\n",
        "       train_regret_winner_7[slice15],\r\n",
        "       train_regret_winner_8[slice15],\r\n",
        "       train_regret_winner_9[slice15],\r\n",
        "       train_regret_winner_10[slice15],\r\n",
        "       train_regret_winner_11[slice15],\r\n",
        "       train_regret_winner_12[slice15],\r\n",
        "       train_regret_winner_13[slice15],\r\n",
        "       train_regret_winner_14[slice15],\r\n",
        "       train_regret_winner_15[slice15],\r\n",
        "       train_regret_winner_16[slice15],\r\n",
        "       train_regret_winner_17[slice15],\r\n",
        "       train_regret_winner_18[slice15],\r\n",
        "       train_regret_winner_19[slice15],\r\n",
        "       train_regret_winner_20[slice15]]\r\n",
        "\r\n",
        "loser15_results = pd.DataFrame(loser15).sort_values(by=[0], ascending=False)\r\n",
        "winner15_results = pd.DataFrame(winner15).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser15 = np.asarray(loser15_results[4:5][0])[0]\r\n",
        "median_loser15 = np.asarray(loser15_results[9:10][0])[0]\r\n",
        "upper_loser15 = np.asarray(loser15_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner15 = np.asarray(winner15_results[4:5][0])[0]\r\n",
        "median_winner15 = np.asarray(winner15_results[9:10][0])[0]\r\n",
        "upper_winner15 = np.asarray(winner15_results[14:15][0])[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADBmBA3ew_zK"
      },
      "source": [
        "# Iteration25 :\r\n",
        "\r\n",
        "slice25 = 24\r\n",
        "\r\n",
        "loser25 = [train_regret_loser_1[slice25],\r\n",
        "       train_regret_loser_2[slice25],\r\n",
        "       train_regret_loser_3[slice25],\r\n",
        "       train_regret_loser_4[slice25],\r\n",
        "       train_regret_loser_5[slice25],\r\n",
        "       train_regret_loser_6[slice25],\r\n",
        "       train_regret_loser_7[slice25],\r\n",
        "       train_regret_loser_8[slice25],\r\n",
        "       train_regret_loser_9[slice25],\r\n",
        "       train_regret_loser_10[slice25],\r\n",
        "       train_regret_loser_11[slice25],\r\n",
        "       train_regret_loser_12[slice25],\r\n",
        "       train_regret_loser_13[slice25],\r\n",
        "       train_regret_loser_14[slice25],\r\n",
        "       train_regret_loser_15[slice25],\r\n",
        "       train_regret_loser_16[slice25],\r\n",
        "       train_regret_loser_17[slice25],\r\n",
        "       train_regret_loser_18[slice25],\r\n",
        "       train_regret_loser_19[slice25],\r\n",
        "       train_regret_loser_20[slice25]]\r\n",
        "\r\n",
        "winner25 = [train_regret_winner_1[slice25],\r\n",
        "       train_regret_winner_2[slice25],\r\n",
        "       train_regret_winner_3[slice25],\r\n",
        "       train_regret_winner_4[slice25],\r\n",
        "       train_regret_winner_5[slice25],\r\n",
        "       train_regret_winner_6[slice25],\r\n",
        "       train_regret_winner_7[slice25],\r\n",
        "       train_regret_winner_8[slice25],\r\n",
        "       train_regret_winner_9[slice25],\r\n",
        "       train_regret_winner_10[slice25],\r\n",
        "       train_regret_winner_11[slice25],\r\n",
        "       train_regret_winner_12[slice25],\r\n",
        "       train_regret_winner_13[slice25],\r\n",
        "       train_regret_winner_14[slice25],\r\n",
        "       train_regret_winner_15[slice25],\r\n",
        "       train_regret_winner_16[slice25],\r\n",
        "       train_regret_winner_17[slice25],\r\n",
        "       train_regret_winner_18[slice25],\r\n",
        "       train_regret_winner_19[slice25],\r\n",
        "       train_regret_winner_20[slice25]]\r\n",
        "\r\n",
        "loser25_results = pd.DataFrame(loser25).sort_values(by=[0], ascending=False)\r\n",
        "winner25_results = pd.DataFrame(winner25).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser25 = np.asarray(loser25_results[4:5][0])[0]\r\n",
        "median_loser25 = np.asarray(loser25_results[9:10][0])[0]\r\n",
        "upper_loser25 = np.asarray(loser25_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner25 = np.asarray(winner25_results[4:5][0])[0]\r\n",
        "median_winner25 = np.asarray(winner25_results[9:10][0])[0]\r\n",
        "upper_winner25 = np.asarray(winner25_results[14:15][0])[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUk9UtSTw_2N"
      },
      "source": [
        "# Iteration35 :\r\n",
        "\r\n",
        "slice35 = 34\r\n",
        "\r\n",
        "loser35 = [train_regret_loser_1[slice35],\r\n",
        "       train_regret_loser_2[slice35],\r\n",
        "       train_regret_loser_3[slice35],\r\n",
        "       train_regret_loser_4[slice35],\r\n",
        "       train_regret_loser_5[slice35],\r\n",
        "       train_regret_loser_6[slice35],\r\n",
        "       train_regret_loser_7[slice35],\r\n",
        "       train_regret_loser_8[slice35],\r\n",
        "       train_regret_loser_9[slice35],\r\n",
        "       train_regret_loser_10[slice35],\r\n",
        "       train_regret_loser_11[slice35],\r\n",
        "       train_regret_loser_12[slice35],\r\n",
        "       train_regret_loser_13[slice35],\r\n",
        "       train_regret_loser_14[slice35],\r\n",
        "       train_regret_loser_15[slice35],\r\n",
        "       train_regret_loser_16[slice35],\r\n",
        "       train_regret_loser_17[slice35],\r\n",
        "       train_regret_loser_18[slice35],\r\n",
        "       train_regret_loser_19[slice35],\r\n",
        "       train_regret_loser_20[slice35]]\r\n",
        "\r\n",
        "winner35 = [train_regret_winner_1[slice35],\r\n",
        "       train_regret_winner_2[slice35],\r\n",
        "       train_regret_winner_3[slice35],\r\n",
        "       train_regret_winner_4[slice35],\r\n",
        "       train_regret_winner_5[slice35],\r\n",
        "       train_regret_winner_6[slice35],\r\n",
        "       train_regret_winner_7[slice35],\r\n",
        "       train_regret_winner_8[slice35],\r\n",
        "       train_regret_winner_9[slice35],\r\n",
        "       train_regret_winner_10[slice35],\r\n",
        "       train_regret_winner_11[slice35],\r\n",
        "       train_regret_winner_12[slice35],\r\n",
        "       train_regret_winner_13[slice35],\r\n",
        "       train_regret_winner_14[slice35],\r\n",
        "       train_regret_winner_15[slice35],\r\n",
        "       train_regret_winner_16[slice35],\r\n",
        "       train_regret_winner_17[slice35],\r\n",
        "       train_regret_winner_18[slice35],\r\n",
        "       train_regret_winner_19[slice35],\r\n",
        "       train_regret_winner_20[slice35]]\r\n",
        "\r\n",
        "loser35_results = pd.DataFrame(loser35).sort_values(by=[0], ascending=False)\r\n",
        "winner35_results = pd.DataFrame(winner35).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser35 = np.asarray(loser35_results[4:5][0])[0]\r\n",
        "median_loser35 = np.asarray(loser35_results[9:10][0])[0]\r\n",
        "upper_loser35 = np.asarray(loser35_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner35 = np.asarray(winner35_results[4:5][0])[0]\r\n",
        "median_winner35 = np.asarray(winner35_results[9:10][0])[0]\r\n",
        "upper_winner35 = np.asarray(winner35_results[14:15][0])[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xX3VgqD_w_4z"
      },
      "source": [
        "# Iteration45 :\r\n",
        "\r\n",
        "slice45 = 44\r\n",
        "\r\n",
        "loser45 = [train_regret_loser_1[slice45],\r\n",
        "       train_regret_loser_2[slice45],\r\n",
        "       train_regret_loser_3[slice45],\r\n",
        "       train_regret_loser_4[slice45],\r\n",
        "       train_regret_loser_5[slice45],\r\n",
        "       train_regret_loser_6[slice45],\r\n",
        "       train_regret_loser_7[slice45],\r\n",
        "       train_regret_loser_8[slice45],\r\n",
        "       train_regret_loser_9[slice45],\r\n",
        "       train_regret_loser_10[slice45],\r\n",
        "       train_regret_loser_11[slice45],\r\n",
        "       train_regret_loser_12[slice45],\r\n",
        "       train_regret_loser_13[slice45],\r\n",
        "       train_regret_loser_14[slice45],\r\n",
        "       train_regret_loser_15[slice45],\r\n",
        "       train_regret_loser_16[slice45],\r\n",
        "       train_regret_loser_17[slice45],\r\n",
        "       train_regret_loser_18[slice45],\r\n",
        "       train_regret_loser_19[slice45],\r\n",
        "       train_regret_loser_20[slice45]]\r\n",
        "\r\n",
        "winner45 = [train_regret_winner_1[slice45],\r\n",
        "       train_regret_winner_2[slice45],\r\n",
        "       train_regret_winner_3[slice45],\r\n",
        "       train_regret_winner_4[slice45],\r\n",
        "       train_regret_winner_5[slice45],\r\n",
        "       train_regret_winner_6[slice45],\r\n",
        "       train_regret_winner_7[slice45],\r\n",
        "       train_regret_winner_8[slice45],\r\n",
        "       train_regret_winner_9[slice45],\r\n",
        "       train_regret_winner_10[slice45],\r\n",
        "       train_regret_winner_11[slice45],\r\n",
        "       train_regret_winner_12[slice45],\r\n",
        "       train_regret_winner_13[slice45],\r\n",
        "       train_regret_winner_14[slice45],\r\n",
        "       train_regret_winner_15[slice45],\r\n",
        "       train_regret_winner_16[slice45],\r\n",
        "       train_regret_winner_17[slice45],\r\n",
        "       train_regret_winner_18[slice45],\r\n",
        "       train_regret_winner_19[slice45],\r\n",
        "       train_regret_winner_20[slice45]]\r\n",
        "\r\n",
        "loser45_results = pd.DataFrame(loser45).sort_values(by=[0], ascending=False)\r\n",
        "winner45_results = pd.DataFrame(winner45).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser45 = np.asarray(loser45_results[4:5][0])[0]\r\n",
        "median_loser45 = np.asarray(loser45_results[9:10][0])[0]\r\n",
        "upper_loser45 = np.asarray(loser45_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner45 = np.asarray(winner45_results[4:5][0])[0]\r\n",
        "median_winner45 = np.asarray(winner45_results[9:10][0])[0]\r\n",
        "upper_winner45 = np.asarray(winner45_results[14:15][0])[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o35W-AOzw_8F"
      },
      "source": [
        "# Iteration55 :\r\n",
        "\r\n",
        "slice55 = 54\r\n",
        "\r\n",
        "loser55 = [train_regret_loser_1[slice55],\r\n",
        "       train_regret_loser_2[slice55],\r\n",
        "       train_regret_loser_3[slice55],\r\n",
        "       train_regret_loser_4[slice55],\r\n",
        "       train_regret_loser_5[slice55],\r\n",
        "       train_regret_loser_6[slice55],\r\n",
        "       train_regret_loser_7[slice55],\r\n",
        "       train_regret_loser_8[slice55],\r\n",
        "       train_regret_loser_9[slice55],\r\n",
        "       train_regret_loser_10[slice55],\r\n",
        "       train_regret_loser_11[slice55],\r\n",
        "       train_regret_loser_12[slice55],\r\n",
        "       train_regret_loser_13[slice55],\r\n",
        "       train_regret_loser_14[slice55],\r\n",
        "       train_regret_loser_15[slice55],\r\n",
        "       train_regret_loser_16[slice55],\r\n",
        "       train_regret_loser_17[slice55],\r\n",
        "       train_regret_loser_18[slice55],\r\n",
        "       train_regret_loser_19[slice55],\r\n",
        "       train_regret_loser_20[slice55]]\r\n",
        "\r\n",
        "winner55 = [train_regret_winner_1[slice55],\r\n",
        "       train_regret_winner_2[slice55],\r\n",
        "       train_regret_winner_3[slice55],\r\n",
        "       train_regret_winner_4[slice55],\r\n",
        "       train_regret_winner_5[slice55],\r\n",
        "       train_regret_winner_6[slice55],\r\n",
        "       train_regret_winner_7[slice55],\r\n",
        "       train_regret_winner_8[slice55],\r\n",
        "       train_regret_winner_9[slice55],\r\n",
        "       train_regret_winner_10[slice55],\r\n",
        "       train_regret_winner_11[slice55],\r\n",
        "       train_regret_winner_12[slice55],\r\n",
        "       train_regret_winner_13[slice55],\r\n",
        "       train_regret_winner_14[slice55],\r\n",
        "       train_regret_winner_15[slice55],\r\n",
        "       train_regret_winner_16[slice55],\r\n",
        "       train_regret_winner_17[slice55],\r\n",
        "       train_regret_winner_18[slice55],\r\n",
        "       train_regret_winner_19[slice55],\r\n",
        "       train_regret_winner_20[slice55]]\r\n",
        "\r\n",
        "loser55_results = pd.DataFrame(loser55).sort_values(by=[0], ascending=False)\r\n",
        "winner55_results = pd.DataFrame(winner55).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser55 = np.asarray(loser55_results[4:5][0])[0]\r\n",
        "median_loser55 = np.asarray(loser55_results[9:10][0])[0]\r\n",
        "upper_loser55 = np.asarray(loser55_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner55 = np.asarray(winner55_results[4:5][0])[0]\r\n",
        "median_winner55 = np.asarray(winner55_results[9:10][0])[0]\r\n",
        "upper_winner55 = np.asarray(winner55_results[14:15][0])[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUAjKXWdw__J"
      },
      "source": [
        "# Iteration65 :\r\n",
        "\r\n",
        "slice65 = 64\r\n",
        "\r\n",
        "loser65 = [train_regret_loser_1[slice65],\r\n",
        "       train_regret_loser_2[slice65],\r\n",
        "       train_regret_loser_3[slice65],\r\n",
        "       train_regret_loser_4[slice65],\r\n",
        "       train_regret_loser_5[slice65],\r\n",
        "       train_regret_loser_6[slice65],\r\n",
        "       train_regret_loser_7[slice65],\r\n",
        "       train_regret_loser_8[slice65],\r\n",
        "       train_regret_loser_9[slice65],\r\n",
        "       train_regret_loser_10[slice65],\r\n",
        "       train_regret_loser_11[slice65],\r\n",
        "       train_regret_loser_12[slice65],\r\n",
        "       train_regret_loser_13[slice65],\r\n",
        "       train_regret_loser_14[slice65],\r\n",
        "       train_regret_loser_15[slice65],\r\n",
        "       train_regret_loser_16[slice65],\r\n",
        "       train_regret_loser_17[slice65],\r\n",
        "       train_regret_loser_18[slice65],\r\n",
        "       train_regret_loser_19[slice65],\r\n",
        "       train_regret_loser_20[slice65]]\r\n",
        "\r\n",
        "winner65 = [train_regret_winner_1[slice65],\r\n",
        "       train_regret_winner_2[slice65],\r\n",
        "       train_regret_winner_3[slice65],\r\n",
        "       train_regret_winner_4[slice65],\r\n",
        "       train_regret_winner_5[slice65],\r\n",
        "       train_regret_winner_6[slice65],\r\n",
        "       train_regret_winner_7[slice65],\r\n",
        "       train_regret_winner_8[slice65],\r\n",
        "       train_regret_winner_9[slice65],\r\n",
        "       train_regret_winner_10[slice65],\r\n",
        "       train_regret_winner_11[slice65],\r\n",
        "       train_regret_winner_12[slice65],\r\n",
        "       train_regret_winner_13[slice65],\r\n",
        "       train_regret_winner_14[slice65],\r\n",
        "       train_regret_winner_15[slice65],\r\n",
        "       train_regret_winner_16[slice65],\r\n",
        "       train_regret_winner_17[slice65],\r\n",
        "       train_regret_winner_18[slice65],\r\n",
        "       train_regret_winner_19[slice65],\r\n",
        "       train_regret_winner_20[slice65]]\r\n",
        "\r\n",
        "loser65_results = pd.DataFrame(loser65).sort_values(by=[0], ascending=False)\r\n",
        "winner65_results = pd.DataFrame(winner65).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser65 = np.asarray(loser65_results[4:5][0])[0]\r\n",
        "median_loser65 = np.asarray(loser65_results[9:10][0])[0]\r\n",
        "upper_loser65 = np.asarray(loser65_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner65 = np.asarray(winner65_results[4:5][0])[0]\r\n",
        "median_winner65 = np.asarray(winner65_results[9:10][0])[0]\r\n",
        "upper_winner65 = np.asarray(winner65_results[14:15][0])[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D00FcmZwxAB9"
      },
      "source": [
        "# Iteration75 :\r\n",
        "\r\n",
        "slice75 = 74\r\n",
        "\r\n",
        "loser75 = [train_regret_loser_1[slice75],\r\n",
        "       train_regret_loser_2[slice75],\r\n",
        "       train_regret_loser_3[slice75],\r\n",
        "       train_regret_loser_4[slice75],\r\n",
        "       train_regret_loser_5[slice75],\r\n",
        "       train_regret_loser_6[slice75],\r\n",
        "       train_regret_loser_7[slice75],\r\n",
        "       train_regret_loser_8[slice75],\r\n",
        "       train_regret_loser_9[slice75],\r\n",
        "       train_regret_loser_10[slice75],\r\n",
        "       train_regret_loser_11[slice75],\r\n",
        "       train_regret_loser_12[slice75],\r\n",
        "       train_regret_loser_13[slice75],\r\n",
        "       train_regret_loser_14[slice75],\r\n",
        "       train_regret_loser_15[slice75],\r\n",
        "       train_regret_loser_16[slice75],\r\n",
        "       train_regret_loser_17[slice75],\r\n",
        "       train_regret_loser_18[slice75],\r\n",
        "       train_regret_loser_19[slice75],\r\n",
        "       train_regret_loser_20[slice75]]\r\n",
        "\r\n",
        "winner75 = [train_regret_winner_1[slice75],\r\n",
        "       train_regret_winner_2[slice75],\r\n",
        "       train_regret_winner_3[slice75],\r\n",
        "       train_regret_winner_4[slice75],\r\n",
        "       train_regret_winner_5[slice75],\r\n",
        "       train_regret_winner_6[slice75],\r\n",
        "       train_regret_winner_7[slice75],\r\n",
        "       train_regret_winner_8[slice75],\r\n",
        "       train_regret_winner_9[slice75],\r\n",
        "       train_regret_winner_10[slice75],\r\n",
        "       train_regret_winner_11[slice75],\r\n",
        "       train_regret_winner_12[slice75],\r\n",
        "       train_regret_winner_13[slice75],\r\n",
        "       train_regret_winner_14[slice75],\r\n",
        "       train_regret_winner_15[slice75],\r\n",
        "       train_regret_winner_16[slice75],\r\n",
        "       train_regret_winner_17[slice75],\r\n",
        "       train_regret_winner_18[slice75],\r\n",
        "       train_regret_winner_19[slice75],\r\n",
        "       train_regret_winner_20[slice75]]\r\n",
        "\r\n",
        "loser75_results = pd.DataFrame(loser75).sort_values(by=[0], ascending=False)\r\n",
        "winner75_results = pd.DataFrame(winner75).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser75 = np.asarray(loser75_results[4:5][0])[0]\r\n",
        "median_loser75 = np.asarray(loser75_results[9:10][0])[0]\r\n",
        "upper_loser75 = np.asarray(loser75_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner75 = np.asarray(winner75_results[4:5][0])[0]\r\n",
        "median_winner75 = np.asarray(winner75_results[9:10][0])[0]\r\n",
        "upper_winner75 = np.asarray(winner75_results[14:15][0])[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvnzmprxxAFB"
      },
      "source": [
        "# Iteration85 :\r\n",
        "\r\n",
        "slice85 = 84\r\n",
        "\r\n",
        "loser85 = [train_regret_loser_1[slice85],\r\n",
        "       train_regret_loser_2[slice85],\r\n",
        "       train_regret_loser_3[slice85],\r\n",
        "       train_regret_loser_4[slice85],\r\n",
        "       train_regret_loser_5[slice85],\r\n",
        "       train_regret_loser_6[slice85],\r\n",
        "       train_regret_loser_7[slice85],\r\n",
        "       train_regret_loser_8[slice85],\r\n",
        "       train_regret_loser_9[slice85],\r\n",
        "       train_regret_loser_10[slice85],\r\n",
        "       train_regret_loser_11[slice85],\r\n",
        "       train_regret_loser_12[slice85],\r\n",
        "       train_regret_loser_13[slice85],\r\n",
        "       train_regret_loser_14[slice85],\r\n",
        "       train_regret_loser_15[slice85],\r\n",
        "       train_regret_loser_16[slice85],\r\n",
        "       train_regret_loser_17[slice85],\r\n",
        "       train_regret_loser_18[slice85],\r\n",
        "       train_regret_loser_19[slice85],\r\n",
        "       train_regret_loser_20[slice85]]\r\n",
        "\r\n",
        "winner85 = [train_regret_winner_1[slice85],\r\n",
        "       train_regret_winner_2[slice85],\r\n",
        "       train_regret_winner_3[slice85],\r\n",
        "       train_regret_winner_4[slice85],\r\n",
        "       train_regret_winner_5[slice85],\r\n",
        "       train_regret_winner_6[slice85],\r\n",
        "       train_regret_winner_7[slice85],\r\n",
        "       train_regret_winner_8[slice85],\r\n",
        "       train_regret_winner_9[slice85],\r\n",
        "       train_regret_winner_10[slice85],\r\n",
        "       train_regret_winner_11[slice85],\r\n",
        "       train_regret_winner_12[slice85],\r\n",
        "       train_regret_winner_13[slice85],\r\n",
        "       train_regret_winner_14[slice85],\r\n",
        "       train_regret_winner_15[slice85],\r\n",
        "       train_regret_winner_16[slice85],\r\n",
        "       train_regret_winner_17[slice85],\r\n",
        "       train_regret_winner_18[slice85],\r\n",
        "       train_regret_winner_19[slice85],\r\n",
        "       train_regret_winner_20[slice85]]\r\n",
        "\r\n",
        "loser85_results = pd.DataFrame(loser85).sort_values(by=[0], ascending=False)\r\n",
        "winner85_results = pd.DataFrame(winner85).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser85 = np.asarray(loser85_results[4:5][0])[0]\r\n",
        "median_loser85 = np.asarray(loser85_results[9:10][0])[0]\r\n",
        "upper_loser85 = np.asarray(loser85_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner85 = np.asarray(winner85_results[4:5][0])[0]\r\n",
        "median_winner85 = np.asarray(winner85_results[9:10][0])[0]\r\n",
        "upper_winner85 = np.asarray(winner85_results[14:15][0])[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztvJhrN6xAH1"
      },
      "source": [
        "# Iteration95 :\r\n",
        "\r\n",
        "slice95 = 94\r\n",
        "\r\n",
        "loser95 = [train_regret_loser_1[slice95],\r\n",
        "       train_regret_loser_2[slice95],\r\n",
        "       train_regret_loser_3[slice95],\r\n",
        "       train_regret_loser_4[slice95],\r\n",
        "       train_regret_loser_5[slice95],\r\n",
        "       train_regret_loser_6[slice95],\r\n",
        "       train_regret_loser_7[slice95],\r\n",
        "       train_regret_loser_8[slice95],\r\n",
        "       train_regret_loser_9[slice95],\r\n",
        "       train_regret_loser_10[slice95],\r\n",
        "       train_regret_loser_11[slice95],\r\n",
        "       train_regret_loser_12[slice95],\r\n",
        "       train_regret_loser_13[slice95],\r\n",
        "       train_regret_loser_14[slice95],\r\n",
        "       train_regret_loser_15[slice95],\r\n",
        "       train_regret_loser_16[slice95],\r\n",
        "       train_regret_loser_17[slice95],\r\n",
        "       train_regret_loser_18[slice95],\r\n",
        "       train_regret_loser_19[slice95],\r\n",
        "       train_regret_loser_20[slice95]]\r\n",
        "\r\n",
        "winner95 = [train_regret_winner_1[slice95],\r\n",
        "       train_regret_winner_2[slice95],\r\n",
        "       train_regret_winner_3[slice95],\r\n",
        "       train_regret_winner_4[slice95],\r\n",
        "       train_regret_winner_5[slice95],\r\n",
        "       train_regret_winner_6[slice95],\r\n",
        "       train_regret_winner_7[slice95],\r\n",
        "       train_regret_winner_8[slice95],\r\n",
        "       train_regret_winner_9[slice95],\r\n",
        "       train_regret_winner_10[slice95],\r\n",
        "       train_regret_winner_11[slice95],\r\n",
        "       train_regret_winner_12[slice95],\r\n",
        "       train_regret_winner_13[slice95],\r\n",
        "       train_regret_winner_14[slice95],\r\n",
        "       train_regret_winner_15[slice95],\r\n",
        "       train_regret_winner_16[slice95],\r\n",
        "       train_regret_winner_17[slice95],\r\n",
        "       train_regret_winner_18[slice95],\r\n",
        "       train_regret_winner_19[slice95],\r\n",
        "       train_regret_winner_20[slice95]]\r\n",
        "\r\n",
        "loser95_results = pd.DataFrame(loser95).sort_values(by=[0], ascending=False)\r\n",
        "winner95_results = pd.DataFrame(winner95).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser95 = np.asarray(loser95_results[4:5][0])[0]\r\n",
        "median_loser95 = np.asarray(loser95_results[9:10][0])[0]\r\n",
        "upper_loser95 = np.asarray(loser95_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner95 = np.asarray(winner95_results[4:5][0])[0]\r\n",
        "median_winner95 = np.asarray(winner95_results[9:10][0])[0]\r\n",
        "upper_winner95 = np.asarray(winner95_results[14:15][0])[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SK-4nr-OxALA"
      },
      "source": [
        "# Iteration6 :\r\n",
        "\r\n",
        "slice6 = 5\r\n",
        "\r\n",
        "loser6 = [train_regret_loser_1[slice6],\r\n",
        "       train_regret_loser_2[slice6],\r\n",
        "       train_regret_loser_3[slice6],\r\n",
        "       train_regret_loser_4[slice6],\r\n",
        "       train_regret_loser_5[slice6],\r\n",
        "       train_regret_loser_6[slice6],\r\n",
        "       train_regret_loser_7[slice6],\r\n",
        "       train_regret_loser_8[slice6],\r\n",
        "       train_regret_loser_9[slice6],\r\n",
        "       train_regret_loser_10[slice6],\r\n",
        "       train_regret_loser_11[slice6],\r\n",
        "       train_regret_loser_12[slice6],\r\n",
        "       train_regret_loser_13[slice6],\r\n",
        "       train_regret_loser_14[slice6],\r\n",
        "       train_regret_loser_15[slice6],\r\n",
        "       train_regret_loser_16[slice6],\r\n",
        "       train_regret_loser_17[slice6],\r\n",
        "       train_regret_loser_18[slice6],\r\n",
        "       train_regret_loser_19[slice6],\r\n",
        "       train_regret_loser_20[slice6]]\r\n",
        "\r\n",
        "winner6 = [train_regret_winner_1[slice6],\r\n",
        "       train_regret_winner_2[slice6],\r\n",
        "       train_regret_winner_3[slice6],\r\n",
        "       train_regret_winner_4[slice6],\r\n",
        "       train_regret_winner_5[slice6],\r\n",
        "       train_regret_winner_6[slice6],\r\n",
        "       train_regret_winner_7[slice6],\r\n",
        "       train_regret_winner_8[slice6],\r\n",
        "       train_regret_winner_9[slice6],\r\n",
        "       train_regret_winner_10[slice6],\r\n",
        "       train_regret_winner_11[slice6],\r\n",
        "       train_regret_winner_12[slice6],\r\n",
        "       train_regret_winner_13[slice6],\r\n",
        "       train_regret_winner_14[slice6],\r\n",
        "       train_regret_winner_15[slice6],\r\n",
        "       train_regret_winner_16[slice6],\r\n",
        "       train_regret_winner_17[slice6],\r\n",
        "       train_regret_winner_18[slice6],\r\n",
        "       train_regret_winner_19[slice6],\r\n",
        "       train_regret_winner_20[slice6]]\r\n",
        "\r\n",
        "loser6_results = pd.DataFrame(loser6).sort_values(by=[0], ascending=False)\r\n",
        "winner6_results = pd.DataFrame(winner6).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser6 = np.asarray(loser6_results[4:5][0])[0]\r\n",
        "median_loser6 = np.asarray(loser6_results[9:10][0])[0]\r\n",
        "upper_loser6 = np.asarray(loser6_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner6 = np.asarray(winner6_results[4:5][0])[0]\r\n",
        "median_winner6 = np.asarray(winner6_results[9:10][0])[0]\r\n",
        "upper_winner6 = np.asarray(winner6_results[14:15][0])[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSwfNX5ExANt"
      },
      "source": [
        "# Iteration16 :\r\n",
        "\r\n",
        "slice16 = 15\r\n",
        "\r\n",
        "loser16 = [train_regret_loser_1[slice16],\r\n",
        "       train_regret_loser_2[slice16],\r\n",
        "       train_regret_loser_3[slice16],\r\n",
        "       train_regret_loser_4[slice16],\r\n",
        "       train_regret_loser_5[slice16],\r\n",
        "       train_regret_loser_6[slice16],\r\n",
        "       train_regret_loser_7[slice16],\r\n",
        "       train_regret_loser_8[slice16],\r\n",
        "       train_regret_loser_9[slice16],\r\n",
        "       train_regret_loser_10[slice16],\r\n",
        "       train_regret_loser_11[slice16],\r\n",
        "       train_regret_loser_12[slice16],\r\n",
        "       train_regret_loser_13[slice16],\r\n",
        "       train_regret_loser_14[slice16],\r\n",
        "       train_regret_loser_15[slice16],\r\n",
        "       train_regret_loser_16[slice16],\r\n",
        "       train_regret_loser_17[slice16],\r\n",
        "       train_regret_loser_18[slice16],\r\n",
        "       train_regret_loser_19[slice16],\r\n",
        "       train_regret_loser_20[slice16]]\r\n",
        "\r\n",
        "winner16 = [train_regret_winner_1[slice16],\r\n",
        "       train_regret_winner_2[slice16],\r\n",
        "       train_regret_winner_3[slice16],\r\n",
        "       train_regret_winner_4[slice16],\r\n",
        "       train_regret_winner_5[slice16],\r\n",
        "       train_regret_winner_6[slice16],\r\n",
        "       train_regret_winner_7[slice16],\r\n",
        "       train_regret_winner_8[slice16],\r\n",
        "       train_regret_winner_9[slice16],\r\n",
        "       train_regret_winner_10[slice16],\r\n",
        "       train_regret_winner_11[slice16],\r\n",
        "       train_regret_winner_12[slice16],\r\n",
        "       train_regret_winner_13[slice16],\r\n",
        "       train_regret_winner_14[slice16],\r\n",
        "       train_regret_winner_15[slice16],\r\n",
        "       train_regret_winner_16[slice16],\r\n",
        "       train_regret_winner_17[slice16],\r\n",
        "       train_regret_winner_18[slice16],\r\n",
        "       train_regret_winner_19[slice16],\r\n",
        "       train_regret_winner_20[slice16]]\r\n",
        "\r\n",
        "loser16_results = pd.DataFrame(loser16).sort_values(by=[0], ascending=False)\r\n",
        "winner16_results = pd.DataFrame(winner16).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser16 = np.asarray(loser16_results[4:5][0])[0]\r\n",
        "median_loser16 = np.asarray(loser16_results[9:10][0])[0]\r\n",
        "upper_loser16 = np.asarray(loser16_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner16 = np.asarray(winner16_results[4:5][0])[0]\r\n",
        "median_winner16 = np.asarray(winner16_results[9:10][0])[0]\r\n",
        "upper_winner16 = np.asarray(winner16_results[14:15][0])[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOB3hOLIxAQy"
      },
      "source": [
        "# Iteration26 :\r\n",
        "\r\n",
        "slice26 = 25\r\n",
        "\r\n",
        "loser26 = [train_regret_loser_1[slice26],\r\n",
        "       train_regret_loser_2[slice26],\r\n",
        "       train_regret_loser_3[slice26],\r\n",
        "       train_regret_loser_4[slice26],\r\n",
        "       train_regret_loser_5[slice26],\r\n",
        "       train_regret_loser_6[slice26],\r\n",
        "       train_regret_loser_7[slice26],\r\n",
        "       train_regret_loser_8[slice26],\r\n",
        "       train_regret_loser_9[slice26],\r\n",
        "       train_regret_loser_10[slice26],\r\n",
        "       train_regret_loser_11[slice26],\r\n",
        "       train_regret_loser_12[slice26],\r\n",
        "       train_regret_loser_13[slice26],\r\n",
        "       train_regret_loser_14[slice26],\r\n",
        "       train_regret_loser_15[slice26],\r\n",
        "       train_regret_loser_16[slice26],\r\n",
        "       train_regret_loser_17[slice26],\r\n",
        "       train_regret_loser_18[slice26],\r\n",
        "       train_regret_loser_19[slice26],\r\n",
        "       train_regret_loser_20[slice26]]\r\n",
        "\r\n",
        "winner26 = [train_regret_winner_1[slice26],\r\n",
        "       train_regret_winner_2[slice26],\r\n",
        "       train_regret_winner_3[slice26],\r\n",
        "       train_regret_winner_4[slice26],\r\n",
        "       train_regret_winner_5[slice26],\r\n",
        "       train_regret_winner_6[slice26],\r\n",
        "       train_regret_winner_7[slice26],\r\n",
        "       train_regret_winner_8[slice26],\r\n",
        "       train_regret_winner_9[slice26],\r\n",
        "       train_regret_winner_10[slice26],\r\n",
        "       train_regret_winner_11[slice26],\r\n",
        "       train_regret_winner_12[slice26],\r\n",
        "       train_regret_winner_13[slice26],\r\n",
        "       train_regret_winner_14[slice26],\r\n",
        "       train_regret_winner_15[slice26],\r\n",
        "       train_regret_winner_16[slice26],\r\n",
        "       train_regret_winner_17[slice26],\r\n",
        "       train_regret_winner_18[slice26],\r\n",
        "       train_regret_winner_19[slice26],\r\n",
        "       train_regret_winner_20[slice26]]\r\n",
        "\r\n",
        "loser26_results = pd.DataFrame(loser26).sort_values(by=[0], ascending=False)\r\n",
        "winner26_results = pd.DataFrame(winner26).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser26 = np.asarray(loser26_results[4:5][0])[0]\r\n",
        "median_loser26 = np.asarray(loser26_results[9:10][0])[0]\r\n",
        "upper_loser26 = np.asarray(loser26_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner26 = np.asarray(winner26_results[4:5][0])[0]\r\n",
        "median_winner26 = np.asarray(winner26_results[9:10][0])[0]\r\n",
        "upper_winner26 = np.asarray(winner26_results[14:15][0])[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-0hQMtOxATh"
      },
      "source": [
        "# Iteration36 :\r\n",
        "\r\n",
        "slice36 = 35\r\n",
        "\r\n",
        "loser36 = [train_regret_loser_1[slice36],\r\n",
        "       train_regret_loser_2[slice36],\r\n",
        "       train_regret_loser_3[slice36],\r\n",
        "       train_regret_loser_4[slice36],\r\n",
        "       train_regret_loser_5[slice36],\r\n",
        "       train_regret_loser_6[slice36],\r\n",
        "       train_regret_loser_7[slice36],\r\n",
        "       train_regret_loser_8[slice36],\r\n",
        "       train_regret_loser_9[slice36],\r\n",
        "       train_regret_loser_10[slice36],\r\n",
        "       train_regret_loser_11[slice36],\r\n",
        "       train_regret_loser_12[slice36],\r\n",
        "       train_regret_loser_13[slice36],\r\n",
        "       train_regret_loser_14[slice36],\r\n",
        "       train_regret_loser_15[slice36],\r\n",
        "       train_regret_loser_16[slice36],\r\n",
        "       train_regret_loser_17[slice36],\r\n",
        "       train_regret_loser_18[slice36],\r\n",
        "       train_regret_loser_19[slice36],\r\n",
        "       train_regret_loser_20[slice36]]\r\n",
        "\r\n",
        "winner36 = [train_regret_winner_1[slice36],\r\n",
        "       train_regret_winner_2[slice36],\r\n",
        "       train_regret_winner_3[slice36],\r\n",
        "       train_regret_winner_4[slice36],\r\n",
        "       train_regret_winner_5[slice36],\r\n",
        "       train_regret_winner_6[slice36],\r\n",
        "       train_regret_winner_7[slice36],\r\n",
        "       train_regret_winner_8[slice36],\r\n",
        "       train_regret_winner_9[slice36],\r\n",
        "       train_regret_winner_10[slice36],\r\n",
        "       train_regret_winner_11[slice36],\r\n",
        "       train_regret_winner_12[slice36],\r\n",
        "       train_regret_winner_13[slice36],\r\n",
        "       train_regret_winner_14[slice36],\r\n",
        "       train_regret_winner_15[slice36],\r\n",
        "       train_regret_winner_16[slice36],\r\n",
        "       train_regret_winner_17[slice36],\r\n",
        "       train_regret_winner_18[slice36],\r\n",
        "       train_regret_winner_19[slice36],\r\n",
        "       train_regret_winner_20[slice36]]\r\n",
        "\r\n",
        "loser36_results = pd.DataFrame(loser36).sort_values(by=[0], ascending=False)\r\n",
        "winner36_results = pd.DataFrame(winner36).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser36 = np.asarray(loser36_results[4:5][0])[0]\r\n",
        "median_loser36 = np.asarray(loser36_results[9:10][0])[0]\r\n",
        "upper_loser36 = np.asarray(loser36_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner36 = np.asarray(winner36_results[4:5][0])[0]\r\n",
        "median_winner36 = np.asarray(winner36_results[9:10][0])[0]\r\n",
        "upper_winner36 = np.asarray(winner36_results[14:15][0])[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSqBLqTgxAWs"
      },
      "source": [
        "# Iteration46 :\r\n",
        "\r\n",
        "slice46 = 45\r\n",
        "\r\n",
        "loser46 = [train_regret_loser_1[slice46],\r\n",
        "       train_regret_loser_2[slice46],\r\n",
        "       train_regret_loser_3[slice46],\r\n",
        "       train_regret_loser_4[slice46],\r\n",
        "       train_regret_loser_5[slice46],\r\n",
        "       train_regret_loser_6[slice46],\r\n",
        "       train_regret_loser_7[slice46],\r\n",
        "       train_regret_loser_8[slice46],\r\n",
        "       train_regret_loser_9[slice46],\r\n",
        "       train_regret_loser_10[slice46],\r\n",
        "       train_regret_loser_11[slice46],\r\n",
        "       train_regret_loser_12[slice46],\r\n",
        "       train_regret_loser_13[slice46],\r\n",
        "       train_regret_loser_14[slice46],\r\n",
        "       train_regret_loser_15[slice46],\r\n",
        "       train_regret_loser_16[slice46],\r\n",
        "       train_regret_loser_17[slice46],\r\n",
        "       train_regret_loser_18[slice46],\r\n",
        "       train_regret_loser_19[slice46],\r\n",
        "       train_regret_loser_20[slice46]]\r\n",
        "\r\n",
        "winner46 = [train_regret_winner_1[slice46],\r\n",
        "       train_regret_winner_2[slice46],\r\n",
        "       train_regret_winner_3[slice46],\r\n",
        "       train_regret_winner_4[slice46],\r\n",
        "       train_regret_winner_5[slice46],\r\n",
        "       train_regret_winner_6[slice46],\r\n",
        "       train_regret_winner_7[slice46],\r\n",
        "       train_regret_winner_8[slice46],\r\n",
        "       train_regret_winner_9[slice46],\r\n",
        "       train_regret_winner_10[slice46],\r\n",
        "       train_regret_winner_11[slice46],\r\n",
        "       train_regret_winner_12[slice46],\r\n",
        "       train_regret_winner_13[slice46],\r\n",
        "       train_regret_winner_14[slice46],\r\n",
        "       train_regret_winner_15[slice46],\r\n",
        "       train_regret_winner_16[slice46],\r\n",
        "       train_regret_winner_17[slice46],\r\n",
        "       train_regret_winner_18[slice46],\r\n",
        "       train_regret_winner_19[slice46],\r\n",
        "       train_regret_winner_20[slice46]]\r\n",
        "\r\n",
        "loser46_results = pd.DataFrame(loser46).sort_values(by=[0], ascending=False)\r\n",
        "winner46_results = pd.DataFrame(winner46).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser46 = np.asarray(loser46_results[4:5][0])[0]\r\n",
        "median_loser46 = np.asarray(loser46_results[9:10][0])[0]\r\n",
        "upper_loser46 = np.asarray(loser46_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner46 = np.asarray(winner46_results[4:5][0])[0]\r\n",
        "median_winner46 = np.asarray(winner46_results[9:10][0])[0]\r\n",
        "upper_winner46 = np.asarray(winner46_results[14:15][0])[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFwO80w6xAZZ"
      },
      "source": [
        "# Iteration56 :\r\n",
        "\r\n",
        "slice56 = 55\r\n",
        "\r\n",
        "loser56 = [train_regret_loser_1[slice56],\r\n",
        "       train_regret_loser_2[slice56],\r\n",
        "       train_regret_loser_3[slice56],\r\n",
        "       train_regret_loser_4[slice56],\r\n",
        "       train_regret_loser_5[slice56],\r\n",
        "       train_regret_loser_6[slice56],\r\n",
        "       train_regret_loser_7[slice56],\r\n",
        "       train_regret_loser_8[slice56],\r\n",
        "       train_regret_loser_9[slice56],\r\n",
        "       train_regret_loser_10[slice56],\r\n",
        "       train_regret_loser_11[slice56],\r\n",
        "       train_regret_loser_12[slice56],\r\n",
        "       train_regret_loser_13[slice56],\r\n",
        "       train_regret_loser_14[slice56],\r\n",
        "       train_regret_loser_15[slice56],\r\n",
        "       train_regret_loser_16[slice56],\r\n",
        "       train_regret_loser_17[slice56],\r\n",
        "       train_regret_loser_18[slice56],\r\n",
        "       train_regret_loser_19[slice56],\r\n",
        "       train_regret_loser_20[slice56]]\r\n",
        "\r\n",
        "winner56 = [train_regret_winner_1[slice56],\r\n",
        "       train_regret_winner_2[slice56],\r\n",
        "       train_regret_winner_3[slice56],\r\n",
        "       train_regret_winner_4[slice56],\r\n",
        "       train_regret_winner_5[slice56],\r\n",
        "       train_regret_winner_6[slice56],\r\n",
        "       train_regret_winner_7[slice56],\r\n",
        "       train_regret_winner_8[slice56],\r\n",
        "       train_regret_winner_9[slice56],\r\n",
        "       train_regret_winner_10[slice56],\r\n",
        "       train_regret_winner_11[slice56],\r\n",
        "       train_regret_winner_12[slice56],\r\n",
        "       train_regret_winner_13[slice56],\r\n",
        "       train_regret_winner_14[slice56],\r\n",
        "       train_regret_winner_15[slice56],\r\n",
        "       train_regret_winner_16[slice56],\r\n",
        "       train_regret_winner_17[slice56],\r\n",
        "       train_regret_winner_18[slice56],\r\n",
        "       train_regret_winner_19[slice56],\r\n",
        "       train_regret_winner_20[slice56]]\r\n",
        "\r\n",
        "loser56_results = pd.DataFrame(loser56).sort_values(by=[0], ascending=False)\r\n",
        "winner56_results = pd.DataFrame(winner56).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser56 = np.asarray(loser56_results[4:5][0])[0]\r\n",
        "median_loser56 = np.asarray(loser56_results[9:10][0])[0]\r\n",
        "upper_loser56 = np.asarray(loser56_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner56 = np.asarray(winner56_results[4:5][0])[0]\r\n",
        "median_winner56 = np.asarray(winner56_results[9:10][0])[0]\r\n",
        "upper_winner56 = np.asarray(winner56_results[14:15][0])[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfVZBDP8xAcC"
      },
      "source": [
        "# Iteration66 :\r\n",
        "\r\n",
        "slice66 = 65\r\n",
        "\r\n",
        "loser66 = [train_regret_loser_1[slice66],\r\n",
        "       train_regret_loser_2[slice66],\r\n",
        "       train_regret_loser_3[slice66],\r\n",
        "       train_regret_loser_4[slice66],\r\n",
        "       train_regret_loser_5[slice66],\r\n",
        "       train_regret_loser_6[slice66],\r\n",
        "       train_regret_loser_7[slice66],\r\n",
        "       train_regret_loser_8[slice66],\r\n",
        "       train_regret_loser_9[slice66],\r\n",
        "       train_regret_loser_10[slice66],\r\n",
        "       train_regret_loser_11[slice66],\r\n",
        "       train_regret_loser_12[slice66],\r\n",
        "       train_regret_loser_13[slice66],\r\n",
        "       train_regret_loser_14[slice66],\r\n",
        "       train_regret_loser_15[slice66],\r\n",
        "       train_regret_loser_16[slice66],\r\n",
        "       train_regret_loser_17[slice66],\r\n",
        "       train_regret_loser_18[slice66],\r\n",
        "       train_regret_loser_19[slice66],\r\n",
        "       train_regret_loser_20[slice66]]\r\n",
        "\r\n",
        "winner66 = [train_regret_winner_1[slice66],\r\n",
        "       train_regret_winner_2[slice66],\r\n",
        "       train_regret_winner_3[slice66],\r\n",
        "       train_regret_winner_4[slice66],\r\n",
        "       train_regret_winner_5[slice66],\r\n",
        "       train_regret_winner_6[slice66],\r\n",
        "       train_regret_winner_7[slice66],\r\n",
        "       train_regret_winner_8[slice66],\r\n",
        "       train_regret_winner_9[slice66],\r\n",
        "       train_regret_winner_10[slice66],\r\n",
        "       train_regret_winner_11[slice66],\r\n",
        "       train_regret_winner_12[slice66],\r\n",
        "       train_regret_winner_13[slice66],\r\n",
        "       train_regret_winner_14[slice66],\r\n",
        "       train_regret_winner_15[slice66],\r\n",
        "       train_regret_winner_16[slice66],\r\n",
        "       train_regret_winner_17[slice66],\r\n",
        "       train_regret_winner_18[slice66],\r\n",
        "       train_regret_winner_19[slice66],\r\n",
        "       train_regret_winner_20[slice66]]\r\n",
        "\r\n",
        "loser66_results = pd.DataFrame(loser66).sort_values(by=[0], ascending=False)\r\n",
        "winner66_results = pd.DataFrame(winner66).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser66 = np.asarray(loser66_results[4:5][0])[0]\r\n",
        "median_loser66 = np.asarray(loser66_results[9:10][0])[0]\r\n",
        "upper_loser66 = np.asarray(loser66_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner66 = np.asarray(winner66_results[4:5][0])[0]\r\n",
        "median_winner66 = np.asarray(winner66_results[9:10][0])[0]\r\n",
        "upper_winner66 = np.asarray(winner66_results[14:15][0])[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KnSYoG7bxAfL"
      },
      "source": [
        "# Iteration76 :\r\n",
        "\r\n",
        "slice76 = 75\r\n",
        "\r\n",
        "loser76 = [train_regret_loser_1[slice76],\r\n",
        "       train_regret_loser_2[slice76],\r\n",
        "       train_regret_loser_3[slice76],\r\n",
        "       train_regret_loser_4[slice76],\r\n",
        "       train_regret_loser_5[slice76],\r\n",
        "       train_regret_loser_6[slice76],\r\n",
        "       train_regret_loser_7[slice76],\r\n",
        "       train_regret_loser_8[slice76],\r\n",
        "       train_regret_loser_9[slice76],\r\n",
        "       train_regret_loser_10[slice76],\r\n",
        "       train_regret_loser_11[slice76],\r\n",
        "       train_regret_loser_12[slice76],\r\n",
        "       train_regret_loser_13[slice76],\r\n",
        "       train_regret_loser_14[slice76],\r\n",
        "       train_regret_loser_15[slice76],\r\n",
        "       train_regret_loser_16[slice76],\r\n",
        "       train_regret_loser_17[slice76],\r\n",
        "       train_regret_loser_18[slice76],\r\n",
        "       train_regret_loser_19[slice76],\r\n",
        "       train_regret_loser_20[slice76]]\r\n",
        "\r\n",
        "winner76 = [train_regret_winner_1[slice76],\r\n",
        "       train_regret_winner_2[slice76],\r\n",
        "       train_regret_winner_3[slice76],\r\n",
        "       train_regret_winner_4[slice76],\r\n",
        "       train_regret_winner_5[slice76],\r\n",
        "       train_regret_winner_6[slice76],\r\n",
        "       train_regret_winner_7[slice76],\r\n",
        "       train_regret_winner_8[slice76],\r\n",
        "       train_regret_winner_9[slice76],\r\n",
        "       train_regret_winner_10[slice76],\r\n",
        "       train_regret_winner_11[slice76],\r\n",
        "       train_regret_winner_12[slice76],\r\n",
        "       train_regret_winner_13[slice76],\r\n",
        "       train_regret_winner_14[slice76],\r\n",
        "       train_regret_winner_15[slice76],\r\n",
        "       train_regret_winner_16[slice76],\r\n",
        "       train_regret_winner_17[slice76],\r\n",
        "       train_regret_winner_18[slice76],\r\n",
        "       train_regret_winner_19[slice76],\r\n",
        "       train_regret_winner_20[slice76]]\r\n",
        "\r\n",
        "loser76_results = pd.DataFrame(loser76).sort_values(by=[0], ascending=False)\r\n",
        "winner76_results = pd.DataFrame(winner76).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser76 = np.asarray(loser76_results[4:5][0])[0]\r\n",
        "median_loser76 = np.asarray(loser76_results[9:10][0])[0]\r\n",
        "upper_loser76 = np.asarray(loser76_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner76 = np.asarray(winner76_results[4:5][0])[0]\r\n",
        "median_winner76 = np.asarray(winner76_results[9:10][0])[0]\r\n",
        "upper_winner76 = np.asarray(winner76_results[14:15][0])[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ze_Oy9mxAiO"
      },
      "source": [
        "# Iteration86 :\r\n",
        "\r\n",
        "slice86 = 85\r\n",
        "\r\n",
        "loser86 = [train_regret_loser_1[slice86],\r\n",
        "       train_regret_loser_2[slice86],\r\n",
        "       train_regret_loser_3[slice86],\r\n",
        "       train_regret_loser_4[slice86],\r\n",
        "       train_regret_loser_5[slice86],\r\n",
        "       train_regret_loser_6[slice86],\r\n",
        "       train_regret_loser_7[slice86],\r\n",
        "       train_regret_loser_8[slice86],\r\n",
        "       train_regret_loser_9[slice86],\r\n",
        "       train_regret_loser_10[slice86],\r\n",
        "       train_regret_loser_11[slice86],\r\n",
        "       train_regret_loser_12[slice86],\r\n",
        "       train_regret_loser_13[slice86],\r\n",
        "       train_regret_loser_14[slice86],\r\n",
        "       train_regret_loser_15[slice86],\r\n",
        "       train_regret_loser_16[slice86],\r\n",
        "       train_regret_loser_17[slice86],\r\n",
        "       train_regret_loser_18[slice86],\r\n",
        "       train_regret_loser_19[slice86],\r\n",
        "       train_regret_loser_20[slice86]]\r\n",
        "\r\n",
        "winner86 = [train_regret_winner_1[slice86],\r\n",
        "       train_regret_winner_2[slice86],\r\n",
        "       train_regret_winner_3[slice86],\r\n",
        "       train_regret_winner_4[slice86],\r\n",
        "       train_regret_winner_5[slice86],\r\n",
        "       train_regret_winner_6[slice86],\r\n",
        "       train_regret_winner_7[slice86],\r\n",
        "       train_regret_winner_8[slice86],\r\n",
        "       train_regret_winner_9[slice86],\r\n",
        "       train_regret_winner_10[slice86],\r\n",
        "       train_regret_winner_11[slice86],\r\n",
        "       train_regret_winner_12[slice86],\r\n",
        "       train_regret_winner_13[slice86],\r\n",
        "       train_regret_winner_14[slice86],\r\n",
        "       train_regret_winner_15[slice86],\r\n",
        "       train_regret_winner_16[slice86],\r\n",
        "       train_regret_winner_17[slice86],\r\n",
        "       train_regret_winner_18[slice86],\r\n",
        "       train_regret_winner_19[slice86],\r\n",
        "       train_regret_winner_20[slice86]]\r\n",
        "\r\n",
        "loser86_results = pd.DataFrame(loser86).sort_values(by=[0], ascending=False)\r\n",
        "winner86_results = pd.DataFrame(winner86).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser86 = np.asarray(loser86_results[4:5][0])[0]\r\n",
        "median_loser86 = np.asarray(loser86_results[9:10][0])[0]\r\n",
        "upper_loser86 = np.asarray(loser86_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner86 = np.asarray(winner86_results[4:5][0])[0]\r\n",
        "median_winner86 = np.asarray(winner86_results[9:10][0])[0]\r\n",
        "upper_winner86 = np.asarray(winner86_results[14:15][0])[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glsE9_RPxAlS"
      },
      "source": [
        "# Iteration96 :\r\n",
        "\r\n",
        "slice96 = 95\r\n",
        "\r\n",
        "loser96 = [train_regret_loser_1[slice96],\r\n",
        "       train_regret_loser_2[slice96],\r\n",
        "       train_regret_loser_3[slice96],\r\n",
        "       train_regret_loser_4[slice96],\r\n",
        "       train_regret_loser_5[slice96],\r\n",
        "       train_regret_loser_6[slice96],\r\n",
        "       train_regret_loser_7[slice96],\r\n",
        "       train_regret_loser_8[slice96],\r\n",
        "       train_regret_loser_9[slice96],\r\n",
        "       train_regret_loser_10[slice96],\r\n",
        "       train_regret_loser_11[slice96],\r\n",
        "       train_regret_loser_12[slice96],\r\n",
        "       train_regret_loser_13[slice96],\r\n",
        "       train_regret_loser_14[slice96],\r\n",
        "       train_regret_loser_15[slice96],\r\n",
        "       train_regret_loser_16[slice96],\r\n",
        "       train_regret_loser_17[slice96],\r\n",
        "       train_regret_loser_18[slice96],\r\n",
        "       train_regret_loser_19[slice96],\r\n",
        "       train_regret_loser_20[slice96]]\r\n",
        "\r\n",
        "winner96 = [train_regret_winner_1[slice96],\r\n",
        "       train_regret_winner_2[slice96],\r\n",
        "       train_regret_winner_3[slice96],\r\n",
        "       train_regret_winner_4[slice96],\r\n",
        "       train_regret_winner_5[slice96],\r\n",
        "       train_regret_winner_6[slice96],\r\n",
        "       train_regret_winner_7[slice96],\r\n",
        "       train_regret_winner_8[slice96],\r\n",
        "       train_regret_winner_9[slice96],\r\n",
        "       train_regret_winner_10[slice96],\r\n",
        "       train_regret_winner_11[slice96],\r\n",
        "       train_regret_winner_12[slice96],\r\n",
        "       train_regret_winner_13[slice96],\r\n",
        "       train_regret_winner_14[slice96],\r\n",
        "       train_regret_winner_15[slice96],\r\n",
        "       train_regret_winner_16[slice96],\r\n",
        "       train_regret_winner_17[slice96],\r\n",
        "       train_regret_winner_18[slice96],\r\n",
        "       train_regret_winner_19[slice96],\r\n",
        "       train_regret_winner_20[slice96]]\r\n",
        "\r\n",
        "loser96_results = pd.DataFrame(loser96).sort_values(by=[0], ascending=False)\r\n",
        "winner96_results = pd.DataFrame(winner96).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser96 = np.asarray(loser96_results[4:5][0])[0]\r\n",
        "median_loser96 = np.asarray(loser96_results[9:10][0])[0]\r\n",
        "upper_loser96 = np.asarray(loser96_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner96 = np.asarray(winner96_results[4:5][0])[0]\r\n",
        "median_winner96 = np.asarray(winner96_results[9:10][0])[0]\r\n",
        "upper_winner96 = np.asarray(winner96_results[14:15][0])[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IhsEfl1RxAoA"
      },
      "source": [
        "# Iteration7 :\r\n",
        "\r\n",
        "slice7 = 6\r\n",
        "\r\n",
        "loser7 = [train_regret_loser_1[slice7],\r\n",
        "       train_regret_loser_2[slice7],\r\n",
        "       train_regret_loser_3[slice7],\r\n",
        "       train_regret_loser_4[slice7],\r\n",
        "       train_regret_loser_5[slice7],\r\n",
        "       train_regret_loser_6[slice7],\r\n",
        "       train_regret_loser_7[slice7],\r\n",
        "       train_regret_loser_8[slice7],\r\n",
        "       train_regret_loser_9[slice7],\r\n",
        "       train_regret_loser_10[slice7],\r\n",
        "       train_regret_loser_11[slice7],\r\n",
        "       train_regret_loser_12[slice7],\r\n",
        "       train_regret_loser_13[slice7],\r\n",
        "       train_regret_loser_14[slice7],\r\n",
        "       train_regret_loser_15[slice7],\r\n",
        "       train_regret_loser_16[slice7],\r\n",
        "       train_regret_loser_17[slice7],\r\n",
        "       train_regret_loser_18[slice7],\r\n",
        "       train_regret_loser_19[slice7],\r\n",
        "       train_regret_loser_20[slice7]]\r\n",
        "\r\n",
        "winner7 = [train_regret_winner_1[slice7],\r\n",
        "       train_regret_winner_2[slice7],\r\n",
        "       train_regret_winner_3[slice7],\r\n",
        "       train_regret_winner_4[slice7],\r\n",
        "       train_regret_winner_5[slice7],\r\n",
        "       train_regret_winner_6[slice7],\r\n",
        "       train_regret_winner_7[slice7],\r\n",
        "       train_regret_winner_8[slice7],\r\n",
        "       train_regret_winner_9[slice7],\r\n",
        "       train_regret_winner_10[slice7],\r\n",
        "       train_regret_winner_11[slice7],\r\n",
        "       train_regret_winner_12[slice7],\r\n",
        "       train_regret_winner_13[slice7],\r\n",
        "       train_regret_winner_14[slice7],\r\n",
        "       train_regret_winner_15[slice7],\r\n",
        "       train_regret_winner_16[slice7],\r\n",
        "       train_regret_winner_17[slice7],\r\n",
        "       train_regret_winner_18[slice7],\r\n",
        "       train_regret_winner_19[slice7],\r\n",
        "       train_regret_winner_20[slice7]]\r\n",
        "\r\n",
        "loser7_results = pd.DataFrame(loser7).sort_values(by=[0], ascending=False)\r\n",
        "winner7_results = pd.DataFrame(winner7).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser7 = np.asarray(loser7_results[4:5][0])[0]\r\n",
        "median_loser7 = np.asarray(loser7_results[9:10][0])[0]\r\n",
        "upper_loser7 = np.asarray(loser7_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner7 = np.asarray(winner7_results[4:5][0])[0]\r\n",
        "median_winner7 = np.asarray(winner7_results[9:10][0])[0]\r\n",
        "upper_winner7 = np.asarray(winner7_results[14:15][0])[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUCMheLKxArL"
      },
      "source": [
        "# Iteration17 :\r\n",
        "\r\n",
        "slice17 = 16\r\n",
        "\r\n",
        "loser17 = [train_regret_loser_1[slice17],\r\n",
        "       train_regret_loser_2[slice17],\r\n",
        "       train_regret_loser_3[slice17],\r\n",
        "       train_regret_loser_4[slice17],\r\n",
        "       train_regret_loser_5[slice17],\r\n",
        "       train_regret_loser_6[slice17],\r\n",
        "       train_regret_loser_7[slice17],\r\n",
        "       train_regret_loser_8[slice17],\r\n",
        "       train_regret_loser_9[slice17],\r\n",
        "       train_regret_loser_10[slice17],\r\n",
        "       train_regret_loser_11[slice17],\r\n",
        "       train_regret_loser_12[slice17],\r\n",
        "       train_regret_loser_13[slice17],\r\n",
        "       train_regret_loser_14[slice17],\r\n",
        "       train_regret_loser_15[slice17],\r\n",
        "       train_regret_loser_16[slice17],\r\n",
        "       train_regret_loser_17[slice17],\r\n",
        "       train_regret_loser_18[slice17],\r\n",
        "       train_regret_loser_19[slice17],\r\n",
        "       train_regret_loser_20[slice17]]\r\n",
        "\r\n",
        "winner17 = [train_regret_winner_1[slice17],\r\n",
        "       train_regret_winner_2[slice17],\r\n",
        "       train_regret_winner_3[slice17],\r\n",
        "       train_regret_winner_4[slice17],\r\n",
        "       train_regret_winner_5[slice17],\r\n",
        "       train_regret_winner_6[slice17],\r\n",
        "       train_regret_winner_7[slice17],\r\n",
        "       train_regret_winner_8[slice17],\r\n",
        "       train_regret_winner_9[slice17],\r\n",
        "       train_regret_winner_10[slice17],\r\n",
        "       train_regret_winner_11[slice17],\r\n",
        "       train_regret_winner_12[slice17],\r\n",
        "       train_regret_winner_13[slice17],\r\n",
        "       train_regret_winner_14[slice17],\r\n",
        "       train_regret_winner_15[slice17],\r\n",
        "       train_regret_winner_16[slice17],\r\n",
        "       train_regret_winner_17[slice17],\r\n",
        "       train_regret_winner_18[slice17],\r\n",
        "       train_regret_winner_19[slice17],\r\n",
        "       train_regret_winner_20[slice17]]\r\n",
        "\r\n",
        "loser17_results = pd.DataFrame(loser17).sort_values(by=[0], ascending=False)\r\n",
        "winner17_results = pd.DataFrame(winner17).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser17 = np.asarray(loser17_results[4:5][0])[0]\r\n",
        "median_loser17 = np.asarray(loser17_results[9:10][0])[0]\r\n",
        "upper_loser17 = np.asarray(loser17_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner17 = np.asarray(winner17_results[4:5][0])[0]\r\n",
        "median_winner17 = np.asarray(winner17_results[9:10][0])[0]\r\n",
        "upper_winner17 = np.asarray(winner17_results[14:15][0])[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYnBmq7_xAuB"
      },
      "source": [
        "# Iteration27 :\r\n",
        "\r\n",
        "slice27 = 26\r\n",
        "\r\n",
        "loser27 = [train_regret_loser_1[slice27],\r\n",
        "       train_regret_loser_2[slice27],\r\n",
        "       train_regret_loser_3[slice27],\r\n",
        "       train_regret_loser_4[slice27],\r\n",
        "       train_regret_loser_5[slice27],\r\n",
        "       train_regret_loser_6[slice27],\r\n",
        "       train_regret_loser_7[slice27],\r\n",
        "       train_regret_loser_8[slice27],\r\n",
        "       train_regret_loser_9[slice27],\r\n",
        "       train_regret_loser_10[slice27],\r\n",
        "       train_regret_loser_11[slice27],\r\n",
        "       train_regret_loser_12[slice27],\r\n",
        "       train_regret_loser_13[slice27],\r\n",
        "       train_regret_loser_14[slice27],\r\n",
        "       train_regret_loser_15[slice27],\r\n",
        "       train_regret_loser_16[slice27],\r\n",
        "       train_regret_loser_17[slice27],\r\n",
        "       train_regret_loser_18[slice27],\r\n",
        "       train_regret_loser_19[slice27],\r\n",
        "       train_regret_loser_20[slice27]]\r\n",
        "\r\n",
        "winner27 = [train_regret_winner_1[slice27],\r\n",
        "       train_regret_winner_2[slice27],\r\n",
        "       train_regret_winner_3[slice27],\r\n",
        "       train_regret_winner_4[slice27],\r\n",
        "       train_regret_winner_5[slice27],\r\n",
        "       train_regret_winner_6[slice27],\r\n",
        "       train_regret_winner_7[slice27],\r\n",
        "       train_regret_winner_8[slice27],\r\n",
        "       train_regret_winner_9[slice27],\r\n",
        "       train_regret_winner_10[slice27],\r\n",
        "       train_regret_winner_11[slice27],\r\n",
        "       train_regret_winner_12[slice27],\r\n",
        "       train_regret_winner_13[slice27],\r\n",
        "       train_regret_winner_14[slice27],\r\n",
        "       train_regret_winner_15[slice27],\r\n",
        "       train_regret_winner_16[slice27],\r\n",
        "       train_regret_winner_17[slice27],\r\n",
        "       train_regret_winner_18[slice27],\r\n",
        "       train_regret_winner_19[slice27],\r\n",
        "       train_regret_winner_20[slice27]]\r\n",
        "\r\n",
        "loser27_results = pd.DataFrame(loser27).sort_values(by=[0], ascending=False)\r\n",
        "winner27_results = pd.DataFrame(winner27).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser27 = np.asarray(loser27_results[4:5][0])[0]\r\n",
        "median_loser27 = np.asarray(loser27_results[9:10][0])[0]\r\n",
        "upper_loser27 = np.asarray(loser27_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner27 = np.asarray(winner27_results[4:5][0])[0]\r\n",
        "median_winner27 = np.asarray(winner27_results[9:10][0])[0]\r\n",
        "upper_winner27 = np.asarray(winner27_results[14:15][0])[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ChA8KOUxAw6"
      },
      "source": [
        "# Iteration37 :\r\n",
        "\r\n",
        "slice37 = 36\r\n",
        "\r\n",
        "loser37 = [train_regret_loser_1[slice37],\r\n",
        "       train_regret_loser_2[slice37],\r\n",
        "       train_regret_loser_3[slice37],\r\n",
        "       train_regret_loser_4[slice37],\r\n",
        "       train_regret_loser_5[slice37],\r\n",
        "       train_regret_loser_6[slice37],\r\n",
        "       train_regret_loser_7[slice37],\r\n",
        "       train_regret_loser_8[slice37],\r\n",
        "       train_regret_loser_9[slice37],\r\n",
        "       train_regret_loser_10[slice37],\r\n",
        "       train_regret_loser_11[slice37],\r\n",
        "       train_regret_loser_12[slice37],\r\n",
        "       train_regret_loser_13[slice37],\r\n",
        "       train_regret_loser_14[slice37],\r\n",
        "       train_regret_loser_15[slice37],\r\n",
        "       train_regret_loser_16[slice37],\r\n",
        "       train_regret_loser_17[slice37],\r\n",
        "       train_regret_loser_18[slice37],\r\n",
        "       train_regret_loser_19[slice37],\r\n",
        "       train_regret_loser_20[slice37]]\r\n",
        "\r\n",
        "winner37 = [train_regret_winner_1[slice37],\r\n",
        "       train_regret_winner_2[slice37],\r\n",
        "       train_regret_winner_3[slice37],\r\n",
        "       train_regret_winner_4[slice37],\r\n",
        "       train_regret_winner_5[slice37],\r\n",
        "       train_regret_winner_6[slice37],\r\n",
        "       train_regret_winner_7[slice37],\r\n",
        "       train_regret_winner_8[slice37],\r\n",
        "       train_regret_winner_9[slice37],\r\n",
        "       train_regret_winner_10[slice37],\r\n",
        "       train_regret_winner_11[slice37],\r\n",
        "       train_regret_winner_12[slice37],\r\n",
        "       train_regret_winner_13[slice37],\r\n",
        "       train_regret_winner_14[slice37],\r\n",
        "       train_regret_winner_15[slice37],\r\n",
        "       train_regret_winner_16[slice37],\r\n",
        "       train_regret_winner_17[slice37],\r\n",
        "       train_regret_winner_18[slice37],\r\n",
        "       train_regret_winner_19[slice37],\r\n",
        "       train_regret_winner_20[slice37]]\r\n",
        "\r\n",
        "loser37_results = pd.DataFrame(loser37).sort_values(by=[0], ascending=False)\r\n",
        "winner37_results = pd.DataFrame(winner37).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser37 = np.asarray(loser37_results[4:5][0])[0]\r\n",
        "median_loser37 = np.asarray(loser37_results[9:10][0])[0]\r\n",
        "upper_loser37 = np.asarray(loser37_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner37 = np.asarray(winner37_results[4:5][0])[0]\r\n",
        "median_winner37 = np.asarray(winner37_results[9:10][0])[0]\r\n",
        "upper_winner37 = np.asarray(winner37_results[14:15][0])[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVsY1buDxAzx"
      },
      "source": [
        "# Iteration47 :\r\n",
        "\r\n",
        "slice47 = 46\r\n",
        "\r\n",
        "loser47 = [train_regret_loser_1[slice47],\r\n",
        "       train_regret_loser_2[slice47],\r\n",
        "       train_regret_loser_3[slice47],\r\n",
        "       train_regret_loser_4[slice47],\r\n",
        "       train_regret_loser_5[slice47],\r\n",
        "       train_regret_loser_6[slice47],\r\n",
        "       train_regret_loser_7[slice47],\r\n",
        "       train_regret_loser_8[slice47],\r\n",
        "       train_regret_loser_9[slice47],\r\n",
        "       train_regret_loser_10[slice47],\r\n",
        "       train_regret_loser_11[slice47],\r\n",
        "       train_regret_loser_12[slice47],\r\n",
        "       train_regret_loser_13[slice47],\r\n",
        "       train_regret_loser_14[slice47],\r\n",
        "       train_regret_loser_15[slice47],\r\n",
        "       train_regret_loser_16[slice47],\r\n",
        "       train_regret_loser_17[slice47],\r\n",
        "       train_regret_loser_18[slice47],\r\n",
        "       train_regret_loser_19[slice47],\r\n",
        "       train_regret_loser_20[slice47]]\r\n",
        "\r\n",
        "winner47 = [train_regret_winner_1[slice47],\r\n",
        "       train_regret_winner_2[slice47],\r\n",
        "       train_regret_winner_3[slice47],\r\n",
        "       train_regret_winner_4[slice47],\r\n",
        "       train_regret_winner_5[slice47],\r\n",
        "       train_regret_winner_6[slice47],\r\n",
        "       train_regret_winner_7[slice47],\r\n",
        "       train_regret_winner_8[slice47],\r\n",
        "       train_regret_winner_9[slice47],\r\n",
        "       train_regret_winner_10[slice47],\r\n",
        "       train_regret_winner_11[slice47],\r\n",
        "       train_regret_winner_12[slice47],\r\n",
        "       train_regret_winner_13[slice47],\r\n",
        "       train_regret_winner_14[slice47],\r\n",
        "       train_regret_winner_15[slice47],\r\n",
        "       train_regret_winner_16[slice47],\r\n",
        "       train_regret_winner_17[slice47],\r\n",
        "       train_regret_winner_18[slice47],\r\n",
        "       train_regret_winner_19[slice47],\r\n",
        "       train_regret_winner_20[slice47]]\r\n",
        "\r\n",
        "loser47_results = pd.DataFrame(loser47).sort_values(by=[0], ascending=False)\r\n",
        "winner47_results = pd.DataFrame(winner47).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser47 = np.asarray(loser47_results[4:5][0])[0]\r\n",
        "median_loser47 = np.asarray(loser47_results[9:10][0])[0]\r\n",
        "upper_loser47 = np.asarray(loser47_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner47 = np.asarray(winner47_results[4:5][0])[0]\r\n",
        "median_winner47 = np.asarray(winner47_results[9:10][0])[0]\r\n",
        "upper_winner47 = np.asarray(winner47_results[14:15][0])[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fIy_EyTxA20"
      },
      "source": [
        "# Iteration57 :\r\n",
        "\r\n",
        "slice57 = 56\r\n",
        "\r\n",
        "loser57 = [train_regret_loser_1[slice57],\r\n",
        "       train_regret_loser_2[slice57],\r\n",
        "       train_regret_loser_3[slice57],\r\n",
        "       train_regret_loser_4[slice57],\r\n",
        "       train_regret_loser_5[slice57],\r\n",
        "       train_regret_loser_6[slice57],\r\n",
        "       train_regret_loser_7[slice57],\r\n",
        "       train_regret_loser_8[slice57],\r\n",
        "       train_regret_loser_9[slice57],\r\n",
        "       train_regret_loser_10[slice57],\r\n",
        "       train_regret_loser_11[slice57],\r\n",
        "       train_regret_loser_12[slice57],\r\n",
        "       train_regret_loser_13[slice57],\r\n",
        "       train_regret_loser_14[slice57],\r\n",
        "       train_regret_loser_15[slice57],\r\n",
        "       train_regret_loser_16[slice57],\r\n",
        "       train_regret_loser_17[slice57],\r\n",
        "       train_regret_loser_18[slice57],\r\n",
        "       train_regret_loser_19[slice57],\r\n",
        "       train_regret_loser_20[slice57]]\r\n",
        "\r\n",
        "winner57 = [train_regret_winner_1[slice57],\r\n",
        "       train_regret_winner_2[slice57],\r\n",
        "       train_regret_winner_3[slice57],\r\n",
        "       train_regret_winner_4[slice57],\r\n",
        "       train_regret_winner_5[slice57],\r\n",
        "       train_regret_winner_6[slice57],\r\n",
        "       train_regret_winner_7[slice57],\r\n",
        "       train_regret_winner_8[slice57],\r\n",
        "       train_regret_winner_9[slice57],\r\n",
        "       train_regret_winner_10[slice57],\r\n",
        "       train_regret_winner_11[slice57],\r\n",
        "       train_regret_winner_12[slice57],\r\n",
        "       train_regret_winner_13[slice57],\r\n",
        "       train_regret_winner_14[slice57],\r\n",
        "       train_regret_winner_15[slice57],\r\n",
        "       train_regret_winner_16[slice57],\r\n",
        "       train_regret_winner_17[slice57],\r\n",
        "       train_regret_winner_18[slice57],\r\n",
        "       train_regret_winner_19[slice57],\r\n",
        "       train_regret_winner_20[slice57]]\r\n",
        "\r\n",
        "loser57_results = pd.DataFrame(loser57).sort_values(by=[0], ascending=False)\r\n",
        "winner57_results = pd.DataFrame(winner57).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser57 = np.asarray(loser57_results[4:5][0])[0]\r\n",
        "median_loser57 = np.asarray(loser57_results[9:10][0])[0]\r\n",
        "upper_loser57 = np.asarray(loser57_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner57 = np.asarray(winner57_results[4:5][0])[0]\r\n",
        "median_winner57 = np.asarray(winner57_results[9:10][0])[0]\r\n",
        "upper_winner57 = np.asarray(winner57_results[14:15][0])[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umwdBl8XxA5r"
      },
      "source": [
        "# Iteration67 :\r\n",
        "\r\n",
        "slice67 = 66\r\n",
        "\r\n",
        "loser67 = [train_regret_loser_1[slice67],\r\n",
        "       train_regret_loser_2[slice67],\r\n",
        "       train_regret_loser_3[slice67],\r\n",
        "       train_regret_loser_4[slice67],\r\n",
        "       train_regret_loser_5[slice67],\r\n",
        "       train_regret_loser_6[slice67],\r\n",
        "       train_regret_loser_7[slice67],\r\n",
        "       train_regret_loser_8[slice67],\r\n",
        "       train_regret_loser_9[slice67],\r\n",
        "       train_regret_loser_10[slice67],\r\n",
        "       train_regret_loser_11[slice67],\r\n",
        "       train_regret_loser_12[slice67],\r\n",
        "       train_regret_loser_13[slice67],\r\n",
        "       train_regret_loser_14[slice67],\r\n",
        "       train_regret_loser_15[slice67],\r\n",
        "       train_regret_loser_16[slice67],\r\n",
        "       train_regret_loser_17[slice67],\r\n",
        "       train_regret_loser_18[slice67],\r\n",
        "       train_regret_loser_19[slice67],\r\n",
        "       train_regret_loser_20[slice67]]\r\n",
        "\r\n",
        "winner67 = [train_regret_winner_1[slice67],\r\n",
        "       train_regret_winner_2[slice67],\r\n",
        "       train_regret_winner_3[slice67],\r\n",
        "       train_regret_winner_4[slice67],\r\n",
        "       train_regret_winner_5[slice67],\r\n",
        "       train_regret_winner_6[slice67],\r\n",
        "       train_regret_winner_7[slice67],\r\n",
        "       train_regret_winner_8[slice67],\r\n",
        "       train_regret_winner_9[slice67],\r\n",
        "       train_regret_winner_10[slice67],\r\n",
        "       train_regret_winner_11[slice67],\r\n",
        "       train_regret_winner_12[slice67],\r\n",
        "       train_regret_winner_13[slice67],\r\n",
        "       train_regret_winner_14[slice67],\r\n",
        "       train_regret_winner_15[slice67],\r\n",
        "       train_regret_winner_16[slice67],\r\n",
        "       train_regret_winner_17[slice67],\r\n",
        "       train_regret_winner_18[slice67],\r\n",
        "       train_regret_winner_19[slice67],\r\n",
        "       train_regret_winner_20[slice67]]\r\n",
        "\r\n",
        "loser67_results = pd.DataFrame(loser67).sort_values(by=[0], ascending=False)\r\n",
        "winner67_results = pd.DataFrame(winner67).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser67 = np.asarray(loser67_results[4:5][0])[0]\r\n",
        "median_loser67 = np.asarray(loser67_results[9:10][0])[0]\r\n",
        "upper_loser67 = np.asarray(loser67_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner67 = np.asarray(winner67_results[4:5][0])[0]\r\n",
        "median_winner67 = np.asarray(winner67_results[9:10][0])[0]\r\n",
        "upper_winner67 = np.asarray(winner67_results[14:15][0])[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lF3KFX-ZxA8q"
      },
      "source": [
        "# Iteration77 :\r\n",
        "\r\n",
        "slice77 = 76\r\n",
        "\r\n",
        "loser77 = [train_regret_loser_1[slice77],\r\n",
        "       train_regret_loser_2[slice77],\r\n",
        "       train_regret_loser_3[slice77],\r\n",
        "       train_regret_loser_4[slice77],\r\n",
        "       train_regret_loser_5[slice77],\r\n",
        "       train_regret_loser_6[slice77],\r\n",
        "       train_regret_loser_7[slice77],\r\n",
        "       train_regret_loser_8[slice77],\r\n",
        "       train_regret_loser_9[slice77],\r\n",
        "       train_regret_loser_10[slice77],\r\n",
        "       train_regret_loser_11[slice77],\r\n",
        "       train_regret_loser_12[slice77],\r\n",
        "       train_regret_loser_13[slice77],\r\n",
        "       train_regret_loser_14[slice77],\r\n",
        "       train_regret_loser_15[slice77],\r\n",
        "       train_regret_loser_16[slice77],\r\n",
        "       train_regret_loser_17[slice77],\r\n",
        "       train_regret_loser_18[slice77],\r\n",
        "       train_regret_loser_19[slice77],\r\n",
        "       train_regret_loser_20[slice77]]\r\n",
        "\r\n",
        "winner77 = [train_regret_winner_1[slice77],\r\n",
        "       train_regret_winner_2[slice77],\r\n",
        "       train_regret_winner_3[slice77],\r\n",
        "       train_regret_winner_4[slice77],\r\n",
        "       train_regret_winner_5[slice77],\r\n",
        "       train_regret_winner_6[slice77],\r\n",
        "       train_regret_winner_7[slice77],\r\n",
        "       train_regret_winner_8[slice77],\r\n",
        "       train_regret_winner_9[slice77],\r\n",
        "       train_regret_winner_10[slice77],\r\n",
        "       train_regret_winner_11[slice77],\r\n",
        "       train_regret_winner_12[slice77],\r\n",
        "       train_regret_winner_13[slice77],\r\n",
        "       train_regret_winner_14[slice77],\r\n",
        "       train_regret_winner_15[slice77],\r\n",
        "       train_regret_winner_16[slice77],\r\n",
        "       train_regret_winner_17[slice77],\r\n",
        "       train_regret_winner_18[slice77],\r\n",
        "       train_regret_winner_19[slice77],\r\n",
        "       train_regret_winner_20[slice77]]\r\n",
        "\r\n",
        "loser77_results = pd.DataFrame(loser77).sort_values(by=[0], ascending=False)\r\n",
        "winner77_results = pd.DataFrame(winner77).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser77 = np.asarray(loser77_results[4:5][0])[0]\r\n",
        "median_loser77 = np.asarray(loser77_results[9:10][0])[0]\r\n",
        "upper_loser77 = np.asarray(loser77_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner77 = np.asarray(winner77_results[4:5][0])[0]\r\n",
        "median_winner77 = np.asarray(winner77_results[9:10][0])[0]\r\n",
        "upper_winner77 = np.asarray(winner77_results[14:15][0])[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7CVYft39xA_t"
      },
      "source": [
        "# Iteration87 :\r\n",
        "\r\n",
        "slice87 = 86\r\n",
        "\r\n",
        "loser87 = [train_regret_loser_1[slice87],\r\n",
        "       train_regret_loser_2[slice87],\r\n",
        "       train_regret_loser_3[slice87],\r\n",
        "       train_regret_loser_4[slice87],\r\n",
        "       train_regret_loser_5[slice87],\r\n",
        "       train_regret_loser_6[slice87],\r\n",
        "       train_regret_loser_7[slice87],\r\n",
        "       train_regret_loser_8[slice87],\r\n",
        "       train_regret_loser_9[slice87],\r\n",
        "       train_regret_loser_10[slice87],\r\n",
        "       train_regret_loser_11[slice87],\r\n",
        "       train_regret_loser_12[slice87],\r\n",
        "       train_regret_loser_13[slice87],\r\n",
        "       train_regret_loser_14[slice87],\r\n",
        "       train_regret_loser_15[slice87],\r\n",
        "       train_regret_loser_16[slice87],\r\n",
        "       train_regret_loser_17[slice87],\r\n",
        "       train_regret_loser_18[slice87],\r\n",
        "       train_regret_loser_19[slice87],\r\n",
        "       train_regret_loser_20[slice87]]\r\n",
        "\r\n",
        "winner87 = [train_regret_winner_1[slice87],\r\n",
        "       train_regret_winner_2[slice87],\r\n",
        "       train_regret_winner_3[slice87],\r\n",
        "       train_regret_winner_4[slice87],\r\n",
        "       train_regret_winner_5[slice87],\r\n",
        "       train_regret_winner_6[slice87],\r\n",
        "       train_regret_winner_7[slice87],\r\n",
        "       train_regret_winner_8[slice87],\r\n",
        "       train_regret_winner_9[slice87],\r\n",
        "       train_regret_winner_10[slice87],\r\n",
        "       train_regret_winner_11[slice87],\r\n",
        "       train_regret_winner_12[slice87],\r\n",
        "       train_regret_winner_13[slice87],\r\n",
        "       train_regret_winner_14[slice87],\r\n",
        "       train_regret_winner_15[slice87],\r\n",
        "       train_regret_winner_16[slice87],\r\n",
        "       train_regret_winner_17[slice87],\r\n",
        "       train_regret_winner_18[slice87],\r\n",
        "       train_regret_winner_19[slice87],\r\n",
        "       train_regret_winner_20[slice87]]\r\n",
        "\r\n",
        "loser87_results = pd.DataFrame(loser87).sort_values(by=[0], ascending=False)\r\n",
        "winner87_results = pd.DataFrame(winner87).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser87 = np.asarray(loser87_results[4:5][0])[0]\r\n",
        "median_loser87 = np.asarray(loser87_results[9:10][0])[0]\r\n",
        "upper_loser87 = np.asarray(loser87_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner87 = np.asarray(winner87_results[4:5][0])[0]\r\n",
        "median_winner87 = np.asarray(winner87_results[9:10][0])[0]\r\n",
        "upper_winner87 = np.asarray(winner87_results[14:15][0])[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0gPkLC0yxBCs"
      },
      "source": [
        "# Iteration97 :\r\n",
        "\r\n",
        "slice97 = 96\r\n",
        "\r\n",
        "loser97 = [train_regret_loser_1[slice97],\r\n",
        "       train_regret_loser_2[slice97],\r\n",
        "       train_regret_loser_3[slice97],\r\n",
        "       train_regret_loser_4[slice97],\r\n",
        "       train_regret_loser_5[slice97],\r\n",
        "       train_regret_loser_6[slice97],\r\n",
        "       train_regret_loser_7[slice97],\r\n",
        "       train_regret_loser_8[slice97],\r\n",
        "       train_regret_loser_9[slice97],\r\n",
        "       train_regret_loser_10[slice97],\r\n",
        "       train_regret_loser_11[slice97],\r\n",
        "       train_regret_loser_12[slice97],\r\n",
        "       train_regret_loser_13[slice97],\r\n",
        "       train_regret_loser_14[slice97],\r\n",
        "       train_regret_loser_15[slice97],\r\n",
        "       train_regret_loser_16[slice97],\r\n",
        "       train_regret_loser_17[slice97],\r\n",
        "       train_regret_loser_18[slice97],\r\n",
        "       train_regret_loser_19[slice97],\r\n",
        "       train_regret_loser_20[slice97]]\r\n",
        "\r\n",
        "winner97 = [train_regret_winner_1[slice97],\r\n",
        "       train_regret_winner_2[slice97],\r\n",
        "       train_regret_winner_3[slice97],\r\n",
        "       train_regret_winner_4[slice97],\r\n",
        "       train_regret_winner_5[slice97],\r\n",
        "       train_regret_winner_6[slice97],\r\n",
        "       train_regret_winner_7[slice97],\r\n",
        "       train_regret_winner_8[slice97],\r\n",
        "       train_regret_winner_9[slice97],\r\n",
        "       train_regret_winner_10[slice97],\r\n",
        "       train_regret_winner_11[slice97],\r\n",
        "       train_regret_winner_12[slice97],\r\n",
        "       train_regret_winner_13[slice97],\r\n",
        "       train_regret_winner_14[slice97],\r\n",
        "       train_regret_winner_15[slice97],\r\n",
        "       train_regret_winner_16[slice97],\r\n",
        "       train_regret_winner_17[slice97],\r\n",
        "       train_regret_winner_18[slice97],\r\n",
        "       train_regret_winner_19[slice97],\r\n",
        "       train_regret_winner_20[slice97]]\r\n",
        "\r\n",
        "loser97_results = pd.DataFrame(loser97).sort_values(by=[0], ascending=False)\r\n",
        "winner97_results = pd.DataFrame(winner97).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser97 = np.asarray(loser97_results[4:5][0])[0]\r\n",
        "median_loser97 = np.asarray(loser97_results[9:10][0])[0]\r\n",
        "upper_loser97 = np.asarray(loser97_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner97 = np.asarray(winner97_results[4:5][0])[0]\r\n",
        "median_winner97 = np.asarray(winner97_results[9:10][0])[0]\r\n",
        "upper_winner97 = np.asarray(winner97_results[14:15][0])[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BbgYEpYUxBFr"
      },
      "source": [
        "# Iteration8 :\r\n",
        "\r\n",
        "slice8 = 7\r\n",
        "\r\n",
        "loser8 = [train_regret_loser_1[slice8],\r\n",
        "       train_regret_loser_2[slice8],\r\n",
        "       train_regret_loser_3[slice8],\r\n",
        "       train_regret_loser_4[slice8],\r\n",
        "       train_regret_loser_5[slice8],\r\n",
        "       train_regret_loser_6[slice8],\r\n",
        "       train_regret_loser_7[slice8],\r\n",
        "       train_regret_loser_8[slice8],\r\n",
        "       train_regret_loser_9[slice8],\r\n",
        "       train_regret_loser_10[slice8],\r\n",
        "       train_regret_loser_11[slice8],\r\n",
        "       train_regret_loser_12[slice8],\r\n",
        "       train_regret_loser_13[slice8],\r\n",
        "       train_regret_loser_14[slice8],\r\n",
        "       train_regret_loser_15[slice8],\r\n",
        "       train_regret_loser_16[slice8],\r\n",
        "       train_regret_loser_17[slice8],\r\n",
        "       train_regret_loser_18[slice8],\r\n",
        "       train_regret_loser_19[slice8],\r\n",
        "       train_regret_loser_20[slice8]]\r\n",
        "\r\n",
        "winner8 = [train_regret_winner_1[slice8],\r\n",
        "       train_regret_winner_2[slice8],\r\n",
        "       train_regret_winner_3[slice8],\r\n",
        "       train_regret_winner_4[slice8],\r\n",
        "       train_regret_winner_5[slice8],\r\n",
        "       train_regret_winner_6[slice8],\r\n",
        "       train_regret_winner_7[slice8],\r\n",
        "       train_regret_winner_8[slice8],\r\n",
        "       train_regret_winner_9[slice8],\r\n",
        "       train_regret_winner_10[slice8],\r\n",
        "       train_regret_winner_11[slice8],\r\n",
        "       train_regret_winner_12[slice8],\r\n",
        "       train_regret_winner_13[slice8],\r\n",
        "       train_regret_winner_14[slice8],\r\n",
        "       train_regret_winner_15[slice8],\r\n",
        "       train_regret_winner_16[slice8],\r\n",
        "       train_regret_winner_17[slice8],\r\n",
        "       train_regret_winner_18[slice8],\r\n",
        "       train_regret_winner_19[slice8],\r\n",
        "       train_regret_winner_20[slice8]]\r\n",
        "\r\n",
        "loser8_results = pd.DataFrame(loser8).sort_values(by=[0], ascending=False)\r\n",
        "winner8_results = pd.DataFrame(winner8).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser8 = np.asarray(loser8_results[4:5][0])[0]\r\n",
        "median_loser8 = np.asarray(loser8_results[9:10][0])[0]\r\n",
        "upper_loser8 = np.asarray(loser8_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner8 = np.asarray(winner8_results[4:5][0])[0]\r\n",
        "median_winner8 = np.asarray(winner8_results[9:10][0])[0]\r\n",
        "upper_winner8 = np.asarray(winner8_results[14:15][0])[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "im9dHUqvxBH9"
      },
      "source": [
        "# Iteration18 :\r\n",
        "\r\n",
        "slice18 = 17\r\n",
        "\r\n",
        "loser18 = [train_regret_loser_1[slice18],\r\n",
        "       train_regret_loser_2[slice18],\r\n",
        "       train_regret_loser_3[slice18],\r\n",
        "       train_regret_loser_4[slice18],\r\n",
        "       train_regret_loser_5[slice18],\r\n",
        "       train_regret_loser_6[slice18],\r\n",
        "       train_regret_loser_7[slice18],\r\n",
        "       train_regret_loser_8[slice18],\r\n",
        "       train_regret_loser_9[slice18],\r\n",
        "       train_regret_loser_10[slice18],\r\n",
        "       train_regret_loser_11[slice18],\r\n",
        "       train_regret_loser_12[slice18],\r\n",
        "       train_regret_loser_13[slice18],\r\n",
        "       train_regret_loser_14[slice18],\r\n",
        "       train_regret_loser_15[slice18],\r\n",
        "       train_regret_loser_16[slice18],\r\n",
        "       train_regret_loser_17[slice18],\r\n",
        "       train_regret_loser_18[slice18],\r\n",
        "       train_regret_loser_19[slice18],\r\n",
        "       train_regret_loser_20[slice18]]\r\n",
        "\r\n",
        "winner18 = [train_regret_winner_1[slice18],\r\n",
        "       train_regret_winner_2[slice18],\r\n",
        "       train_regret_winner_3[slice18],\r\n",
        "       train_regret_winner_4[slice18],\r\n",
        "       train_regret_winner_5[slice18],\r\n",
        "       train_regret_winner_6[slice18],\r\n",
        "       train_regret_winner_7[slice18],\r\n",
        "       train_regret_winner_8[slice18],\r\n",
        "       train_regret_winner_9[slice18],\r\n",
        "       train_regret_winner_10[slice18],\r\n",
        "       train_regret_winner_11[slice18],\r\n",
        "       train_regret_winner_12[slice18],\r\n",
        "       train_regret_winner_13[slice18],\r\n",
        "       train_regret_winner_14[slice18],\r\n",
        "       train_regret_winner_15[slice18],\r\n",
        "       train_regret_winner_16[slice18],\r\n",
        "       train_regret_winner_17[slice18],\r\n",
        "       train_regret_winner_18[slice18],\r\n",
        "       train_regret_winner_19[slice18],\r\n",
        "       train_regret_winner_20[slice18]]\r\n",
        "\r\n",
        "loser18_results = pd.DataFrame(loser18).sort_values(by=[0], ascending=False)\r\n",
        "winner18_results = pd.DataFrame(winner18).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser18 = np.asarray(loser18_results[4:5][0])[0]\r\n",
        "median_loser18 = np.asarray(loser18_results[9:10][0])[0]\r\n",
        "upper_loser18 = np.asarray(loser18_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner18 = np.asarray(winner18_results[4:5][0])[0]\r\n",
        "median_winner18 = np.asarray(winner18_results[9:10][0])[0]\r\n",
        "upper_winner18 = np.asarray(winner18_results[14:15][0])[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IvqFjlBDxBK4"
      },
      "source": [
        "# Iteration28 :\r\n",
        "\r\n",
        "slice28 = 27\r\n",
        "\r\n",
        "loser28 = [train_regret_loser_1[slice28],\r\n",
        "       train_regret_loser_2[slice28],\r\n",
        "       train_regret_loser_3[slice28],\r\n",
        "       train_regret_loser_4[slice28],\r\n",
        "       train_regret_loser_5[slice28],\r\n",
        "       train_regret_loser_6[slice28],\r\n",
        "       train_regret_loser_7[slice28],\r\n",
        "       train_regret_loser_8[slice28],\r\n",
        "       train_regret_loser_9[slice28],\r\n",
        "       train_regret_loser_10[slice28],\r\n",
        "       train_regret_loser_11[slice28],\r\n",
        "       train_regret_loser_12[slice28],\r\n",
        "       train_regret_loser_13[slice28],\r\n",
        "       train_regret_loser_14[slice28],\r\n",
        "       train_regret_loser_15[slice28],\r\n",
        "       train_regret_loser_16[slice28],\r\n",
        "       train_regret_loser_17[slice28],\r\n",
        "       train_regret_loser_18[slice28],\r\n",
        "       train_regret_loser_19[slice28],\r\n",
        "       train_regret_loser_20[slice28]]\r\n",
        "\r\n",
        "winner28 = [train_regret_winner_1[slice28],\r\n",
        "       train_regret_winner_2[slice28],\r\n",
        "       train_regret_winner_3[slice28],\r\n",
        "       train_regret_winner_4[slice28],\r\n",
        "       train_regret_winner_5[slice28],\r\n",
        "       train_regret_winner_6[slice28],\r\n",
        "       train_regret_winner_7[slice28],\r\n",
        "       train_regret_winner_8[slice28],\r\n",
        "       train_regret_winner_9[slice28],\r\n",
        "       train_regret_winner_10[slice28],\r\n",
        "       train_regret_winner_11[slice28],\r\n",
        "       train_regret_winner_12[slice28],\r\n",
        "       train_regret_winner_13[slice28],\r\n",
        "       train_regret_winner_14[slice28],\r\n",
        "       train_regret_winner_15[slice28],\r\n",
        "       train_regret_winner_16[slice28],\r\n",
        "       train_regret_winner_17[slice28],\r\n",
        "       train_regret_winner_18[slice28],\r\n",
        "       train_regret_winner_19[slice28],\r\n",
        "       train_regret_winner_20[slice28]]\r\n",
        "\r\n",
        "loser28_results = pd.DataFrame(loser28).sort_values(by=[0], ascending=False)\r\n",
        "winner28_results = pd.DataFrame(winner28).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser28 = np.asarray(loser28_results[4:5][0])[0]\r\n",
        "median_loser28 = np.asarray(loser28_results[9:10][0])[0]\r\n",
        "upper_loser28 = np.asarray(loser28_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner28 = np.asarray(winner28_results[4:5][0])[0]\r\n",
        "median_winner28 = np.asarray(winner28_results[9:10][0])[0]\r\n",
        "upper_winner28 = np.asarray(winner28_results[14:15][0])[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "It5daJCEted3"
      },
      "source": [
        "# Iteration38 :\r\n",
        "\r\n",
        "slice38 = 37\r\n",
        "\r\n",
        "loser38 = [train_regret_loser_1[slice38],\r\n",
        "       train_regret_loser_2[slice38],\r\n",
        "       train_regret_loser_3[slice38],\r\n",
        "       train_regret_loser_4[slice38],\r\n",
        "       train_regret_loser_5[slice38],\r\n",
        "       train_regret_loser_6[slice38],\r\n",
        "       train_regret_loser_7[slice38],\r\n",
        "       train_regret_loser_8[slice38],\r\n",
        "       train_regret_loser_9[slice38],\r\n",
        "       train_regret_loser_10[slice38],\r\n",
        "       train_regret_loser_11[slice38],\r\n",
        "       train_regret_loser_12[slice38],\r\n",
        "       train_regret_loser_13[slice38],\r\n",
        "       train_regret_loser_14[slice38],\r\n",
        "       train_regret_loser_15[slice38],\r\n",
        "       train_regret_loser_16[slice38],\r\n",
        "       train_regret_loser_17[slice38],\r\n",
        "       train_regret_loser_18[slice38],\r\n",
        "       train_regret_loser_19[slice38],\r\n",
        "       train_regret_loser_20[slice38]]\r\n",
        "\r\n",
        "winner38 = [train_regret_winner_1[slice38],\r\n",
        "       train_regret_winner_2[slice38],\r\n",
        "       train_regret_winner_3[slice38],\r\n",
        "       train_regret_winner_4[slice38],\r\n",
        "       train_regret_winner_5[slice38],\r\n",
        "       train_regret_winner_6[slice38],\r\n",
        "       train_regret_winner_7[slice38],\r\n",
        "       train_regret_winner_8[slice38],\r\n",
        "       train_regret_winner_9[slice38],\r\n",
        "       train_regret_winner_10[slice38],\r\n",
        "       train_regret_winner_11[slice38],\r\n",
        "       train_regret_winner_12[slice38],\r\n",
        "       train_regret_winner_13[slice38],\r\n",
        "       train_regret_winner_14[slice38],\r\n",
        "       train_regret_winner_15[slice38],\r\n",
        "       train_regret_winner_16[slice38],\r\n",
        "       train_regret_winner_17[slice38],\r\n",
        "       train_regret_winner_18[slice38],\r\n",
        "       train_regret_winner_19[slice38],\r\n",
        "       train_regret_winner_20[slice38]]\r\n",
        "\r\n",
        "loser38_results = pd.DataFrame(loser38).sort_values(by=[0], ascending=False)\r\n",
        "winner38_results = pd.DataFrame(winner38).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser38 = np.asarray(loser38_results[4:5][0])[0]\r\n",
        "median_loser38 = np.asarray(loser38_results[9:10][0])[0]\r\n",
        "upper_loser38 = np.asarray(loser38_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner38 = np.asarray(winner38_results[4:5][0])[0]\r\n",
        "median_winner38 = np.asarray(winner38_results[9:10][0])[0]\r\n",
        "upper_winner38 = np.asarray(winner38_results[14:15][0])[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GekXOf_gtegw"
      },
      "source": [
        "# Iteration48 :\r\n",
        "\r\n",
        "slice48 = 47\r\n",
        "\r\n",
        "loser48 = [train_regret_loser_1[slice48],\r\n",
        "       train_regret_loser_2[slice48],\r\n",
        "       train_regret_loser_3[slice48],\r\n",
        "       train_regret_loser_4[slice48],\r\n",
        "       train_regret_loser_5[slice48],\r\n",
        "       train_regret_loser_6[slice48],\r\n",
        "       train_regret_loser_7[slice48],\r\n",
        "       train_regret_loser_8[slice48],\r\n",
        "       train_regret_loser_9[slice48],\r\n",
        "       train_regret_loser_10[slice48],\r\n",
        "       train_regret_loser_11[slice48],\r\n",
        "       train_regret_loser_12[slice48],\r\n",
        "       train_regret_loser_13[slice48],\r\n",
        "       train_regret_loser_14[slice48],\r\n",
        "       train_regret_loser_15[slice48],\r\n",
        "       train_regret_loser_16[slice48],\r\n",
        "       train_regret_loser_17[slice48],\r\n",
        "       train_regret_loser_18[slice48],\r\n",
        "       train_regret_loser_19[slice48],\r\n",
        "       train_regret_loser_20[slice48]]\r\n",
        "\r\n",
        "winner48 = [train_regret_winner_1[slice48],\r\n",
        "       train_regret_winner_2[slice48],\r\n",
        "       train_regret_winner_3[slice48],\r\n",
        "       train_regret_winner_4[slice48],\r\n",
        "       train_regret_winner_5[slice48],\r\n",
        "       train_regret_winner_6[slice48],\r\n",
        "       train_regret_winner_7[slice48],\r\n",
        "       train_regret_winner_8[slice48],\r\n",
        "       train_regret_winner_9[slice48],\r\n",
        "       train_regret_winner_10[slice48],\r\n",
        "       train_regret_winner_11[slice48],\r\n",
        "       train_regret_winner_12[slice48],\r\n",
        "       train_regret_winner_13[slice48],\r\n",
        "       train_regret_winner_14[slice48],\r\n",
        "       train_regret_winner_15[slice48],\r\n",
        "       train_regret_winner_16[slice48],\r\n",
        "       train_regret_winner_17[slice48],\r\n",
        "       train_regret_winner_18[slice48],\r\n",
        "       train_regret_winner_19[slice48],\r\n",
        "       train_regret_winner_20[slice48]]\r\n",
        "\r\n",
        "loser48_results = pd.DataFrame(loser48).sort_values(by=[0], ascending=False)\r\n",
        "winner48_results = pd.DataFrame(winner48).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser48 = np.asarray(loser48_results[4:5][0])[0]\r\n",
        "median_loser48 = np.asarray(loser48_results[9:10][0])[0]\r\n",
        "upper_loser48 = np.asarray(loser48_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner48 = np.asarray(winner48_results[4:5][0])[0]\r\n",
        "median_winner48 = np.asarray(winner48_results[9:10][0])[0]\r\n",
        "upper_winner48 = np.asarray(winner48_results[14:15][0])[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNpV6knOtejv"
      },
      "source": [
        "# Iteration58 :\r\n",
        "\r\n",
        "slice58 = 57\r\n",
        "\r\n",
        "loser58 = [train_regret_loser_1[slice58],\r\n",
        "       train_regret_loser_2[slice58],\r\n",
        "       train_regret_loser_3[slice58],\r\n",
        "       train_regret_loser_4[slice58],\r\n",
        "       train_regret_loser_5[slice58],\r\n",
        "       train_regret_loser_6[slice58],\r\n",
        "       train_regret_loser_7[slice58],\r\n",
        "       train_regret_loser_8[slice58],\r\n",
        "       train_regret_loser_9[slice58],\r\n",
        "       train_regret_loser_10[slice58],\r\n",
        "       train_regret_loser_11[slice58],\r\n",
        "       train_regret_loser_12[slice58],\r\n",
        "       train_regret_loser_13[slice58],\r\n",
        "       train_regret_loser_14[slice58],\r\n",
        "       train_regret_loser_15[slice58],\r\n",
        "       train_regret_loser_16[slice58],\r\n",
        "       train_regret_loser_17[slice58],\r\n",
        "       train_regret_loser_18[slice58],\r\n",
        "       train_regret_loser_19[slice58],\r\n",
        "       train_regret_loser_20[slice58]]\r\n",
        "\r\n",
        "winner58 = [train_regret_winner_1[slice58],\r\n",
        "       train_regret_winner_2[slice58],\r\n",
        "       train_regret_winner_3[slice58],\r\n",
        "       train_regret_winner_4[slice58],\r\n",
        "       train_regret_winner_5[slice58],\r\n",
        "       train_regret_winner_6[slice58],\r\n",
        "       train_regret_winner_7[slice58],\r\n",
        "       train_regret_winner_8[slice58],\r\n",
        "       train_regret_winner_9[slice58],\r\n",
        "       train_regret_winner_10[slice58],\r\n",
        "       train_regret_winner_11[slice58],\r\n",
        "       train_regret_winner_12[slice58],\r\n",
        "       train_regret_winner_13[slice58],\r\n",
        "       train_regret_winner_14[slice58],\r\n",
        "       train_regret_winner_15[slice58],\r\n",
        "       train_regret_winner_16[slice58],\r\n",
        "       train_regret_winner_17[slice58],\r\n",
        "       train_regret_winner_18[slice58],\r\n",
        "       train_regret_winner_19[slice58],\r\n",
        "       train_regret_winner_20[slice58]]\r\n",
        "\r\n",
        "loser58_results = pd.DataFrame(loser58).sort_values(by=[0], ascending=False)\r\n",
        "winner58_results = pd.DataFrame(winner58).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser58 = np.asarray(loser58_results[4:5][0])[0]\r\n",
        "median_loser58 = np.asarray(loser58_results[9:10][0])[0]\r\n",
        "upper_loser58 = np.asarray(loser58_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner58 = np.asarray(winner58_results[4:5][0])[0]\r\n",
        "median_winner58 = np.asarray(winner58_results[9:10][0])[0]\r\n",
        "upper_winner58 = np.asarray(winner58_results[14:15][0])[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VFsqaBqtemn"
      },
      "source": [
        "# Iteration68 :\r\n",
        "\r\n",
        "slice68 = 67\r\n",
        "\r\n",
        "loser68 = [train_regret_loser_1[slice68],\r\n",
        "       train_regret_loser_2[slice68],\r\n",
        "       train_regret_loser_3[slice68],\r\n",
        "       train_regret_loser_4[slice68],\r\n",
        "       train_regret_loser_5[slice68],\r\n",
        "       train_regret_loser_6[slice68],\r\n",
        "       train_regret_loser_7[slice68],\r\n",
        "       train_regret_loser_8[slice68],\r\n",
        "       train_regret_loser_9[slice68],\r\n",
        "       train_regret_loser_10[slice68],\r\n",
        "       train_regret_loser_11[slice68],\r\n",
        "       train_regret_loser_12[slice68],\r\n",
        "       train_regret_loser_13[slice68],\r\n",
        "       train_regret_loser_14[slice68],\r\n",
        "       train_regret_loser_15[slice68],\r\n",
        "       train_regret_loser_16[slice68],\r\n",
        "       train_regret_loser_17[slice68],\r\n",
        "       train_regret_loser_18[slice68],\r\n",
        "       train_regret_loser_19[slice68],\r\n",
        "       train_regret_loser_20[slice68]]\r\n",
        "\r\n",
        "winner68 = [train_regret_winner_1[slice68],\r\n",
        "       train_regret_winner_2[slice68],\r\n",
        "       train_regret_winner_3[slice68],\r\n",
        "       train_regret_winner_4[slice68],\r\n",
        "       train_regret_winner_5[slice68],\r\n",
        "       train_regret_winner_6[slice68],\r\n",
        "       train_regret_winner_7[slice68],\r\n",
        "       train_regret_winner_8[slice68],\r\n",
        "       train_regret_winner_9[slice68],\r\n",
        "       train_regret_winner_10[slice68],\r\n",
        "       train_regret_winner_11[slice68],\r\n",
        "       train_regret_winner_12[slice68],\r\n",
        "       train_regret_winner_13[slice68],\r\n",
        "       train_regret_winner_14[slice68],\r\n",
        "       train_regret_winner_15[slice68],\r\n",
        "       train_regret_winner_16[slice68],\r\n",
        "       train_regret_winner_17[slice68],\r\n",
        "       train_regret_winner_18[slice68],\r\n",
        "       train_regret_winner_19[slice68],\r\n",
        "       train_regret_winner_20[slice68]]\r\n",
        "\r\n",
        "loser68_results = pd.DataFrame(loser68).sort_values(by=[0], ascending=False)\r\n",
        "winner68_results = pd.DataFrame(winner68).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser68 = np.asarray(loser68_results[4:5][0])[0]\r\n",
        "median_loser68 = np.asarray(loser68_results[9:10][0])[0]\r\n",
        "upper_loser68 = np.asarray(loser68_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner68 = np.asarray(winner68_results[4:5][0])[0]\r\n",
        "median_winner68 = np.asarray(winner68_results[9:10][0])[0]\r\n",
        "upper_winner68 = np.asarray(winner68_results[14:15][0])[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XlFH0Wxtepo"
      },
      "source": [
        "# Iteration78 :\r\n",
        "\r\n",
        "slice78 = 77\r\n",
        "\r\n",
        "loser78 = [train_regret_loser_1[slice78],\r\n",
        "       train_regret_loser_2[slice78],\r\n",
        "       train_regret_loser_3[slice78],\r\n",
        "       train_regret_loser_4[slice78],\r\n",
        "       train_regret_loser_5[slice78],\r\n",
        "       train_regret_loser_6[slice78],\r\n",
        "       train_regret_loser_7[slice78],\r\n",
        "       train_regret_loser_8[slice78],\r\n",
        "       train_regret_loser_9[slice78],\r\n",
        "       train_regret_loser_10[slice78],\r\n",
        "       train_regret_loser_11[slice78],\r\n",
        "       train_regret_loser_12[slice78],\r\n",
        "       train_regret_loser_13[slice78],\r\n",
        "       train_regret_loser_14[slice78],\r\n",
        "       train_regret_loser_15[slice78],\r\n",
        "       train_regret_loser_16[slice78],\r\n",
        "       train_regret_loser_17[slice78],\r\n",
        "       train_regret_loser_18[slice78],\r\n",
        "       train_regret_loser_19[slice78],\r\n",
        "       train_regret_loser_20[slice78]]\r\n",
        "\r\n",
        "winner78 = [train_regret_winner_1[slice78],\r\n",
        "       train_regret_winner_2[slice78],\r\n",
        "       train_regret_winner_3[slice78],\r\n",
        "       train_regret_winner_4[slice78],\r\n",
        "       train_regret_winner_5[slice78],\r\n",
        "       train_regret_winner_6[slice78],\r\n",
        "       train_regret_winner_7[slice78],\r\n",
        "       train_regret_winner_8[slice78],\r\n",
        "       train_regret_winner_9[slice78],\r\n",
        "       train_regret_winner_10[slice78],\r\n",
        "       train_regret_winner_11[slice78],\r\n",
        "       train_regret_winner_12[slice78],\r\n",
        "       train_regret_winner_13[slice78],\r\n",
        "       train_regret_winner_14[slice78],\r\n",
        "       train_regret_winner_15[slice78],\r\n",
        "       train_regret_winner_16[slice78],\r\n",
        "       train_regret_winner_17[slice78],\r\n",
        "       train_regret_winner_18[slice78],\r\n",
        "       train_regret_winner_19[slice78],\r\n",
        "       train_regret_winner_20[slice78]]\r\n",
        "\r\n",
        "loser78_results = pd.DataFrame(loser78).sort_values(by=[0], ascending=False)\r\n",
        "winner78_results = pd.DataFrame(winner78).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser78 = np.asarray(loser78_results[4:5][0])[0]\r\n",
        "median_loser78 = np.asarray(loser78_results[9:10][0])[0]\r\n",
        "upper_loser78 = np.asarray(loser78_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner78 = np.asarray(winner78_results[4:5][0])[0]\r\n",
        "median_winner78 = np.asarray(winner78_results[9:10][0])[0]\r\n",
        "upper_winner78 = np.asarray(winner78_results[14:15][0])[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsL0PJVetesC"
      },
      "source": [
        "# Iteration88 :\r\n",
        "\r\n",
        "slice88 = 87\r\n",
        "\r\n",
        "loser88 = [train_regret_loser_1[slice88],\r\n",
        "       train_regret_loser_2[slice88],\r\n",
        "       train_regret_loser_3[slice88],\r\n",
        "       train_regret_loser_4[slice88],\r\n",
        "       train_regret_loser_5[slice88],\r\n",
        "       train_regret_loser_6[slice88],\r\n",
        "       train_regret_loser_7[slice88],\r\n",
        "       train_regret_loser_8[slice88],\r\n",
        "       train_regret_loser_9[slice88],\r\n",
        "       train_regret_loser_10[slice88],\r\n",
        "       train_regret_loser_11[slice88],\r\n",
        "       train_regret_loser_12[slice88],\r\n",
        "       train_regret_loser_13[slice88],\r\n",
        "       train_regret_loser_14[slice88],\r\n",
        "       train_regret_loser_15[slice88],\r\n",
        "       train_regret_loser_16[slice88],\r\n",
        "       train_regret_loser_17[slice88],\r\n",
        "       train_regret_loser_18[slice88],\r\n",
        "       train_regret_loser_19[slice88],\r\n",
        "       train_regret_loser_20[slice88]]\r\n",
        "\r\n",
        "winner88 = [train_regret_winner_1[slice88],\r\n",
        "       train_regret_winner_2[slice88],\r\n",
        "       train_regret_winner_3[slice88],\r\n",
        "       train_regret_winner_4[slice88],\r\n",
        "       train_regret_winner_5[slice88],\r\n",
        "       train_regret_winner_6[slice88],\r\n",
        "       train_regret_winner_7[slice88],\r\n",
        "       train_regret_winner_8[slice88],\r\n",
        "       train_regret_winner_9[slice88],\r\n",
        "       train_regret_winner_10[slice88],\r\n",
        "       train_regret_winner_11[slice88],\r\n",
        "       train_regret_winner_12[slice88],\r\n",
        "       train_regret_winner_13[slice88],\r\n",
        "       train_regret_winner_14[slice88],\r\n",
        "       train_regret_winner_15[slice88],\r\n",
        "       train_regret_winner_16[slice88],\r\n",
        "       train_regret_winner_17[slice88],\r\n",
        "       train_regret_winner_18[slice88],\r\n",
        "       train_regret_winner_19[slice88],\r\n",
        "       train_regret_winner_20[slice88]]\r\n",
        "\r\n",
        "loser88_results = pd.DataFrame(loser88).sort_values(by=[0], ascending=False)\r\n",
        "winner88_results = pd.DataFrame(winner88).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser88 = np.asarray(loser88_results[4:5][0])[0]\r\n",
        "median_loser88 = np.asarray(loser88_results[9:10][0])[0]\r\n",
        "upper_loser88 = np.asarray(loser88_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner88 = np.asarray(winner88_results[4:5][0])[0]\r\n",
        "median_winner88 = np.asarray(winner88_results[9:10][0])[0]\r\n",
        "upper_winner88 = np.asarray(winner88_results[14:15][0])[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYWEWGku1o_9"
      },
      "source": [
        "# Iteration98 :\r\n",
        "\r\n",
        "slice98 = 97\r\n",
        "\r\n",
        "loser98 = [train_regret_loser_1[slice98],\r\n",
        "       train_regret_loser_2[slice98],\r\n",
        "       train_regret_loser_3[slice98],\r\n",
        "       train_regret_loser_4[slice98],\r\n",
        "       train_regret_loser_5[slice98],\r\n",
        "       train_regret_loser_6[slice98],\r\n",
        "       train_regret_loser_7[slice98],\r\n",
        "       train_regret_loser_8[slice98],\r\n",
        "       train_regret_loser_9[slice98],\r\n",
        "       train_regret_loser_10[slice98],\r\n",
        "       train_regret_loser_11[slice98],\r\n",
        "       train_regret_loser_12[slice98],\r\n",
        "       train_regret_loser_13[slice98],\r\n",
        "       train_regret_loser_14[slice98],\r\n",
        "       train_regret_loser_15[slice98],\r\n",
        "       train_regret_loser_16[slice98],\r\n",
        "       train_regret_loser_17[slice98],\r\n",
        "       train_regret_loser_18[slice98],\r\n",
        "       train_regret_loser_19[slice98],\r\n",
        "       train_regret_loser_20[slice98]]\r\n",
        "\r\n",
        "winner98 = [train_regret_winner_1[slice98],\r\n",
        "       train_regret_winner_2[slice98],\r\n",
        "       train_regret_winner_3[slice98],\r\n",
        "       train_regret_winner_4[slice98],\r\n",
        "       train_regret_winner_5[slice98],\r\n",
        "       train_regret_winner_6[slice98],\r\n",
        "       train_regret_winner_7[slice98],\r\n",
        "       train_regret_winner_8[slice98],\r\n",
        "       train_regret_winner_9[slice98],\r\n",
        "       train_regret_winner_10[slice98],\r\n",
        "       train_regret_winner_11[slice98],\r\n",
        "       train_regret_winner_12[slice98],\r\n",
        "       train_regret_winner_13[slice98],\r\n",
        "       train_regret_winner_14[slice98],\r\n",
        "       train_regret_winner_15[slice98],\r\n",
        "       train_regret_winner_16[slice98],\r\n",
        "       train_regret_winner_17[slice98],\r\n",
        "       train_regret_winner_18[slice98],\r\n",
        "       train_regret_winner_19[slice98],\r\n",
        "       train_regret_winner_20[slice98]]\r\n",
        "\r\n",
        "loser98_results = pd.DataFrame(loser98).sort_values(by=[0], ascending=False)\r\n",
        "winner98_results = pd.DataFrame(winner98).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser98 = np.asarray(loser98_results[4:5][0])[0]\r\n",
        "median_loser98 = np.asarray(loser98_results[9:10][0])[0]\r\n",
        "upper_loser98 = np.asarray(loser98_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner98 = np.asarray(winner98_results[4:5][0])[0]\r\n",
        "median_winner98 = np.asarray(winner98_results[9:10][0])[0]\r\n",
        "upper_winner98 = np.asarray(winner98_results[14:15][0])[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "guLGgcX91pDK"
      },
      "source": [
        "# Iteration9 :\r\n",
        "\r\n",
        "slice9 = 8\r\n",
        "\r\n",
        "loser9 = [train_regret_loser_1[slice9],\r\n",
        "       train_regret_loser_2[slice9],\r\n",
        "       train_regret_loser_3[slice9],\r\n",
        "       train_regret_loser_4[slice9],\r\n",
        "       train_regret_loser_5[slice9],\r\n",
        "       train_regret_loser_6[slice9],\r\n",
        "       train_regret_loser_7[slice9],\r\n",
        "       train_regret_loser_8[slice9],\r\n",
        "       train_regret_loser_9[slice9],\r\n",
        "       train_regret_loser_10[slice9],\r\n",
        "       train_regret_loser_11[slice9],\r\n",
        "       train_regret_loser_12[slice9],\r\n",
        "       train_regret_loser_13[slice9],\r\n",
        "       train_regret_loser_14[slice9],\r\n",
        "       train_regret_loser_15[slice9],\r\n",
        "       train_regret_loser_16[slice9],\r\n",
        "       train_regret_loser_17[slice9],\r\n",
        "       train_regret_loser_18[slice9],\r\n",
        "       train_regret_loser_19[slice9],\r\n",
        "       train_regret_loser_20[slice9]]\r\n",
        "\r\n",
        "winner9 = [train_regret_winner_1[slice9],\r\n",
        "       train_regret_winner_2[slice9],\r\n",
        "       train_regret_winner_3[slice9],\r\n",
        "       train_regret_winner_4[slice9],\r\n",
        "       train_regret_winner_5[slice9],\r\n",
        "       train_regret_winner_6[slice9],\r\n",
        "       train_regret_winner_7[slice9],\r\n",
        "       train_regret_winner_8[slice9],\r\n",
        "       train_regret_winner_9[slice9],\r\n",
        "       train_regret_winner_10[slice9],\r\n",
        "       train_regret_winner_11[slice9],\r\n",
        "       train_regret_winner_12[slice9],\r\n",
        "       train_regret_winner_13[slice9],\r\n",
        "       train_regret_winner_14[slice9],\r\n",
        "       train_regret_winner_15[slice9],\r\n",
        "       train_regret_winner_16[slice9],\r\n",
        "       train_regret_winner_17[slice9],\r\n",
        "       train_regret_winner_18[slice9],\r\n",
        "       train_regret_winner_19[slice9],\r\n",
        "       train_regret_winner_20[slice9]]\r\n",
        "\r\n",
        "loser9_results = pd.DataFrame(loser9).sort_values(by=[0], ascending=False)\r\n",
        "winner9_results = pd.DataFrame(winner9).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser9 = np.asarray(loser9_results[4:5][0])[0]\r\n",
        "median_loser9 = np.asarray(loser9_results[9:10][0])[0]\r\n",
        "upper_loser9 = np.asarray(loser9_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner9 = np.asarray(winner9_results[4:5][0])[0]\r\n",
        "median_winner9 = np.asarray(winner9_results[9:10][0])[0]\r\n",
        "upper_winner9 = np.asarray(winner9_results[14:15][0])[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3_9-eoT1pGd"
      },
      "source": [
        "# Iteration19 :\r\n",
        "\r\n",
        "slice19 = 18\r\n",
        "\r\n",
        "loser19 = [train_regret_loser_1[slice19],\r\n",
        "       train_regret_loser_2[slice19],\r\n",
        "       train_regret_loser_3[slice19],\r\n",
        "       train_regret_loser_4[slice19],\r\n",
        "       train_regret_loser_5[slice19],\r\n",
        "       train_regret_loser_6[slice19],\r\n",
        "       train_regret_loser_7[slice19],\r\n",
        "       train_regret_loser_8[slice19],\r\n",
        "       train_regret_loser_9[slice19],\r\n",
        "       train_regret_loser_10[slice19],\r\n",
        "       train_regret_loser_11[slice19],\r\n",
        "       train_regret_loser_12[slice19],\r\n",
        "       train_regret_loser_13[slice19],\r\n",
        "       train_regret_loser_14[slice19],\r\n",
        "       train_regret_loser_15[slice19],\r\n",
        "       train_regret_loser_16[slice19],\r\n",
        "       train_regret_loser_17[slice19],\r\n",
        "       train_regret_loser_18[slice19],\r\n",
        "       train_regret_loser_19[slice19],\r\n",
        "       train_regret_loser_20[slice19]]\r\n",
        "\r\n",
        "winner19 = [train_regret_winner_1[slice19],\r\n",
        "       train_regret_winner_2[slice19],\r\n",
        "       train_regret_winner_3[slice19],\r\n",
        "       train_regret_winner_4[slice19],\r\n",
        "       train_regret_winner_5[slice19],\r\n",
        "       train_regret_winner_6[slice19],\r\n",
        "       train_regret_winner_7[slice19],\r\n",
        "       train_regret_winner_8[slice19],\r\n",
        "       train_regret_winner_9[slice19],\r\n",
        "       train_regret_winner_10[slice19],\r\n",
        "       train_regret_winner_11[slice19],\r\n",
        "       train_regret_winner_12[slice19],\r\n",
        "       train_regret_winner_13[slice19],\r\n",
        "       train_regret_winner_14[slice19],\r\n",
        "       train_regret_winner_15[slice19],\r\n",
        "       train_regret_winner_16[slice19],\r\n",
        "       train_regret_winner_17[slice19],\r\n",
        "       train_regret_winner_18[slice19],\r\n",
        "       train_regret_winner_19[slice19],\r\n",
        "       train_regret_winner_20[slice19]]\r\n",
        "\r\n",
        "loser19_results = pd.DataFrame(loser19).sort_values(by=[0], ascending=False)\r\n",
        "winner19_results = pd.DataFrame(winner19).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser19 = np.asarray(loser19_results[4:5][0])[0]\r\n",
        "median_loser19 = np.asarray(loser19_results[9:10][0])[0]\r\n",
        "upper_loser19 = np.asarray(loser19_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner19 = np.asarray(winner19_results[4:5][0])[0]\r\n",
        "median_winner19 = np.asarray(winner19_results[9:10][0])[0]\r\n",
        "upper_winner19 = np.asarray(winner19_results[14:15][0])[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ae5x13M1pRp"
      },
      "source": [
        "# Iteration29 :\r\n",
        "\r\n",
        "slice29 = 28\r\n",
        "\r\n",
        "loser29 = [train_regret_loser_1[slice29],\r\n",
        "       train_regret_loser_2[slice29],\r\n",
        "       train_regret_loser_3[slice29],\r\n",
        "       train_regret_loser_4[slice29],\r\n",
        "       train_regret_loser_5[slice29],\r\n",
        "       train_regret_loser_6[slice29],\r\n",
        "       train_regret_loser_7[slice29],\r\n",
        "       train_regret_loser_8[slice29],\r\n",
        "       train_regret_loser_9[slice29],\r\n",
        "       train_regret_loser_10[slice29],\r\n",
        "       train_regret_loser_11[slice29],\r\n",
        "       train_regret_loser_12[slice29],\r\n",
        "       train_regret_loser_13[slice29],\r\n",
        "       train_regret_loser_14[slice29],\r\n",
        "       train_regret_loser_15[slice29],\r\n",
        "       train_regret_loser_16[slice29],\r\n",
        "       train_regret_loser_17[slice29],\r\n",
        "       train_regret_loser_18[slice29],\r\n",
        "       train_regret_loser_19[slice29],\r\n",
        "       train_regret_loser_20[slice29]]\r\n",
        "\r\n",
        "winner29 = [train_regret_winner_1[slice29],\r\n",
        "       train_regret_winner_2[slice29],\r\n",
        "       train_regret_winner_3[slice29],\r\n",
        "       train_regret_winner_4[slice29],\r\n",
        "       train_regret_winner_5[slice29],\r\n",
        "       train_regret_winner_6[slice29],\r\n",
        "       train_regret_winner_7[slice29],\r\n",
        "       train_regret_winner_8[slice29],\r\n",
        "       train_regret_winner_9[slice29],\r\n",
        "       train_regret_winner_10[slice29],\r\n",
        "       train_regret_winner_11[slice29],\r\n",
        "       train_regret_winner_12[slice29],\r\n",
        "       train_regret_winner_13[slice29],\r\n",
        "       train_regret_winner_14[slice29],\r\n",
        "       train_regret_winner_15[slice29],\r\n",
        "       train_regret_winner_16[slice29],\r\n",
        "       train_regret_winner_17[slice29],\r\n",
        "       train_regret_winner_18[slice29],\r\n",
        "       train_regret_winner_19[slice29],\r\n",
        "       train_regret_winner_20[slice29]]\r\n",
        "\r\n",
        "loser29_results = pd.DataFrame(loser29).sort_values(by=[0], ascending=False)\r\n",
        "winner29_results = pd.DataFrame(winner29).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser29 = np.asarray(loser29_results[4:5][0])[0]\r\n",
        "median_loser29 = np.asarray(loser29_results[9:10][0])[0]\r\n",
        "upper_loser29 = np.asarray(loser29_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner29 = np.asarray(winner29_results[4:5][0])[0]\r\n",
        "median_winner29 = np.asarray(winner29_results[9:10][0])[0]\r\n",
        "upper_winner29 = np.asarray(winner29_results[14:15][0])[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XR9aPvBy1pUc"
      },
      "source": [
        "# Iteration39 :\r\n",
        "\r\n",
        "slice39 = 38\r\n",
        "\r\n",
        "loser39 = [train_regret_loser_1[slice39],\r\n",
        "       train_regret_loser_2[slice39],\r\n",
        "       train_regret_loser_3[slice39],\r\n",
        "       train_regret_loser_4[slice39],\r\n",
        "       train_regret_loser_5[slice39],\r\n",
        "       train_regret_loser_6[slice39],\r\n",
        "       train_regret_loser_7[slice39],\r\n",
        "       train_regret_loser_8[slice39],\r\n",
        "       train_regret_loser_9[slice39],\r\n",
        "       train_regret_loser_10[slice39],\r\n",
        "       train_regret_loser_11[slice39],\r\n",
        "       train_regret_loser_12[slice39],\r\n",
        "       train_regret_loser_13[slice39],\r\n",
        "       train_regret_loser_14[slice39],\r\n",
        "       train_regret_loser_15[slice39],\r\n",
        "       train_regret_loser_16[slice39],\r\n",
        "       train_regret_loser_17[slice39],\r\n",
        "       train_regret_loser_18[slice39],\r\n",
        "       train_regret_loser_19[slice39],\r\n",
        "       train_regret_loser_20[slice39]]\r\n",
        "\r\n",
        "winner39 = [train_regret_winner_1[slice39],\r\n",
        "       train_regret_winner_2[slice39],\r\n",
        "       train_regret_winner_3[slice39],\r\n",
        "       train_regret_winner_4[slice39],\r\n",
        "       train_regret_winner_5[slice39],\r\n",
        "       train_regret_winner_6[slice39],\r\n",
        "       train_regret_winner_7[slice39],\r\n",
        "       train_regret_winner_8[slice39],\r\n",
        "       train_regret_winner_9[slice39],\r\n",
        "       train_regret_winner_10[slice39],\r\n",
        "       train_regret_winner_11[slice39],\r\n",
        "       train_regret_winner_12[slice39],\r\n",
        "       train_regret_winner_13[slice39],\r\n",
        "       train_regret_winner_14[slice39],\r\n",
        "       train_regret_winner_15[slice39],\r\n",
        "       train_regret_winner_16[slice39],\r\n",
        "       train_regret_winner_17[slice39],\r\n",
        "       train_regret_winner_18[slice39],\r\n",
        "       train_regret_winner_19[slice39],\r\n",
        "       train_regret_winner_20[slice39]]\r\n",
        "\r\n",
        "loser39_results = pd.DataFrame(loser39).sort_values(by=[0], ascending=False)\r\n",
        "winner39_results = pd.DataFrame(winner39).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser39 = np.asarray(loser39_results[4:5][0])[0]\r\n",
        "median_loser39 = np.asarray(loser39_results[9:10][0])[0]\r\n",
        "upper_loser39 = np.asarray(loser39_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner39 = np.asarray(winner39_results[4:5][0])[0]\r\n",
        "median_winner39 = np.asarray(winner39_results[9:10][0])[0]\r\n",
        "upper_winner39 = np.asarray(winner39_results[14:15][0])[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vb7vh2K1pXp"
      },
      "source": [
        "# Iteration49 :\r\n",
        "\r\n",
        "slice49 = 48\r\n",
        "\r\n",
        "loser49 = [train_regret_loser_1[slice49],\r\n",
        "       train_regret_loser_2[slice49],\r\n",
        "       train_regret_loser_3[slice49],\r\n",
        "       train_regret_loser_4[slice49],\r\n",
        "       train_regret_loser_5[slice49],\r\n",
        "       train_regret_loser_6[slice49],\r\n",
        "       train_regret_loser_7[slice49],\r\n",
        "       train_regret_loser_8[slice49],\r\n",
        "       train_regret_loser_9[slice49],\r\n",
        "       train_regret_loser_10[slice49],\r\n",
        "       train_regret_loser_11[slice49],\r\n",
        "       train_regret_loser_12[slice49],\r\n",
        "       train_regret_loser_13[slice49],\r\n",
        "       train_regret_loser_14[slice49],\r\n",
        "       train_regret_loser_15[slice49],\r\n",
        "       train_regret_loser_16[slice49],\r\n",
        "       train_regret_loser_17[slice49],\r\n",
        "       train_regret_loser_18[slice49],\r\n",
        "       train_regret_loser_19[slice49],\r\n",
        "       train_regret_loser_20[slice49]]\r\n",
        "\r\n",
        "winner49 = [train_regret_winner_1[slice49],\r\n",
        "       train_regret_winner_2[slice49],\r\n",
        "       train_regret_winner_3[slice49],\r\n",
        "       train_regret_winner_4[slice49],\r\n",
        "       train_regret_winner_5[slice49],\r\n",
        "       train_regret_winner_6[slice49],\r\n",
        "       train_regret_winner_7[slice49],\r\n",
        "       train_regret_winner_8[slice49],\r\n",
        "       train_regret_winner_9[slice49],\r\n",
        "       train_regret_winner_10[slice49],\r\n",
        "       train_regret_winner_11[slice49],\r\n",
        "       train_regret_winner_12[slice49],\r\n",
        "       train_regret_winner_13[slice49],\r\n",
        "       train_regret_winner_14[slice49],\r\n",
        "       train_regret_winner_15[slice49],\r\n",
        "       train_regret_winner_16[slice49],\r\n",
        "       train_regret_winner_17[slice49],\r\n",
        "       train_regret_winner_18[slice49],\r\n",
        "       train_regret_winner_19[slice49],\r\n",
        "       train_regret_winner_20[slice49]]\r\n",
        "\r\n",
        "loser49_results = pd.DataFrame(loser49).sort_values(by=[0], ascending=False)\r\n",
        "winner49_results = pd.DataFrame(winner49).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser49 = np.asarray(loser49_results[4:5][0])[0]\r\n",
        "median_loser49 = np.asarray(loser49_results[9:10][0])[0]\r\n",
        "upper_loser49 = np.asarray(loser49_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner49 = np.asarray(winner49_results[4:5][0])[0]\r\n",
        "median_winner49 = np.asarray(winner49_results[9:10][0])[0]\r\n",
        "upper_winner49 = np.asarray(winner49_results[14:15][0])[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3wN7DWwt1paM"
      },
      "source": [
        "# Iteration59 :\r\n",
        "\r\n",
        "slice59 = 58\r\n",
        "\r\n",
        "loser59 = [train_regret_loser_1[slice59],\r\n",
        "       train_regret_loser_2[slice59],\r\n",
        "       train_regret_loser_3[slice59],\r\n",
        "       train_regret_loser_4[slice59],\r\n",
        "       train_regret_loser_5[slice59],\r\n",
        "       train_regret_loser_6[slice59],\r\n",
        "       train_regret_loser_7[slice59],\r\n",
        "       train_regret_loser_8[slice59],\r\n",
        "       train_regret_loser_9[slice59],\r\n",
        "       train_regret_loser_10[slice59],\r\n",
        "       train_regret_loser_11[slice59],\r\n",
        "       train_regret_loser_12[slice59],\r\n",
        "       train_regret_loser_13[slice59],\r\n",
        "       train_regret_loser_14[slice59],\r\n",
        "       train_regret_loser_15[slice59],\r\n",
        "       train_regret_loser_16[slice59],\r\n",
        "       train_regret_loser_17[slice59],\r\n",
        "       train_regret_loser_18[slice59],\r\n",
        "       train_regret_loser_19[slice59],\r\n",
        "       train_regret_loser_20[slice59]]\r\n",
        "\r\n",
        "winner59 = [train_regret_winner_1[slice59],\r\n",
        "       train_regret_winner_2[slice59],\r\n",
        "       train_regret_winner_3[slice59],\r\n",
        "       train_regret_winner_4[slice59],\r\n",
        "       train_regret_winner_5[slice59],\r\n",
        "       train_regret_winner_6[slice59],\r\n",
        "       train_regret_winner_7[slice59],\r\n",
        "       train_regret_winner_8[slice59],\r\n",
        "       train_regret_winner_9[slice59],\r\n",
        "       train_regret_winner_10[slice59],\r\n",
        "       train_regret_winner_11[slice59],\r\n",
        "       train_regret_winner_12[slice59],\r\n",
        "       train_regret_winner_13[slice59],\r\n",
        "       train_regret_winner_14[slice59],\r\n",
        "       train_regret_winner_15[slice59],\r\n",
        "       train_regret_winner_16[slice59],\r\n",
        "       train_regret_winner_17[slice59],\r\n",
        "       train_regret_winner_18[slice59],\r\n",
        "       train_regret_winner_19[slice59],\r\n",
        "       train_regret_winner_20[slice59]]\r\n",
        "\r\n",
        "loser59_results = pd.DataFrame(loser59).sort_values(by=[0], ascending=False)\r\n",
        "winner59_results = pd.DataFrame(winner59).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser59 = np.asarray(loser59_results[4:5][0])[0]\r\n",
        "median_loser59 = np.asarray(loser59_results[9:10][0])[0]\r\n",
        "upper_loser59 = np.asarray(loser59_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner59 = np.asarray(winner59_results[4:5][0])[0]\r\n",
        "median_winner59 = np.asarray(winner59_results[9:10][0])[0]\r\n",
        "upper_winner59 = np.asarray(winner59_results[14:15][0])[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqtgekml1pde"
      },
      "source": [
        "# Iteration69 :\r\n",
        "\r\n",
        "slice69 = 68\r\n",
        "\r\n",
        "loser69 = [train_regret_loser_1[slice69],\r\n",
        "       train_regret_loser_2[slice69],\r\n",
        "       train_regret_loser_3[slice69],\r\n",
        "       train_regret_loser_4[slice69],\r\n",
        "       train_regret_loser_5[slice69],\r\n",
        "       train_regret_loser_6[slice69],\r\n",
        "       train_regret_loser_7[slice69],\r\n",
        "       train_regret_loser_8[slice69],\r\n",
        "       train_regret_loser_9[slice69],\r\n",
        "       train_regret_loser_10[slice69],\r\n",
        "       train_regret_loser_11[slice69],\r\n",
        "       train_regret_loser_12[slice69],\r\n",
        "       train_regret_loser_13[slice69],\r\n",
        "       train_regret_loser_14[slice69],\r\n",
        "       train_regret_loser_15[slice69],\r\n",
        "       train_regret_loser_16[slice69],\r\n",
        "       train_regret_loser_17[slice69],\r\n",
        "       train_regret_loser_18[slice69],\r\n",
        "       train_regret_loser_19[slice69],\r\n",
        "       train_regret_loser_20[slice69]]\r\n",
        "\r\n",
        "winner69 = [train_regret_winner_1[slice69],\r\n",
        "       train_regret_winner_2[slice69],\r\n",
        "       train_regret_winner_3[slice69],\r\n",
        "       train_regret_winner_4[slice69],\r\n",
        "       train_regret_winner_5[slice69],\r\n",
        "       train_regret_winner_6[slice69],\r\n",
        "       train_regret_winner_7[slice69],\r\n",
        "       train_regret_winner_8[slice69],\r\n",
        "       train_regret_winner_9[slice69],\r\n",
        "       train_regret_winner_10[slice69],\r\n",
        "       train_regret_winner_11[slice69],\r\n",
        "       train_regret_winner_12[slice69],\r\n",
        "       train_regret_winner_13[slice69],\r\n",
        "       train_regret_winner_14[slice69],\r\n",
        "       train_regret_winner_15[slice69],\r\n",
        "       train_regret_winner_16[slice69],\r\n",
        "       train_regret_winner_17[slice69],\r\n",
        "       train_regret_winner_18[slice69],\r\n",
        "       train_regret_winner_19[slice69],\r\n",
        "       train_regret_winner_20[slice69]]\r\n",
        "\r\n",
        "loser69_results = pd.DataFrame(loser69).sort_values(by=[0], ascending=False)\r\n",
        "winner69_results = pd.DataFrame(winner69).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser69 = np.asarray(loser69_results[4:5][0])[0]\r\n",
        "median_loser69 = np.asarray(loser69_results[9:10][0])[0]\r\n",
        "upper_loser69 = np.asarray(loser69_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner69 = np.asarray(winner69_results[4:5][0])[0]\r\n",
        "median_winner69 = np.asarray(winner69_results[9:10][0])[0]\r\n",
        "upper_winner69 = np.asarray(winner69_results[14:15][0])[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPFTa1171pgF"
      },
      "source": [
        "# Iteration79 :\r\n",
        "\r\n",
        "slice79 = 78\r\n",
        "\r\n",
        "loser79 = [train_regret_loser_1[slice79],\r\n",
        "       train_regret_loser_2[slice79],\r\n",
        "       train_regret_loser_3[slice79],\r\n",
        "       train_regret_loser_4[slice79],\r\n",
        "       train_regret_loser_5[slice79],\r\n",
        "       train_regret_loser_6[slice79],\r\n",
        "       train_regret_loser_7[slice79],\r\n",
        "       train_regret_loser_8[slice79],\r\n",
        "       train_regret_loser_9[slice79],\r\n",
        "       train_regret_loser_10[slice79],\r\n",
        "       train_regret_loser_11[slice79],\r\n",
        "       train_regret_loser_12[slice79],\r\n",
        "       train_regret_loser_13[slice79],\r\n",
        "       train_regret_loser_14[slice79],\r\n",
        "       train_regret_loser_15[slice79],\r\n",
        "       train_regret_loser_16[slice79],\r\n",
        "       train_regret_loser_17[slice79],\r\n",
        "       train_regret_loser_18[slice79],\r\n",
        "       train_regret_loser_19[slice79],\r\n",
        "       train_regret_loser_20[slice79]]\r\n",
        "\r\n",
        "winner79 = [train_regret_winner_1[slice79],\r\n",
        "       train_regret_winner_2[slice79],\r\n",
        "       train_regret_winner_3[slice79],\r\n",
        "       train_regret_winner_4[slice79],\r\n",
        "       train_regret_winner_5[slice79],\r\n",
        "       train_regret_winner_6[slice79],\r\n",
        "       train_regret_winner_7[slice79],\r\n",
        "       train_regret_winner_8[slice79],\r\n",
        "       train_regret_winner_9[slice79],\r\n",
        "       train_regret_winner_10[slice79],\r\n",
        "       train_regret_winner_11[slice79],\r\n",
        "       train_regret_winner_12[slice79],\r\n",
        "       train_regret_winner_13[slice79],\r\n",
        "       train_regret_winner_14[slice79],\r\n",
        "       train_regret_winner_15[slice79],\r\n",
        "       train_regret_winner_16[slice79],\r\n",
        "       train_regret_winner_17[slice79],\r\n",
        "       train_regret_winner_18[slice79],\r\n",
        "       train_regret_winner_19[slice79],\r\n",
        "       train_regret_winner_20[slice79]]\r\n",
        "\r\n",
        "loser79_results = pd.DataFrame(loser79).sort_values(by=[0], ascending=False)\r\n",
        "winner79_results = pd.DataFrame(winner79).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser79 = np.asarray(loser79_results[4:5][0])[0]\r\n",
        "median_loser79 = np.asarray(loser79_results[9:10][0])[0]\r\n",
        "upper_loser79 = np.asarray(loser79_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner79 = np.asarray(winner79_results[4:5][0])[0]\r\n",
        "median_winner79 = np.asarray(winner79_results[9:10][0])[0]\r\n",
        "upper_winner79 = np.asarray(winner79_results[14:15][0])[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTnRF4lx1pio"
      },
      "source": [
        "# Iteration89 :\r\n",
        "\r\n",
        "slice89 = 88\r\n",
        "\r\n",
        "loser89 = [train_regret_loser_1[slice89],\r\n",
        "       train_regret_loser_2[slice89],\r\n",
        "       train_regret_loser_3[slice89],\r\n",
        "       train_regret_loser_4[slice89],\r\n",
        "       train_regret_loser_5[slice89],\r\n",
        "       train_regret_loser_6[slice89],\r\n",
        "       train_regret_loser_7[slice89],\r\n",
        "       train_regret_loser_8[slice89],\r\n",
        "       train_regret_loser_9[slice89],\r\n",
        "       train_regret_loser_10[slice89],\r\n",
        "       train_regret_loser_11[slice89],\r\n",
        "       train_regret_loser_12[slice89],\r\n",
        "       train_regret_loser_13[slice89],\r\n",
        "       train_regret_loser_14[slice89],\r\n",
        "       train_regret_loser_15[slice89],\r\n",
        "       train_regret_loser_16[slice89],\r\n",
        "       train_regret_loser_17[slice89],\r\n",
        "       train_regret_loser_18[slice89],\r\n",
        "       train_regret_loser_19[slice89],\r\n",
        "       train_regret_loser_20[slice89]]\r\n",
        "\r\n",
        "winner89 = [train_regret_winner_1[slice89],\r\n",
        "       train_regret_winner_2[slice89],\r\n",
        "       train_regret_winner_3[slice89],\r\n",
        "       train_regret_winner_4[slice89],\r\n",
        "       train_regret_winner_5[slice89],\r\n",
        "       train_regret_winner_6[slice89],\r\n",
        "       train_regret_winner_7[slice89],\r\n",
        "       train_regret_winner_8[slice89],\r\n",
        "       train_regret_winner_9[slice89],\r\n",
        "       train_regret_winner_10[slice89],\r\n",
        "       train_regret_winner_11[slice89],\r\n",
        "       train_regret_winner_12[slice89],\r\n",
        "       train_regret_winner_13[slice89],\r\n",
        "       train_regret_winner_14[slice89],\r\n",
        "       train_regret_winner_15[slice89],\r\n",
        "       train_regret_winner_16[slice89],\r\n",
        "       train_regret_winner_17[slice89],\r\n",
        "       train_regret_winner_18[slice89],\r\n",
        "       train_regret_winner_19[slice89],\r\n",
        "       train_regret_winner_20[slice89]]\r\n",
        "\r\n",
        "loser89_results = pd.DataFrame(loser89).sort_values(by=[0], ascending=False)\r\n",
        "winner89_results = pd.DataFrame(winner89).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser89 = np.asarray(loser89_results[4:5][0])[0]\r\n",
        "median_loser89 = np.asarray(loser89_results[9:10][0])[0]\r\n",
        "upper_loser89 = np.asarray(loser89_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner89 = np.asarray(winner89_results[4:5][0])[0]\r\n",
        "median_winner89 = np.asarray(winner89_results[9:10][0])[0]\r\n",
        "upper_winner89 = np.asarray(winner89_results[14:15][0])[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hiLAF38i1plO"
      },
      "source": [
        "# Iteration99 :\r\n",
        "\r\n",
        "slice99 = 98\r\n",
        "\r\n",
        "loser99 = [train_regret_loser_1[slice99],\r\n",
        "       train_regret_loser_2[slice99],\r\n",
        "       train_regret_loser_3[slice99],\r\n",
        "       train_regret_loser_4[slice99],\r\n",
        "       train_regret_loser_5[slice99],\r\n",
        "       train_regret_loser_6[slice99],\r\n",
        "       train_regret_loser_7[slice99],\r\n",
        "       train_regret_loser_8[slice99],\r\n",
        "       train_regret_loser_9[slice99],\r\n",
        "       train_regret_loser_10[slice99],\r\n",
        "       train_regret_loser_11[slice99],\r\n",
        "       train_regret_loser_12[slice99],\r\n",
        "       train_regret_loser_13[slice99],\r\n",
        "       train_regret_loser_14[slice99],\r\n",
        "       train_regret_loser_15[slice99],\r\n",
        "       train_regret_loser_16[slice99],\r\n",
        "       train_regret_loser_17[slice99],\r\n",
        "       train_regret_loser_18[slice99],\r\n",
        "       train_regret_loser_19[slice99],\r\n",
        "       train_regret_loser_20[slice99]]\r\n",
        "\r\n",
        "winner99 = [train_regret_winner_1[slice99],\r\n",
        "       train_regret_winner_2[slice99],\r\n",
        "       train_regret_winner_3[slice99],\r\n",
        "       train_regret_winner_4[slice99],\r\n",
        "       train_regret_winner_5[slice99],\r\n",
        "       train_regret_winner_6[slice99],\r\n",
        "       train_regret_winner_7[slice99],\r\n",
        "       train_regret_winner_8[slice99],\r\n",
        "       train_regret_winner_9[slice99],\r\n",
        "       train_regret_winner_10[slice99],\r\n",
        "       train_regret_winner_11[slice99],\r\n",
        "       train_regret_winner_12[slice99],\r\n",
        "       train_regret_winner_13[slice99],\r\n",
        "       train_regret_winner_14[slice99],\r\n",
        "       train_regret_winner_15[slice99],\r\n",
        "       train_regret_winner_16[slice99],\r\n",
        "       train_regret_winner_17[slice99],\r\n",
        "       train_regret_winner_18[slice99],\r\n",
        "       train_regret_winner_19[slice99],\r\n",
        "       train_regret_winner_20[slice99]]\r\n",
        "\r\n",
        "loser99_results = pd.DataFrame(loser99).sort_values(by=[0], ascending=False)\r\n",
        "winner99_results = pd.DataFrame(winner99).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser99 = np.asarray(loser99_results[4:5][0])[0]\r\n",
        "median_loser99 = np.asarray(loser99_results[9:10][0])[0]\r\n",
        "upper_loser99 = np.asarray(loser99_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner99 = np.asarray(winner99_results[4:5][0])[0]\r\n",
        "median_winner99 = np.asarray(winner99_results[9:10][0])[0]\r\n",
        "upper_winner99 = np.asarray(winner99_results[14:15][0])[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBd_N0F21pn0"
      },
      "source": [
        "# Iteration10 :\r\n",
        "\r\n",
        "slice10 = 9\r\n",
        "\r\n",
        "loser10 = [train_regret_loser_1[slice10],\r\n",
        "       train_regret_loser_2[slice10],\r\n",
        "       train_regret_loser_3[slice10],\r\n",
        "       train_regret_loser_4[slice10],\r\n",
        "       train_regret_loser_5[slice10],\r\n",
        "       train_regret_loser_6[slice10],\r\n",
        "       train_regret_loser_7[slice10],\r\n",
        "       train_regret_loser_8[slice10],\r\n",
        "       train_regret_loser_9[slice10],\r\n",
        "       train_regret_loser_10[slice10],\r\n",
        "       train_regret_loser_11[slice10],\r\n",
        "       train_regret_loser_12[slice10],\r\n",
        "       train_regret_loser_13[slice10],\r\n",
        "       train_regret_loser_14[slice10],\r\n",
        "       train_regret_loser_15[slice10],\r\n",
        "       train_regret_loser_16[slice10],\r\n",
        "       train_regret_loser_17[slice10],\r\n",
        "       train_regret_loser_18[slice10],\r\n",
        "       train_regret_loser_19[slice10],\r\n",
        "       train_regret_loser_20[slice10]]\r\n",
        "\r\n",
        "winner10 = [train_regret_winner_1[slice10],\r\n",
        "       train_regret_winner_2[slice10],\r\n",
        "       train_regret_winner_3[slice10],\r\n",
        "       train_regret_winner_4[slice10],\r\n",
        "       train_regret_winner_5[slice10],\r\n",
        "       train_regret_winner_6[slice10],\r\n",
        "       train_regret_winner_7[slice10],\r\n",
        "       train_regret_winner_8[slice10],\r\n",
        "       train_regret_winner_9[slice10],\r\n",
        "       train_regret_winner_10[slice10],\r\n",
        "       train_regret_winner_11[slice10],\r\n",
        "       train_regret_winner_12[slice10],\r\n",
        "       train_regret_winner_13[slice10],\r\n",
        "       train_regret_winner_14[slice10],\r\n",
        "       train_regret_winner_15[slice10],\r\n",
        "       train_regret_winner_16[slice10],\r\n",
        "       train_regret_winner_17[slice10],\r\n",
        "       train_regret_winner_18[slice10],\r\n",
        "       train_regret_winner_19[slice10],\r\n",
        "       train_regret_winner_20[slice10]]\r\n",
        "\r\n",
        "loser10_results = pd.DataFrame(loser10).sort_values(by=[0], ascending=False)\r\n",
        "winner10_results = pd.DataFrame(winner10).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser10 = np.asarray(loser10_results[4:5][0])[0]\r\n",
        "median_loser10 = np.asarray(loser10_results[9:10][0])[0]\r\n",
        "upper_loser10 = np.asarray(loser10_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner10 = np.asarray(winner10_results[4:5][0])[0]\r\n",
        "median_winner10 = np.asarray(winner10_results[9:10][0])[0]\r\n",
        "upper_winner10 = np.asarray(winner10_results[14:15][0])[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXjdqc4s1pqb"
      },
      "source": [
        "# Iteration20 :\r\n",
        "\r\n",
        "slice20 = 19\r\n",
        "\r\n",
        "loser20 = [train_regret_loser_1[slice20],\r\n",
        "       train_regret_loser_2[slice20],\r\n",
        "       train_regret_loser_3[slice20],\r\n",
        "       train_regret_loser_4[slice20],\r\n",
        "       train_regret_loser_5[slice20],\r\n",
        "       train_regret_loser_6[slice20],\r\n",
        "       train_regret_loser_7[slice20],\r\n",
        "       train_regret_loser_8[slice20],\r\n",
        "       train_regret_loser_9[slice20],\r\n",
        "       train_regret_loser_10[slice20],\r\n",
        "       train_regret_loser_11[slice20],\r\n",
        "       train_regret_loser_12[slice20],\r\n",
        "       train_regret_loser_13[slice20],\r\n",
        "       train_regret_loser_14[slice20],\r\n",
        "       train_regret_loser_15[slice20],\r\n",
        "       train_regret_loser_16[slice20],\r\n",
        "       train_regret_loser_17[slice20],\r\n",
        "       train_regret_loser_18[slice20],\r\n",
        "       train_regret_loser_19[slice20],\r\n",
        "       train_regret_loser_20[slice20]]\r\n",
        "\r\n",
        "winner20 = [train_regret_winner_1[slice20],\r\n",
        "       train_regret_winner_2[slice20],\r\n",
        "       train_regret_winner_3[slice20],\r\n",
        "       train_regret_winner_4[slice20],\r\n",
        "       train_regret_winner_5[slice20],\r\n",
        "       train_regret_winner_6[slice20],\r\n",
        "       train_regret_winner_7[slice20],\r\n",
        "       train_regret_winner_8[slice20],\r\n",
        "       train_regret_winner_9[slice20],\r\n",
        "       train_regret_winner_10[slice20],\r\n",
        "       train_regret_winner_11[slice20],\r\n",
        "       train_regret_winner_12[slice20],\r\n",
        "       train_regret_winner_13[slice20],\r\n",
        "       train_regret_winner_14[slice20],\r\n",
        "       train_regret_winner_15[slice20],\r\n",
        "       train_regret_winner_16[slice20],\r\n",
        "       train_regret_winner_17[slice20],\r\n",
        "       train_regret_winner_18[slice20],\r\n",
        "       train_regret_winner_19[slice20],\r\n",
        "       train_regret_winner_20[slice20]]\r\n",
        "\r\n",
        "loser20_results = pd.DataFrame(loser20).sort_values(by=[0], ascending=False)\r\n",
        "winner20_results = pd.DataFrame(winner20).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser20 = np.asarray(loser20_results[4:5][0])[0]\r\n",
        "median_loser20 = np.asarray(loser20_results[9:10][0])[0]\r\n",
        "upper_loser20 = np.asarray(loser20_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner20 = np.asarray(winner20_results[4:5][0])[0]\r\n",
        "median_winner20 = np.asarray(winner20_results[9:10][0])[0]\r\n",
        "upper_winner20 = np.asarray(winner20_results[14:15][0])[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQBVS6dx1ptB"
      },
      "source": [
        "# Iteration30 :\r\n",
        "\r\n",
        "slice30 = 29\r\n",
        "\r\n",
        "loser30 = [train_regret_loser_1[slice30],\r\n",
        "       train_regret_loser_2[slice30],\r\n",
        "       train_regret_loser_3[slice30],\r\n",
        "       train_regret_loser_4[slice30],\r\n",
        "       train_regret_loser_5[slice30],\r\n",
        "       train_regret_loser_6[slice30],\r\n",
        "       train_regret_loser_7[slice30],\r\n",
        "       train_regret_loser_8[slice30],\r\n",
        "       train_regret_loser_9[slice30],\r\n",
        "       train_regret_loser_10[slice30],\r\n",
        "       train_regret_loser_11[slice30],\r\n",
        "       train_regret_loser_12[slice30],\r\n",
        "       train_regret_loser_13[slice30],\r\n",
        "       train_regret_loser_14[slice30],\r\n",
        "       train_regret_loser_15[slice30],\r\n",
        "       train_regret_loser_16[slice30],\r\n",
        "       train_regret_loser_17[slice30],\r\n",
        "       train_regret_loser_18[slice30],\r\n",
        "       train_regret_loser_19[slice30],\r\n",
        "       train_regret_loser_20[slice30]]\r\n",
        "\r\n",
        "winner30 = [train_regret_winner_1[slice30],\r\n",
        "       train_regret_winner_2[slice30],\r\n",
        "       train_regret_winner_3[slice30],\r\n",
        "       train_regret_winner_4[slice30],\r\n",
        "       train_regret_winner_5[slice30],\r\n",
        "       train_regret_winner_6[slice30],\r\n",
        "       train_regret_winner_7[slice30],\r\n",
        "       train_regret_winner_8[slice30],\r\n",
        "       train_regret_winner_9[slice30],\r\n",
        "       train_regret_winner_10[slice30],\r\n",
        "       train_regret_winner_11[slice30],\r\n",
        "       train_regret_winner_12[slice30],\r\n",
        "       train_regret_winner_13[slice30],\r\n",
        "       train_regret_winner_14[slice30],\r\n",
        "       train_regret_winner_15[slice30],\r\n",
        "       train_regret_winner_16[slice30],\r\n",
        "       train_regret_winner_17[slice30],\r\n",
        "       train_regret_winner_18[slice30],\r\n",
        "       train_regret_winner_19[slice30],\r\n",
        "       train_regret_winner_20[slice30]]\r\n",
        "\r\n",
        "loser30_results = pd.DataFrame(loser30).sort_values(by=[0], ascending=False)\r\n",
        "winner30_results = pd.DataFrame(winner30).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser30 = np.asarray(loser30_results[4:5][0])[0]\r\n",
        "median_loser30 = np.asarray(loser30_results[9:10][0])[0]\r\n",
        "upper_loser30 = np.asarray(loser30_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner30 = np.asarray(winner30_results[4:5][0])[0]\r\n",
        "median_winner30 = np.asarray(winner30_results[9:10][0])[0]\r\n",
        "upper_winner30 = np.asarray(winner30_results[14:15][0])[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-iWfLkj1pvn"
      },
      "source": [
        "# Iteration40 :\r\n",
        "\r\n",
        "slice40 = 39\r\n",
        "\r\n",
        "loser40 = [train_regret_loser_1[slice40],\r\n",
        "       train_regret_loser_2[slice40],\r\n",
        "       train_regret_loser_3[slice40],\r\n",
        "       train_regret_loser_4[slice40],\r\n",
        "       train_regret_loser_5[slice40],\r\n",
        "       train_regret_loser_6[slice40],\r\n",
        "       train_regret_loser_7[slice40],\r\n",
        "       train_regret_loser_8[slice40],\r\n",
        "       train_regret_loser_9[slice40],\r\n",
        "       train_regret_loser_10[slice40],\r\n",
        "       train_regret_loser_11[slice40],\r\n",
        "       train_regret_loser_12[slice40],\r\n",
        "       train_regret_loser_13[slice40],\r\n",
        "       train_regret_loser_14[slice40],\r\n",
        "       train_regret_loser_15[slice40],\r\n",
        "       train_regret_loser_16[slice40],\r\n",
        "       train_regret_loser_17[slice40],\r\n",
        "       train_regret_loser_18[slice40],\r\n",
        "       train_regret_loser_19[slice40],\r\n",
        "       train_regret_loser_20[slice40]]\r\n",
        "\r\n",
        "winner40 = [train_regret_winner_1[slice40],\r\n",
        "       train_regret_winner_2[slice40],\r\n",
        "       train_regret_winner_3[slice40],\r\n",
        "       train_regret_winner_4[slice40],\r\n",
        "       train_regret_winner_5[slice40],\r\n",
        "       train_regret_winner_6[slice40],\r\n",
        "       train_regret_winner_7[slice40],\r\n",
        "       train_regret_winner_8[slice40],\r\n",
        "       train_regret_winner_9[slice40],\r\n",
        "       train_regret_winner_10[slice40],\r\n",
        "       train_regret_winner_11[slice40],\r\n",
        "       train_regret_winner_12[slice40],\r\n",
        "       train_regret_winner_13[slice40],\r\n",
        "       train_regret_winner_14[slice40],\r\n",
        "       train_regret_winner_15[slice40],\r\n",
        "       train_regret_winner_16[slice40],\r\n",
        "       train_regret_winner_17[slice40],\r\n",
        "       train_regret_winner_18[slice40],\r\n",
        "       train_regret_winner_19[slice40],\r\n",
        "       train_regret_winner_20[slice40]]\r\n",
        "\r\n",
        "loser40_results = pd.DataFrame(loser40).sort_values(by=[0], ascending=False)\r\n",
        "winner40_results = pd.DataFrame(winner40).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser40 = np.asarray(loser40_results[4:5][0])[0]\r\n",
        "median_loser40 = np.asarray(loser40_results[9:10][0])[0]\r\n",
        "upper_loser40 = np.asarray(loser40_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner40 = np.asarray(winner40_results[4:5][0])[0]\r\n",
        "median_winner40 = np.asarray(winner40_results[9:10][0])[0]\r\n",
        "upper_winner40 = np.asarray(winner40_results[14:15][0])[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-G5i-Cjw2U7K"
      },
      "source": [
        "# Iteration50 :\r\n",
        "\r\n",
        "slice50 = 49\r\n",
        "\r\n",
        "loser50 = [train_regret_loser_1[slice50],\r\n",
        "       train_regret_loser_2[slice50],\r\n",
        "       train_regret_loser_3[slice50],\r\n",
        "       train_regret_loser_4[slice50],\r\n",
        "       train_regret_loser_5[slice50],\r\n",
        "       train_regret_loser_6[slice50],\r\n",
        "       train_regret_loser_7[slice50],\r\n",
        "       train_regret_loser_8[slice50],\r\n",
        "       train_regret_loser_9[slice50],\r\n",
        "       train_regret_loser_10[slice50],\r\n",
        "       train_regret_loser_11[slice50],\r\n",
        "       train_regret_loser_12[slice50],\r\n",
        "       train_regret_loser_13[slice50],\r\n",
        "       train_regret_loser_14[slice50],\r\n",
        "       train_regret_loser_15[slice50],\r\n",
        "       train_regret_loser_16[slice50],\r\n",
        "       train_regret_loser_17[slice50],\r\n",
        "       train_regret_loser_18[slice50],\r\n",
        "       train_regret_loser_19[slice50],\r\n",
        "       train_regret_loser_20[slice50]]\r\n",
        "\r\n",
        "winner50 = [train_regret_winner_1[slice50],\r\n",
        "       train_regret_winner_2[slice50],\r\n",
        "       train_regret_winner_3[slice50],\r\n",
        "       train_regret_winner_4[slice50],\r\n",
        "       train_regret_winner_5[slice50],\r\n",
        "       train_regret_winner_6[slice50],\r\n",
        "       train_regret_winner_7[slice50],\r\n",
        "       train_regret_winner_8[slice50],\r\n",
        "       train_regret_winner_9[slice50],\r\n",
        "       train_regret_winner_10[slice50],\r\n",
        "       train_regret_winner_11[slice50],\r\n",
        "       train_regret_winner_12[slice50],\r\n",
        "       train_regret_winner_13[slice50],\r\n",
        "       train_regret_winner_14[slice50],\r\n",
        "       train_regret_winner_15[slice50],\r\n",
        "       train_regret_winner_16[slice50],\r\n",
        "       train_regret_winner_17[slice50],\r\n",
        "       train_regret_winner_18[slice50],\r\n",
        "       train_regret_winner_19[slice50],\r\n",
        "       train_regret_winner_20[slice50]]\r\n",
        "\r\n",
        "loser50_results = pd.DataFrame(loser50).sort_values(by=[0], ascending=False)\r\n",
        "winner50_results = pd.DataFrame(winner50).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser50 = np.asarray(loser50_results[4:5][0])[0]\r\n",
        "median_loser50 = np.asarray(loser50_results[9:10][0])[0]\r\n",
        "upper_loser50 = np.asarray(loser50_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner50 = np.asarray(winner50_results[4:5][0])[0]\r\n",
        "median_winner50 = np.asarray(winner50_results[9:10][0])[0]\r\n",
        "upper_winner50 = np.asarray(winner50_results[14:15][0])[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RY6qxZQA2U-r"
      },
      "source": [
        "# Iteration60 :\r\n",
        "\r\n",
        "slice60 = 59\r\n",
        "\r\n",
        "loser60 = [train_regret_loser_1[slice60],\r\n",
        "       train_regret_loser_2[slice60],\r\n",
        "       train_regret_loser_3[slice60],\r\n",
        "       train_regret_loser_4[slice60],\r\n",
        "       train_regret_loser_5[slice60],\r\n",
        "       train_regret_loser_6[slice60],\r\n",
        "       train_regret_loser_7[slice60],\r\n",
        "       train_regret_loser_8[slice60],\r\n",
        "       train_regret_loser_9[slice60],\r\n",
        "       train_regret_loser_10[slice60],\r\n",
        "       train_regret_loser_11[slice60],\r\n",
        "       train_regret_loser_12[slice60],\r\n",
        "       train_regret_loser_13[slice60],\r\n",
        "       train_regret_loser_14[slice60],\r\n",
        "       train_regret_loser_15[slice60],\r\n",
        "       train_regret_loser_16[slice60],\r\n",
        "       train_regret_loser_17[slice60],\r\n",
        "       train_regret_loser_18[slice60],\r\n",
        "       train_regret_loser_19[slice60],\r\n",
        "       train_regret_loser_20[slice60]]\r\n",
        "\r\n",
        "winner60 = [train_regret_winner_1[slice60],\r\n",
        "       train_regret_winner_2[slice60],\r\n",
        "       train_regret_winner_3[slice60],\r\n",
        "       train_regret_winner_4[slice60],\r\n",
        "       train_regret_winner_5[slice60],\r\n",
        "       train_regret_winner_6[slice60],\r\n",
        "       train_regret_winner_7[slice60],\r\n",
        "       train_regret_winner_8[slice60],\r\n",
        "       train_regret_winner_9[slice60],\r\n",
        "       train_regret_winner_10[slice60],\r\n",
        "       train_regret_winner_11[slice60],\r\n",
        "       train_regret_winner_12[slice60],\r\n",
        "       train_regret_winner_13[slice60],\r\n",
        "       train_regret_winner_14[slice60],\r\n",
        "       train_regret_winner_15[slice60],\r\n",
        "       train_regret_winner_16[slice60],\r\n",
        "       train_regret_winner_17[slice60],\r\n",
        "       train_regret_winner_18[slice60],\r\n",
        "       train_regret_winner_19[slice60],\r\n",
        "       train_regret_winner_20[slice60]]\r\n",
        "\r\n",
        "loser60_results = pd.DataFrame(loser60).sort_values(by=[0], ascending=False)\r\n",
        "winner60_results = pd.DataFrame(winner60).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser60 = np.asarray(loser60_results[4:5][0])[0]\r\n",
        "median_loser60 = np.asarray(loser60_results[9:10][0])[0]\r\n",
        "upper_loser60 = np.asarray(loser60_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner60 = np.asarray(winner60_results[4:5][0])[0]\r\n",
        "median_winner60 = np.asarray(winner60_results[9:10][0])[0]\r\n",
        "upper_winner60 = np.asarray(winner60_results[14:15][0])[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBqgVxVH2VCU"
      },
      "source": [
        "# Iteration70 :\r\n",
        "\r\n",
        "slice70 = 69\r\n",
        "\r\n",
        "loser70 = [train_regret_loser_1[slice70],\r\n",
        "       train_regret_loser_2[slice70],\r\n",
        "       train_regret_loser_3[slice70],\r\n",
        "       train_regret_loser_4[slice70],\r\n",
        "       train_regret_loser_5[slice70],\r\n",
        "       train_regret_loser_6[slice70],\r\n",
        "       train_regret_loser_7[slice70],\r\n",
        "       train_regret_loser_8[slice70],\r\n",
        "       train_regret_loser_9[slice70],\r\n",
        "       train_regret_loser_10[slice70],\r\n",
        "       train_regret_loser_11[slice70],\r\n",
        "       train_regret_loser_12[slice70],\r\n",
        "       train_regret_loser_13[slice70],\r\n",
        "       train_regret_loser_14[slice70],\r\n",
        "       train_regret_loser_15[slice70],\r\n",
        "       train_regret_loser_16[slice70],\r\n",
        "       train_regret_loser_17[slice70],\r\n",
        "       train_regret_loser_18[slice70],\r\n",
        "       train_regret_loser_19[slice70],\r\n",
        "       train_regret_loser_20[slice70]]\r\n",
        "\r\n",
        "winner70 = [train_regret_winner_1[slice70],\r\n",
        "       train_regret_winner_2[slice70],\r\n",
        "       train_regret_winner_3[slice70],\r\n",
        "       train_regret_winner_4[slice70],\r\n",
        "       train_regret_winner_5[slice70],\r\n",
        "       train_regret_winner_6[slice70],\r\n",
        "       train_regret_winner_7[slice70],\r\n",
        "       train_regret_winner_8[slice70],\r\n",
        "       train_regret_winner_9[slice70],\r\n",
        "       train_regret_winner_10[slice70],\r\n",
        "       train_regret_winner_11[slice70],\r\n",
        "       train_regret_winner_12[slice70],\r\n",
        "       train_regret_winner_13[slice70],\r\n",
        "       train_regret_winner_14[slice70],\r\n",
        "       train_regret_winner_15[slice70],\r\n",
        "       train_regret_winner_16[slice70],\r\n",
        "       train_regret_winner_17[slice70],\r\n",
        "       train_regret_winner_18[slice70],\r\n",
        "       train_regret_winner_19[slice70],\r\n",
        "       train_regret_winner_20[slice70]]\r\n",
        "\r\n",
        "loser70_results = pd.DataFrame(loser70).sort_values(by=[0], ascending=False)\r\n",
        "winner70_results = pd.DataFrame(winner70).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser70 = np.asarray(loser70_results[4:5][0])[0]\r\n",
        "median_loser70 = np.asarray(loser70_results[9:10][0])[0]\r\n",
        "upper_loser70 = np.asarray(loser70_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner70 = np.asarray(winner70_results[4:5][0])[0]\r\n",
        "median_winner70 = np.asarray(winner70_results[9:10][0])[0]\r\n",
        "upper_winner70 = np.asarray(winner70_results[14:15][0])[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YF7mlDZL1pyN"
      },
      "source": [
        "# Iteration80 :\r\n",
        "\r\n",
        "slice80 = 79\r\n",
        "\r\n",
        "loser80 = [train_regret_loser_1[slice80],\r\n",
        "       train_regret_loser_2[slice80],\r\n",
        "       train_regret_loser_3[slice80],\r\n",
        "       train_regret_loser_4[slice80],\r\n",
        "       train_regret_loser_5[slice80],\r\n",
        "       train_regret_loser_6[slice80],\r\n",
        "       train_regret_loser_7[slice80],\r\n",
        "       train_regret_loser_8[slice80],\r\n",
        "       train_regret_loser_9[slice80],\r\n",
        "       train_regret_loser_10[slice80],\r\n",
        "       train_regret_loser_11[slice80],\r\n",
        "       train_regret_loser_12[slice80],\r\n",
        "       train_regret_loser_13[slice80],\r\n",
        "       train_regret_loser_14[slice80],\r\n",
        "       train_regret_loser_15[slice80],\r\n",
        "       train_regret_loser_16[slice80],\r\n",
        "       train_regret_loser_17[slice80],\r\n",
        "       train_regret_loser_18[slice80],\r\n",
        "       train_regret_loser_19[slice80],\r\n",
        "       train_regret_loser_20[slice80]]\r\n",
        "\r\n",
        "winner80 = [train_regret_winner_1[slice80],\r\n",
        "       train_regret_winner_2[slice80],\r\n",
        "       train_regret_winner_3[slice80],\r\n",
        "       train_regret_winner_4[slice80],\r\n",
        "       train_regret_winner_5[slice80],\r\n",
        "       train_regret_winner_6[slice80],\r\n",
        "       train_regret_winner_7[slice80],\r\n",
        "       train_regret_winner_8[slice80],\r\n",
        "       train_regret_winner_9[slice80],\r\n",
        "       train_regret_winner_10[slice80],\r\n",
        "       train_regret_winner_11[slice80],\r\n",
        "       train_regret_winner_12[slice80],\r\n",
        "       train_regret_winner_13[slice80],\r\n",
        "       train_regret_winner_14[slice80],\r\n",
        "       train_regret_winner_15[slice80],\r\n",
        "       train_regret_winner_16[slice80],\r\n",
        "       train_regret_winner_17[slice80],\r\n",
        "       train_regret_winner_18[slice80],\r\n",
        "       train_regret_winner_19[slice80],\r\n",
        "       train_regret_winner_20[slice80]]\r\n",
        "\r\n",
        "loser80_results = pd.DataFrame(loser80).sort_values(by=[0], ascending=False)\r\n",
        "winner80_results = pd.DataFrame(winner80).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser80 = np.asarray(loser80_results[4:5][0])[0]\r\n",
        "median_loser80 = np.asarray(loser80_results[9:10][0])[0]\r\n",
        "upper_loser80 = np.asarray(loser80_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner80 = np.asarray(winner80_results[4:5][0])[0]\r\n",
        "median_winner80 = np.asarray(winner80_results[9:10][0])[0]\r\n",
        "upper_winner80 = np.asarray(winner80_results[14:15][0])[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1Rw8IkD1p0z"
      },
      "source": [
        "# Iteration90 :\r\n",
        "\r\n",
        "slice90 = 89\r\n",
        "\r\n",
        "loser90 = [train_regret_loser_1[slice90],\r\n",
        "       train_regret_loser_2[slice90],\r\n",
        "       train_regret_loser_3[slice90],\r\n",
        "       train_regret_loser_4[slice90],\r\n",
        "       train_regret_loser_5[slice90],\r\n",
        "       train_regret_loser_6[slice90],\r\n",
        "       train_regret_loser_7[slice90],\r\n",
        "       train_regret_loser_8[slice90],\r\n",
        "       train_regret_loser_9[slice90],\r\n",
        "       train_regret_loser_10[slice90],\r\n",
        "       train_regret_loser_11[slice90],\r\n",
        "       train_regret_loser_12[slice90],\r\n",
        "       train_regret_loser_13[slice90],\r\n",
        "       train_regret_loser_14[slice90],\r\n",
        "       train_regret_loser_15[slice90],\r\n",
        "       train_regret_loser_16[slice90],\r\n",
        "       train_regret_loser_17[slice90],\r\n",
        "       train_regret_loser_18[slice90],\r\n",
        "       train_regret_loser_19[slice90],\r\n",
        "       train_regret_loser_20[slice90]]\r\n",
        "\r\n",
        "winner90 = [train_regret_winner_1[slice90],\r\n",
        "       train_regret_winner_2[slice90],\r\n",
        "       train_regret_winner_3[slice90],\r\n",
        "       train_regret_winner_4[slice90],\r\n",
        "       train_regret_winner_5[slice90],\r\n",
        "       train_regret_winner_6[slice90],\r\n",
        "       train_regret_winner_7[slice90],\r\n",
        "       train_regret_winner_8[slice90],\r\n",
        "       train_regret_winner_9[slice90],\r\n",
        "       train_regret_winner_10[slice90],\r\n",
        "       train_regret_winner_11[slice90],\r\n",
        "       train_regret_winner_12[slice90],\r\n",
        "       train_regret_winner_13[slice90],\r\n",
        "       train_regret_winner_14[slice90],\r\n",
        "       train_regret_winner_15[slice90],\r\n",
        "       train_regret_winner_16[slice90],\r\n",
        "       train_regret_winner_17[slice90],\r\n",
        "       train_regret_winner_18[slice90],\r\n",
        "       train_regret_winner_19[slice90],\r\n",
        "       train_regret_winner_20[slice90]]\r\n",
        "\r\n",
        "loser90_results = pd.DataFrame(loser90).sort_values(by=[0], ascending=False)\r\n",
        "winner90_results = pd.DataFrame(winner90).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser90 = np.asarray(loser90_results[4:5][0])[0]\r\n",
        "median_loser90 = np.asarray(loser90_results[9:10][0])[0]\r\n",
        "upper_loser90 = np.asarray(loser90_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner90 = np.asarray(winner90_results[4:5][0])[0]\r\n",
        "median_winner90 = np.asarray(winner90_results[9:10][0])[0]\r\n",
        "upper_winner90 = np.asarray(winner90_results[14:15][0])[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iiljCrq21p3a"
      },
      "source": [
        "# Iteration100 :\r\n",
        "\r\n",
        "slice100 = 99\r\n",
        "\r\n",
        "loser100 = [train_regret_loser_1[slice100],\r\n",
        "       train_regret_loser_2[slice100],\r\n",
        "       train_regret_loser_3[slice100],\r\n",
        "       train_regret_loser_4[slice100],\r\n",
        "       train_regret_loser_5[slice100],\r\n",
        "       train_regret_loser_6[slice100],\r\n",
        "       train_regret_loser_7[slice100],\r\n",
        "       train_regret_loser_8[slice100],\r\n",
        "       train_regret_loser_9[slice100],\r\n",
        "       train_regret_loser_10[slice100],\r\n",
        "       train_regret_loser_11[slice100],\r\n",
        "       train_regret_loser_12[slice100],\r\n",
        "       train_regret_loser_13[slice100],\r\n",
        "       train_regret_loser_14[slice100],\r\n",
        "       train_regret_loser_15[slice100],\r\n",
        "       train_regret_loser_16[slice100],\r\n",
        "       train_regret_loser_17[slice100],\r\n",
        "       train_regret_loser_18[slice100],\r\n",
        "       train_regret_loser_19[slice100],\r\n",
        "       train_regret_loser_20[slice100]]\r\n",
        "\r\n",
        "winner100 = [train_regret_winner_1[slice100],\r\n",
        "       train_regret_winner_2[slice100],\r\n",
        "       train_regret_winner_3[slice100],\r\n",
        "       train_regret_winner_4[slice100],\r\n",
        "       train_regret_winner_5[slice100],\r\n",
        "       train_regret_winner_6[slice100],\r\n",
        "       train_regret_winner_7[slice100],\r\n",
        "       train_regret_winner_8[slice100],\r\n",
        "       train_regret_winner_9[slice100],\r\n",
        "       train_regret_winner_10[slice100],\r\n",
        "       train_regret_winner_11[slice100],\r\n",
        "       train_regret_winner_12[slice100],\r\n",
        "       train_regret_winner_13[slice100],\r\n",
        "       train_regret_winner_14[slice100],\r\n",
        "       train_regret_winner_15[slice100],\r\n",
        "       train_regret_winner_16[slice100],\r\n",
        "       train_regret_winner_17[slice100],\r\n",
        "       train_regret_winner_18[slice100],\r\n",
        "       train_regret_winner_19[slice100],\r\n",
        "       train_regret_winner_20[slice100]]\r\n",
        "\r\n",
        "loser100_results = pd.DataFrame(loser100).sort_values(by=[0], ascending=False)\r\n",
        "winner100_results = pd.DataFrame(winner100).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser100 = np.asarray(loser100_results[4:5][0])[0]\r\n",
        "median_loser100 = np.asarray(loser100_results[9:10][0])[0]\r\n",
        "upper_loser100 = np.asarray(loser100_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner100 = np.asarray(winner100_results[4:5][0])[0]\r\n",
        "median_winner100 = np.asarray(winner100_results[9:10][0])[0]\r\n",
        "upper_winner100 = np.asarray(winner100_results[14:15][0])[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OItJgqUT2oNb"
      },
      "source": [
        "### Summarize arrays: 'Loser'\r\n",
        "\r\n",
        "lower_loser = [lower_loser1,\r\n",
        "            lower_loser2,\r\n",
        "            lower_loser3,\r\n",
        "            lower_loser4,\r\n",
        "            lower_loser5,\r\n",
        "            lower_loser6,\r\n",
        "            lower_loser7,\r\n",
        "            lower_loser8,\r\n",
        "            lower_loser9,\r\n",
        "            lower_loser10,\r\n",
        "            lower_loser11,\r\n",
        "            lower_loser12,\r\n",
        "            lower_loser13,\r\n",
        "            lower_loser14,\r\n",
        "            lower_loser15,\r\n",
        "            lower_loser16,\r\n",
        "            lower_loser17,\r\n",
        "            lower_loser18,\r\n",
        "            lower_loser19,\r\n",
        "            lower_loser20,\r\n",
        "            lower_loser21,\r\n",
        "            lower_loser22,\r\n",
        "            lower_loser23,\r\n",
        "            lower_loser24,\r\n",
        "            lower_loser25,\r\n",
        "            lower_loser26,\r\n",
        "            lower_loser27,\r\n",
        "            lower_loser28,\r\n",
        "            lower_loser29,\r\n",
        "            lower_loser30,\r\n",
        "            lower_loser31,\r\n",
        "            lower_loser32,\r\n",
        "            lower_loser33,\r\n",
        "            lower_loser34,\r\n",
        "            lower_loser35,\r\n",
        "            lower_loser36,\r\n",
        "            lower_loser37,\r\n",
        "            lower_loser38,\r\n",
        "            lower_loser39,\r\n",
        "            lower_loser40,\r\n",
        "            lower_loser41,\r\n",
        "            lower_loser42,\r\n",
        "            lower_loser43,\r\n",
        "            lower_loser44,\r\n",
        "            lower_loser45,\r\n",
        "            lower_loser46,\r\n",
        "            lower_loser47,\r\n",
        "            lower_loser48,\r\n",
        "            lower_loser49,\r\n",
        "            lower_loser50,\r\n",
        "            lower_loser51,\r\n",
        "            lower_loser52,\r\n",
        "            lower_loser53,\r\n",
        "            lower_loser54,\r\n",
        "            lower_loser55,\r\n",
        "            lower_loser56,\r\n",
        "            lower_loser57,\r\n",
        "            lower_loser58,\r\n",
        "            lower_loser59,\r\n",
        "            lower_loser60,\r\n",
        "            lower_loser61,\r\n",
        "            lower_loser62,\r\n",
        "            lower_loser63,\r\n",
        "            lower_loser64,\r\n",
        "            lower_loser65,\r\n",
        "            lower_loser66,\r\n",
        "            lower_loser67,\r\n",
        "            lower_loser68,\r\n",
        "            lower_loser69,\r\n",
        "            lower_loser70,\r\n",
        "            lower_loser71,\r\n",
        "            lower_loser72,\r\n",
        "            lower_loser73,\r\n",
        "            lower_loser74,\r\n",
        "            lower_loser75,\r\n",
        "            lower_loser76,\r\n",
        "            lower_loser77,\r\n",
        "            lower_loser78,\r\n",
        "            lower_loser79,\r\n",
        "            lower_loser80,\r\n",
        "            lower_loser81,\r\n",
        "            lower_loser82,\r\n",
        "            lower_loser83,\r\n",
        "            lower_loser84,\r\n",
        "            lower_loser85,\r\n",
        "            lower_loser86,\r\n",
        "            lower_loser87,\r\n",
        "            lower_loser88,\r\n",
        "            lower_loser89,\r\n",
        "            lower_loser90,\r\n",
        "            lower_loser91,\r\n",
        "            lower_loser92,\r\n",
        "            lower_loser93,\r\n",
        "            lower_loser94,\r\n",
        "            lower_loser95,\r\n",
        "            lower_loser96,\r\n",
        "            lower_loser97,\r\n",
        "            lower_loser98,\r\n",
        "            lower_loser99,\r\n",
        "            lower_loser100,\r\n",
        "            lower_loser101]\r\n",
        "\r\n",
        "median_loser = [median_loser1,\r\n",
        "            median_loser2,\r\n",
        "            median_loser3,\r\n",
        "            median_loser4,\r\n",
        "            median_loser5,\r\n",
        "            median_loser6,\r\n",
        "            median_loser7,\r\n",
        "            median_loser8,\r\n",
        "            median_loser9,\r\n",
        "            median_loser10,\r\n",
        "            median_loser11,\r\n",
        "            median_loser12,\r\n",
        "            median_loser13,\r\n",
        "            median_loser14,\r\n",
        "            median_loser15,\r\n",
        "            median_loser16,\r\n",
        "            median_loser17,\r\n",
        "            median_loser18,\r\n",
        "            median_loser19,\r\n",
        "            median_loser20,\r\n",
        "            median_loser21,\r\n",
        "            median_loser22,\r\n",
        "            median_loser23,\r\n",
        "            median_loser24,\r\n",
        "            median_loser25,\r\n",
        "            median_loser26,\r\n",
        "            median_loser27,\r\n",
        "            median_loser28,\r\n",
        "            median_loser29,\r\n",
        "            median_loser30,\r\n",
        "            median_loser31,\r\n",
        "            median_loser32,\r\n",
        "            median_loser33,\r\n",
        "            median_loser34,\r\n",
        "            median_loser35,\r\n",
        "            median_loser36,\r\n",
        "            median_loser37,\r\n",
        "            median_loser38,\r\n",
        "            median_loser39,\r\n",
        "            median_loser40,\r\n",
        "            median_loser41,\r\n",
        "            median_loser42,\r\n",
        "            median_loser43,\r\n",
        "            median_loser44,\r\n",
        "            median_loser45,\r\n",
        "            median_loser46,\r\n",
        "            median_loser47,\r\n",
        "            median_loser48,\r\n",
        "            median_loser49,\r\n",
        "            median_loser50,\r\n",
        "            median_loser51,\r\n",
        "            median_loser52,\r\n",
        "            median_loser53,\r\n",
        "            median_loser54,\r\n",
        "            median_loser55,\r\n",
        "            median_loser56,\r\n",
        "            median_loser57,\r\n",
        "            median_loser58,\r\n",
        "            median_loser59,\r\n",
        "            median_loser60,\r\n",
        "            median_loser61,\r\n",
        "            median_loser62,\r\n",
        "            median_loser63,\r\n",
        "            median_loser64,\r\n",
        "            median_loser65,\r\n",
        "            median_loser66,\r\n",
        "            median_loser67,\r\n",
        "            median_loser68,\r\n",
        "            median_loser69,\r\n",
        "            median_loser70,\r\n",
        "            median_loser71,\r\n",
        "            median_loser72,\r\n",
        "            median_loser73,\r\n",
        "            median_loser74,\r\n",
        "            median_loser75,\r\n",
        "            median_loser76,\r\n",
        "            median_loser77,\r\n",
        "            median_loser78,\r\n",
        "            median_loser79,\r\n",
        "            median_loser80,\r\n",
        "            median_loser81,\r\n",
        "            median_loser82,\r\n",
        "            median_loser83,\r\n",
        "            median_loser84,\r\n",
        "            median_loser85,\r\n",
        "            median_loser86,\r\n",
        "            median_loser87,\r\n",
        "            median_loser88,\r\n",
        "            median_loser89,\r\n",
        "            median_loser90,\r\n",
        "            median_loser91,\r\n",
        "            median_loser92,\r\n",
        "            median_loser93,\r\n",
        "            median_loser94,\r\n",
        "            median_loser95,\r\n",
        "            median_loser96,\r\n",
        "            median_loser97,\r\n",
        "            median_loser98,\r\n",
        "            median_loser99,\r\n",
        "            median_loser100,\r\n",
        "            median_loser101]\r\n",
        "\r\n",
        "upper_loser = [upper_loser1,\r\n",
        "            upper_loser2,\r\n",
        "            upper_loser3,\r\n",
        "            upper_loser4,\r\n",
        "            upper_loser5,\r\n",
        "            upper_loser6,\r\n",
        "            upper_loser7,\r\n",
        "            upper_loser8,\r\n",
        "            upper_loser9,\r\n",
        "            upper_loser10,\r\n",
        "            upper_loser11,\r\n",
        "            upper_loser12,\r\n",
        "            upper_loser13,\r\n",
        "            upper_loser14,\r\n",
        "            upper_loser15,\r\n",
        "            upper_loser16,\r\n",
        "            upper_loser17,\r\n",
        "            upper_loser18,\r\n",
        "            upper_loser19,\r\n",
        "            upper_loser20,\r\n",
        "            upper_loser21,\r\n",
        "            upper_loser22,\r\n",
        "            upper_loser23,\r\n",
        "            upper_loser24,\r\n",
        "            upper_loser25,\r\n",
        "            upper_loser26,\r\n",
        "            upper_loser27,\r\n",
        "            upper_loser28,\r\n",
        "            upper_loser29,\r\n",
        "            upper_loser30,\r\n",
        "            upper_loser31,\r\n",
        "            upper_loser32,\r\n",
        "            upper_loser33,\r\n",
        "            upper_loser34,\r\n",
        "            upper_loser35,\r\n",
        "            upper_loser36,\r\n",
        "            upper_loser37,\r\n",
        "            upper_loser38,\r\n",
        "            upper_loser39,\r\n",
        "            upper_loser40,\r\n",
        "            upper_loser41,\r\n",
        "            upper_loser42,\r\n",
        "            upper_loser43,\r\n",
        "            upper_loser44,\r\n",
        "            upper_loser45,\r\n",
        "            upper_loser46,\r\n",
        "            upper_loser47,\r\n",
        "            upper_loser48,\r\n",
        "            upper_loser49,\r\n",
        "            upper_loser50,\r\n",
        "            upper_loser51,\r\n",
        "            upper_loser52,\r\n",
        "            upper_loser53,\r\n",
        "            upper_loser54,\r\n",
        "            upper_loser55,\r\n",
        "            upper_loser56,\r\n",
        "            upper_loser57,\r\n",
        "            upper_loser58,\r\n",
        "            upper_loser59,\r\n",
        "            upper_loser60,\r\n",
        "            upper_loser61,\r\n",
        "            upper_loser62,\r\n",
        "            upper_loser63,\r\n",
        "            upper_loser64,\r\n",
        "            upper_loser65,\r\n",
        "            upper_loser66,\r\n",
        "            upper_loser67,\r\n",
        "            upper_loser68,\r\n",
        "            upper_loser69,\r\n",
        "            upper_loser70,\r\n",
        "            upper_loser71,\r\n",
        "            upper_loser72,\r\n",
        "            upper_loser73,\r\n",
        "            upper_loser74,\r\n",
        "            upper_loser75,\r\n",
        "            upper_loser76,\r\n",
        "            upper_loser77,\r\n",
        "            upper_loser78,\r\n",
        "            upper_loser79,\r\n",
        "            upper_loser80,\r\n",
        "            upper_loser81,\r\n",
        "            upper_loser82,\r\n",
        "            upper_loser83,\r\n",
        "            upper_loser84,\r\n",
        "            upper_loser85,\r\n",
        "            upper_loser86,\r\n",
        "            upper_loser87,\r\n",
        "            upper_loser88,\r\n",
        "            upper_loser89,\r\n",
        "            upper_loser90,\r\n",
        "            upper_loser91,\r\n",
        "            upper_loser92,\r\n",
        "            upper_loser93,\r\n",
        "            upper_loser94,\r\n",
        "            upper_loser95,\r\n",
        "            upper_loser96,\r\n",
        "            upper_loser97,\r\n",
        "            upper_loser98,\r\n",
        "            upper_loser99,\r\n",
        "            upper_loser100,\r\n",
        "            upper_loser101]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u19FndLT2oU3"
      },
      "source": [
        "### Summarize arrays: 'Winner'\r\n",
        "\r\n",
        "lower_winner = [lower_winner1,\r\n",
        "            lower_winner2,\r\n",
        "            lower_winner3,\r\n",
        "            lower_winner4,\r\n",
        "            lower_winner5,\r\n",
        "            lower_winner6,\r\n",
        "            lower_winner7,\r\n",
        "            lower_winner8,\r\n",
        "            lower_winner9,\r\n",
        "            lower_winner10,\r\n",
        "            lower_winner11,\r\n",
        "            lower_winner12,\r\n",
        "            lower_winner13,\r\n",
        "            lower_winner14,\r\n",
        "            lower_winner15,\r\n",
        "            lower_winner16,\r\n",
        "            lower_winner17,\r\n",
        "            lower_winner18,\r\n",
        "            lower_winner19,\r\n",
        "            lower_winner20,\r\n",
        "            lower_winner21,\r\n",
        "            lower_winner22,\r\n",
        "            lower_winner23,\r\n",
        "            lower_winner24,\r\n",
        "            lower_winner25,\r\n",
        "            lower_winner26,\r\n",
        "            lower_winner27,\r\n",
        "            lower_winner28,\r\n",
        "            lower_winner29,\r\n",
        "            lower_winner30,\r\n",
        "            lower_winner31,\r\n",
        "            lower_winner32,\r\n",
        "            lower_winner33,\r\n",
        "            lower_winner34,\r\n",
        "            lower_winner35,\r\n",
        "            lower_winner36,\r\n",
        "            lower_winner37,\r\n",
        "            lower_winner38,\r\n",
        "            lower_winner39,\r\n",
        "            lower_winner40,\r\n",
        "            lower_winner41,\r\n",
        "            lower_winner42,\r\n",
        "            lower_winner43,\r\n",
        "            lower_winner44,\r\n",
        "            lower_winner45,\r\n",
        "            lower_winner46,\r\n",
        "            lower_winner47,\r\n",
        "            lower_winner48,\r\n",
        "            lower_winner49,\r\n",
        "            lower_winner50,\r\n",
        "            lower_winner51,\r\n",
        "            lower_winner52,\r\n",
        "            lower_winner53,\r\n",
        "            lower_winner54,\r\n",
        "            lower_winner55,\r\n",
        "            lower_winner56,\r\n",
        "            lower_winner57,\r\n",
        "            lower_winner58,\r\n",
        "            lower_winner59,\r\n",
        "            lower_winner60,\r\n",
        "            lower_winner61,\r\n",
        "            lower_winner62,\r\n",
        "            lower_winner63,\r\n",
        "            lower_winner64,\r\n",
        "            lower_winner65,\r\n",
        "            lower_winner66,\r\n",
        "            lower_winner67,\r\n",
        "            lower_winner68,\r\n",
        "            lower_winner69,\r\n",
        "            lower_winner70,\r\n",
        "            lower_winner71,\r\n",
        "            lower_winner72,\r\n",
        "            lower_winner73,\r\n",
        "            lower_winner74,\r\n",
        "            lower_winner75,\r\n",
        "            lower_winner76,\r\n",
        "            lower_winner77,\r\n",
        "            lower_winner78,\r\n",
        "            lower_winner79,\r\n",
        "            lower_winner80,\r\n",
        "            lower_winner81,\r\n",
        "            lower_winner82,\r\n",
        "            lower_winner83,\r\n",
        "            lower_winner84,\r\n",
        "            lower_winner85,\r\n",
        "            lower_winner86,\r\n",
        "            lower_winner87,\r\n",
        "            lower_winner88,\r\n",
        "            lower_winner89,\r\n",
        "            lower_winner90,\r\n",
        "            lower_winner91,\r\n",
        "            lower_winner92,\r\n",
        "            lower_winner93,\r\n",
        "            lower_winner94,\r\n",
        "            lower_winner95,\r\n",
        "            lower_winner96,\r\n",
        "            lower_winner97,\r\n",
        "            lower_winner98,\r\n",
        "            lower_winner99,\r\n",
        "            lower_winner100,\r\n",
        "            lower_winner101]\r\n",
        "\r\n",
        "median_winner = [median_winner1,\r\n",
        "            median_winner2,\r\n",
        "            median_winner3,\r\n",
        "            median_winner4,\r\n",
        "            median_winner5,\r\n",
        "            median_winner6,\r\n",
        "            median_winner7,\r\n",
        "            median_winner8,\r\n",
        "            median_winner9,\r\n",
        "            median_winner10,\r\n",
        "            median_winner11,\r\n",
        "            median_winner12,\r\n",
        "            median_winner13,\r\n",
        "            median_winner14,\r\n",
        "            median_winner15,\r\n",
        "            median_winner16,\r\n",
        "            median_winner17,\r\n",
        "            median_winner18,\r\n",
        "            median_winner19,\r\n",
        "            median_winner20,\r\n",
        "            median_winner21,\r\n",
        "            median_winner22,\r\n",
        "            median_winner23,\r\n",
        "            median_winner24,\r\n",
        "            median_winner25,\r\n",
        "            median_winner26,\r\n",
        "            median_winner27,\r\n",
        "            median_winner28,\r\n",
        "            median_winner29,\r\n",
        "            median_winner30,\r\n",
        "            median_winner31,\r\n",
        "            median_winner32,\r\n",
        "            median_winner33,\r\n",
        "            median_winner34,\r\n",
        "            median_winner35,\r\n",
        "            median_winner36,\r\n",
        "            median_winner37,\r\n",
        "            median_winner38,\r\n",
        "            median_winner39,\r\n",
        "            median_winner40,\r\n",
        "            median_winner41,\r\n",
        "            median_winner42,\r\n",
        "            median_winner43,\r\n",
        "            median_winner44,\r\n",
        "            median_winner45,\r\n",
        "            median_winner46,\r\n",
        "            median_winner47,\r\n",
        "            median_winner48,\r\n",
        "            median_winner49,\r\n",
        "            median_winner50,\r\n",
        "            median_winner51,\r\n",
        "            median_winner52,\r\n",
        "            median_winner53,\r\n",
        "            median_winner54,\r\n",
        "            median_winner55,\r\n",
        "            median_winner56,\r\n",
        "            median_winner57,\r\n",
        "            median_winner58,\r\n",
        "            median_winner59,\r\n",
        "            median_winner60,\r\n",
        "            median_winner61,\r\n",
        "            median_winner62,\r\n",
        "            median_winner63,\r\n",
        "            median_winner64,\r\n",
        "            median_winner65,\r\n",
        "            median_winner66,\r\n",
        "            median_winner67,\r\n",
        "            median_winner68,\r\n",
        "            median_winner69,\r\n",
        "            median_winner70,\r\n",
        "            median_winner71,\r\n",
        "            median_winner72,\r\n",
        "            median_winner73,\r\n",
        "            median_winner74,\r\n",
        "            median_winner75,\r\n",
        "            median_winner76,\r\n",
        "            median_winner77,\r\n",
        "            median_winner78,\r\n",
        "            median_winner79,\r\n",
        "            median_winner80,\r\n",
        "            median_winner81,\r\n",
        "            median_winner82,\r\n",
        "            median_winner83,\r\n",
        "            median_winner84,\r\n",
        "            median_winner85,\r\n",
        "            median_winner86,\r\n",
        "            median_winner87,\r\n",
        "            median_winner88,\r\n",
        "            median_winner89,\r\n",
        "            median_winner90,\r\n",
        "            median_winner91,\r\n",
        "            median_winner92,\r\n",
        "            median_winner93,\r\n",
        "            median_winner94,\r\n",
        "            median_winner95,\r\n",
        "            median_winner96,\r\n",
        "            median_winner97,\r\n",
        "            median_winner98,\r\n",
        "            median_winner99,\r\n",
        "            median_winner100,\r\n",
        "            median_winner101]\r\n",
        "\r\n",
        "upper_winner = [upper_winner1,\r\n",
        "            upper_winner2,\r\n",
        "            upper_winner3,\r\n",
        "            upper_winner4,\r\n",
        "            upper_winner5,\r\n",
        "            upper_winner6,\r\n",
        "            upper_winner7,\r\n",
        "            upper_winner8,\r\n",
        "            upper_winner9,\r\n",
        "            upper_winner10,\r\n",
        "            upper_winner11,\r\n",
        "            upper_winner12,\r\n",
        "            upper_winner13,\r\n",
        "            upper_winner14,\r\n",
        "            upper_winner15,\r\n",
        "            upper_winner16,\r\n",
        "            upper_winner17,\r\n",
        "            upper_winner18,\r\n",
        "            upper_winner19,\r\n",
        "            upper_winner20,\r\n",
        "            upper_winner21,\r\n",
        "            upper_winner22,\r\n",
        "            upper_winner23,\r\n",
        "            upper_winner24,\r\n",
        "            upper_winner25,\r\n",
        "            upper_winner26,\r\n",
        "            upper_winner27,\r\n",
        "            upper_winner28,\r\n",
        "            upper_winner29,\r\n",
        "            upper_winner30,\r\n",
        "            upper_winner31,\r\n",
        "            upper_winner32,\r\n",
        "            upper_winner33,\r\n",
        "            upper_winner34,\r\n",
        "            upper_winner35,\r\n",
        "            upper_winner36,\r\n",
        "            upper_winner37,\r\n",
        "            upper_winner38,\r\n",
        "            upper_winner39,\r\n",
        "            upper_winner40,\r\n",
        "            upper_winner41,\r\n",
        "            upper_winner42,\r\n",
        "            upper_winner43,\r\n",
        "            upper_winner44,\r\n",
        "            upper_winner45,\r\n",
        "            upper_winner46,\r\n",
        "            upper_winner47,\r\n",
        "            upper_winner48,\r\n",
        "            upper_winner49,\r\n",
        "            upper_winner50,\r\n",
        "            upper_winner51,\r\n",
        "            upper_winner52,\r\n",
        "            upper_winner53,\r\n",
        "            upper_winner54,\r\n",
        "            upper_winner55,\r\n",
        "            upper_winner56,\r\n",
        "            upper_winner57,\r\n",
        "            upper_winner58,\r\n",
        "            upper_winner59,\r\n",
        "            upper_winner60,\r\n",
        "            upper_winner61,\r\n",
        "            upper_winner62,\r\n",
        "            upper_winner63,\r\n",
        "            upper_winner64,\r\n",
        "            upper_winner65,\r\n",
        "            upper_winner66,\r\n",
        "            upper_winner67,\r\n",
        "            upper_winner68,\r\n",
        "            upper_winner69,\r\n",
        "            upper_winner70,\r\n",
        "            upper_winner71,\r\n",
        "            upper_winner72,\r\n",
        "            upper_winner73,\r\n",
        "            upper_winner74,\r\n",
        "            upper_winner75,\r\n",
        "            upper_winner76,\r\n",
        "            upper_winner77,\r\n",
        "            upper_winner78,\r\n",
        "            upper_winner79,\r\n",
        "            upper_winner80,\r\n",
        "            upper_winner81,\r\n",
        "            upper_winner82,\r\n",
        "            upper_winner83,\r\n",
        "            upper_winner84,\r\n",
        "            upper_winner85,\r\n",
        "            upper_winner86,\r\n",
        "            upper_winner87,\r\n",
        "            upper_winner88,\r\n",
        "            upper_winner89,\r\n",
        "            upper_winner90,\r\n",
        "            upper_winner91,\r\n",
        "            upper_winner92,\r\n",
        "            upper_winner93,\r\n",
        "            upper_winner94,\r\n",
        "            upper_winner95,\r\n",
        "            upper_winner96,\r\n",
        "            upper_winner97,\r\n",
        "            upper_winner98,\r\n",
        "            upper_winner99,\r\n",
        "            upper_winner100,\r\n",
        "            upper_winner101]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84XIzmWD2oba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "outputId": "d25b50b4-bc08-4a45-c838-ba87af82bad5"
      },
      "source": [
        "### Visualize!\r\n",
        "\r\n",
        "title = obj_func\r\n",
        "\r\n",
        "plt.figure()\r\n",
        "\r\n",
        "plt.plot(median_loser, color = 'Green')\r\n",
        "plt.plot(median_winner, color = 'Red')\r\n",
        "\r\n",
        "xstar = np.arange(0, max_iter+1, step=1)\r\n",
        "plt.fill_between(xstar, lower_loser, upper_loser, facecolor = 'Green', alpha=0.4, label='GP EI Regret IQR: dEI')\r\n",
        "plt.fill_between(xstar, lower_winner, upper_winner, facecolor = 'Red', alpha=0.4, label='STP ERM Regret IQR: dERM')\r\n",
        "\r\n",
        "plt.title(title, weight = 'bold', family = 'Arial')\r\n",
        "plt.xlabel('(Post-initialization) iteration $\\it{k}$', weight = 'bold', family = 'Arial') \r\n",
        "plt.ylabel('log(Regret)', weight = 'bold', family = 'Arial') \r\n",
        "plt.legend(loc=1) # add plot legend\r\n",
        "\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "findfont: Font family ['Arial'] not found. Falling back to DejaVu Sans.\n",
            "findfont: Font family ['Arial'] not found. Falling back to DejaVu Sans.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEaCAYAAAAYOoCaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdeXxU1fn48c+ZyWTfF0KAkIQlYQchqIAsoqJ1oaDWpVq1tj+su9ZiUfutWmurxVpr3Wqrba1ate77gqLsZVF2CGtIAoGQFbIvc35/nCQkZJtM7mQyyfN+veaV5M6de88kcJ+5Z3kepbVGCCFE32PzdgOEEEJ4hwQAIYTooyQACCFEHyUBQAgh+igJAEII0UdJABBCiD5KAoAQQvRREgCET1JKZSqltFJqXpNts+q3FXfx2F/XH+e6LjfUw5RSI5VS/1NKFSulqpRSB5RSf1ZKBXq7baLnkwAgRD2llMPbbXBDDFAL/Bd4A4gDbgPu8WajhG+QACB6LaXUq0qpg/WfjI8rpb5SSo1t8nzDXcR9SqltQKVS6mtgZv0u/6h//oGmdxdKqbuVUkX1n7bnKKVuUkodVUodUkpd0+T4v1BK7VZKldW3YZNS6tImz/+z/pjPKaU+UEqVK6U2K6UmNNlH1z9uUUrtqn8fLyul/AG01iu01tO01v9Pa/0j4O/1L03x3G9W9BZKUkEIX6SUygSSgA+BvfWbBwGXACVa60il1EogEygGxgAzgJ1a65EnHaMOeBOwA98Ai4CBwBfAduBToBJYCmhgK3AUmA0cA0qAdcDFQDkwQGtdopR6GkgAcjGfzC/GfFofobXOVEr9E7i2vu3vAqnAKGCF1np6fRsb/oMWAh8AlwFBwE+11i/U7xMN/BpzN3AJUAWcr7Ve7d5vV/QVft5ugBBddGE7z12GuegOBDZjAsAIpdQArfWhJvv9Tmv964Yf6j+lDwRe1Vr/s37brIangfMx/3f2A+HAlVrrj5VS+ZiLcComINyNuSAPB6oxQaM/MBUTmBp8rLWer5Q6E/gKOKWV9/IzrfV/lVIKuOakfcKB25v8/A2Q1c7vRQhAAoDwffO11u9C40V6af33w4FvgdBWXhMHNA0AKztxvlKtdY5SKrLJtoyG5zABIKS+i2YN5s6jtfM39V3914bB65BWXnPyPo3vS2udCSilVBzwKPBj4J/AOR29GdG3yRiA6K0uwFwkNwKRQHyT59RJ+1ad9HNd/dfW/n/UubhtFObiXwsMrT/W9jbOX1v/tb3+2Fb3UUqFNXyvtT6K6bYCcxciRLvkDkD0Vkfqv6YCfwYmtLPvybLrv96ulBoH/MON8+cDTsz/sT8CYZiuIKs9qZQaBWwBAoC59ds/88C5RC8jdwCit3oDeAHz6fxs4PedeO0fMWMGozB9652+cGutc4BbMYFoNrABWNXZ47hgNeZO50pgPqZr66H6cwvRLpkFJIQQfZTcAQghRB8lAUAIIfooCQBCCNFHSQAQQog+yqemgcbGxurk5GRvN0MIIXzKhg0b8rXWJy9A9K0AkJyczPr1673dDCGE8ClKqQOtbZcuICGE6KMkAAghRB8lAUAIIfoonxoDEMKbampqyMnJobKy0ttNEaJVgYGBDBo0CIfDteJ2EgCEcFFOTg5hYWEkJydj0vIL0XNorSkoKCAnJ4eUFNcKwkkXkBAuqqysJCYmRi7+okdSShETE9OpO1QJAEJ0glz8RU/W2X+fXg0ASqkXlVJ5Sqmt3myHEEL0Rd4eA/gn8BTwkqdPtOvLN0icMJOgmPiOdxbCBc9veN7S4y2YtKDDfY4cOcKdd97JmjVriIqKwt/fn7vvvpv58+fz9ddf8/3vf5+UlBSqqqq44ooruP/++5u9PjMzk5EjR5KWlta47ec//znXXHNN40LL2NjYZq9JTk4mLCwMpRRRUVG89NJLJCUlWfOmW1FcXMyrr77KTTfd1OrzoaGhlJaWArBt2zZuvfVWDh48SG1tLVdffTX3338/NpuNf/7znyxcuJCBAwdSWVnJDTfcwJ133tmptjT9ndjtdsaOHdv43BVXXMGiRYuYNWsWjz32GOnp6e6/aS/x6h2A1noZUNgd5yrYtIZvn/glu994jtpjxR2/QIgeRmvNvHnzmDFjBvv27WPDhg289tpr5OTkNO4zffp0Nm7cyPr163n55Zf59ttvWxxn6NChbNy4sfFxzTXXdHjupUuXsnnzZmbNmsVvf/tbS96L0+ls9bni4mKeeeaZDo9RUVHB3LlzWbRoERkZGWzZsoW1a9fy5z//uXGfyy+/nI0bN7Jy5UoefvhhsrOz2zli+4KCgpr93hYtWuT2sXqKHj8GoJRaoJRar5Raf/To0S4dq85ZS+6mFex77Vmore34BUL0IF999RX+/v787Gc/a9yWlJTErbe2LP4VEhLCpEmT2LNnj6VtmDJlCgcPHgTg6NGjXHLJJUyePJnJkyezcuXKxu3nnHMOo0eP5qc//SlJSUnk5+eTmZlJWloa11xzDWPGjCE7O5vFixczefJkxo0b13i3smjRIvbu3cuECRNYuHBhm2159dVXmTZtGnPmzAEgODiYp556isWLF7fYNyYmhmHDhpGbm9vu+ysoKGDOnDmNbe/tBbN6fADQWj+vtU7XWqfHxbXIZeSWvKwdVHwpJVOFb9m2bRsTJ050ad+CggLWrFnD6NGjWzzXcHFteCxfvtzlNnz66afMmzcPgNtvv50777yTdevW8dZbb/HTn/4UgAcffJDZs2ezbds2Lr30UrKyshpfv3v3bm666Sa2bdtGRkYGu3fvZu3atWzcuJENGzawbNkyHnnkkca7lNYu5k1/H5MmTWq2bejQoVRUVFBc3PwuPysri8rKSsaNGwfAr3/9a95///0Wx3zwwQc544wz2LZtG/Pnz2/W9oqKima/t9dff93l31tP5e0xAK9waicHNnzFiIFJMGaMt5sjhFtuvvlmVqxYgb+/P+vWrQNg+fLlnHLKKdhsNhYtWtRqAGi4uHbGmWeeSWFhIaGhoTz00EMALFmyhO3btzfuc+zYMUpLS1mxYgXvvPMOAOeddx5RUVGN+yQlJXH66acD8Pnnn/P5559zyimnAFBaWsru3bsZPHhwp9rWntdff51ly5axc+dOnnrqKQIDAwH4zW9+0+r+y5Yt4+233wbgggsuaNb2hi6g3qRPBgCAvLIjDFz2BWEhIeDiogkhvGn06NG89dZbjT8//fTT5OfnNxt8nD59Oh9++KHl5166dCmRkZFcddVV3H///Tz++OM4nU7WrFnTeFF1RUhISOP3Wmvuuecebrjhhmb7ZGZmunSsUaNGsWzZsmbb9u3bR0xMDJGRkYAZA3jqqadYv349c+bMYe7cufTv39/l9vZ23p4G+h9gNZCmlMpRSv2kO8+fWbAPvvjCPMrKuvPUQnTa7Nmzqays5Nlnn23cVl5e3m3n9/Pz44knnuCll16isLCQOXPm8Je//KXx+YZPx9OmTeONN94AzKf8oqKiVo937rnn8uKLLzbO6Dl48CB5eXmEhYVx/PjxDttz1VVXsWLFCpYsWQKYLprbbruNBx98sMW+6enp/OhHP2o2QNyaGTNm8OqrrwLwySeftNn23sKrdwBa6yu9ef6iyiJKKkuI2L8fcnIgNRXsdvNkUBAMHw7Bwd5soujBXJm2aSWlFO+++y533nknf/jDH4iLiyMkJIRHH320U8dpGANocP3113Pbbbe59NqEhASuvPJKnn76aZ588kluvvlmxo0bR21tLTNmzOC5557j/vvv58orr+Tf//43U6ZMoX///oSFhTVe6BvMmTOHHTt2MGXKFMBM73z55ZcZOnQo06ZNY8yYMXzve99rcxwgKCiI999/n1tvvZWbbrqJgwcP8qtf/Yqrrrqq1f1/+ctfMnHiRO69914WL15Meno6c+fObbZPQ9tHjx7N1KlTm3VHNYwBNDjvvPN45JFHXPq99VTKl0a509PTtTsFYVbccxV+77zH8LoIAMr6RZE5czwoRWJ4IilRbXQBKQVJSTBsGISHQ1gYBAR05S0IH7Zjxw5Gjhzp7Wb0eFVVVdjtdvz8/Fi9ejU33nhjt/Sdv/vuu/z85z9n6dKlHl2n0NO19u9UKbVBa91ioULfGAM4dgy/Y2VUaUVErZ2YPYcoj40gb0wKxVUlbb9Oa8jMNI8Gfn7mLsFuN3cJ48bB0KFg6/ETqoToFllZWVx22WU4nU78/f3529/+1i3nnTdvXuMMJeGaPhEATv/LO4zon0BpXTn/qj6fKf/6kiFffkvBsIGUcpxaZy1+Nhd/FbW1J9YQlJfD0qWwYQNMnQoWzl4QwlcNHz6c7777ztvNEC7oEx9b/Wx+3BVwJkdUOS/7bWPPuZNxlFWStHwzGk1JZTt3Aa44dgy++gokT7wQwof0iQAAMMFvEOfWpfCabQc7EhzkThzOwPW7CDlSRHGlBakhqquhfi62EEL4gj4TAAB+VncKAdj5s9969s8cR02gP8M+W0txuUXpiHbsgPx8a44lhBAe1qcCQDRBXF83jnW2XFYEF7Dv7IlE5OQTt2QV1XXV1pxk1SprjiOEEB7WJwaBm5rnTOVdvYtn/L5l8tgLiMg+yuDV2ylJXYX/1FldP8Hhw+ZOYOBAM420rQINDbOIhO963tp00CzoeF3Bww8/zKuvvordbsdms/HXv/6VRx55hP3791NaWsrRo0cbywE+88wz3HvvveTm5hIYGEhoaCgvvvhis1TQANdddx3ffPMNERFmmnRwcDCrVq1qN53yAw88wIMPPsju3bsZNmwYAE888URjbqCTUyPPmjWrsR0NM4Oazqn3hN/97nfce++9rT7XNM1zTk4ON998M9u3b6euro7zzz+fP/7xjwQEBDRLsV1ZWcmFF17IY4891ql2NE0X3ZBa216/3mjGjBk8+eSTzf4GWmsef/xxzjrrrMbX79u3jwMHDjQWfJk3bx5Llixpsbais/pcAPDDxo21E7nX8Q0f2PbgmJNOyNFiwv7zJiSnwoABXT+JK8m17Ha4+GJokmtEiPasXr2aDz/8kG+//ZaAgADy8/Oprq5uzLvz9ddf89hjj7VIBfHKK6+Qnp7O888/z8KFC1tNgrZ48WIuvfTSFtsbUikUFBSQlpbGpZdeSmJiIgBjx47ltdde41e/+hUA//3vf1vNPXRyO/7xj3+wcOFCvvjiC7d/Fw3q6uoaL6Ynay8ANNBac/HFF3PjjTfy3nvvUVdXx4IFC7j77rsbVw03pNeoqKjglFNOYf78+UybNs3tNi9durRFzQU48TdYunQpCxYsYPfu3Y3PRUZGsnLlSs444wyKi4s7zGrqqj7VBdRgqh7IKc54/mHfzDG/OrZfPJ06hx889hj89rftP/74R+hi1AWgrs7MHGojJ7oQJ8vNzSU2NpaA+sWIsbGxDOjEB5YZM2a4nR66tXTK8+bN47333gPM6uKIiIhWL2wna5pSuqysjOuvv55TTz2VU045pfF45eXlXHbZZYwaNYr58+dz2mmn0bAINDQ0lLvuuovx48ezevVqXn75ZU499VQmTJjADTfcQF1dHYsWLWpcudvWymAwKbYDAwP58Y9/DIDdbudPf/oTL730UotP10FBQUyYMKGx7W2pqKjgiiuuYOTIkcyfP5+KiooOfydt/X4aXHHFFbz22msAvP3221x88cWdOmZb+mQAUChuqpvIMap42b6V6rBgtlw2k7rhwyA6uu1HZCTs2gUrVljTkIICmTkkXDZnzhyys7NJTU3lpptu4ptvvunU6z/44INmFa2aWrhwYWOa49YumCenUwYIDw8nMTGRrVu38tprr3H55Ze71I6mKaUffvhhZs+ezdq1a1m6dCkLFy6krKyMZ555hqioKLZv385DDz3Ehg0bGl9fVlbGaaedxqZNm4iJieH1119n5cqVbNy4EbvdziuvvMIjjzzSmL3zlVdeabMtraWUDg8PJzk5uUWwLCoqYvfu3cyYMQOA5557jueee67FMZ999lmCg4PZsWMHDz74YLO2g8ms2vC7/tOf/tTu76fBWWedxbJly6irq+vU77ojfa4LqEGqjuZc5xDetGVwpjOJEQkx7Esdy/DoYe2/8I9/hGXLYM4ca1b/btoEiYnWdD2JXi00NJQNGzawfPlyli5dyuWXX84jjzzCdddd1+7rrrrqKoKCgkhOTm6WvK2ptrqA2kqn3KDhk+lnn33Gl19+yT/+8Y9221FdXU1paWljaojPP/+c999/v7FfvbKykqysLFasWMHtt98OwJgxY5oFHrvdziWXXALAl19+yYYNG5g8eTJgPn3369ev3d9HZy1fvpzx48eze/du7rjjjsZsok0L8zS1bNmyxtxK48aNa9Z2aLsLaOHChdx7773k5OSwevXqZs/Z7XbOOOMMXnvtNSoqKkhOTrbgnfXRO4AGN9VNJJpAHvBbwXGqyT1+iKySrPZfNHOm+eS+bZt1DfnySzhwwLrjiV7Lbrcza9YsHnzwQZ566qlm6aHb8sorr7Bx40befffdxv57V11++eVs3ryZVatWsWjRIg4fPtzs+QsvvJB///vfDB48mPDw8A7bsW/fPq699trGKmZaa956663GMotZWVkd5lsKDAxs7PfXWnPttdc2vj4jI4MHHnjA5fc3atSoFp/Qjx07xuHDhxsHy6dPn86mTZvYtm0bL7zwgsfyGi1evJhdu3bx6KOPcv3117d4/oorruC2227jsssus+ycfToARBDAA7XTyaOMR/3WoNFkFmeSW3q47RdNmGASw3Xy9rtdFRXw2WewZIn5XohWNFTQarBx48ZuS3rWVjrl4OBgHn30Ue677z6XjqOU4qGHHmLNmjXs3LmTc889l7/85S+NpRcbUkg0TSm9fft2tmzZ0urxzjrrLN58803y8vIAKCws5ED9hymHw0FNTU277TnrrLMoLy/npZdeAsyg8l133cUtt9xC0Emz9FJSUli0aFGH2VebppTeunUrmzdvbnf/k91yyy04nU4++6x51cLp06dzzz33cOWV1iVR7rNdQA1G61huqDuFZ/y+5U1bBj9wjmBPwW5q6qoJ8Q8l0B5AoF8gdlv9TAM/P5g2DT791NwJxMRY15h9++DQIbjwQjPmIHo2F6ZtWqm0tJRbb72V4uJi/Pz8GDZsGM9bNBV14cKFzYq9r127tsU+TdMpN3XFFVd06lxBQUHcddddLF68mKeeeoo77riDcePG4XQ6SUlJ4cMPP+Smm27i2muvZdSoUYwYMYLRo0c3TlNtatSoUfz2t79lzpw5OJ1OHA4HTz/9NElJSSxYsIBx48YxceLENscBlFK888473HzzzTz00EMcPXqUyy+/vM2A9rOf/YzHHnuMzMxMPv3008ZtTd144438+Mc/ZuTIkYwcObLFGMOZZ57ZeAczbty4xuDTtE2/+tWv+MMf/sC5557bbPsvfvGLDn67ndMn0kEDrH7859QU5LX6nEbzK79lrFGH+GvteQzTzadmRgVGMTa+yeBZYSHcey+cdx54IvtgcDBcdBG08g9eeI+kg+4+dXV11NTUEBgYyN69ezn77LPJyMjA39/fo+ddtWoVV155Je+8847L9Zd7GkkH3UkKxd21p3Od40N+b1/Nc7Xn4uDE3OKiyiLyy/OJDa4fuImOhrFjzWwgdwvV2+0mlXRrBWfKy+Gjj2DuXAgNde/4Qviw8vJyzjzzTGpqatBa88wzz3j84g8wderUxi6kvqDPBAB7SCjOKpOt01lehnbWNXs+ggDuqj2V+xzL+Ld9G9fXNR+531e0j6jAqBNdQbNnwxNPwEm3b52SmAh33NH6Rb60FD74AJrWL7Xbwd/fPDxRfyAqyhTAEcLLwsLCcPduX7iuzwSAU2/4TeP3JUey2PH6U1QfOdRsnzN0InPqUnjZtpUznINI1Sf64StrK8k5fpCkiPqc/yNHwuLFJ2oDdFZWFvztbyaI3HknNCmW3ej4cfPoTv36wamnyrTUNmitG5fjC9HTdLZLv8+MAZzMWVfL9o//ReGapc22H6eK6xwfEYKD39TMIJkT/fA2ZSM9IZ1AR+DJh3PP1q3w7LOQkAC33tqz+vxDQ71T5Sw01AyC90D79+8nLCyMmJgYCQKix9FaU1BQwPHjxxvzQTVoawygzwaABp9te5+ckmwAlNOJX0UVO49s44n9r1LprOHMiAn8IHYmMQ4zx3mAI5qpFbGosjJrGtAQBLSGSZNM11JycttJ5PqCuXObd331EDU1NeTk5FAphX9EDxUYGMigQYNwOBzNtksAaMPq7NVsyWs5x7i0upSPd3/MNwe+wc/mx29m/YaIQPMJ/fSBpzGuKtJk/czNhQ7mGnfoyBH4+muTSrqyEgYNgilTTFdMB4treqXUVJg1y9utEKLXkADQhq15W1mV3XYO/8ziTH6/4vf8cOwPmZk0EzBdQfNHzCcmOMYkcysshKNHTYI3MNvKyqCkxPTh19W1PHBtrZnt01RlJfzvfyYQZGaaLpimAeCUU+DSS81ahN7Mboerr4b6pGdCiK6RaaBtCA9o/xN2UkQSscGxbD6yuTEAOLWTL/d/ycUjLzbF5GNjzaMzamvhzTdNPeEGgYEm1cTMmWZB2Nq1JwaBGwrQZ2XBDTf0rPECq9XVwZ490E5qYSFE13k1ACilzgP+DNiBv2utH+nuNnQUAJRSjIsfx7IDy6iqrSLAz3wqLa4s5sNdHxLq33IKZ7AjmNMHnY5NtTOI6ucHZ5wBH3/c+vMDBrRcZLZ+PfzrX/C738FZZ50YpB02zIwb9CY7dkgAEMLDvBYAlFJ24GngHCAHWKeUel9rvb072xHmH9bhPuPix/HV/q/Ykb+DCf1PVDHKK8sjr6z11cVVtVXMSp7V/myRQYNMf/euXa41Nj0d4uPhueegaRIwpeB73zOzZ9oojuFzCgshL89MSxVCeIQ37wBOBfZorfcBKKVeA74PdGsAsNvshDhCKKtpe1bP8OjhBPoFsvnI5mYBoD27C3cT4BfA1MSp7e94+ummW8fVmSWJifDQQyf2r6mBd94xdxLbt8P5558YIwgIOFHHwBtTOrtq61YzGOyLbRfCB3gzAAwEspv8nAOc5o2GhAeEtxsA/Gx+jI4bzZa8LTi1s/2unSa25m0FOJFCoolgRzCDwgeZfv9p00xKaFfZbM1TSFx3nUlN8fLL8Mwzbe/f0dTShAS4+WbTpp5gzx7Izjark4cNM3dMQgjL9PhBYKXUAmABwODBgz1yjrCAMHJL26+xOS5+HBtyN3Cg+AApUSnt7ttUQxA4WURABJePqa/qM3QoFBXBt9+6fNwWJk2CtDQzpbRBRYXpSiko6DjNdG2tmX30xhtwzTXut8NqVVWmi2z3bvjBD8zdjBDCEt4MAAeBptUpBtVva0Zr/TzwPJhpoJ5oSEcDwQBj+o1Bodict7lTAaAtJVUl1NTV4LDXL9hITzdTRzMy3D9oaGjXkseFh8Mnn5jB15NS2Hqd1iZAzp7t7ZYI0Wt4s3N1HTBcKZWilPIHrgDe90ZDXAkAof6hDI0eypYjrRemcEdhRWHzDdOnmz5+b7noIkhJMV1JhYUd79/d9uwxd0pCCEt47Q5Aa12rlLoF+AwzDfRFrbWFdRZd50oAANMN9PaOtymsKCQ6qOsFWwoqCogPjT+xwWYzn3D/8x+oru7y8TvNboef/MQMMj/xhHf63G02M6Np4MDWn9+wAc4+u3vbJEQv5dUxAK31x0AbE+G7j6sBYHTcaN7e8TYZ+RlMSZzS5fPml+e33BgQAOPHw7p1XT6+W+Li4Prr4f33zWK07pafb4LfTTe1/vy+febuRCqmCdFlPX4QuDsE+gXisDmocbaf02dA2ABCHCFkFFgTAArKC1p/YswYMwXSW/WBJ0wwD2947z0zDnH0aNvFdtauhc5Wa4qNlemkQpxEAkC98IBwCirauCDXsykbaTFpZBRkWJIXvrCisPVppQ6Hyfuzqu0cRb3WzJmm3vLSpXDZZa3vk5VlHp0xdaoJrEKIRvKRqF5YQMcrggFSY1MprCjsMFi4ok7XUVJZ0vqTI0f2zXKQkZFmRtTKla4vjnPFunUtk+8J0cdJAKjn6jhAWkwaABn5XZiu2USbgcRuNxfC4OD2H0FBlrSjR5k921z8rbwDqqmBNWusO54QvYB0AdVzNQAkhCYQ5h9GRkEG0wZP6/J5C8oLGBY9rPUnU1PNoyMHDsA331j7idmbUlLMY+lSa1NB7NljFsu1NcNIiD5G7gDquRoAlFKkxqQ2jgN0lRVdSSQlwSWX9K46vrNnm2RwzzwDL7wAL75oaiR01cqVpl6DEELuABq4khW0QVpsGhtyN5BXltd8Hr8bWp0K6o6QELjggub93O+/3/1F5a0yaZLpAmpIbZGfbwbHu5r2urjYpMZoa4aREH2IBIB6YQFhKBSajj/VN44DFGR0OQBU1lZSXlNOsCO44507opQJBA0SEnw3ANjtcMcdJ35evBgOH7bm2IcPSwAQAukCamRTNkL8QzreEYgPiSc8IJxdBS7m8e+AZXcBJ+uBhdXd1r+/qb9sBasCiRA+TgJAE4nhicQFxxEXHEegX9spkZVSzdYDdFWbC8K6qjcFgIQEkyyvtLTrx5IAIAQgXUDNTE+a3vj9zvydLDuwrM1902LSWHdoHb9c8ssWC8IcNgc3Tb6JAWGuDcoeKTvCsSpTG9imbK2WmXRLZKSZJuqtFcVWaghmubkwfHjXjlVRASUlvbuushAukADQhqFRQ1mVvYpaZ22rz08aMImc4znU1DVPH6HRrMpexeYjm10OAFklWWSVmJWtAfYArhl/TZdXGTfq3x/277fmWN5kZQAAcxcgAUD0cRIA2uCwOxgaNZSMgtYXfAU7grlyzJWtPrencA/7i9y76FbVVVFYUUhMcIxbr28hIaF3BIDoaDMLyKrumyNHzJoAIfowGQNoR1qsexeIlMgU9hfvd3t84NBxC7Nw9pZxAJvNvBcrZwIJ0cdJAGhH/9D+RAZ2vgRhSmQKJVUlFFW6V7zE0gAQEwP+/tYdz5usDADFxb1n5bQQbpIA0IGGOf+d0VAy0t1uoNzSXEtmFySNKfsAACAASURBVAFmbUB819Yq9BgJCWYRl1XFcuQuQPRxEgA6MDxmOIrODcgOCh+En82P/cXuBYDqumprUkQ0SEiw7lje1NCdJd1AQlhCAkAHgh3BTOg/gaSIJJIikogI6HjmiJ/Nj6SIJPYV7XP7vLnHLVr0BL1nHEACgBCWkllALpg8cHLj91klWXy659MOX5McmcyyA8uoc9Zht9k7fc5Dxw8xNn5sp1/Xqrg4CAz0/T7vfv1Ml5ZVF+78fFN5rKNso+HhZgaSEL2MBIBOGhQ+iAB7AFV1Ve3ulxKVwpf7vyTnWA5JkUmdPk/DOIAl6wHsdrj6anPhzMw0F77WFBZ6pxi9qxwOE8ysSgnhdMI773S83/jxcNpp1pxTiB5EAkAn2ZSNlKgUdubvbHe/IZFDANhfvN+tANAwDhAbHOtWO1uw2Uy66PZSRu/cCcvaXv3cI1g5E8hV27ebGskBAd17XiE8TMYA3DA0amiH+0QHRRMeEO72TCCweDqoK9LSzIKrnqx/f1MnoK6u+85ZU2OCgBC9jAQANwwIG0CQX/ulGJVSjQvC3NXtAUCpnt/V0b8/1Naa6aDdacsWc14hehHpAnKDUoohUUPYdnRbu/ulRKWw6cgmyqrLXE413VRWSRbPb3je3WY20z+0P1MTp3bcpZSYaEomHjxoyXkt1zClNTfXDAp3l8pKyMiA0aO775xCeJgEADcNjR7acQCINAvC9hXts25Gj5sOlx7m7R1vMzJ2ZIdrG9ToFAL37mj1uQC/AAL8vNgX3jAVdMUKOFR/hzRuXPfU+d20CUaOtK5GsRBe5pUAoJT6AfAAMBI4VWu93hvt6Ir4kHhC/UMprW47P/2QqCGEOEJYnrXc6wGgwY78HezIb/3i3lRwqhNbbct+9iA/P84ZMo3QgC6mrK6qMiUfOzvrKDgYBg+GzZvNA+Djj+GGG2DMmK61qSOlpabMZkNqjfHjpcC88GneugPYClwM/NVL5+8ypRSnDTyNw6WHqXXWUlBR0KKyl7/dn1nJs/h498ccKT3S5fKR3ak8rvUcSKXAR9VbmZsylyBH++MgHYqLMxfvsrLOve6ee04MApeWwtNPm8c118CUKV1rU0fy8k5836+fBADh05RlOWfcOblSXwO/cPUOID09Xa9f3zNvFnKP5/LBrg9abD9WdYx7vryHqYlTuWrsVV5omWfEBsdyYeqF+Nu7mGiurAw++cSsQXBXRQU8+6zpo09NNeseTpaeDmec4f45WjNkCJx9trXHFMIDlFIbtNbpJ2/v8WMASqkFwAKAwYMHe7k1bYsPjcff7k91XfMujfCAcE4feDqrs1czN3UuYQFhXmqhtfLL83l1y6vYlbnYBjmCOHXgqQyO6OTfKCQELrkEGj6I1NTAZ591bq5/UBDceiu8+SZkZbWcrXPkiLlTsDoAFLmX7VWInqLDAKCUCgEuBKYDyfWbDwDfAB9prVu9f1dKLQFaS0Jzn9b6PVcbqLV+HngezB2Aq6/rbjZlY1D4oFbz/5w95GxWZK/gmwPfcGHqhV5onWc0DXYVtRV8uudTEsMTSR+Q7vZAsV3ZCfne9+DTTzu34tfhgCtbL9DDK6/Ahg1utaddJSVmNbEMCgsf1W4AUEo9Dvw/IASoBQoABcwBbgRKlVJ/01rfdfJrtdZ97t44MTyx1QCQEJbA2H5j+Trza6YlTsNus5sLnRtTQ3u67GPZZB/Ldvv1/nZ/rh53NX7f+565E7BiOmpUlOlqqq62tjaC0wnHjpnay0L4oI7uAC4DngA+AL7TWtcAKKX8gVOAi4DrgBYBoC9KjEhs87lzhpzD42seZ9GXiwBQKK4ccyUzk2d2V/N8QnVdNbsLdjMybiRccMGJJ4qL4Y033Dtow+rmoiLrayMUF0sAED6rowCQpLVuMRdQa10N/A/4n1Lq/s6eVCk1H/gLEAd8pJTaqLU+t7PH6WmCHcHEBMW0mss/NSaVBRMXcLz6OABrD67l3Yx3SR+Q3ivvBLpia95WEwCaCu3CtNOoKPO1sND6AFBUBMnJ1h5TiG7Sbudlw8VfKbVPKdX4cUwpNVMp9XnTfTpDa/2O1nqQ1jpAax3fGy7+Ddq6C1BKMWnAJGYlz2JW8ix+OPaHVNRUuJRauq8pqixqmQbDz8+sAXBH0zsAq8lAsPBh7QYApVS4UioJM/ibpJQarJQaDMwEzuqG9vkcV2fBDAofxOmDTuerzK8orOjCFMheamve1pYb3b0LaOii8cTFurjY+mMK0U06mr5wJ7AP0Jgum/31j/uBLM82zTf1C+nn8tz4uWlzAXg/431PNsknZRZncrzqePON4eHuHczhgLAwzwUAL66lEaIrOhoD2AV8ApwPfAccwgSDInx4Fa8ntTcd9GTRQdHMTpnNF3u/ICooCofNvapT4+PHMzC8961I3ZK3hfQBJ9au2IICUc46l4vk2FSTzzdRUZ4JALW1Zo1BWO9Y3yH6lnYDgNb6P8B/6gd6/6u1lqToLhgSNcTlesDnDT2PjYc38vHuj90+39LMpfxm1m+6npqhh9mat7VZV1DEocMkZO9y+fVTE6fiZ6v/Jx4VZco/ekJRkQQA4ZNcXQm8GHhQKXU2cAtwObBCa+3mvLzeLTkyucNEcQ1C/EN4cNaDuJuSI6ski0dXPsqHuz/kB6N+4NYxfEVtUOcWl5VVlxERGGF+iIqCXa4Hj04pLjYJ6oTwMa4uYXwcMx4wDggA7MBCTzXK19mUjTH9XM9MaVM2szjMjUdKVApTE6fy1f6vyD1uUa3cHqo6JLBT+5dVN1mkHh1tcgZVVlrcKmQmkPBZrgaASzB3AQ02AGnWN6f3GBE7wu0+/c6aN2IeAfYA3tj+htt3Er6gs3cApTVN7sCargWwmgQA4aNcDQBOaFZBZDwmM7Bog7/dnxGxI7rlXOEB4cxNm8v2o9v5bO9nbDy8kY2HN/a66aXabutUECirKT/xgyfXAshUUOGjXB0D+Aj4ef33/8Ykefu7R1rUi4zpN4ateVvReP5T+cykmazKXsU7O99p3BbiCOGeM+4hLiTO4+fvLjXBgfhVVLm0b3l1GVprM2uo4Q7AEwGguhrKy91fqCaEl7gaAO7A3AFcADiAfwG/8FSjeouwgDBSolJcnhHUFXabnbun3c3hUpNGuaKmgr9u+CtPr3uaX077Za+ZIVQdEkhQQYlL+9bpOipqKwl2BJnFYEp5rrumqEgCgPA5HXYBKaXsmIVfL2mt+9U/rtdaH+/otQKmD57O7JTZDIseRpBfEHZlMoE2m6NuEX+7P4MjBjM4YjBpsWksmLSAI2VHeOG7F3Bqp+Xn84aa4M7OBKrvqbTbzUIyTwUA6QYSPqjDOwCtdZ1Sah6wA1jq+Sb1LgF+AQyLHsaw6GHNtmutyTmWQ0ZBBpnFmR65QI+IHcFloy/jta2v8Ze1fyEq0HSDDIkawhmDLS6O0k1qgjs/E6ixCywqyjODwGDSQgvhY1ztAvoa+LVSKgBonGuotX7bE43qC5RSJEYkkhiRSFVtFVV1Lfu1a+pqeGfnO10KDrOSZlFcWcyanDUcOn6IytpK1h9az9TEqR65C/G0mk5OBW02Eyg6Gg4danvnrpAAIHyQqwHgx/Vfn6z/qjApIVopvio6K8AvoM0KWoMjBpNZnOn2sZVSzB8xn/kj5gOwMnslL216iaNlR32qSH2DTt8BNJ0JFBUF27aZ3D0uppNwWYlr4xJC9CSuBoDfQDdMZREtDIse1qUAcLLB4WbFataxLN8MAEH+5uLt4nqHqtpKaupqcNgdJgBUVZkZOyEW12A4ftwzgUUID3IpAGitH/BwO0QbkiKSWi02766EsATsyk5OSQ6TB0y25JjdymajJigAR7nrK3rLasqItEc2XwtgdQCoqzNlJ7tSuEaIbuZSAFBKfdXK5mLgC631s9Y2STRlt9kZEjWEnfk7LTmen82PAWEDyDrmu9m8a0ICOxcAqsuIDIxsvhZg0CDrG3bsmAQA4VNc7QKa1cb27yulYrXWD1nUHtGK4dHDLQsAYKqWbTmy5cQiKR/jzkwgwLOLwcAEgAEDPHNsITzA1WkgD2MKw6dicgB9APwJeBW41jNNEw36h/Yn1N+6T5aJ4Ykcrz5OSZVvDlx2Nilc40ygiAiw2WQqqBD1XL0DuBn4vdZ6D4BSajlwF3A1cKmH2ibqKaUYFj2MjYc3WnK8xHBTtzi7JNt0jfiYzi4GK68pb8yU2i88lKq8Q5R4IHNqdZad8pTmdQEiAyNJCEuw/FxCWMHVAHAQeFgpdRFmNtAUzMKwGKDAQ20TTaTFpDUbCC6sKGxM+9BZg8JN/3f2sWzGxo+1pH3dqbNdQE7tZHfhbgCCQ/3RRw83/mylqrpc9qfUNdtmV3YuSruIfiH9LD+fEF3lagD4ISb/T8Py0e+A64Bo4DbrmyVOFhEY0WL1bkVNBVklWewq2EVuqeufaIMcQcQFx5Fdkm11M7tFRXQYxxL7EZ6d1+nXlvWLIn7LPmw1tTgdrv7zd42jrKLFtjpdx+d7P2f+iPmE+Fs880iILnJpDEBrvUVrPRGIBCK11pPqt30jq4G9J8gRRFpsGhelXcQlIy8hLcb1Eg2JEYm+OxPIZuNQehrFyf07/dL8tETsNXVE7nfv7qndZtXWYa9sOV23vKacz/Z+Rq2z1vJzCtEVrk4DDcIsBjsbuEUpJSUhe5iY4BhmJs9Eo9lV0HHpw8TwRL7N/ZaKmgrfzBSqFIdPGY622YhouBPQGlttXbsvKxkcT02gP7G7silMtX4qqH9ZJRWB/i2255fn8+7Odwl2SMZQ0bbwgHCmDJqC3dY9SRZcvQd+AvgJJgVE05KQbgUApdRi4CKgGtgL/FhrLekULTB5wGT2Fe3r8NNm40DwsWxSY1K7o2nWU4ojE4ZxZEJ9oj2tSf1wNbaatt+7ttsoHDaAmN0Hwek0s4Is5CiroCImvNXnCisKe12RHmG94spi5gydg7+95QcJq7n6r/9irC0J+QUwRms9DtgF3NOFY4kmQvxDGBc/rsP9BkeYlBC+Og7QKqUoi+t4VlN+aiKOiioisjo/htAR/9KW4wBCdMah44f4cNeHVNR4/t+Sq3cAlpaE1Fp/3uTHNchUUkuNjx/PjqM7qKht+x9QRGAE4QHhZB/rRQEAKI+LJOxQfrv7FA0ZQJ2fndhdOZS4MY7QHv8yDxSdF31Ofnk+r2x5pTFj77j4caQPSLf8PD2hJOT1wOttPamUWgAsABg8eLBFp+zdHHYHkwdOZtmBZe3uNyh8ENuObuOlTS+5fS5/uz8B9gD87f6WrCq2KRtTBk0hIjDCrdeX9ev4DsDp70fRkARidmWz95xJliZwc0gAEBZxamdjKnhPFXTyWElIpdQSTKA42X1a6/fq97kPqAVeaes4WuvngecB0tPTJSOpi9Ji0lh3cF27dwGTB0wm93gu249ud+scWmuqndVU1lZa+g80vzyfq8dd7dZrq8OCqQ30x6+V2ThNFaQmErsrh9DDhZQmxLh1rtZIF5DwJa5mAz3GiZoAACilxgBb23nN2e0dUyl1HXAhcJbWLub2FS5TStE/tD/7i/e3uc/UxKlMTZxqyfnqnO3PvnHVvzf/m7UH13LpqEsJ9Ovcgq8GZf2iiMg60u4+BcMHopUifst+aluZtVMTFEBdK9s7Yq+u8cgaAyE8ocN/pUqpS4AhwFqt9TdKqbGYKaEXufL6No55HnA3MFNrXd7R/sI9CWEJ7QYAK1k1bW1G0gxW56xm7cG1zEia4dYxyuMiOwwAtUEBFCfFM3B9BgPXZ7R43mmzcXTkYA5NSuX4wNhOdRM5yiqpipSsoKLna/cCrpT6M3AL9RXAlFJPYPIC+WNmArnrKcx00i/q+43XaK1/1oXjiVb0D7V2gLM7pESmMChsEMsOLGP64OlujSuUxbk2frDrgtPbDBRhuYXEb9lH/LZMqsKCcfq1DHBVYUFsuWI2+qTn/EsrJAAIn9DRJ/jLMbN0ngbOBO4EMoHbtdYfuHtSrfWwjvcSXRUTFGNpMZnuoJRietJ0/rP1PxwoOUByZHKnj1EbHEh1aFCH/fFVESHkjR3S6nN5Y4ewf9Z44rfuJzz7KCeHIXt1DTG7DxKbkc3R0c3bGLXvEMcHxXW63UJ0t44CQBzwc631q/WDuj8BftmVi7/oPkop4kPifW6q52kDT+OtHW+x7MAytwIAmHGArg7IOv0d5E5MJXdiKwvltObUZ96j/6a9LQJAcH4J4dl5HEuUBHCiZ+toIZgCfq6Ueh8z80cDdyql3ldKvefx1oku88VuoCBHEJMHTGbdoXVuL4Ypj3VvGqnLlOLwuCFEZh4moLjlkph+W/ahOkhLIYS3uTKIO7H+0eD0+q8yc8cH+Gou+hlJM1iZvZJFXy7CruwopZiZNJOLUi9yaVygrF8kNcGdKx3ZWUfGDSFp+Rb6b97HgRnNV1/7VVYTt+NAm11MQvQEHQWAlG5phfCYuOA47MpOnfatT6PJkclcOupS8svNqt6C8gI+2v0Rx6qO8cOxP2xcIdkWp7+DfWdPIjYjm+jdOSin9QtpqiJCKU7pT/zmvRw4Y0yLvEJRew9RnBRPdbikgRY9U0cBoKSjJG1KqUhJ5NZz2W124kLi3C4e403nDDmn8XutNe9lvMcnez6hvKacy0df7tKdQMmwaLL6BzHwfzvwq6yytH1BODg8figj311J5IEjFKc0v9tSTidDlnQ8Wa42KID9Z57i1roDIbqiowBwUCn1JqYG8DrgEGZcYACQDszFJIqTOW89WEJogk8GgKaUUswbMY9gRzBv7XiLDbluzEK2+PoaqO38YkQ6vw7yp//GvS0CgKv8KqoY9L8dHJg+1vLspEK0p6MAcA8mB9CPaNnnr4ADSCbPHs8XB4LbMmfoHJIiktwKaGEHjxJ81Lqb1aW2A/w28H+cNiaC87/Nxq+iitqgztUrbhBUUEL/jXs43NqMIyE8pN0AoLV+EnhSKTUdUw4ysf6pLExBmBUebp+wQHxoPAqF7iXj9mmxaaTFdj4buRpUR8rS7/A/bs3i8wudw3jBvolfTdzOhetg2c7PeXNyFwq+5EBVSZDbQUT0DIF+gQyNGsrw6OEkRiRiV11fJa+1RmttScLFplzNBbQcWG7pmUW38bf7ExUU1eeLkWg/O4fS00j+eiNYkH7KDxs31J3C8thYMuJXccGmcl6d3MWLd1kZNf7SDeTLCssL2Xh4o+XH/eSqTzhv2HmWHtPVkpAvtrK5GFiitf7Y0hYJj4gLjuvzAQCgMiqMoiEDiNp70LJjTteJ+I0dT9qSDbyYeyrlLhSlac+hwSNkEZmPK6ksYU/hHnJLcy25804ITWBYtPUJFFxN5nYdZgyg4f6j4fvblVI3a62fs7xlwlJxIXFkFLRMetYXHR05mPDsPOzVNZYdM290MilffUv8ln3snz2x4xe0I2rvIQkAPi4iMIJJAyZZdrwJ/Sd4JAC4eq/5GLAamAOcW//9M5jSjrdZ3iphubhgyU3TwOnvaJG+oatqQgIpHDaQflv2m1rDXRBUeIzAouMWtUyItrkaAK4B/qO1XqK1/gJ4FZMO+nEg2UNtExaKDorucPFUX1Kc3J9KizN2Hhk7hICySqL25Xb5WFF7D1nQIiHa5+oVoRz4nVLqX0qpfwG/w1TyCqILtYFF97Hb7EQHRXu7GT2HUhwZN9TSQxYOG0B1UAD9N+/r8rHCc45i76CqmRBd5eoYwE8xZRt/VP/z4fptYZjiMMIHxAbHNqZWEFARG0HG3GmNP/tVVhN6uJDQI0UEHy3udPoIbbeTNyaZget3kf6cSZhbG+hg2w9mURPSuepmyukkMvMwBSOkDrbwHFengX6llEoCRtRv2qm1lo8nPiYuOI6d7PR2M3qUpsVcakKDKBo2kKJhAxmwbifh2XmdPt7BU0fgqKhG1Tmx1dURuyuH6D0HOTK+83cbkZmHKUhLtLRovRBNudQFpJRyAPcCf6t/LKrfJnxIXIgMBLvquJuF4qsiQsmYO5Wd889g+yUzqAoLItrN/nxHeSUhR4rceq0QrnC1C+gPwO1Awz1xOhCJSRMhfETDQLBTW58Zs7cpi49C22xdyyKqFIVDBhC3M8vMDHIjz09k5mHK+svYjfAMV/9FXgb8AwgGQoB/YspFCh9iUzZigtz7ZNvXOB1+lhSVKRoyAL+qGsIPujf2Enq4UAaDhce4GgCCgAytdbXWugrYVb9N+BjpBnJdqQWfvItS+qOVcrsbSDmdRB5ovXC9EF3lagBYBjyslFqulFoGPAR87bFWCY+RBWGusyIA1AX6c2xgbJfWBkRm+nYqb9FzuRoAbgFWAdMwWUFXArd6qlHCc+QOwHU1oUFUWVDNq3DoAMIOF+Jws0i9o6zC0jTWQjRodxC4vhh8gxJgSf33lZhUEN/3ULuEh0QGRvpkiUhvKU2IIeBYWZeOUTR0ACnfbCJqf67bNYJjMky9gc4oj42gNrhz6w9E39LRLKAL23mudySX72Nsysb1p1zf4X6FFYUsz1pOXlnn58L3JscToonJyOrSMUrjo6gODiR67yG3A0BIXhEheZ2cEqoUxxNiKBo6gMqIE3cyToefrC0QgBSF75NcKSoRExzD99O+z/aj2/nu8HfUOfvmHYOOjUWHh2OvMplDbbW1bn30KR46kKg9OfiXVqABp5/d8zWAtSbsUD5hh5rPQKoNCuD4gFiODYylIiZcgkEfprQFhTE6fVKlHsJ0HzmBPOA6rXWH0yTS09P1+vXrPd08Idq2di1sdKPYx7p18Pe/N/6olWL/mRPIOX2UhY3rPKfDj4rocMpjI6h2IV1FeWyEFK/3ggn9J3DqwFPdfr1SaoPWOv3k7a4uBLPaYq31/wEopW4Dfg38zEttEcJ1aWnuBYCJE+G663BWVZJfno9tx06GfPUdfpXVZM4c77VP4baaWkKOFBJyxLViQQWpiRwdIx0DvYVXAoDW+liTH0OQ8QThKyIioH9/ONzJqZl2O0yZgg3oB+TPyKPs9bcZvOo7YnQQ5RPHAaAdDmoHxPfYbpkhx/0pGXC6eT+i28QGx3rkuN66A0Ap9TCmzkAJcGY7+y0AFgAMHiyZEUUPkJbW+QBwktjQfnD9DRD+JiFLlhCyuknX5gUXwNy5XWyk5ww6Hgipqd5uhrCAx8YAlFJLgP6tPHWf1vq9JvvdAwRqre/v6JgyBiB6hJoa+Pe/oba268fSGjIzoaJ+jcCKFfDdd3D33ZDSQ7ta+vWDefO83QrRCd0+BqC1PtvFXV8BPgY6DABC9AgOBwwZArt2df1YSjW/0KekwP798OKL8H//B/49cMA1Lw+OHoU4WVTo67xSI1ApNbzJj98HSVIvfExammeOGxQE111nLrJvveWZc1hh2zZvt0BYwFtFYh9RSm1VSm3GFJq/3UvtEMI9CQkQEwMBAebhRqrnNqWlwVlnwddfuzfjqDvs3QuVld5uhegir6wDcJeMAYgeq6AAPvgAqi1K3VxdDY89BocOwe23w/DhHb+muwUEmGA1cqSZHSV6rLbGACQACGGV3Fz4+GOos2jV9PHjsHgxHDsGd90FiYnWHNcTIiNP3AWlpcHYsd5tj2imrQDgrS4gIXqfhAQ4+2zr5vCHhcEdd0BgIDz5JCxbBhkZUFRkZg/1JMXFUFhoHhs2SPeQj5A7ACGsVlgIVfWZO51OM8WzrMx8bSgxWVMD+/aZrx3JzYUnnjAX2QbJyXDuuTBhgrXjD1YZOxamTPF2K0Q96QISoqeprobt282j4ROz03kiSDTldJoAcOQI5OSYAeL8fLMqOTm5c+dNTobp08HPg+tAbTa44goIDfXcOYTLJAAI4QucTrMwbOvW9lcb19XBt9/CV1+ZMQJX1dWZLqT4ePjBDzzbV5+aCrNmee74wmUSAITwNQUFpvsnP9988i8p6foxtTbB5b//NcecMgWuvdZzuYcuvPDEXYCfHwQHe+Y8ol09LRuoEKIjMTHmAeaT+3vvmWDQFUqZT/0jR8L778Nnn0FSEpzZZjqurvnww+Y/BwaaFcRRUe6PXQQEwLhxPTZhni+RACCEL7DbzQyjt9+2Zq2Bn5/J53PokLkbSEnp/FiCOyorITvbPLqitBSmTbOmTX1YD5w+IIRoVXi4tX3qNhv8+MdmEdfzz5uZSr5i27aeu0rah8gYgBC+ZvVq2LLFuuPt328WnAUHm+6V9ihl7h7sdhg0CK65xru1AWbMMO0A046gIO+1pQeTMQAheotTTzVTQYs6WSS+LSkpsGCBmVXUEa3NeERlJaxZY/rzL7zQmna4Y9myE9/bbHDxxRAd7b32+BgJAEL4GrvddAW9+651K4InTDCPznjhBfjoIxg/vmekqXA6zfqIefN65uK4Hkh+S0L4org4c+H1pssvN1M8//lPa4rjWCE/HzZt8nYrfIYEACF81aRJJgmbt4SGwtVXm+6o//4Xduwwj4MHvdcmMLmIrOoe6+WkC0gIX2W3w8yZZn2At4wfbxaTff21eYDpfnn0UTNryRucTvjyS7O+oSMxMab7yuHwfLt6IAkAQviy+PgTZSS95ZprzGwcp9NUMvvXv2D3bnOH4i0NmUld0TCjafLkPjeALF1AQvi6yZO9uyrWZjM1kocNg9NOM1NJraiX3F3q6uDAATOo7kvttoAEACF8XWSk52oUd5bdboKBL15Ia2tNN9ayZdYV9enhJAAI0RtMmuTdBVlNpaaaFBOlpd5uiXt27jTjKr7a/k6QACBEbxASAmPGeLsVRmqq+bpnj3fb0RX5+SbvUk6Ot1viUTIILERvMWnSieLxWsPy5WZQtrslJZlZNbt2dX5xWU9SWWlqPF98McTGers1HiF3AEL0Fn5+ZhZLdLSZ3njeed5ZJ+Bw+O44QGt68cIyCQBC9FaBgXDBBd4pyzh8uOk+KS/v/nNbbd8+H5aSZAAADpFJREFUOH7c263wCAkAQvRmISFw/vndf97UVNMN5cvjAA0aqqj1Ql4NAEqpu5RSWinVOzvYhOgJIiO7f4FTSorpkuot3UA7dkBVlbdbYTmvBQClVCIwB8jyVhuE6DMGDOje8/n7mwpju3d373k9pbbWBIFexpuzgP4E3A14MZGJEH3EgAHd340xfDh8+ince2/3nM9mM7UJTj/dM8ffutXUU+4p6y0s4JUAoJT6PnBQa71JdbCEXSm1AFgAMHjw4G5onRC9UEJC95/zjDPg2LHuW1V78CC89JJJlT10qPXHLy831djOOMP6Y3uJx0pCKqWWAP1beeo+4F5gjta6RCmVCaRrrfM7OqaUhBSiC95+2yxw6q3KyuD3v4fqanPX4akpsDNmwIgRnjm2h7RVEtJjYwBa67O11mNOfgD7gBRgU/3FfxDwrVKqtWAhhLBKd48DdLeQELjxRrOA669/NQGhsrL9hzt3JytWeGeBnQd4vSi83AEI0U2yskyffG+3fj387W+u7auUWScRFmaymDYIDzeD2CkppjvJ37/564KDTS2G4GBTiL7pa7tCKY+Us5Si8EL0dQkJ5gLj5Q99Hpeebi7Ihw+3v5/W5i7g+HEzVlFdfWL74cMnVgAnJMA99zS/yJeXwyefWN92pcwq7n79zNeGAefoaI+ko/B6ANBaJ3u7DUL0CQ6HGSDtJd0X7Ro71jy6oqwMNm82BW7efBOuusqatrVHazNOc/JYzYQJHgkAshJYiL6kt48DWCkkxJS7POccUyNg40Zvt8hyEgCE6EsGDvR2C3zP3LmmbvBLL0FxsbdbYymvdwEJIbpRfLwZvHRXdTUUFZl6uwUFfaNylsMBP/0p/Pa38PTTcOmlJteRN8twWkQCgBB9iZ+fdeUjKytNH/m2bVBTY80xe6r+/eH66+E//4HHHzf1j+fNO1F/wUdJABBCuCcwEE49FcaPN0XVnU6zvbTUpE1omFXTW0ycaKqurVwJn30GTzwBixaZ7iEfJWMAQoiuCQgwXSIjRphHejr88IfmghkUZObQ+/t7ZH57t/P3hzPPhPvuM+sH/vY3cyfko+QOQAhhPX9/EwjSm6w9KiyE99/vHXcGYWGmS+hPf4LXXoPrrvN2i9wiAUAI0T2io+Hcc02d3d4weJyWZortfPSRmV2VkuLecRwOGDTIK1lGJQAIIbpPQgLMng1ffOHtlljjggtM0Zs33+zacQIDTUAZPrxl2gkwZSn79weLMyJLABBCdK+UFLjoos7VC66rMwPNTQebewK7HW6/3ZS+dDfFRnk5ZGTA9u3tF6CfNUsCgBCiF3CnPkFqqhlw3bevewdet25t/3wOB4wc2bVzNIyVlJa23j02ZowZfLaYBAAhhO8IDIRRo7r3nLW13ZcGIjS09e0xMdZlHG2iF8zLEkIID+rqp/seTAKAEEK0JyzM8r73nkICgBBCdGT0aG+3wCMkAAghREcGDTJVwnoZCQBCCNERpbp/8LkbyCwgIYRwRVoa7N1rfUnNqipTltILJAAIIYQrAgJg/nzPHDsvz6wo3rOnW3MlSQAQQghv69fPPE4/HXbsMHUWyso8floJAEII0VP4+Zli9qNGmdQQ//ufR1NfyCCwEEL0NHa7CQTnn++RFcANJAAIIURPNWCAGXeIjPTI4aULSAgherLwcI+tQZA7ACGE6KO8EgCUUg8opQ4qpTbWP873RjuEEKIv82YX0J+01o958fxCCNGnSReQEEL0Ud4MALcopTYrpV5USkW1tZNSaoFSar1Sav3Ro0e7s31CCNGrKW11XouGAyu1BOjfylP3AWuAfEADDwEJWuvrOzpmenq6Xr9+vaXtFEKI3k4ptUFrnX7ydo+NAWitz3ZlP6XU34APPdUOIYQQrfPWLKCmFaHnA1u90Q4hhOjLvDUL6A9KqQmYLqBM4AYvtUMIIfosj40BeIJS6ihwwM2Xx2LGHfoSec99g7znvqEr7zlJax138kafCgBdoZRa39ogSG8m77lvkPfcN3jiPcs6ACGE6KMkAAghRB/VlwLA895ugBfIe+4b5D33DZa/5z4zBiCEEKK5vnQHIIQQogkJAEII0Uf1iQCglDpPKZWhlNqjlFrk7fZYTSmVqJRaqpTarpTappS6vX57tFLqC6XU7vqvbSbd81VKKbtS6jul1If1P6copf5X/7d+XSnl7+02WkkpFamUelMptVMptUMpNaW3/52VUnfW/7veqpT6j1IqsLf9neuTYuYppbY22dbq31UZT9a/981KqYnunrfXBwCllB14GvgeMAq4Uik1yrutslwtcJfWehRwOnBz/XtcBHyptR4OfFn/c29zO7Cjyc+PYmpNDAOKgJ94pVWe82fgU631CGA85r332r+zUmogcBuQrrUeA9iBK+h9f+d/AuedtK2tv+v3gOH1jwXAs+6etNcHAOBUYI/Wep/Wuhp4Dfi+l9tkKa11rtb62/rvj2MuCgMx7/Nf9bv9C5jnnRZ6hlJqEHAB8Pf6nxUwG3izfpde9Z6VUhHADOAFAK11tda6mF7+d8akrAlSSvkBwUAuvezvrLVeBhSetLmtv+v3gZe0sQaIPCm/msv6QgAYCGQ3+TmnfluvpJRKBk4B/gfE6//f3pmHXFFFAfx3KFPSP1KzQKy0lIigxQK1lMw2MOuPklSkNCP7r8KgTSKlPS2koCKyLDNbLMq+NtAyW23RMLOyJHPLrcywpFxOf5w7Nt9889689733fDFzfnD55s6979xz7tG7zcy9qr+EpE3AkU1Sq1HMAG4E9oV4d+B3Vd0T4nnzdR9gK/BUWPZ6QkQ6k2M/q+oGYDqwFmv4dwBfkm8/R5Tya93atCJ0AIVBRLoALwPXq+of8TS1931z886viIwAtqjql83W5QByMNAfeFRVTwX+JLHck0M/d8VGvH2AnkBn2i6V5J5G+bUIHcAG4KhYvFe4lytEpAPW+M9R1VfC7c3R1DD83dIs/RrAmcDFIrIGW9Ybhq2PHxaWCiB/vl4PrFfVJSE+D+sQ8uznc4GfVHWrqu4GXsF8n2c/R5Tya93atCJ0AJ8D/cJbA4dgD5DmN1mnuhLWvmcC36rqg7Gk+cC4cD0OeO1A69YoVPUWVe2lqr0xn76rqmOB94CRIVvebN4ErBOR48Otc4CV5NjP2NLPQBE5NPw7j2zOrZ9jlPLrfOCK8DbQQGBHbKmoOlQ19wEYDqwCVgOTm61PA+wbjE0PlwNfhTAcWxNfCPwALAC6NVvXBtk/FGgJ18cCnwE/Ai8BHZutX51tPQX4Ivj6VaBr3v0MTAW+ww6Omg10zJufgbnYM47d2EzvqlJ+BQR7s3E18DX2hlS7yvWtIBzHcQpKEZaAHMdxnBS8A3Acxyko3gE4juMUFO8AHMdxCop3AI7jOAXFOwDHcZyC4h2A4zhOQfEOoKCEPdU3ish9ItJbRDQWfhOR50WkeztlHyoiU0RkfJk8UZktFcjbnzdNdqWykvmq0aGEvFa61CovJre7iOwSketLpJetj3pRS123o6xzRGR2PWU6FdDsL+A8NCdgXxoq0BfoHa6XAmOwPYUUmNlO2YeH3y8qk6cztoXDkArk7c+bJrtSWTE7W6rVoRI7a5WXkP0ssIZwbnc19VFlOQdX48d62pgoaxIwqZ4yPVRQ781WwEOTHG+fmK8M18mG8YQQXxHiV2Ofo/+JfX4/ONw/IsjZCfyBbUHdIzRcGgtTUspPlhnFPwbeCvKewz573583TXYivQewLOi0E/gAODGjzBZgfEKuhnvl5CV1mRWXn1F3Je0N6aNC+qBydVeqroEJwPeh3I+B/inlLgA2l7Ixq65rtTFh09PA2dg2D7OAu9Pyeahv8CWgAhJOSRuIbZQXp4OI9OC/gyfWisgw4HFsH/pJwNHA/LA8NBbbhfMB4AZsD6KDgFvD77/FZhTzwnLC4SF0KaPeAGAx1niNwfY5itNGdiJ9H7Zj5HXAvdipWTPKlBfxfpB3BbAN+AfbZ6WcvKQu0+MCM+ouy97IN0My9E6r66HY5oBrgDuxPWVeF5FOsd8NwvbVv62MjVl1XauNcU7Cdrt8B1igqrdq6BmcBtLsHsjDgQ/YwRIK3BPivWk7+l2PbTw2PcTPC3nvCvELgRHh+kOs4RgW8qQtHUyh9Ug5KrPNDCDEbw7xy2k94k2THU/vCXyENWpReZuS+dLi4d6T4d7YEC8nL7kElJRfru5K2hvinUL8kRT/ZdXHtBR/KrZ1dPTbpbH8qTZm1XWtNsZkdsAOellOyozHQ+OCzwCKjSTiS7D91/sDx6nqV7E0TfxFVVuwmcTb2KhuoYicG88T4xngvBDuL6NTdCxedNrTQYn0rFHhtcAZ2Aj2fKwj61T2FwERmQxcCdyuqnMqkFfpCLVN3cUoZW/SN1my07iB/+r8AuCnWNrG2HUpG6sZgbfHxogTsBnPHmBvFWU6NeIdQDHZBuzCRn6t7qvqQlVdpqp/h3tvhr9TReQa7OHxduBTERmJzQLWAd+EfD2xtd59QF8RGSsix6idybwghJU16N5Gdol8XbHzc3tVIlRELgLuwNayV4nIaBHpkyGvlS5AUpeSdVeBSpFvfs7Il1Yfb4S0MdiSzADgIVXdniEraWMldV2LjREnY88JRmPHXebmSMv/O94BFBBV3Qt8ApxeQd53gYnYA98HsdHhxar6K/AXcCnwGHAZ8AIwT+3kpmnAYdjbLFnr2NXoniX7YWw0OQo7J3VFhaJPw0bd/bC92ecCZ5WTl6VLRt1lEflmcblMaTqo6iJsJtMF2zd+ItbAliLVxkr8WKONESdjLxysAm4CXgwn3DkNxs8DKCgiMgF7UNhPVX9stj5Oa0TkWWxZrY/6f1KnQfgMoLjMwU4gurrZijitEZFuwCXADG/8nUbiMwDHcZyC4jMAx3GcguIdgOM4TkHxDsBxHKegeAfgOI5TULwDcBzHKSjeATiO4xQU7wAcx3EKyr89k3cSV/DMhAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5dkR2Id2oiu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f82b5e8-da83-4f90-fa33-237694435bb8"
      },
      "source": [
        "time_lose, time_win"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3568.070971727371, 23300.253478765488)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 211
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNLBF57p2opi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}