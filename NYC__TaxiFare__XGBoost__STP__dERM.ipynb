{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NYC__TaxiFare__XGBoost__STP__dERM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9wHutsqZUcn"
      },
      "source": [
        "XGBoost Regression - 'real-world' example: NYC Taxi-Fare Predictor\n",
        "\n",
        "GP dEI versus STP nu = 3 dERM (winner)\n",
        "\n",
        "https://www.kaggle.com/c/new-york-city-taxi-fare-prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7PwmXsgZO8D",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "adc7b01e-0507-4ee5-de12-e50c73b35423"
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-d7872e85-3aa2-4207-a117-4c9318f062a1\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-d7872e85-3aa2-4207-a117-4c9318f062a1\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"conorc2006\",\"key\":\"c5c5a6382a7d50c022aab991694fc17f\"}'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMwbJ6hjZltI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36ecb80f-2562-4583-aa4a-4ce716dce45b"
      },
      "source": [
        "## Ensure the kaggle.json file is present:\n",
        "!ls -lha kaggle.json"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-rw-r--r-- 1 root root 66 Feb 18 06:47 kaggle.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8Pu-UlWZovH"
      },
      "source": [
        "## Next, install the Kaggle API client:\n",
        "!pip install -q kaggle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUOQ4SE7Zuj3"
      },
      "source": [
        "## The Kaggle API Client expects this file to be ~/.kaggle\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJcEztjCZxOn"
      },
      "source": [
        "## Permissions' change\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-u4Tmj7ZUD3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84bfad84-4e63-4198-f855-c5ccba90359d"
      },
      "source": [
        "!kaggle competitions download -c new-york-city-taxi-fare-prediction"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.10 / client 1.5.4)\n",
            "Downloading GCP-Coupons-Instructions.rtf to /content\n",
            "  0% 0.00/486 [00:00<?, ?B/s]\n",
            "100% 486/486 [00:00<00:00, 828kB/s]\n",
            "Downloading train.csv.zip to /content\n",
            "100% 1.56G/1.56G [00:27<00:00, 64.9MB/s]\n",
            "100% 1.56G/1.56G [00:27<00:00, 61.0MB/s]\n",
            "Downloading sample_submission.csv to /content\n",
            "  0% 0.00/335k [00:00<?, ?B/s]\n",
            "100% 335k/335k [00:00<00:00, 109MB/s]\n",
            "Downloading test.csv to /content\n",
            "  0% 0.00/960k [00:00<?, ?B/s]\n",
            "100% 960k/960k [00:00<00:00, 129MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-0Pe1i4Z2R_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "120fab8c-eda4-44da-f044-339f08c911cd"
      },
      "source": [
        "!pip install pyGPGO"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyGPGO\n",
            "  Downloading https://files.pythonhosted.org/packages/8d/87/a113c91ba014708114f7635d5c0f6a5e5c773480c5f0a537b257a02d180d/pyGPGO-0.4.0.dev1.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pyGPGO) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from pyGPGO) (1.4.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from pyGPGO) (1.0.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from pyGPGO) (0.22.2.post1)\n",
            "Requirement already satisfied: theano in /usr/local/lib/python3.6/dist-packages (from pyGPGO) (1.0.5)\n",
            "Requirement already satisfied: pyMC3 in /usr/local/lib/python3.6/dist-packages (from pyGPGO) (3.7)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from theano->pyGPGO) (1.15.0)\n",
            "Requirement already satisfied: tqdm>=4.8.4 in /usr/local/lib/python3.6/dist-packages (from pyMC3->pyGPGO) (4.41.1)\n",
            "Requirement already satisfied: h5py>=2.7.0 in /usr/local/lib/python3.6/dist-packages (from pyMC3->pyGPGO) (2.10.0)\n",
            "Requirement already satisfied: patsy>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from pyMC3->pyGPGO) (0.5.1)\n",
            "Requirement already satisfied: pandas>=0.18.0 in /usr/local/lib/python3.6/dist-packages (from pyMC3->pyGPGO) (1.1.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.18.0->pyMC3->pyGPGO) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.18.0->pyMC3->pyGPGO) (2.8.1)\n",
            "Building wheels for collected packages: pyGPGO\n",
            "  Building wheel for pyGPGO (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyGPGO: filename=pyGPGO-0.4.0.dev1-cp36-none-any.whl size=19867 sha256=45ed7b07bbf53875b5cf3ae36f658bce790fe16d1ba5e17008a700f0ad3b82d4\n",
            "  Stored in directory: /root/.cache/pip/wheels/d1/27/04/c4fa3bfe194d36e3cd51542132f43415a6813114a5e8301acb\n",
            "Successfully built pyGPGO\n",
            "Installing collected packages: pyGPGO\n",
            "Successfully installed pyGPGO-0.4.0.dev1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7zDTf1naBsH"
      },
      "source": [
        "# Load some default Python modules:\n",
        "\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "import pandas as pd\n",
        "import pymc3 as pm\n",
        "import matplotlib.pyplot as plt\n",
        "import xgboost as xgb\n",
        "import time\n",
        "\n",
        "from matplotlib.pyplot import rc\n",
        "rc('font',**{'family':'sans-serif','sans-serif':['Helvetica']})\n",
        "rc('text', usetex=False)\n",
        "### % matplotlib inline\n",
        "import seaborn as sns\n",
        "plt.style.use('seaborn-whitegrid')\n",
        "\n",
        "from collections import OrderedDict\n",
        "from joblib import Parallel, delayed\n",
        "from numpy.linalg import slogdet, inv, cholesky, solve\n",
        "from scipy.optimize import minimize\n",
        "from scipy.spatial.distance import cdist\n",
        "from scipy.special import gamma\n",
        "from scipy.stats import norm, t\n",
        "from joblib import Parallel, delayed\n",
        "\n",
        "from pyGPGO.logger import EventLogger\n",
        "from pyGPGO.GPGO import GPGO\n",
        "from pyGPGO.surrogates.GaussianProcess import GaussianProcess\n",
        "from pyGPGO.surrogates.tStudentProcess import tStudentProcess\n",
        "from pyGPGO.surrogates.tStudentProcess import logpdf\n",
        "from pyGPGO.acquisition import Acquisition\n",
        "from pyGPGO.covfunc import squaredExponential\n",
        "from sklearn.model_selection import cross_val_score, train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from xgboost import XGBRegressor\n",
        "from pandas_datareader import data\n",
        "\n",
        "import warnings\n",
        "import random\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXicekJhaE0P"
      },
      "source": [
        "# Read data in pandas dataframe:\n",
        "\n",
        "df_train =  pd.read_csv('/content/train.csv.zip', nrows = 1_000_000, parse_dates=[\"pickup_datetime\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQ0mDzt_cBmw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "15c23b2c-4bc9-41c6-cbd7-b32540bad82c"
      },
      "source": [
        "# List first rows:\n",
        "\n",
        "df_train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>key</th>\n",
              "      <th>fare_amount</th>\n",
              "      <th>pickup_datetime</th>\n",
              "      <th>pickup_longitude</th>\n",
              "      <th>pickup_latitude</th>\n",
              "      <th>dropoff_longitude</th>\n",
              "      <th>dropoff_latitude</th>\n",
              "      <th>passenger_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2009-06-15 17:26:21.0000001</td>\n",
              "      <td>4.5</td>\n",
              "      <td>2009-06-15 17:26:21+00:00</td>\n",
              "      <td>-73.844311</td>\n",
              "      <td>40.721319</td>\n",
              "      <td>-73.841610</td>\n",
              "      <td>40.712278</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2010-01-05 16:52:16.0000002</td>\n",
              "      <td>16.9</td>\n",
              "      <td>2010-01-05 16:52:16+00:00</td>\n",
              "      <td>-74.016048</td>\n",
              "      <td>40.711303</td>\n",
              "      <td>-73.979268</td>\n",
              "      <td>40.782004</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2011-08-18 00:35:00.00000049</td>\n",
              "      <td>5.7</td>\n",
              "      <td>2011-08-18 00:35:00+00:00</td>\n",
              "      <td>-73.982738</td>\n",
              "      <td>40.761270</td>\n",
              "      <td>-73.991242</td>\n",
              "      <td>40.750562</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2012-04-21 04:30:42.0000001</td>\n",
              "      <td>7.7</td>\n",
              "      <td>2012-04-21 04:30:42+00:00</td>\n",
              "      <td>-73.987130</td>\n",
              "      <td>40.733143</td>\n",
              "      <td>-73.991567</td>\n",
              "      <td>40.758092</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2010-03-09 07:51:00.000000135</td>\n",
              "      <td>5.3</td>\n",
              "      <td>2010-03-09 07:51:00+00:00</td>\n",
              "      <td>-73.968095</td>\n",
              "      <td>40.768008</td>\n",
              "      <td>-73.956655</td>\n",
              "      <td>40.783762</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                             key  ...  passenger_count\n",
              "0    2009-06-15 17:26:21.0000001  ...                1\n",
              "1    2010-01-05 16:52:16.0000002  ...                1\n",
              "2   2011-08-18 00:35:00.00000049  ...                2\n",
              "3    2012-04-21 04:30:42.0000001  ...                1\n",
              "4  2010-03-09 07:51:00.000000135  ...                1\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9fZujMycFMo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52b636f5-4702-4fb4-c6c3-e8e9ada7a232"
      },
      "source": [
        "# Format 'pickup_datetime' variable:\n",
        "\n",
        "df_train['pickup_datetime'] =  pd.to_datetime(df_train['pickup_datetime'], utc=True, format='%Y-%m-%d %H:%M')\n",
        "df_train['pickup_datetime'].head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0   2009-06-15 17:26:21+00:00\n",
              "1   2010-01-05 16:52:16+00:00\n",
              "2   2011-08-18 00:35:00+00:00\n",
              "3   2012-04-21 04:30:42+00:00\n",
              "4   2010-03-09 07:51:00+00:00\n",
              "Name: pickup_datetime, dtype: datetime64[ns, UTC]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nReKu62HcVFI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "5235ecaf-fdd8-4772-9db6-228c16970ca3"
      },
      "source": [
        "df_train.sort_values(by = 'pickup_datetime').tail() ### June 2015 the final month"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>key</th>\n",
              "      <th>fare_amount</th>\n",
              "      <th>pickup_datetime</th>\n",
              "      <th>pickup_longitude</th>\n",
              "      <th>pickup_latitude</th>\n",
              "      <th>dropoff_longitude</th>\n",
              "      <th>dropoff_latitude</th>\n",
              "      <th>passenger_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>286276</th>\n",
              "      <td>2015-06-30 23:38:21.0000003</td>\n",
              "      <td>26.5</td>\n",
              "      <td>2015-06-30 23:38:21+00:00</td>\n",
              "      <td>-74.008385</td>\n",
              "      <td>40.711571</td>\n",
              "      <td>-73.884071</td>\n",
              "      <td>40.737385</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>955575</th>\n",
              "      <td>2015-06-30 23:45:57.0000003</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2015-06-30 23:45:57+00:00</td>\n",
              "      <td>-74.002342</td>\n",
              "      <td>40.739819</td>\n",
              "      <td>-74.005829</td>\n",
              "      <td>40.745239</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>915826</th>\n",
              "      <td>2015-06-30 23:48:35.0000005</td>\n",
              "      <td>30.5</td>\n",
              "      <td>2015-06-30 23:48:35+00:00</td>\n",
              "      <td>-73.983826</td>\n",
              "      <td>40.729546</td>\n",
              "      <td>-73.927917</td>\n",
              "      <td>40.661186</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>751350</th>\n",
              "      <td>2015-06-30 23:53:23.0000002</td>\n",
              "      <td>3.5</td>\n",
              "      <td>2015-06-30 23:53:23+00:00</td>\n",
              "      <td>-73.978020</td>\n",
              "      <td>40.757439</td>\n",
              "      <td>-73.980705</td>\n",
              "      <td>40.753544</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>785182</th>\n",
              "      <td>2015-06-30 23:53:49.0000003</td>\n",
              "      <td>7.5</td>\n",
              "      <td>2015-06-30 23:53:49+00:00</td>\n",
              "      <td>-73.959969</td>\n",
              "      <td>40.762405</td>\n",
              "      <td>-73.953064</td>\n",
              "      <td>40.782688</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                key  ...  passenger_count\n",
              "286276  2015-06-30 23:38:21.0000003  ...                5\n",
              "955575  2015-06-30 23:45:57.0000003  ...                1\n",
              "915826  2015-06-30 23:48:35.0000005  ...                2\n",
              "751350  2015-06-30 23:53:23.0000002  ...                1\n",
              "785182  2015-06-30 23:53:49.0000003  ...                1\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W9j9LnIfcXcX"
      },
      "source": [
        "# Add time variables:\n",
        "\n",
        "df_train['hour'] = df_train['pickup_datetime'].dt.hour\n",
        "df_train['weekday'] = df_train['pickup_datetime'].dt.weekday\n",
        "df_train['month'] = df_train['pickup_datetime'].dt.month\n",
        "df_train['year'] = df_train['pickup_datetime'].dt.year"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVyFZIVIcaj3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "e3412b77-1831-416e-abf1-dcabd16f89a9"
      },
      "source": [
        "df_train = df_train.drop(['pickup_datetime','key'], axis = 1)\n",
        "df_train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>fare_amount</th>\n",
              "      <th>pickup_longitude</th>\n",
              "      <th>pickup_latitude</th>\n",
              "      <th>dropoff_longitude</th>\n",
              "      <th>dropoff_latitude</th>\n",
              "      <th>passenger_count</th>\n",
              "      <th>hour</th>\n",
              "      <th>weekday</th>\n",
              "      <th>month</th>\n",
              "      <th>year</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4.5</td>\n",
              "      <td>-73.844311</td>\n",
              "      <td>40.721319</td>\n",
              "      <td>-73.841610</td>\n",
              "      <td>40.712278</td>\n",
              "      <td>1</td>\n",
              "      <td>17</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>2009</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>16.9</td>\n",
              "      <td>-74.016048</td>\n",
              "      <td>40.711303</td>\n",
              "      <td>-73.979268</td>\n",
              "      <td>40.782004</td>\n",
              "      <td>1</td>\n",
              "      <td>16</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2010</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5.7</td>\n",
              "      <td>-73.982738</td>\n",
              "      <td>40.761270</td>\n",
              "      <td>-73.991242</td>\n",
              "      <td>40.750562</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>2011</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>7.7</td>\n",
              "      <td>-73.987130</td>\n",
              "      <td>40.733143</td>\n",
              "      <td>-73.991567</td>\n",
              "      <td>40.758092</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>2012</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5.3</td>\n",
              "      <td>-73.968095</td>\n",
              "      <td>40.768008</td>\n",
              "      <td>-73.956655</td>\n",
              "      <td>40.783762</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2010</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   fare_amount  pickup_longitude  pickup_latitude  ...  weekday  month  year\n",
              "0          4.5        -73.844311        40.721319  ...        0      6  2009\n",
              "1         16.9        -74.016048        40.711303  ...        1      1  2010\n",
              "2          5.7        -73.982738        40.761270  ...        3      8  2011\n",
              "3          7.7        -73.987130        40.733143  ...        5      4  2012\n",
              "4          5.3        -73.968095        40.768008  ...        1      3  2010\n",
              "\n",
              "[5 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVfm-KSqcdVY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f867d56-b053-4540-d164-3578002da941"
      },
      "source": [
        "# Remove negative fares and postive outliers:\n",
        "\n",
        "df_train = df_train[df_train.fare_amount>=0]\n",
        "df_train = df_train[df_train.fare_amount<=60]\n",
        "print('New size: %d' % len(df_train))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "New size: 997297\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTVDAD2KchTv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffc56366-e590-446a-86e6-c27cbeed7683"
      },
      "source": [
        "# Remove missing data:\n",
        "\n",
        "df_train = df_train.dropna(how = 'any', axis = 'rows')\n",
        "print('New size: %d' % len(df_train))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "New size: 997288\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUYksJ2cclVQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eeec3c59-3441-4b26-c472-ac5fda64c081"
      },
      "source": [
        "# June 2015 NYC taxi data (Wu et al, 2017):\n",
        "\n",
        "df_train = df_train[df_train.month==6]\n",
        "df_train = df_train[df_train.year==2015]\n",
        "print('New size: %d' % len(df_train))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "New size: 11269\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXgSHPyYcnuv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "outputId": "d27a2fb6-2fad-4f88-9b4e-4a67bf628a03"
      },
      "source": [
        "# Histogram fare plot:\n",
        "\n",
        "df_train[df_train.fare_amount<60].fare_amount.hist(bins=100, figsize=(16,5), color = \"red\")\n",
        "plt.xlabel('$USD')\n",
        "plt.title('NYC Taxi Fares: Dataset');\n",
        "plt.grid(b=None)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1152x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9TMSdAAjcr4o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fd4f7c0-b1ee-492a-f9fe-31dbc5fbba26"
      },
      "source": [
        "y = df_train.fare_amount.values + 1e-10\n",
        "y ### for supervised learning: output vector y"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([22.54,  8.  , 34.  , ...,  4.5 ,  6.5 ,  7.  ])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FOeHvi3cu1n",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "2cdab73d-cd5f-486c-9524-42f09017e5e5"
      },
      "source": [
        "# List first rows (post-cleaning):\n",
        "\n",
        "df_train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>fare_amount</th>\n",
              "      <th>pickup_longitude</th>\n",
              "      <th>pickup_latitude</th>\n",
              "      <th>dropoff_longitude</th>\n",
              "      <th>dropoff_latitude</th>\n",
              "      <th>passenger_count</th>\n",
              "      <th>hour</th>\n",
              "      <th>weekday</th>\n",
              "      <th>month</th>\n",
              "      <th>year</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>22.54</td>\n",
              "      <td>-74.010483</td>\n",
              "      <td>40.717667</td>\n",
              "      <td>-73.985771</td>\n",
              "      <td>40.660366</td>\n",
              "      <td>1</td>\n",
              "      <td>21</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>310</th>\n",
              "      <td>8.00</td>\n",
              "      <td>-74.010727</td>\n",
              "      <td>40.710091</td>\n",
              "      <td>-73.998100</td>\n",
              "      <td>40.722900</td>\n",
              "      <td>1</td>\n",
              "      <td>9</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>314</th>\n",
              "      <td>34.00</td>\n",
              "      <td>-73.974899</td>\n",
              "      <td>40.751095</td>\n",
              "      <td>-73.908546</td>\n",
              "      <td>40.881878</td>\n",
              "      <td>0</td>\n",
              "      <td>23</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>321</th>\n",
              "      <td>8.00</td>\n",
              "      <td>-73.961784</td>\n",
              "      <td>40.759579</td>\n",
              "      <td>-73.978943</td>\n",
              "      <td>40.772606</td>\n",
              "      <td>4</td>\n",
              "      <td>21</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>486</th>\n",
              "      <td>11.50</td>\n",
              "      <td>-73.957443</td>\n",
              "      <td>40.761703</td>\n",
              "      <td>-73.973236</td>\n",
              "      <td>40.787079</td>\n",
              "      <td>1</td>\n",
              "      <td>19</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     fare_amount  pickup_longitude  pickup_latitude  ...  weekday  month  year\n",
              "31         22.54        -74.010483        40.717667  ...        6      6  2015\n",
              "310         8.00        -74.010727        40.710091  ...        5      6  2015\n",
              "314        34.00        -73.974899        40.751095  ...        1      6  2015\n",
              "321         8.00        -73.961784        40.759579  ...        0      6  2015\n",
              "486        11.50        -73.957443        40.761703  ...        0      6  2015\n",
              "\n",
              "[5 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-lT9BBicw4P",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "7e0dd860-be04-4c94-b8cc-58bc010dc237"
      },
      "source": [
        "X = df_train.drop(['fare_amount', 'month', 'year'], axis = 1)\n",
        "X.head() ### for supervised learning: input matrix X"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pickup_longitude</th>\n",
              "      <th>pickup_latitude</th>\n",
              "      <th>dropoff_longitude</th>\n",
              "      <th>dropoff_latitude</th>\n",
              "      <th>passenger_count</th>\n",
              "      <th>hour</th>\n",
              "      <th>weekday</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>-74.010483</td>\n",
              "      <td>40.717667</td>\n",
              "      <td>-73.985771</td>\n",
              "      <td>40.660366</td>\n",
              "      <td>1</td>\n",
              "      <td>21</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>310</th>\n",
              "      <td>-74.010727</td>\n",
              "      <td>40.710091</td>\n",
              "      <td>-73.998100</td>\n",
              "      <td>40.722900</td>\n",
              "      <td>1</td>\n",
              "      <td>9</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>314</th>\n",
              "      <td>-73.974899</td>\n",
              "      <td>40.751095</td>\n",
              "      <td>-73.908546</td>\n",
              "      <td>40.881878</td>\n",
              "      <td>0</td>\n",
              "      <td>23</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>321</th>\n",
              "      <td>-73.961784</td>\n",
              "      <td>40.759579</td>\n",
              "      <td>-73.978943</td>\n",
              "      <td>40.772606</td>\n",
              "      <td>4</td>\n",
              "      <td>21</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>486</th>\n",
              "      <td>-73.957443</td>\n",
              "      <td>40.761703</td>\n",
              "      <td>-73.973236</td>\n",
              "      <td>40.787079</td>\n",
              "      <td>1</td>\n",
              "      <td>19</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     pickup_longitude  pickup_latitude  ...  hour  weekday\n",
              "31         -74.010483        40.717667  ...    21        6\n",
              "310        -74.010727        40.710091  ...     9        5\n",
              "314        -73.974899        40.751095  ...    23        1\n",
              "321        -73.961784        40.759579  ...    21        0\n",
              "486        -73.957443        40.761703  ...    19        0\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2eC8SDPzczNY"
      },
      "source": [
        "### Optimum rmse: regression model objective function is Root Mean Square Error (RMSE); \n",
        "### Should be minimized (as close to zero as possible):\n",
        "\n",
        "y_global_orig = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GoTmWEhSc1qQ"
      },
      "source": [
        "### Bayesian Optimization - inputs:\n",
        "\n",
        "obj_func = 'XGBoost'\n",
        "n_start_AcqFunc = 100\n",
        "n_test = n_start_AcqFunc # test points\n",
        "df = 3 # nu\n",
        "\n",
        "util_loser = 'dEI_GP'\n",
        "util_winner = 'dERM_STP'\n",
        "n_init = 5 # random initialisations\n",
        "\n",
        "test_perc = 0.15\n",
        "train_perc = 1 - test_perc\n",
        "\n",
        "n_test = int(len(df_train) * test_perc)\n",
        "n_train = int(len(df_train) - n_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2ngnRxbc7cg"
      },
      "source": [
        "### Objective function:\n",
        "\n",
        "if obj_func == 'XGBoost': # 6-D\n",
        "            \n",
        "    # Constraints:\n",
        "    param_lb_alpha = 0\n",
        "    param_ub_alpha = 10\n",
        "    \n",
        "    param_lb_gamma = 0\n",
        "    param_ub_gamma = 10\n",
        "    \n",
        "    param_lb_max_depth = 5\n",
        "    param_ub_max_depth = 15\n",
        "    \n",
        "    param_lb_min_child_weight = 1\n",
        "    param_ub_min_child_weight = 20\n",
        "    \n",
        "    param_lb_subsample = .5\n",
        "    param_ub_subsample = 1\n",
        "    \n",
        "    param_lb_colsample = .1\n",
        "    param_ub_colsample = 1\n",
        "    \n",
        "    # 6-D inputs' parameter bounds:\n",
        "    param = { 'alpha':  ('cont', (param_lb_alpha, param_ub_alpha)),\n",
        "         'gamma':  ('cont', (param_lb_gamma, param_ub_gamma)),     \n",
        "         'max_depth':  ('int', (param_lb_max_depth, param_ub_max_depth)),\n",
        "         'subsample':  ('cont', (param_lb_subsample, param_ub_subsample)),\n",
        "          'min_child_weight':  ('int', (param_lb_min_child_weight, param_ub_min_child_weight)),\n",
        "            'colsample': ('cont', (param_lb_colsample, param_ub_colsample))\n",
        "        }\n",
        "       \n",
        "    # True y bounds:\n",
        "    dim = 6\n",
        "    \n",
        "    max_iter = 20  # iterations of Bayesian optimization\n",
        "    \n",
        "    operator = 1 \n",
        "    \n",
        "    n_est = 3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmJsNX29c_xA"
      },
      "source": [
        "### Derivatives - Squared-exponential covariance function:\n",
        "\n",
        "def l2norm_(X, Xstar):\n",
        "    \n",
        "    return cdist(X, Xstar)\n",
        "\n",
        "def kronDelta(X, Xstar):\n",
        "\n",
        "    return cdist(X, Xstar) < np.finfo(np.float32).eps\n",
        "\n",
        "class squaredExponentialDeriv(squaredExponential):\n",
        "    \n",
        "    def K(self, X, Xstar):\n",
        "        \n",
        "        r = (l2norm_(X, Xstar)/self.l)\n",
        "        K = self.sigmaf * np.exp(-1/2*r **2) + self.sigman * kronDelta(X, Xstar)\n",
        "        return K\n",
        "    \n",
        "    def dK(self, X, Xstar):\n",
        "        \n",
        "        r = (l2norm_(X, Xstar)/self.l)\n",
        "        dK = self.sigmaf/self.l**2 * np.exp(-1/2 * r **2) * l2norm_(X, Xstar)\n",
        "        return dK\n",
        "    \n",
        "        \n",
        "    def d2K(self, X, Xstar):\n",
        "        \n",
        "        r = (l2norm_(X, Xstar)/self.l)\n",
        "        d2K = self.sigmaf/self.l**2 * np.exp(-1/2 * r **2) * (1 - r **2)\n",
        "        return d2K\n",
        "    \n",
        "cov_func = squaredExponentialDeriv()\n",
        "d_cov_func = squaredExponentialDeriv()\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9ZuEB2VdE0W"
      },
      "source": [
        "### Set-seeds:\n",
        "\n",
        "run_num_1 = 111\n",
        "run_num_2 = 113\n",
        "run_num_3 = 3333\n",
        "run_num_4 = 4444\n",
        "run_num_5 = 5555\n",
        "run_num_6 = 6\n",
        "run_num_7 = 7777\n",
        "run_num_8 = 8878\n",
        "run_num_9 = 999\n",
        "run_num_10 = 1000\n",
        "run_num_11 = 1113\n",
        "run_num_12 = 1234\n",
        "run_num_13 = 234\n",
        "run_num_14 = 888\n",
        "run_num_15 = 1557\n",
        "run_num_16 = 1666\n",
        "run_num_17 = 71\n",
        "run_num_18 = 8\n",
        "run_num_19 = 1999\n",
        "run_num_20 = 2000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XgHMFEyPdCk4"
      },
      "source": [
        "### Cumulative Regret Calculator:\n",
        "\n",
        "def min_max_array(x):\n",
        "    new_list = []\n",
        "    for i, num in enumerate(x):\n",
        "            new_list.append(np.min(x[0:i+1]))\n",
        "    return new_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJMhL70fdHz_"
      },
      "source": [
        "class Acquisition_new(Acquisition):    \n",
        "    def __init__(self, mode, eps=1e-08, **params):\n",
        "        \n",
        "        self.params = params\n",
        "        self.eps = eps\n",
        "\n",
        "        mode_dict = {\n",
        "            'dEI_GP': self.dEI_GP,\n",
        "            'dERM_STP': self.dERM_STP\n",
        "        }\n",
        "\n",
        "        self.f = mode_dict[mode]\n",
        "    \n",
        "    def dEI_GP(self, tau, mean, std, ds, dm, dvdv, d2v, d2m):\n",
        "        z = -1 * (tau - mean - self.eps) / (std + self.eps)\n",
        "        \n",
        "        dsdx = ds / 2 * (std + self.eps)\n",
        "        d2sdx = -dsdx**2 / ((std + self.eps)) - dvdv / (std + self.eps) - d2v / (std + self.eps)\n",
        "        dmdx = (dm - z * dsdx) / (std + self.eps)\n",
        "        d2mdx = (d2m - (z * d2sdx + 2 * dmdx * dsdx)) / (std + self.eps)\n",
        "        \n",
        "        f = (std + self.eps) * (z * norm.cdf(z) + norm.pdf(z)[0])\n",
        "        df = (f / (std + self.eps) * dsdx + (std + self.eps) * norm.cdf(z) * dmdx)\n",
        "        d2f = (f / (std + self.eps) * d2sdx + dsdx * dmdx * norm.cdf(z) \\\n",
        "            + d2mdx * (std + self.eps) * norm.cdf(z) + dsdx * norm.cdf(z) * dmdx \\\n",
        "            + norm.pdf(z)[0] * (std + self.eps) * dmdx)\n",
        "            \n",
        "        return f, df, d2f\n",
        "\n",
        "    def dERM_STP(self, tau, mean, std, ds, dm, dvdv, d2v, d2m, nu=3.0):\n",
        "    \n",
        "        gamma = -1 * (y_global_orig - mean - self.eps) / (std + self.eps)\n",
        "        dsdx = ds / 2 * (std + self.eps)\n",
        "        dmdx = (dm - gamma * dsdx) / (std + self.eps)\n",
        "        f = (std + self.eps) * (gamma * t.cdf(gamma, df=nu) + (nu + gamma ** 2)/(nu - 1) * t.pdf(gamma, df=nu))\n",
        "        df = (gamma * t.cdf(gamma, df=nu) + (nu + gamma ** 2)/(nu - 1) * t.pdf(gamma,df=nu)[0]) * dsdx \\\n",
        "             + (std + self.eps) * (t.cdf(gamma,df=nu) * dmdx + gamma * t.pdf(gamma, df=nu) * \\\n",
        "             (1 - (nu + gamma ** 2)/(nu - 1) + 2/(nu - 1) * dmdx))\n",
        "        return f, df\n",
        "    \n",
        "    def _eval(self, tau, mean, std):\n",
        "    \n",
        "        return self.f(tau, mean, std, **self.params)\n",
        "    \n",
        "    def d_eval(self, tau, mean, std, ds, dm, dvdv, d2v, d2m):\n",
        "    \n",
        "        return self.f(tau, mean, std, ds, dm, dvdv, d2v, d2m, **self.params)\n",
        "\n",
        "    def d_eval_stp(self, tau, mean, std, ds, dm, dvdv, d2v, d2m, nu=3.0):\n",
        "    \n",
        "        return self.f(tau, mean, std, ds, dm, dvdv, d2v, d2m, nu=3.0, **self.params)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "comHkJv9dH_O"
      },
      "source": [
        "### Surrogate derivatives: \n",
        "\n",
        "from scipy.linalg import cholesky, solve\n",
        "\n",
        "class dGaussianProcess(GaussianProcess):\n",
        "    l = 1\n",
        "    sigmaf = 1\n",
        "    sigman = 1e-6\n",
        "\n",
        "    def AcqGrad(self, Xstar, return_std=False):\n",
        "        r_X = l2norm_(self.X, self.X)/self.l\n",
        "        K = self.sigmaf * np.exp(-1/2*r_X **2) + self.sigman * kronDelta(self.X, self.X)\n",
        "        L = cholesky(K).T\n",
        "        alpha = solve(L.T, solve(L, self.y))\n",
        "        Xstar = np.atleast_2d(Xstar)\n",
        "        Kstar = squaredExponentialDeriv.K(self, self.X, Xstar).T\n",
        "        dKstar = squaredExponentialDeriv.dK(self, self.X, Xstar).T\n",
        "        d2Kstar = squaredExponentialDeriv.d2K(self, self.X, Xstar).T\n",
        "        v = solve(self.L, Kstar.T)\n",
        "        dv = solve(self.L, dKstar.T)\n",
        "        d2v = solve(self.L, d2Kstar.T)\n",
        "        \n",
        "        ds = -2 * np.dot(dv.T, v)\n",
        "        dvdv = np.dot(dv.T, dv)\n",
        "        d2s = -2 * (dvdv + d2v)\n",
        "        \n",
        "        dm = np.dot(dKstar, alpha)\n",
        "        d2m = np.dot(d2Kstar, alpha)\n",
        "        return ds, dm, dvdv, d2v, d2m\n",
        "\n",
        "class dtStudentProcess(tStudentProcess):\n",
        "    l = 1\n",
        "    sigmaf = 1\n",
        "    sigman = 1e-6\n",
        "    \n",
        "    def AcqGrad(self, Xstar, return_std=False):\n",
        "        r_X = l2norm_(self.X, self.X)/self.l\n",
        "        K = self.sigmaf * np.exp(-1/2*r_X **2) + self.sigman * kronDelta(self.X, self.X)\n",
        "        L = cholesky(self.K11).T\n",
        "        alpha = solve(L.T, solve(L, self.y))\n",
        "        Xstar = np.atleast_2d(Xstar)\n",
        "        Kstar = squaredExponentialDeriv.K(self, self.X, Xstar).T\n",
        "        dKstar = squaredExponentialDeriv.dK(self, self.X, Xstar).T\n",
        "        d2Kstar = squaredExponentialDeriv.d2K(self, self.X, Xstar).T\n",
        "        v = solve(L, Kstar.T)\n",
        "        dv = solve(L, dKstar.T)\n",
        "        d2v = solve(L, d2Kstar.T)\n",
        "        \n",
        "        ds = -2 * np.dot(dv.T, v)\n",
        "        dvdv = np.dot(dv.T, dv)\n",
        "        d2s = -2 * (dvdv + d2v)\n",
        "        \n",
        "        dm = np.dot(dKstar, alpha)\n",
        "        d2m = np.dot(d2Kstar, alpha)\n",
        "        return ds, dm, dvdv, d2v, d2m"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S422jNLsdIMm"
      },
      "source": [
        "class dGPGO(GPGO):  \n",
        "    n_start = n_start_AcqFunc\n",
        "    eps = 1e-08\n",
        "        \n",
        "    def func(self, xnew):\n",
        "        new_mean, new_var = self.GP.predict(xnew, return_std=True)\n",
        "        new_std = np.sqrt(new_var + 1e-6)\n",
        "        ds, dm, dvdv, d2v, d2m = self.GP.AcqGrad(xnew, return_std=True)\n",
        "        f  = np.empty((self.n_start,))\n",
        "        df = np.empty((self.n_start,))\n",
        "        f  = -self.A.d_eval(self.tau, new_mean, new_std, ds=ds, dm=dm, dvdv=dvdv, d2v=d2v, d2m=d2m)[0]\n",
        "        df = -self.A.d_eval(self.tau, new_mean, new_std, ds=ds, dm=dm, dvdv=dvdv, d2v=d2v, d2m=d2m)[1]\n",
        "        df_array = np.full((len(xnew),),df)\n",
        "        return f, df_array\n",
        "        \n",
        "    def d_optimizeAcq(self, method='L-BFGS-B', n_start=n_start):\n",
        "        start_points_dict = [self._sampleParam() for i in range(n_start)]\n",
        "        start_points_arr = np.array([list(s.values())\n",
        "                                     for s in start_points_dict])\n",
        "        x_best = np.empty((n_start, len(self.parameter_key)))\n",
        "        f_best = np.empty((n_start,))\n",
        "        opt = Parallel(n_jobs=self.n_jobs)(delayed(minimize)(self.func,\n",
        "                                                                 x0=start_point,\n",
        "                                                                 method=method,\n",
        "                                                                 jac = True,\n",
        "                                                                 bounds=self.parameter_range) for start_point in\n",
        "                                               start_points_arr)\n",
        "        x_best = np.array([res.x for res in opt])\n",
        "        f_best = np.array([np.atleast_1d(res.fun)[0] for res in opt])\n",
        "\n",
        "        self.x_best = x_best\n",
        "        self.f_best = f_best\n",
        "        self.best = x_best[np.argmin(f_best)]\n",
        "    \n",
        "    def run(self, max_iter=10, init_evals=3, resume=False):\n",
        "        \n",
        "        if not resume:\n",
        "            self.init_evals = init_evals\n",
        "            self._firstRun(self.init_evals)\n",
        "            self.logger._printInit(self)\n",
        "        for iteration in range(max_iter):\n",
        "            self.d_optimizeAcq()\n",
        "            self.updateGP()\n",
        "            self.logger._printCurrent(self)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwEwZD0qdIPO"
      },
      "source": [
        "## dGPGO_stp - BayesOpt derivatives' class: Student's-t\n",
        "\n",
        "class dGPGO_stp(GPGO):  \n",
        "    n_start = 100\n",
        "        \n",
        "    def func_stp(self, xnew):\n",
        "        new_mean, new_var = self.GP.predict(xnew, return_std=True)\n",
        "        new_std = np.sqrt(new_var + 1e-6)\n",
        "        ds, dm, dvdv, d2v, d2m = self.GP.AcqGrad(xnew, return_std=True)\n",
        "        f  = np.empty((self.n_start,))\n",
        "        df = np.empty((self.n_start,))\n",
        "        f  = -self.A.d_eval_stp(self.tau, new_mean, new_std, ds=ds, dm=dm, dvdv=dvdv, d2v=d2v, d2m=d2m, nu=df)[0]\n",
        "        df = -self.A.d_eval_stp(self.tau, new_mean, new_std, ds=ds, dm=dm, dvdv=dvdv, d2v=d2v, d2m=d2m, nu=df)[1]\n",
        "        df_array = np.full((len(xnew),),df)\n",
        "        return f, df_array\n",
        "        \n",
        "    def d_optimizeAcq_stp(self, method='L-BFGS-B', n_start=n_start):\n",
        "        start_points_dict = [self._sampleParam() for i in range(n_start)]\n",
        "        start_points_arr = np.array([list(s.values())\n",
        "                                     for s in start_points_dict])\n",
        "        x_best = np.empty((n_start, len(self.parameter_key)))\n",
        "        f_best = np.empty((n_start,))\n",
        "        opt = Parallel(n_jobs=self.n_jobs)(delayed(minimize)(self.func_stp,\n",
        "                                                                 x0=start_point,\n",
        "                                                                 method=method,\n",
        "                                                                 jac = True,\n",
        "                                                                 bounds=self.parameter_range) for start_point in\n",
        "                                               start_points_arr)\n",
        "        x_best = np.array([res.x for res in opt])\n",
        "        f_best = np.array([np.atleast_1d(res.fun)[0] for res in opt])\n",
        "\n",
        "        self.x_best = x_best\n",
        "        self.f_best = f_best\n",
        "        self.best = x_best[np.argmin(f_best)]\n",
        "    \n",
        "    def run(self, max_iter=10, init_evals=3, resume=False):\n",
        "        \n",
        "        if not resume:\n",
        "            self.init_evals = init_evals\n",
        "            self._firstRun(self.init_evals)\n",
        "            self.logger._printInit(self)\n",
        "        for iteration in range(max_iter):\n",
        "            self.d_optimizeAcq_stp()\n",
        "            self.updateGP()\n",
        "            self.logger._printCurrent(self)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HlilveEgdIR_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "019d9e16-47b6-4a0d-87de-1996200eeca2"
      },
      "source": [
        "start_lose = time.time()\n",
        "start_lose"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1613631026.4208772"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wlzDSHbUG-c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ec1f413-603c-4bf8-85f4-d87be99eed9d"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 1\n",
        "\n",
        "np.random.seed(run_num_1)\n",
        "surrogate_loser_1 = dGaussianProcess(cov_func)\n",
        "\n",
        "X_train1, X_test1, y_train1, y_test1 = train_test_split(X, y, test_size=test_perc, random_state=run_num_1)\n",
        "\n",
        "def f_syn_polarity1(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_1, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train1, y=y_train1).mean())\n",
        "    return operator * score\n",
        "\n",
        "loser_1 = dGPGO(surrogate_loser_1, Acquisition_new(util_loser), f_syn_polarity1, param, n_jobs = -1) # define BayesOpt\n",
        "loser_1.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_loser_1 = loser_1.getResult()[0]\n",
        "params_loser_1['max_depth'] = int(params_loser_1['max_depth'])\n",
        "params_loser_1['min_child_weight'] = int(params_loser_1['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_loser_train1 = xgb.DMatrix(X_train1, y_train1)\n",
        "dX_loser_test1 = xgb.DMatrix(X_test1, y_test1)\n",
        "model_loser_1 = xgb.train(params_loser_1, dX_loser_train1)\n",
        "pred_loser_1 = model_loser_1.predict(dX_loser_test1)\n",
        "\n",
        "rmse_loser_1 = np.sqrt(mean_squared_error(pred_loser_1, y_test1))\n",
        "rmse_loser_1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 6.12170176  1.69069754 11.          0.92695323  3.          0.36579277]. \t  -0.5875926438135084 \t -0.45245711231879204\n",
            "init   \t [ 1.49162957  0.22478325  6.          0.85613654 15.          0.19363322]. \t  -0.6811472770543097 \t -0.45245711231879204\n",
            "init   \t [ 5.27957805  2.81819356 11.          0.81062146 14.          0.74826845]. \t  -0.46117277972616416 \t -0.45245711231879204\n",
            "init   \t [3.81060005 6.24236569 7.         0.95038709 5.         0.9861745 ]. \t  -0.45245711231879204 \t -0.45245711231879204\n",
            "init   \t [4.77531134 7.87990376 6.         0.78863691 6.         0.47918609]. \t  -0.5612893767574513 \t -0.45245711231879204\n",
            "1      \t [0.  0.  5.  0.5 1.  0.1]. \t  -0.6826322236836013 \t -0.45245711231879204\n",
            "2      \t [ 9.60774341  8.94642175  5.          0.60295867 15.          0.76784922]. \t  -0.5001571385184761 \t -0.45245711231879204\n",
            "3      \t [ 0.4810761   9.31563532  8.          0.92845546 14.          0.70014982]. \t  -0.5056468669267833 \t -0.45245711231879204\n",
            "4      \t [ 9.85849625  0.83439907  7.          0.7705582  19.          0.15487995]. \t  -0.6815452883308839 \t -0.45245711231879204\n",
            "5      \t [ 9.1348132   9.54667443 13.          0.95557113 18.          0.11016711]. \t  -0.6791252728059957 \t -0.45245711231879204\n",
            "6      \t [ 9.97122318  8.75901621 13.          0.9904996   9.          0.60161775]. \t  -0.4895864123043516 \t -0.45245711231879204\n",
            "7      \t [7.19079936 0.87565629 5.         0.54470564 8.         0.4449021 ]. \t  -0.5736147518216788 \t -0.45245711231879204\n",
            "8      \t [ 5.24989998  8.90398408 13.          0.54165135  1.          0.74771929]. \t  -0.46141851807096074 \t -0.45245711231879204\n",
            "9      \t [ 1.7157918   5.89141512 14.          0.5296881  10.          0.1803042 ]. \t  -0.6825568503758982 \t -0.45245711231879204\n",
            "10     \t [ 1.33491429  4.9488224  14.          0.57825982 18.          0.42075517]. \t  -0.5921600742966667 \t -0.45245711231879204\n",
            "11     \t [8.99095022 4.91206514 6.         0.87212346 1.         0.53198502]. \t  -0.5638457176592562 \t -0.45245711231879204\n",
            "12     \t [0.51753753 1.68511981 9.         0.80360039 9.         0.45200858]. \t  -0.5426731188677099 \t -0.45245711231879204\n",
            "13     \t [ 0.47819106  0.69361363 14.          0.7271902   3.          0.48615024]. \t  -0.5388370338312534 \t -0.45245711231879204\n",
            "14     \t [ 3.39786838  7.15897283  5.          0.7138544  19.          0.22047835]. \t  -0.6821252760743777 \t -0.45245711231879204\n",
            "15     \t [0.4939583  9.37390762 8.         0.7641092  1.         0.4917045 ]. \t  -0.5481894484993145 \t -0.45245711231879204\n",
            "16     \t [ 8.70209222  0.21363429 14.          0.98391211 18.          0.50653374]. \t  -0.5341942124085062 \t -0.45245711231879204\n",
            "17     \t [ 5.30693639  9.58195731  9.          0.53796245 11.          0.46099251]. \t  -0.5549909911648074 \t -0.45245711231879204\n",
            "18     \t [ 3.83524595  9.61883757 12.          0.81445794 17.          0.17700713]. \t  -0.6797728963073484 \t -0.45245711231879204\n",
            "19     \t [ 0.25234021  8.87254669 14.          0.7101496   5.          0.12345742]. \t  -0.684133223230854 \t -0.45245711231879204\n",
            "20     \t [9.81917647 9.19103216 5.         0.82619792 9.         0.10869763]. \t  -0.6822789922076578 \t -0.45245711231879204\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.87018129255363"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ClJ9rN2KUJzy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab556bc5-db86-4675-8cb8-c34934a013c6"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 2\n",
        "\n",
        "np.random.seed(run_num_2)\n",
        "surrogate_loser_2 = dGaussianProcess(cov_func)\n",
        "\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X, y, test_size=test_perc, random_state=run_num_2)\n",
        "\n",
        "def f_syn_polarity2(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_2, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train2, y=y_train2).mean())\n",
        "    return operator * score\n",
        "\n",
        "loser_2 = dGPGO(surrogate_loser_2, Acquisition_new(util_loser), f_syn_polarity2, param, n_jobs = -1) # define BayesOpt\n",
        "loser_2.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_loser_2 = loser_2.getResult()[0]\n",
        "params_loser_2['max_depth'] = int(params_loser_2['max_depth'])\n",
        "params_loser_2['min_child_weight'] = int(params_loser_2['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_loser_train2 = xgb.DMatrix(X_train2, y_train2)\n",
        "dX_loser_test2 = xgb.DMatrix(X_test2, y_test2)\n",
        "model_loser_2 = xgb.train(params_loser_2, dX_loser_train2)\n",
        "pred_loser_2 = model_loser_2.predict(dX_loser_test2)\n",
        "\n",
        "rmse_loser_2 = np.sqrt(mean_squared_error(pred_loser_2, y_test2))\n",
        "rmse_loser_2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 8.51985493  0.73903599  7.          0.71824677 17.          0.61827209]. \t  -0.5233872966482411 \t -0.42848969661986275\n",
            "init   \t [ 8.4047092   4.35120551 12.          0.84231905 16.          0.80172484]. \t  -0.42848969661986275 \t -0.42848969661986275\n",
            "init   \t [ 6.42749368  9.61026168 10.          0.73363448 15.          0.84932207]. \t  -0.43419399571353007 \t -0.42848969661986275\n",
            "init   \t [ 2.66008363  8.36685389  6.          0.92082163 12.          0.69999264]. \t  -0.5353675801921922 \t -0.42848969661986275\n",
            "init   \t [ 1.74779478  2.17691512 12.          0.71794562  2.          0.48990646]. \t  -0.5346620615324694 \t -0.42848969661986275\n",
            "1      \t [0.65899411 0.08911785 5.         0.57513718 8.         0.6649358 ]. \t  -0.5435492302872315 \t -0.42848969661986275\n",
            "2      \t [7.50154303 9.49874736 7.         0.83842364 4.         0.5714185 ]. \t  -0.5422231033003287 \t -0.42848969661986275\n",
            "3      \t [9.88263606 1.71181353 7.         0.97275193 1.         0.74825108]. \t  -0.441747506473179 \t -0.42848969661986275\n",
            "4      \t [ 0.50045611  0.9041258  14.          0.89701685 13.          0.41057236]. \t  -0.5651338432083423 \t -0.42848969661986275\n",
            "5      \t [ 8.92213252  4.56062357 13.          0.93869871  8.          0.53052713]. \t  -0.5289605254310144 \t -0.42848969661986275\n",
            "6      \t [ 3.60181415  8.33097203 14.          0.9882562   5.          0.89449689]. \t  \u001b[92m-0.4137972346428497\u001b[0m \t -0.4137972346428497\n",
            "7      \t [ 0.11546318  7.06183776 14.          0.68323086 19.          0.99255453]. \t  -0.42239160139562504 \t -0.4137972346428497\n",
            "8      \t [0.  0.  5.  0.5 1.  0.1]. \t  -0.6860882831213788 \t -0.4137972346428497\n",
            "9      \t [ 0.10082071  2.20765876  6.          0.79154439 17.          0.83321591]. \t  -0.45448270216379194 \t -0.4137972346428497\n",
            "10     \t [0.32359376 9.19341132 6.         0.74153554 1.         0.25113354]. \t  -0.6853813278526151 \t -0.4137972346428497\n",
            "11     \t [ 9.07851132  6.67468648 14.          0.65555017  1.          0.96610958]. \t  -0.424975755228691 \t -0.4137972346428497\n",
            "12     \t [ 9.80974078  5.67295504  6.          0.91127307 13.          0.44957804]. \t  -0.5591662642332185 \t -0.4137972346428497\n",
            "13     \t [ 2.77198081  0.67245678 14.          0.56883806 19.          0.9553332 ]. \t  -0.4333861030323646 \t -0.4137972346428497\n",
            "14     \t [6.86928577 3.72419603 7.         0.55484489 8.         0.11419186]. \t  -0.6867308079831548 \t -0.4137972346428497\n",
            "15     \t [ 0.83926825  9.31286624 13.          0.83595717 11.          0.92691071]. \t  -0.41668725948491847 \t -0.4137972346428497\n",
            "16     \t [ 2.2599948   8.5777314   7.          0.92101066 18.          0.81278145]. \t  -0.44458847008023056 \t -0.4137972346428497\n",
            "17     \t [ 7.48756016  0.13555104 14.          0.75269236 12.          0.41177876]. \t  -0.5678014017572689 \t -0.4137972346428497\n",
            "18     \t [2.94548912 4.25555055 5.         0.68007425 4.         0.7533029 ]. \t  -0.45906401928062657 \t -0.4137972346428497\n",
            "19     \t [ 8.06744935  9.40296538  6.          0.60147647 19.          0.17678037]. \t  -0.6864884735391246 \t -0.4137972346428497\n",
            "20     \t [ 3.87077387  0.04333662 11.          0.79311966  8.          0.7797114 ]. \t  -0.423652004810958 \t -0.4137972346428497\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.343487868410211"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-45l3NU4UNiI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d632da6-62a8-4da1-d943-877d510a9511"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 3\n",
        "\n",
        "np.random.seed(run_num_3)\n",
        "surrogate_loser_3 = dGaussianProcess(cov_func)\n",
        "\n",
        "X_train3, X_test3, y_train3, y_test3 = train_test_split(X, y, test_size=test_perc, random_state=run_num_3)\n",
        "\n",
        "def f_syn_polarity3(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_3, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train3, y=y_train3).mean())\n",
        "    return operator * score\n",
        "\n",
        "loser_3 = dGPGO(surrogate_loser_3, Acquisition_new(util_loser), f_syn_polarity3, param, n_jobs = -1) # define BayesOpt\n",
        "loser_3.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_loser_3 = loser_3.getResult()[0]\n",
        "params_loser_3['max_depth'] = int(params_loser_3['max_depth'])\n",
        "params_loser_3['min_child_weight'] = int(params_loser_3['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_loser_train3 = xgb.DMatrix(X_train3, y_train3)\n",
        "dX_loser_test3 = xgb.DMatrix(X_test3, y_test3)\n",
        "model_loser_3 = xgb.train(params_loser_3, dX_loser_train3)\n",
        "pred_loser_3 = model_loser_3.predict(dX_loser_test3)\n",
        "\n",
        "rmse_loser_3 = np.sqrt(mean_squared_error(pred_loser_3, y_test3))\n",
        "rmse_loser_3"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 7.51575607  1.09251275 13.          0.80377147 16.          0.31229624]. \t  -0.6858957949546655 \t -0.5081673732303724\n",
            "init   \t [6.12794891 1.19652403 5.         0.74822174 8.         0.33695838]. \t  -0.6849895205987827 \t -0.5081673732303724\n",
            "init   \t [ 6.21631223  4.4909976  10.          0.66764529  9.          0.77207323]. \t  -0.5081673732303724 \t -0.5081673732303724\n",
            "init   \t [ 0.58127233  4.79918477 11.          0.64526552 15.          0.62431773]. \t  -0.5106501747238601 \t -0.5081673732303724\n",
            "init   \t [ 5.76228258  0.67219724  7.          0.88228691 13.          0.29867219]. \t  -0.6839256266327581 \t -0.5081673732303724\n",
            "1      \t [9.79994489 9.57810833 8.         0.93763368 1.         0.1772549 ]. \t  -0.7316297465421655 \t -0.5081673732303724\n",
            "2      \t [ 3.27075931  5.07207678 14.          0.99587284  2.          0.51564135]. \t  -0.6620507374994331 \t -0.5081673732303724\n",
            "3      \t [0.         0.         5.         0.5        1.19792586 0.1       ]. \t  -0.732320887566852 \t -0.5081673732303724\n",
            "4      \t [ 8.88346709  9.03427619  8.          0.525814   19.          0.22866902]. \t  -0.732406939137863 \t -0.5081673732303724\n",
            "5      \t [1.21637479 8.34226514 5.         0.57939054 5.         0.55479603]. \t  -0.6531491980452168 \t -0.5081673732303724\n",
            "6      \t [ 2.93278685  9.15897164 14.          0.67343746 10.          0.85376406]. \t  \u001b[92m-0.504811636744922\u001b[0m \t -0.504811636744922\n",
            "7      \t [ 1.1995358   9.87930749  5.          0.84448568 18.          0.26022822]. \t  -0.7297224033867351 \t -0.504811636744922\n",
            "8      \t [ 1.96525323  0.16048396 14.          0.65133015  9.          0.3003508 ]. \t  -0.6860931128073492 \t -0.504811636744922\n",
            "9      \t [6.19566593 0.54575662 9.         0.53247577 1.         0.58149275]. \t  -0.5171483404397559 \t -0.504811636744922\n",
            "10     \t [ 3.97093318  9.58939295  6.          0.5381492  12.          0.74760437]. \t  -0.5217629748454513 \t -0.504811636744922\n",
            "11     \t [ 9.94719165  9.26136843 13.          0.64663072 11.          0.47018827]. \t  -0.6546346492088932 \t -0.504811636744922\n",
            "12     \t [9.55919323 7.66342182 6.         0.63144136 7.         0.82664124]. \t  -0.5195220027397864 \t -0.504811636744922\n",
            "13     \t [ 2.80079796  0.61761702  5.          0.62606225 19.          0.71334647]. \t  -0.5274647167409906 \t -0.504811636744922\n",
            "14     \t [ 9.59599852  3.11576364  5.          0.59256945 17.          0.56621848]. \t  -0.6538676177721936 \t -0.504811636744922\n",
            "15     \t [ 8.91533927  0.16704192 13.          0.66647549  6.          0.25014   ]. \t  -0.7314177593008601 \t -0.504811636744922\n",
            "16     \t [ 4.33253274  9.14516752 14.          0.94398901 19.          0.74581708]. \t  \u001b[92m-0.5007721901039625\u001b[0m \t -0.5007721901039625\n",
            "17     \t [9.75857066 2.84550968 5.         0.7181157  3.         0.67923616]. \t  -0.5274197984747595 \t -0.5007721901039625\n",
            "18     \t [ 0.          0.          5.          0.5        10.44274008  0.1       ]. \t  -0.7323784626096901 \t -0.5007721901039625\n",
            "19     \t [ 9.94497871  9.34276152 14.          0.66308198  5.          0.66061477]. \t  -0.5102482930096841 \t -0.5007721901039625\n",
            "20     \t [ 0.45956381  8.89338741 11.          0.67604128  4.          0.20043998]. \t  -0.7307540586635348 \t -0.5007721901039625\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.6249009343932785"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voPfk1UDUQU0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb2c3b2f-b174-467c-ae33-6a5619f47929"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 4\n",
        "\n",
        "np.random.seed(run_num_4)\n",
        "surrogate_loser_4 = dGaussianProcess(cov_func)\n",
        "\n",
        "X_train4, X_test4, y_train4, y_test4 = train_test_split(X, y, test_size=test_perc, random_state=run_num_4)\n",
        "\n",
        "def f_syn_polarity4(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_4, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train4, y=y_train4).mean())\n",
        "    return operator * score\n",
        "\n",
        "loser_4 = dGPGO(surrogate_loser_4, Acquisition_new(util_loser), f_syn_polarity4, param, n_jobs = -1) # define BayesOpt\n",
        "loser_4.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_loser_4 = loser_4.getResult()[0]\n",
        "params_loser_4['max_depth'] = int(params_loser_4['max_depth'])\n",
        "params_loser_4['min_child_weight'] = int(params_loser_4['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_loser_train4 = xgb.DMatrix(X_train4, y_train4)\n",
        "dX_loser_test4 = xgb.DMatrix(X_test4, y_test4)\n",
        "model_loser_4 = xgb.train(params_loser_4, dX_loser_train4)\n",
        "pred_loser_4 = model_loser_4.predict(dX_loser_test4)\n",
        "\n",
        "rmse_loser_4 = np.sqrt(mean_squared_error(pred_loser_4, y_test4))\n",
        "rmse_loser_4"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 9.42798506  1.23319667 14.          0.81910755  7.          0.7021549 ]. \t  -0.5581090891563811 \t -0.5581090891563811\n",
            "init   \t [ 5.54158317  7.71233962 10.          0.59117644  6.          0.31476458]. \t  -0.6204181120564479 \t -0.5581090891563811\n",
            "init   \t [2.18282634 2.93794127 7.         0.84739327 5.         0.62716229]. \t  -0.5626656925564042 \t -0.5581090891563811\n",
            "init   \t [ 5.66159581  4.68678814 11.          0.54147808  9.          0.58609221]. \t  -0.5595885941852147 \t -0.5581090891563811\n",
            "init   \t [ 3.02160958  6.07432614  6.          0.92038559 14.          0.12778609]. \t  -0.6361772582153176 \t -0.5581090891563811\n",
            "1      \t [ 1.89460697  0.10324856 13.          0.70356349 16.          0.67776197]. \t  \u001b[92m-0.5568243993303088\u001b[0m \t -0.5568243993303088\n",
            "2      \t [ 9.69969     4.32607677 14.          0.87739158 16.          0.66014279]. \t  -0.5573098211239071 \t -0.5568243993303088\n",
            "3      \t [ 2.2210328   8.31054196 13.          0.70317243 19.          0.56953182]. \t  -0.573504425404327 \t -0.5568243993303088\n",
            "4      \t [ 0.14706294  0.75702217 14.          0.56309891  8.          0.29887971]. \t  -0.6254087880261816 \t -0.5568243993303088\n",
            "5      \t [ 6.94218243  0.32144048  6.          0.59286079 19.          0.70636825]. \t  -0.5706574487524134 \t -0.5568243993303088\n",
            "6      \t [ 9.24159749  9.43575685  7.          0.98380459 19.          0.34937346]. \t  -0.6187589419936593 \t -0.5568243993303088\n",
            "7      \t [7.57184449 0.0324327  9.         0.86318723 1.         0.6413689 ]. \t  -0.5611723012436254 \t -0.5568243993303088\n",
            "8      \t [ 1.62755426  6.7921099  14.          0.91903975  1.          0.61791343]. \t  -0.5680865264437001 \t -0.5568243993303088\n",
            "9      \t [ 9.43074486  0.98044049  5.          0.96290856 10.          0.27397556]. \t  -0.6390681992417737 \t -0.5568243993303088\n",
            "10     \t [ 8.73128599  9.22658128 14.          0.98344346  2.          0.13489277]. \t  -0.6368164613179 \t -0.5568243993303088\n",
            "11     \t [ 0.          0.          7.37879691  0.5        12.37879691  0.1       ]. \t  -0.6353530029272869 \t -0.5568243993303088\n",
            "12     \t [8.91398595 7.00476796 6.         0.72156795 1.         0.41673037]. \t  -0.6210807206819127 \t -0.5568243993303088\n",
            "13     \t [ 9.58851274  9.75423983 14.          0.60815214 12.          0.97572229]. \t  \u001b[92m-0.44045254560081915\u001b[0m \t -0.44045254560081915\n",
            "14     \t [ 9.87497198  6.68701692  6.          0.55980787 13.          0.32605223]. \t  -0.6200123103666009 \t -0.44045254560081915\n",
            "15     \t [ 2.16464782  9.35692886 11.          0.97431128 12.          0.43257419]. \t  -0.5747879420555002 \t -0.44045254560081915\n",
            "16     \t [0.  0.  5.  0.5 1.  0.1]. \t  -0.6351353738423114 \t -0.44045254560081915\n",
            "17     \t [0.94508697 7.16234343 8.48038022 0.5        1.48038022 0.1       ]. \t  -0.6346230590373485 \t -0.44045254560081915\n",
            "18     \t [7.36355448 5.3077281  5.         0.5709887  7.         0.87569884]. \t  -0.467633753337906 \t -0.44045254560081915\n",
            "19     \t [ 7.93069008  0.11830338 11.          0.97110397 13.          0.22887984]. \t  -0.6356672520315936 \t -0.44045254560081915\n",
            "20     \t [ 6.39787933  3.84205635 14.          0.72880814  3.          0.21974221]. \t  -0.6364956155454153 \t -0.44045254560081915\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.469425637772456"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kEnTd7MUdlv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed09ddc6-91bb-4a72-85c6-a40db27c7806"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 5\n",
        "\n",
        "np.random.seed(run_num_5)\n",
        "surrogate_loser_5 = dGaussianProcess(cov_func)\n",
        "\n",
        "X_train5, X_test5, y_train5, y_test5 = train_test_split(X, y, test_size=test_perc, random_state=run_num_5)\n",
        "\n",
        "def f_syn_polarity5(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_5, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train5, y=y_train5).mean())\n",
        "    return operator * score\n",
        "\n",
        "loser_5 = dGPGO(surrogate_loser_5, Acquisition_new(util_loser), f_syn_polarity5, param, n_jobs = -1) # define BayesOpt\n",
        "loser_5.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_loser_5 = loser_5.getResult()[0]\n",
        "params_loser_5['max_depth'] = int(params_loser_5['max_depth'])\n",
        "params_loser_5['min_child_weight'] = int(params_loser_5['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_loser_train5 = xgb.DMatrix(X_train5, y_train5)\n",
        "dX_loser_test5 = xgb.DMatrix(X_test5, y_test5)\n",
        "model_loser_5 = xgb.train(params_loser_5, dX_loser_train5)\n",
        "pred_loser_5 = model_loser_5.predict(dX_loser_test5)\n",
        "\n",
        "rmse_loser_5 = np.sqrt(mean_squared_error(pred_loser_5, y_test5))\n",
        "rmse_loser_5"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 5.57353274  5.41760627  8.          0.53710825 19.          0.26034376]. \t  -0.6523797617249512 \t -0.466901759574539\n",
            "init   \t [5.17826437 9.78817462 5.         0.84048437 6.         0.79712615]. \t  -0.4886603987288415 \t -0.466901759574539\n",
            "init   \t [ 9.84476201  9.65484856  5.          0.59464714 19.          0.34528747]. \t  -0.564220001422124 \t -0.466901759574539\n",
            "init   \t [ 9.17476842  4.53262239 13.          0.64368565 10.          0.515308  ]. \t  -0.5049042831858346 \t -0.466901759574539\n",
            "init   \t [ 8.66719465  7.99685372 12.          0.79833571 17.          0.72443466]. \t  -0.466901759574539 \t -0.466901759574539\n",
            "1      \t [ 3.31480734  6.81602635 12.          0.95330947  1.          0.27334927]. \t  -0.6531167190113543 \t -0.466901759574539\n",
            "2      \t [2.53140694 0.29740057 9.         0.7505917  7.         0.82287404]. \t  -0.47475119282304395 \t -0.466901759574539\n",
            "3      \t [ 9.84728748  0.28222034 13.          0.93969018  3.          0.22339517]. \t  -0.6522247903981228 \t -0.466901759574539\n",
            "4      \t [ 0.68116737  3.94603776 14.          0.80380753 14.          0.77317996]. \t  \u001b[92m-0.4613270217842209\u001b[0m \t -0.4613270217842209\n",
            "5      \t [ 9.6928838   0.38736829  6.          0.56167539 11.          0.4775038 ]. \t  -0.5239028011959734 \t -0.4613270217842209\n",
            "6      \t [ 1.62699639  9.0444625   7.          0.60549869 13.          0.13190699]. \t  -0.6526723866694732 \t -0.4613270217842209\n",
            "7      \t [1.904453  0.        5.1118395 0.5       1.        0.1      ]. \t  -0.654517615956795 \t -0.4613270217842209\n",
            "8      \t [ 9.56204849  0.80573312 12.          0.60476955 16.          0.23612747]. \t  -0.6524022297735614 \t -0.4613270217842209\n",
            "9      \t [ 1.05461775  1.17661042  5.          0.5648338  17.          0.90649901]. \t  -0.4702337390488589 \t -0.4613270217842209\n",
            "10     \t [ 0.29281635  9.83144868 11.          0.57517626 19.          0.13846438]. \t  -0.6543975191191339 \t -0.4613270217842209\n",
            "11     \t [9.66604659 1.51730123 5.         0.59597224 4.         0.66643448]. \t  -0.5016316198301449 \t -0.4613270217842209\n",
            "12     \t [0.57140531 6.2823043  5.         0.70475062 2.         0.3328708 ]. \t  -0.5594485185267623 \t -0.4613270217842209\n",
            "13     \t [ 2.5240907   6.84169344 14.          0.61623097  8.          0.50182056]. \t  -0.5037582468108619 \t -0.4613270217842209\n",
            "14     \t [0.0858834  4.38043443 5.         0.57114514 9.         0.11102852]. \t  -0.6531285980012218 \t -0.4613270217842209\n",
            "15     \t [ 9.08851157  7.7443441   6.          0.74314397 12.          0.52508293]. \t  -0.5238407378055661 \t -0.4613270217842209\n",
            "16     \t [ 0.62266475  1.9018111  14.          0.63329124  3.          0.94054341]. \t  \u001b[92m-0.42529491633232686\u001b[0m \t -0.42529491633232686\n",
            "17     \t [ 9.23494676  7.58606552 12.          0.78487568  2.          0.96478853]. \t  -0.42939677763278594 \t -0.42529491633232686\n",
            "18     \t [ 5.11817103  4.38089559  9.          0.53776664 13.          0.7209118 ]. \t  -0.4792507830025482 \t -0.42529491633232686\n",
            "19     \t [8.44899175 7.77761284 6.         0.53217155 1.         0.55958377]. \t  -0.5238817589404079 \t -0.42529491633232686\n",
            "20     \t [ 3.67612744  5.42599858 14.          0.93140269 19.          0.80853077]. \t  -0.4626498801811607 \t -0.42529491633232686\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.31722690080873"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjVSH6caUgyy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7518631-29bd-4f29-809f-547528a8786b"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 6\n",
        "\n",
        "np.random.seed(run_num_6)\n",
        "surrogate_loser_6 = dGaussianProcess(cov_func)\n",
        "\n",
        "X_train6, X_test6, y_train6, y_test6 = train_test_split(X, y, test_size=test_perc, random_state=run_num_6)\n",
        "\n",
        "def f_syn_polarity6(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_6, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train6, y=y_train6).mean())\n",
        "    return operator * score\n",
        "\n",
        "loser_6 = dGPGO(surrogate_loser_6, Acquisition_new(util_loser), f_syn_polarity6, param, n_jobs = -1) # define BayesOpt\n",
        "loser_6.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_loser_6 = loser_6.getResult()[0]\n",
        "params_loser_6['max_depth'] = int(params_loser_6['max_depth'])\n",
        "params_loser_6['min_child_weight'] = int(params_loser_6['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_loser_train6 = xgb.DMatrix(X_train6, y_train6)\n",
        "dX_loser_test6 = xgb.DMatrix(X_test6, y_test6)\n",
        "model_loser_6 = xgb.train(params_loser_6, dX_loser_train6)\n",
        "pred_loser_6 = model_loser_6.predict(dX_loser_test6)\n",
        "\n",
        "rmse_loser_6 = np.sqrt(mean_squared_error(pred_loser_6, y_test6))\n",
        "rmse_loser_6"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [8.92860151 3.31979805 5.         0.99251441 2.         0.57683563]. \t  -0.5471181110640053 \t -0.5219867521874206\n",
            "init   \t [4.18807429 3.35407849 9.         0.87750649 3.         0.56623277]. \t  -0.629286701398377 \t -0.5219867521874206\n",
            "init   \t [ 5.788586    6.45355096 14.          0.70660047 12.          0.82154882]. \t  -0.5219867521874206 \t -0.5219867521874206\n",
            "init   \t [4.58184578 6.73834679 5.         0.90108528 3.         0.65482895]. \t  -0.5476166257785141 \t -0.5219867521874206\n",
            "init   \t [ 4.42510505  5.75952352 14.          0.97882365 15.          0.29525604]. \t  -0.6655867879161768 \t -0.5219867521874206\n",
            "1      \t [ 7.09628606  0.82458553  6.          0.52978945 17.          0.78827589]. \t  -0.5368551114917748 \t -0.5219867521874206\n",
            "2      \t [ 1.88885215  8.62524025  6.          0.90916563 14.          0.40379086]. \t  -0.6680072912825885 \t -0.5219867521874206\n",
            "3      \t [ 0.5654966   9.52584762 14.          0.82016688  4.          0.10717254]. \t  -0.7389555129670606 \t -0.5219867521874206\n",
            "4      \t [ 9.63033822  0.03793409 13.          0.68076302  7.          0.91391681]. \t  \u001b[92m-0.4301920669254681\u001b[0m \t -0.4301920669254681\n",
            "5      \t [ 9.97469028  9.15579642 12.          0.66745021 19.          0.28959158]. \t  -0.6668248751122167 \t -0.4301920669254681\n",
            "6      \t [ 9.85880187  8.48475629 13.          0.76006171  2.          0.58751173]. \t  -0.5403277210407594 \t -0.4301920669254681\n",
            "7      \t [ 9.51844     9.62786679  5.          0.56989798 15.          0.51068013]. \t  -0.6345288616058514 \t -0.4301920669254681\n",
            "8      \t [ 0.15151883  3.04875397  5.          0.61495161 19.          0.79420245]. \t  -0.5357132607573185 \t -0.4301920669254681\n",
            "9      \t [ 3.08077333  0.89569015  6.          0.84149914 11.          0.94992477]. \t  -0.4472711154368675 \t -0.4301920669254681\n",
            "10     \t [ 0.19816257  1.95434851 14.          0.59292238  7.          0.17971262]. \t  -0.7383261448869979 \t -0.4301920669254681\n",
            "11     \t [ 0.56219975  9.42928268 11.          0.51903033 19.          0.52214278]. \t  -0.6326946461691906 \t -0.4301920669254681\n",
            "12     \t [9.58176304 9.94093363 6.         0.96280567 8.         0.32855082]. \t  -0.6677341620025121 \t -0.4301920669254681\n",
            "13     \t [ 5.43877488  0.17504551 13.          0.58721411 19.          0.60115325]. \t  -0.5401916158723044 \t -0.4301920669254681\n",
            "14     \t [0.  0.  5.  0.5 1.  0.1]. \t  -0.7382709735940614 \t -0.4301920669254681\n",
            "15     \t [9.18163606 3.89469538 8.         0.59772622 8.         0.18901946]. \t  -0.7380580836797348 \t -0.4301920669254681\n",
            "16     \t [0.52725495 9.82259272 6.         0.62080595 7.         0.68770749]. \t  -0.5497290569747493 \t -0.4301920669254681\n",
            "17     \t [ 6.71573436  6.18076517  8.          0.50736725 18.          0.99692636]. \t  -0.4548470613295324 \t -0.4301920669254681\n",
            "18     \t [ 7.23381367  0.21465732 13.          0.75484136 13.          0.44717401]. \t  -0.6306144837329356 \t -0.4301920669254681\n",
            "19     \t [ 0.17525712  4.19183624 13.          0.93093846  1.          0.77496282]. \t  -0.5254340278392874 \t -0.4301920669254681\n",
            "20     \t [ 0.22896089  2.69806987 14.          0.53297649 18.          0.65056758]. \t  -0.5430966302037052 \t -0.4301920669254681\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.1451215046465775"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1WsphKSUj19",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25fa0ecc-08c5-4a0f-b61f-363940733fa7"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 7\n",
        "\n",
        "np.random.seed(run_num_7)\n",
        "surrogate_loser_7 = dGaussianProcess(cov_func)\n",
        "\n",
        "X_train7, X_test7, y_train7, y_test7 = train_test_split(X, y, test_size=test_perc, random_state=run_num_7)\n",
        "\n",
        "def f_syn_polarity7(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_7, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train7, y=y_train7).mean())\n",
        "    return operator * score\n",
        "\n",
        "loser_7 = dGPGO(surrogate_loser_7, Acquisition_new(util_loser), f_syn_polarity7, param, n_jobs = -1) # define BayesOpt\n",
        "loser_7.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_loser_7 = loser_7.getResult()[0]\n",
        "params_loser_7['max_depth'] = int(params_loser_7['max_depth'])\n",
        "params_loser_7['min_child_weight'] = int(params_loser_7['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_loser_train7 = xgb.DMatrix(X_train7, y_train7)\n",
        "dX_loser_test7 = xgb.DMatrix(X_test7, y_test7)\n",
        "model_loser_7 = xgb.train(params_loser_7, dX_loser_train7)\n",
        "pred_loser_7 = model_loser_7.predict(dX_loser_test7)\n",
        "\n",
        "rmse_loser_7 = np.sqrt(mean_squared_error(pred_loser_7, y_test7))\n",
        "rmse_loser_7"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 8.11505126  6.92324143 13.          0.94046175 12.          0.3650384 ]. \t  -0.612852517751432 \t -0.4284992540085738\n",
            "init   \t [ 5.15357029  9.0066636   6.          0.74104227 15.          0.8966337 ]. \t  -0.45500673865987984 \t -0.4284992540085738\n",
            "init   \t [ 2.63548913  9.6610934  14.          0.64712408  1.          0.80820948]. \t  -0.43730487111474525 \t -0.4284992540085738\n",
            "init   \t [ 1.40821426  1.72758589 12.          0.70495069  5.          0.86632715]. \t  -0.4284992540085738 \t -0.4284992540085738\n",
            "init   \t [2.85126987 2.73434054 7.         0.88394733 8.         0.65545939]. \t  -0.48253390492139586 \t -0.4284992540085738\n",
            "1      \t [1.03697999e-02 2.51695028e+00 1.40000000e+01 6.19497248e-01\n",
            " 1.90000000e+01 7.01827417e-01]. \t  -0.47758423778274467 \t -0.4284992540085738\n",
            "2      \t [9.9089961  7.50235155 7.         0.67925503 5.         0.2471192 ]. \t  -0.6736712023667494 \t -0.4284992540085738\n",
            "3      \t [ 7.21196079  0.11384945  8.          0.67755731 17.          0.74323626]. \t  -0.4647919397201027 \t -0.4284992540085738\n",
            "4      \t [0.35283087 9.2950614  5.         0.93054416 3.         0.27625531]. \t  -0.670508947897148 \t -0.4284992540085738\n",
            "5      \t [0.  0.  5.  0.5 1.  0.1]. \t  -0.6745888440506937 \t -0.4284992540085738\n",
            "6      \t [9.76759527 0.48761621 7.         0.96362191 1.         0.137157  ]. \t  -0.6715547907624335 \t -0.4284992540085738\n",
            "7      \t [ 8.53387117  0.35946342 13.          0.65332712  6.          0.68302293]. \t  -0.4712802345283902 \t -0.4284992540085738\n",
            "8      \t [ 0.45155781  2.33209808  5.          0.73660734 17.          0.83890491]. \t  -0.4804814871680856 \t -0.4284992540085738\n",
            "9      \t [ 0.30506512  9.51761383 13.          0.5277377  16.          0.92234588]. \t  -0.4390633386106087 \t -0.4284992540085738\n",
            "10     \t [ 9.21287749  3.65759887  5.          0.85687073 12.          0.88459516]. \t  -0.45867733996321497 \t -0.4284992540085738\n",
            "11     \t [ 1.14642054  8.54710172 12.          0.83923566  8.          0.25317439]. \t  -0.6716592254658202 \t -0.4284992540085738\n",
            "12     \t [ 6.43879816  6.53341304 14.          0.70290258 19.          0.43578856]. \t  -0.5879212739277196 \t -0.4284992540085738\n",
            "13     \t [ 0.28968499  1.98768694 14.          0.97996073 12.          0.15615797]. \t  -0.6725851052423111 \t -0.4284992540085738\n",
            "14     \t [5.4721942  4.70341961 9.         0.675658   1.         0.91263074]. \t  -0.44024324260844283 \t -0.4284992540085738\n",
            "15     \t [ 7.56572451  6.67046024 14.          0.88731871  4.          0.29940932]. \t  -0.6117537685654454 \t -0.4284992540085738\n",
            "16     \t [4.54036829 9.23672573 7.         0.78071654 7.         0.925943  ]. \t  -0.44422074369565967 \t -0.4284992540085738\n",
            "17     \t [ 9.52566208  5.8884151   6.          0.78376025 19.          0.40148749]. \t  -0.6258103990077785 \t -0.4284992540085738\n",
            "18     \t [ 7.79919254  0.6285715  14.          0.87477175 15.          0.15632002]. \t  -0.6708453498973665 \t -0.4284992540085738\n",
            "19     \t [ 0.          0.          8.44191182  0.5        11.44191182  0.1       ]. \t  -0.6744471853974358 \t -0.4284992540085738\n",
            "20     \t [ 3.1237334   5.58211355  9.          0.8525378  19.          0.1292964 ]. \t  -0.6703702828267247 \t -0.4284992540085738\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.492050628419268"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hI8sFP4ZUmOs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0bc3a37-a280-4bcd-dbfa-2ef74a09f71d"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 8\n",
        "\n",
        "np.random.seed(run_num_8)\n",
        "surrogate_loser_8 = dGaussianProcess(cov_func)\n",
        "\n",
        "X_train8, X_test8, y_train8, y_test8 = train_test_split(X, y, test_size=test_perc, random_state=run_num_8)\n",
        "\n",
        "def f_syn_polarity8(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_8, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train8, y=y_train8).mean())\n",
        "    return operator * score\n",
        "\n",
        "loser_8 = dGPGO(surrogate_loser_8, Acquisition_new(util_loser), f_syn_polarity8, param, n_jobs = -1) # define BayesOpt\n",
        "loser_8.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_loser_8 = loser_8.getResult()[0]\n",
        "params_loser_8['max_depth'] = int(params_loser_8['max_depth'])\n",
        "params_loser_8['min_child_weight'] = int(params_loser_8['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_loser_train8 = xgb.DMatrix(X_train8, y_train8)\n",
        "dX_loser_test8 = xgb.DMatrix(X_test8, y_test8)\n",
        "model_loser_8 = xgb.train(params_loser_8, dX_loser_train8)\n",
        "pred_loser_8 = model_loser_8.predict(dX_loser_test8)\n",
        "\n",
        "rmse_loser_8 = np.sqrt(mean_squared_error(pred_loser_8, y_test8))\n",
        "rmse_loser_8"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 1.29221489  0.77396898 11.          0.56062468  3.          0.25451879]. \t  -0.6231794271723858 \t -0.4674926150198253\n",
            "init   \t [ 0.10611429  1.37648061  8.          0.6050375  15.          0.790205  ]. \t  -0.4674926150198253 \t -0.4674926150198253\n",
            "init   \t [ 8.35239571  4.16908043  7.          0.67902746 14.          0.16683749]. \t  -0.6176439249709439 \t -0.4674926150198253\n",
            "init   \t [ 6.06301777  7.27127491 12.          0.87295531 11.          0.37694382]. \t  -0.5951520828762356 \t -0.4674926150198253\n",
            "init   \t [8.85209637 7.89269955 6.         0.72444854 3.         0.47766986]. \t  -0.5688519156580504 \t -0.4674926150198253\n",
            "1      \t [ 1.47825581  9.16010046 11.          0.96606546  1.          0.92358292]. \t  \u001b[92m-0.43518063190798095\u001b[0m \t -0.43518063190798095\n",
            "2      \t [ 7.52273592  2.30141271 14.          0.75505047 19.          0.77804046]. \t  -0.45158957529537797 \t -0.43518063190798095\n",
            "3      \t [1.64420908 9.47353882 5.         0.79993338 7.         0.15673599]. \t  -0.6175838614360663 \t -0.43518063190798095\n",
            "4      \t [ 1.02399475  9.65113789  9.          0.53947972 17.          0.8292164 ]. \t  -0.4719352875928771 \t -0.43518063190798095\n",
            "5      \t [8.14212583 0.61314553 6.         0.86488725 6.         0.28574491]. \t  -0.5954481959734638 \t -0.43518063190798095\n",
            "6      \t [ 8.76387285  7.83054608 13.          0.54159783  4.          0.24710422]. \t  -0.6207082177098945 \t -0.43518063190798095\n",
            "7      \t [ 9.0517509   9.45530828 11.          0.85305105 19.          0.79961763]. \t  -0.4589725747508552 \t -0.43518063190798095\n",
            "8      \t [ 7.77762087  0.97608919 14.          0.56979605  8.          0.73436998]. \t  -0.4483480686509709 \t -0.43518063190798095\n",
            "9      \t [2.67676373 3.95611485 5.         0.79739163 2.         0.89987835]. \t  -0.45674643004535687 \t -0.43518063190798095\n",
            "10     \t [ 0.62854908  4.77207773 14.          0.78182185 18.          0.73284936]. \t  -0.4461225555243119 \t -0.43518063190798095\n",
            "11     \t [3.06290827 1.66002331 6.         0.51411222 9.         0.98529356]. \t  -0.461628022517463 \t -0.43518063190798095\n",
            "12     \t [ 0.55545334  6.5757175   9.          0.88529579 12.          0.24203797]. \t  -0.6187020960077592 \t -0.43518063190798095\n",
            "13     \t [ 9.10053278  1.48753729 11.          0.9741735   1.          0.65977303]. \t  -0.5223704878111852 \t -0.43518063190798095\n",
            "14     \t [ 9.48329103  0.2076913   7.          0.76655383 19.          0.88164364]. \t  -0.4616094756962891 \t -0.43518063190798095\n",
            "15     \t [ 0.81390458  8.42248516 12.          0.78729971  7.          0.71328057]. \t  -0.5176932464043367 \t -0.43518063190798095\n",
            "16     \t [ 5.51535579  8.63063044  6.          0.64760098 18.          0.14185561]. \t  -0.6172074564902792 \t -0.43518063190798095\n",
            "17     \t [ 0.81210087  0.8533868  13.          0.66456412 12.          0.37769434]. \t  -0.5971747957799627 \t -0.43518063190798095\n",
            "18     \t [ 8.74769008  9.46041181  5.          0.76888057 12.          0.28611188]. \t  -0.5996998823178632 \t -0.43518063190798095\n",
            "19     \t [ 6.67918913  1.91467598 12.          0.83062216 13.          0.72788563]. \t  -0.45035357506572193 \t -0.43518063190798095\n",
            "20     \t [ 4.53564763  5.14157265 14.          0.52558575  1.          0.72718907]. \t  -0.45119962298136185 \t -0.43518063190798095\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.711175290813375"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vw5IYus6UpAn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2ad5766-c259-406d-82ac-0d920b1f803d"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 9\n",
        "\n",
        "np.random.seed(run_num_9)\n",
        "surrogate_loser_9 = dGaussianProcess(cov_func)\n",
        "\n",
        "X_train9, X_test9, y_train9, y_test9 = train_test_split(X, y, test_size=test_perc, random_state=run_num_9)\n",
        "\n",
        "def f_syn_polarity9(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_9, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train9, y=y_train9).mean())\n",
        "    return operator * score\n",
        "\n",
        "loser_9 = dGPGO(surrogate_loser_9, Acquisition_new(util_loser), f_syn_polarity9, param, n_jobs = -1) # define BayesOpt\n",
        "loser_9.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_loser_9 = loser_9.getResult()[0]\n",
        "params_loser_9['max_depth'] = int(params_loser_9['max_depth'])\n",
        "params_loser_9['min_child_weight'] = int(params_loser_9['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_loser_train9 = xgb.DMatrix(X_train9, y_train9)\n",
        "dX_loser_test9 = xgb.DMatrix(X_test9, y_test9)\n",
        "model_loser_9 = xgb.train(params_loser_9, dX_loser_train9)\n",
        "pred_loser_9 = model_loser_9.predict(dX_loser_test9)\n",
        "\n",
        "rmse_loser_9 = np.sqrt(mean_squared_error(pred_loser_9, y_test9))\n",
        "rmse_loser_9"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 8.0342804   5.27522296 13.          0.60741904 17.          0.15336222]. \t  -0.6890593508433319 \t -0.4886355541929722\n",
            "init   \t [7.56063644 1.79876369 7.         0.84869647 5.         0.55956942]. \t  -0.5532305414240943 \t -0.4886355541929722\n",
            "init   \t [2.43675539 0.20225128 6.         0.51669963 7.         0.4646446 ]. \t  -0.5609155518755553 \t -0.4886355541929722\n",
            "init   \t [ 7.6043547   4.73758376  9.          0.87564624 15.          0.65751286]. \t  -0.4886355541929722 \t -0.4886355541929722\n",
            "init   \t [ 4.70349314  6.71029586  6.          0.97680389 11.          0.41971652]. \t  -0.6548660487332898 \t -0.4886355541929722\n",
            "1      \t [ 0.20910387  7.25545521 14.          0.59407892  3.          0.95410586]. \t  \u001b[92m-0.4312356520914486\u001b[0m \t -0.4312356520914486\n",
            "2      \t [ 2.14892068  9.25577985 14.          0.84828472 11.          0.2793841 ]. \t  -0.6883959263608518 \t -0.4312356520914486\n",
            "3      \t [ 0.37305557  1.02756836 13.          0.57348052 13.          0.68410192]. \t  -0.4836872799099695 \t -0.4312356520914486\n",
            "4      \t [ 7.98252507  8.04957695 12.          0.6367811   3.          0.85775382]. \t  -0.4368210491589304 \t -0.4312356520914486\n",
            "5      \t [ 1.2588195   0.04438454  7.          0.70342864 18.          0.58554745]. \t  -0.4951281025154505 \t -0.4312356520914486\n",
            "6      \t [ 9.17710587  9.71573287  5.          0.9584376  19.          0.78072441]. \t  -0.4682275209845145 \t -0.4312356520914486\n",
            "7      \t [6.11416909 9.54959201 5.         0.91943284 1.         0.10968146]. \t  -0.6901756354808697 \t -0.4312356520914486\n",
            "8      \t [ 0.31270412  9.94818438  8.          0.84397658 19.          0.43974503]. \t  -0.5483333357303245 \t -0.4312356520914486\n",
            "9      \t [ 8.76504058  0.46155195 14.          0.61459938 10.          0.19095511]. \t  -0.6902913672662703 \t -0.4312356520914486\n",
            "10     \t [0.  0.  5.  0.5 1.  0.1]. \t  -0.6893514282293631 \t -0.4312356520914486\n",
            "11     \t [ 0.76412088  5.02527282 14.          0.53794557 19.          0.6779987 ]. \t  -0.4862552398836468 \t -0.4312356520914486\n",
            "12     \t [ 8.7007577   8.84182108 11.          0.88074977 10.          0.89589347]. \t  -0.43700927635364534 \t -0.4312356520914486\n",
            "13     \t [1.26882135 8.44257007 8.         0.93932154 4.         0.79202967]. \t  -0.44680835713197 \t -0.4312356520914486\n",
            "14     \t [ 3.38351629  2.89927867 12.          0.76452212  7.          0.61222347]. \t  -0.4809111856529227 \t -0.4312356520914486\n",
            "15     \t [ 7.69267031  0.52709121  5.          0.97712342 11.          0.47111142]. \t  -0.5671609865381374 \t -0.4312356520914486\n",
            "16     \t [ 5.04465897  1.7242392  11.          0.82507363  1.          0.66070113]. \t  -0.48737194712175996 \t -0.4312356520914486\n",
            "17     \t [ 9.35257803  1.01913578 10.          0.61034129 19.          0.44798121]. \t  -0.5514224578726403 \t -0.4312356520914486\n",
            "18     \t [ 3.56621236  4.91536137  7.          0.71255145 19.          0.80604757]. \t  -0.4579607623923458 \t -0.4312356520914486\n",
            "19     \t [ 0.         0.         5.         0.5       12.6024416  0.1      ]. \t  -0.6889398896191611 \t -0.4312356520914486\n",
            "20     \t [9.96752263 8.45749074 7.         0.92612294 6.         0.22771617]. \t  -0.6878594070452866 \t -0.4312356520914486\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.130636454660636"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YD494io_Ur7V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e94b186-5dc3-4ad1-8010-eb60a60b039a"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 10\n",
        "\n",
        "np.random.seed(run_num_10)\n",
        "surrogate_loser_10 = dGaussianProcess(cov_func)\n",
        "\n",
        "X_train10, X_test10, y_train10, y_test10 = train_test_split(X, y, test_size=test_perc, random_state=run_num_10)\n",
        "\n",
        "def f_syn_polarity10(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_10, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train10, y=y_train10).mean())\n",
        "    return operator * score\n",
        "\n",
        "loser_10 = dGPGO(surrogate_loser_10, Acquisition_new(util_loser), f_syn_polarity10, param, n_jobs = -1) # define BayesOpt\n",
        "loser_10.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_loser_10 = loser_10.getResult()[0]\n",
        "params_loser_10['max_depth'] = int(params_loser_10['max_depth'])\n",
        "params_loser_10['min_child_weight'] = int(params_loser_10['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_loser_train10 = xgb.DMatrix(X_train10, y_train10)\n",
        "dX_loser_test10 = xgb.DMatrix(X_test10, y_test10)\n",
        "model_loser_10 = xgb.train(params_loser_10, dX_loser_train10)\n",
        "pred_loser_10 = model_loser_10.predict(dX_loser_test10)\n",
        "\n",
        "rmse_loser_10 = np.sqrt(mean_squared_error(pred_loser_10, y_test10))\n",
        "rmse_loser_10"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 6.53589585  1.15006943  6.          0.93623727 14.          0.13663866]. \t  -0.6929852048256443 \t -0.4483221221388197\n",
            "init   \t [ 3.97194461  2.33132197 14.          0.9533253  19.          0.26403087]. \t  -0.6932702114662359 \t -0.4483221221388197\n",
            "init   \t [ 7.43539415  0.69582081  9.          0.9763222  11.          0.12608349]. \t  -0.6931242497450955 \t -0.4483221221388197\n",
            "init   \t [ 9.82027485  3.39637684  9.          0.78522537 19.          0.87618843]. \t  -0.4483221221388197 \t -0.4483221221388197\n",
            "init   \t [ 0.57576207  5.82646405 12.          0.77704362 18.          0.11865044]. \t  -0.6946096327074635 \t -0.4483221221388197\n",
            "1      \t [ 3.17878299  6.40666671 10.          0.71928645  2.          0.37920433]. \t  -0.6059745513602156 \t -0.4483221221388197\n",
            "2      \t [ 3.37448201  8.10780464 13.          0.50477079 10.          0.914384  ]. \t  \u001b[92m-0.43629727149692393\u001b[0m \t -0.43629727149692393\n",
            "3      \t [ 9.185601    0.61355962 14.          0.74945204  2.          0.47381106]. \t  -0.5791988425691649 \t -0.43629727149692393\n",
            "4      \t [8.35411693 8.91554079 5.         0.73130633 7.         0.67857606]. \t  -0.5771647264709212 \t -0.43629727149692393\n",
            "5      \t [ 6.07704325  9.68005151  5.          0.52932858 19.          0.88896368]. \t  -0.4736105624167554 \t -0.43629727149692393\n",
            "6      \t [ 8.63718494  8.55246455 14.          0.55824531 18.          0.4228152 ]. \t  -0.6085826477793935 \t -0.43629727149692393\n",
            "7      \t [9.97805381 2.33960922 6.         0.72769906 2.         0.16112568]. \t  -0.693543381593374 \t -0.43629727149692393\n",
            "8      \t [0.  0.  5.  0.5 1.  0.1]. \t  -0.6914754878687801 \t -0.43629727149692393\n",
            "9      \t [ 0.03680135  0.50694581 14.          0.91124751  8.          0.87026982]. \t  \u001b[92m-0.4215933675272626\u001b[0m \t -0.4215933675272626\n",
            "10     \t [ 1.86936633  8.33951478  6.          0.64608958 13.          0.87352832]. \t  -0.45750043698066534 \t -0.4215933675272626\n",
            "11     \t [ 9.77077394  8.65786606 12.          0.7706968   5.          0.42456779]. \t  -0.6030224397086726 \t -0.4215933675272626\n",
            "12     \t [ 0.1202295   0.91999335  5.          0.65757131 10.          0.24500243]. \t  -0.6930477249479814 \t -0.4215933675272626\n",
            "13     \t [0.50440288 9.75457432 5.         0.8137431  6.         0.15250773]. \t  -0.6941630128319229 \t -0.4215933675272626\n",
            "14     \t [ 9.91477928  9.35319949  6.          0.75202808 14.          0.13042843]. \t  -0.6936633884299808 \t -0.4215933675272626\n",
            "15     \t [2.78752086e+00 7.83961972e-03 1.10000000e+01 9.72670765e-01\n",
            " 2.00000000e+00 4.67980644e-01]. \t  -0.5809464009440812 \t -0.4215933675272626\n",
            "16     \t [ 0.60051063  0.93658443 10.          0.71787415 13.          0.3650692 ]. \t  -0.6037929489655063 \t -0.4215933675272626\n",
            "17     \t [7.64536232 8.37707893 5.         0.8688534  1.         0.76688319]. \t  -0.5294614889177678 \t -0.4215933675272626\n",
            "18     \t [ 8.58082048  3.39962732 14.          0.7151638   9.          0.76214186]. \t  -0.5049104315367501 \t -0.4215933675272626\n",
            "19     \t [ 8.52646717  0.12245704 14.          0.80155281 15.          0.7100538 ]. \t  -0.5709119331337676 \t -0.4215933675272626\n",
            "20     \t [5.51569281 2.48923068 5.         0.97468437 6.         0.88188042]. \t  -0.4524796762835521 \t -0.4215933675272626\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.436171669561738"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N03Sq0TvUuhp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a85d6076-e55e-4064-ad38-72f1a7b7852d"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 11\n",
        "\n",
        "np.random.seed(run_num_11)\n",
        "surrogate_loser_11 = dGaussianProcess(cov_func)\n",
        "\n",
        "X_train11, X_test11, y_train11, y_test11 = train_test_split(X, y, test_size=test_perc, random_state=run_num_11)\n",
        "\n",
        "def f_syn_polarity11(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_11, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train11, y=y_train11).mean())\n",
        "    return operator * score\n",
        "\n",
        "loser_11 = dGPGO(surrogate_loser_11, Acquisition_new(util_loser), f_syn_polarity11, param, n_jobs = -1) # define BayesOpt\n",
        "loser_11.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_loser_11 = loser_11.getResult()[0]\n",
        "params_loser_11['max_depth'] = int(params_loser_11['max_depth'])\n",
        "params_loser_11['min_child_weight'] = int(params_loser_11['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_loser_train11 = xgb.DMatrix(X_train11, y_train11)\n",
        "dX_loser_test11 = xgb.DMatrix(X_test11, y_test11)\n",
        "model_loser_11 = xgb.train(params_loser_11, dX_loser_train11)\n",
        "pred_loser_11 = model_loser_11.predict(dX_loser_test11)\n",
        "\n",
        "rmse_loser_11 = np.sqrt(mean_squared_error(pred_loser_11, y_test11))\n",
        "rmse_loser_11"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 9.81035663  8.2124785   9.          0.68487173 15.          0.28207988]. \t  -0.7133787013189323 \t -0.47317045732943896\n",
            "init   \t [ 4.91906771  1.38558332  6.          0.618878   19.          0.10263204]. \t  -0.7112023662943636 \t -0.47317045732943896\n",
            "init   \t [ 5.23426175  1.89971159 13.          0.76313779 17.          0.46597853]. \t  -0.47317045732943896 \t -0.47317045732943896\n",
            "init   \t [ 4.0204056   3.75167577  9.          0.86204818 17.          0.26560056]. \t  -0.7132057194891379 \t -0.47317045732943896\n",
            "init   \t [8.8704459  5.64917815 5.         0.51406171 1.         0.82826731]. \t  -0.48048685248123163 \t -0.47317045732943896\n",
            "1      \t [0.08349406 1.89250335 6.         0.66386636 1.         0.88641723]. \t  \u001b[92m-0.4484239383130805\u001b[0m \t -0.4484239383130805\n",
            "2      \t [ 5.35837408  0.31510164 14.          0.97519815  6.          0.3034681 ]. \t  -0.607905230502132 \t -0.4484239383130805\n",
            "3      \t [ 4.66315557  9.30076713 11.          0.83534598  8.          0.21303816]. \t  -0.7135282416096518 \t -0.4484239383130805\n",
            "4      \t [9.02046973 1.16865322 7.         0.6807515  8.         0.2962635 ]. \t  -0.6068266615066733 \t -0.4484239383130805\n",
            "5      \t [0.79988529 9.85424348 5.         0.5743044  3.         0.67901433]. \t  -0.48209943859810334 \t -0.4484239383130805\n",
            "6      \t [ 0.02057794  7.14089873 13.          0.90855163  1.          0.61823707]. \t  -0.45805892074172583 \t -0.4484239383130805\n",
            "7      \t [ 0.97603689  3.49871064  9.          0.91846837 10.          0.35158251]. \t  -0.6085109103402477 \t -0.4484239383130805\n",
            "8      \t [ 9.27240029  9.57413463 14.          0.58544186  3.          0.73779027]. \t  -0.45267877796490286 \t -0.4484239383130805\n",
            "9      \t [ 0.4027375   9.38447167 12.          0.94595612 15.          0.85310792]. \t  -0.451136347669006 \t -0.4484239383130805\n",
            "10     \t [ 1.3440527   8.97080429  5.          0.80277597 12.          0.63284111]. \t  -0.48349651102568225 \t -0.4484239383130805\n",
            "11     \t [ 7.94225687  3.68352192 13.          0.94902199 11.          0.95471054]. \t  \u001b[92m-0.422771965919715\u001b[0m \t -0.422771965919715\n",
            "12     \t [9.20332934 9.98167579 5.         0.57575366 6.         0.3717594 ]. \t  -0.6046860081027627 \t -0.422771965919715\n",
            "13     \t [5.68758712 2.62033511 9.         0.67614771 3.         0.67632008]. \t  -0.47052528509931824 \t -0.422771965919715\n",
            "14     \t [2.94143112e-03 7.03829535e-01 1.30000000e+01 5.98722168e-01\n",
            " 2.00000000e+00 1.01335355e-01]. \t  -0.7122743790062083 \t -0.422771965919715\n",
            "15     \t [ 0.81561387  9.31451412  5.          0.70594876 19.          0.98284163]. \t  -0.45988817082889427 \t -0.422771965919715\n",
            "16     \t [ 9.8968071   1.89203294 13.          0.7888373   2.          0.82539259]. \t  -0.45294282860990764 \t -0.422771965919715\n",
            "17     \t [ 8.37835592  2.52983026  5.          0.97806207 14.          0.6500905 ]. \t  -0.48098936825483635 \t -0.422771965919715\n",
            "18     \t [ 0.56769151  4.18340522 14.          0.91967306 13.          0.92813967]. \t  \u001b[92m-0.41912566738485735\u001b[0m \t -0.41912566738485735\n",
            "19     \t [ 4.05595265  0.          5.          0.5        10.09765104  0.1       ]. \t  -0.7116217008807669 \t -0.41912566738485735\n",
            "20     \t [ 8.98906297  7.01351387 13.          0.77197065 19.          0.47265223]. \t  -0.475345282554151 \t -0.41912566738485735\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.3224670398414355"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_nP9lQjUztV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff7968e3-e93c-4892-ddb7-83c0923ca4d0"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 12\n",
        "\n",
        "np.random.seed(run_num_12)\n",
        "surrogate_loser_12 = dGaussianProcess(cov_func)\n",
        "\n",
        "X_train12, X_test12, y_train12, y_test12 = train_test_split(X, y, test_size=test_perc, random_state=run_num_12)\n",
        "\n",
        "def f_syn_polarity12(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_12, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train12, y=y_train12).mean())\n",
        "    return operator * score\n",
        "\n",
        "loser_12 = dGPGO(surrogate_loser_12, Acquisition_new(util_loser), f_syn_polarity12, param, n_jobs = -1) # define BayesOpt\n",
        "loser_12.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_loser_12 = loser_12.getResult()[0]\n",
        "params_loser_12['max_depth'] = int(params_loser_12['max_depth'])\n",
        "params_loser_12['min_child_weight'] = int(params_loser_12['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_loser_train12 = xgb.DMatrix(X_train12, y_train12)\n",
        "dX_loser_test12 = xgb.DMatrix(X_test12, y_test12)\n",
        "model_loser_12 = xgb.train(params_loser_12, dX_loser_train12)\n",
        "pred_loser_12 = model_loser_12.predict(dX_loser_test12)\n",
        "\n",
        "rmse_loser_12 = np.sqrt(mean_squared_error(pred_loser_12, y_test12))\n",
        "rmse_loser_12"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 1.9151945   6.22108771  9.          0.89267929 16.          0.87460279]. \t  -0.4420077321695894 \t -0.4420077321695894\n",
            "init   \t [ 1.50636966  1.98518759 11.          0.67890863 17.          0.5381501 ]. \t  -0.5998539491151165 \t -0.4420077321695894\n",
            "init   \t [ 3.31015428  8.02639569  7.          0.75154158 13.          0.11992952]. \t  -0.7324503600335561 \t -0.4420077321695894\n",
            "init   \t [ 2.90728553  2.46394443  5.          0.94461307 17.          0.43194161]. \t  -0.5995790338913064 \t -0.4420077321695894\n",
            "init   \t [ 9.33140102  6.51378143  8.          0.76907392 18.          0.38515251]. \t  -0.5982523495604211 \t -0.4420077321695894\n",
            "1      \t [5.68098653 8.6912739  6.         0.52027807 1.         0.51631894]. \t  -0.5999713561677879 \t -0.4420077321695894\n",
            "2      \t [ 4.61089026  7.78540446 14.          0.61560224  6.          0.60910898]. \t  -0.5310432089168232 \t -0.4420077321695894\n",
            "3      \t [ 7.71510638  1.71782806 11.          0.9748513   2.          0.33442931]. \t  -0.5954035506799399 \t -0.4420077321695894\n",
            "4      \t [ 0.42901773  1.02682351 10.          0.62582208  7.          0.40519792]. \t  -0.600033100066767 \t -0.4420077321695894\n",
            "5      \t [ 9.48743388  2.23858916 14.          0.84020064 13.          0.41361313]. \t  -0.5921503481987865 \t -0.4420077321695894\n",
            "6      \t [ 7.09099339  0.08964775  6.          0.68605854 11.          0.16899055]. \t  -0.7330665763811677 \t -0.4420077321695894\n",
            "7      \t [0.  0.  5.  0.5 1.  0.1]. \t  -0.7334076855722197 \t -0.4420077321695894\n",
            "8      \t [ 4.8875203   9.38595347 14.          0.7902783  19.          0.67669595]. \t  -0.5289545232453541 \t -0.4420077321695894\n",
            "9      \t [ 0.50387213  8.28483675 14.          0.60639358 12.          0.19061796]. \t  -0.7343015225078143 \t -0.4420077321695894\n",
            "10     \t [ 1.81216609  5.49695067 10.52048254  0.5         1.          0.1       ]. \t  -0.7336073261479503 \t -0.4420077321695894\n",
            "11     \t [ 9.8847664   8.41680653 10.          0.63099084  9.          0.83145315]. \t  -0.5042925534600994 \t -0.4420077321695894\n",
            "12     \t [ 0.          0.          5.          0.5        11.52941427  0.1       ]. \t  -0.7334775686570922 \t -0.4420077321695894\n",
            "13     \t [9.43877299 3.79912488 5.         0.53841687 4.         0.20857531]. \t  -0.7351835256464294 \t -0.4420077321695894\n",
            "14     \t [0.41721862 8.87551388 5.         0.69088851 6.         0.86973069]. \t  -0.468169334397084 \t -0.4420077321695894\n",
            "15     \t [ 9.68675553  3.68641979 14.          0.63310045  7.          0.87631691]. \t  \u001b[92m-0.44169445577667543\u001b[0m \t -0.44169445577667543\n",
            "16     \t [ 9.64220337  0.08866879  7.          0.55877602 19.          0.80403324]. \t  -0.5167375732665379 \t -0.44169445577667543\n",
            "17     \t [ 8.95792842  8.74686238 12.          0.60340754  1.          0.64076061]. \t  -0.5341583767040248 \t -0.44169445577667543\n",
            "18     \t [ 3.80794615  1.17683    14.          0.6618018  11.          0.78665992]. \t  -0.497041017578099 \t -0.44169445577667543\n",
            "19     \t [ 0.60473619  9.7310361   5.          0.764657   19.          0.44899982]. \t  -0.602200593591361 \t -0.44169445577667543\n",
            "20     \t [ 7.25143535  1.86402411 14.          0.59310957 19.          0.53879904]. \t  -0.5987977809908415 \t -0.44169445577667543\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3.9962203731848227"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDI2Bi9vU05U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "caf41f3c-d016-43f8-84a2-147b907eee91"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 13\n",
        "\n",
        "np.random.seed(run_num_13)\n",
        "surrogate_loser_13 = dGaussianProcess(cov_func)\n",
        "\n",
        "X_train13, X_test13, y_train13, y_test13 = train_test_split(X, y, test_size=test_perc, random_state=run_num_13)\n",
        "\n",
        "def f_syn_polarity13(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_13, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train13, y=y_train13).mean())\n",
        "    return operator * score\n",
        "\n",
        "loser_13 = dGPGO(surrogate_loser_13, Acquisition_new(util_loser), f_syn_polarity13, param, n_jobs = -1) # define BayesOpt\n",
        "loser_13.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_loser_13 = loser_13.getResult()[0]\n",
        "params_loser_13['max_depth'] = int(params_loser_13['max_depth'])\n",
        "params_loser_13['min_child_weight'] = int(params_loser_13['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_loser_train13 = xgb.DMatrix(X_train13, y_train13)\n",
        "dX_loser_test13 = xgb.DMatrix(X_test13, y_test13)\n",
        "model_loser_13 = xgb.train(params_loser_13, dX_loser_train13)\n",
        "pred_loser_13 = model_loser_13.predict(dX_loser_test13)\n",
        "\n",
        "rmse_loser_13 = np.sqrt(mean_squared_error(pred_loser_13, y_test13))\n",
        "rmse_loser_13"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 0.30967343  2.54282073  6.          0.9850871  17.          0.50848087]. \t  -0.5693403578256062 \t -0.4459810811310791\n",
            "init   \t [ 6.95119021  3.25593423 14.          0.8602375   8.          0.82055713]. \t  -0.5112217679187058 \t -0.4459810811310791\n",
            "init   \t [ 5.09252675  8.19921021 10.          0.78792002 16.          0.382158  ]. \t  -0.581693558072188 \t -0.4459810811310791\n",
            "init   \t [ 5.4465191   3.19841462 14.          0.90654581  5.          0.10359415]. \t  -0.6818818523119694 \t -0.4459810811310791\n",
            "init   \t [ 9.8393864   9.89619708  9.          0.63387763 10.          0.88979174]. \t  -0.4459810811310791 \t -0.4459810811310791\n",
            "1      \t [ 9.27118046  9.4344906  10.          0.71578295  1.          0.5463837 ]. \t  -0.5604994598662556 \t -0.4459810811310791\n",
            "2      \t [0.21910824 0.53051294 5.         0.62754372 4.         0.55299122]. \t  -0.578556985871312 \t -0.4459810811310791\n",
            "3      \t [0.32904954 9.80442032 8.         0.7285653  2.         0.94646442]. \t  \u001b[92m-0.43885135786496293\u001b[0m \t -0.43885135786496293\n",
            "4      \t [ 8.54553338  0.49573541 12.          0.81406546 19.          0.20999807]. \t  -0.6795516887212301 \t -0.43885135786496293\n",
            "5      \t [9.40145549 1.43629395 5.         0.63655396 1.         0.63002732]. \t  -0.5662893508870628 \t -0.43885135786496293\n",
            "6      \t [ 8.7238127   0.85665784  6.          0.75815556 12.          0.87180469]. \t  -0.4644522650882914 \t -0.43885135786496293\n",
            "7      \t [ 0.11400844  9.5774521   6.          0.90472821 11.          0.18513309]. \t  -0.6793968950126906 \t -0.43885135786496293\n",
            "8      \t [ 1.95513735  0.12334702 13.          0.54467307 14.          0.86324917]. \t  \u001b[92m-0.43794830902637205\u001b[0m \t -0.43794830902637205\n",
            "9      \t [ 2.89959158  8.10209185 14.          0.9094105   1.          0.86003598]. \t  \u001b[92m-0.42582758559630707\u001b[0m \t -0.42582758559630707\n",
            "10     \t [ 0.63533588  7.88973169 13.          0.71777205  8.          0.22258282]. \t  -0.6847473450330067 \t -0.42582758559630707\n",
            "11     \t [4.59853484 7.19189873 5.         0.92777191 6.         0.5760847 ]. \t  -0.5604008171822471 \t -0.42582758559630707\n",
            "12     \t [ 9.78022669  6.34740254  5.          0.81941991 15.          0.17866883]. \t  -0.6810440437950237 \t -0.42582758559630707\n",
            "13     \t [ 0.44924058  6.01817024 13.          0.62113095 19.          0.22629481]. \t  -0.6838258323776948 \t -0.42582758559630707\n",
            "14     \t [2.08434216 0.20801851 5.85566131 0.5        9.85566131 0.1       ]. \t  -0.6829533902190683 \t -0.42582758559630707\n",
            "15     \t [ 8.93459678  5.75013258 14.          0.86506851 14.          0.11026757]. \t  -0.6798789215383618 \t -0.42582758559630707\n",
            "16     \t [ 1.2199304   9.32862427  5.          0.98694875 18.          0.69275841]. \t  -0.5593626212561581 \t -0.42582758559630707\n",
            "17     \t [7.30733696 1.06961871 9.         0.6583612  6.         0.41488291]. \t  -0.5857440675365031 \t -0.42582758559630707\n",
            "18     \t [ 0.25964514  2.37009305 11.          0.85577938  4.          0.42985865]. \t  -0.556481072440868 \t -0.42582758559630707\n",
            "19     \t [ 5.54919942  8.87232522  5.          0.84416975 12.          0.31830005]. \t  -0.5998464916577737 \t -0.42582758559630707\n",
            "20     \t [ 9.03439965  9.39442441 14.          0.98025678 19.          0.55921955]. \t  -0.550861248380592 \t -0.42582758559630707\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.159065314760995"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z2F_Q194U3uu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74646d1b-c330-4f14-bd86-65bf3b923f62"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 14\n",
        "\n",
        "np.random.seed(run_num_14)\n",
        "surrogate_loser_14 = dGaussianProcess(cov_func)\n",
        "\n",
        "X_train14, X_test14, y_train14, y_test14 = train_test_split(X, y, test_size=test_perc, random_state=run_num_14)\n",
        "\n",
        "def f_syn_polarity14(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_14, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train14, y=y_train14).mean())\n",
        "    return operator * score\n",
        "\n",
        "loser_14 = dGPGO(surrogate_loser_14, Acquisition_new(util_loser), f_syn_polarity14, param, n_jobs = -1) # define BayesOpt\n",
        "loser_14.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_loser_14 = loser_14.getResult()[0]\n",
        "params_loser_14['max_depth'] = int(params_loser_14['max_depth'])\n",
        "params_loser_14['min_child_weight'] = int(params_loser_14['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_loser_train14 = xgb.DMatrix(X_train14, y_train14)\n",
        "dX_loser_test14 = xgb.DMatrix(X_test14, y_test14)\n",
        "model_loser_14 = xgb.train(params_loser_14, dX_loser_train14)\n",
        "pred_loser_14 = model_loser_14.predict(dX_loser_test14)\n",
        "\n",
        "rmse_loser_14 = np.sqrt(mean_squared_error(pred_loser_14, y_test14))\n",
        "rmse_loser_14"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 8.59560606  1.64569495 11.          0.89021195 15.          0.4857008 ]. \t  -0.6102441482204597 \t -0.4517949441804544\n",
            "init   \t [ 0.57460093  9.25007434  5.          0.56647642 17.          0.7533211 ]. \t  -0.5614002279921607 \t -0.4517949441804544\n",
            "init   \t [1.90674396 4.32019547 9.         0.93708225 3.         0.91775905]. \t  -0.4517949441804544 \t -0.4517949441804544\n",
            "init   \t [ 0.36770475  8.43368681 14.          0.6826844  15.          0.6291966 ]. \t  -0.5407924410004543 \t -0.4517949441804544\n",
            "init   \t [ 5.33840324  0.449182   14.          0.52045439  2.          0.48604556]. \t  -0.6242722768101235 \t -0.4517949441804544\n",
            "1      \t [ 9.54492219  9.07631824  8.          0.54579372 12.          0.38295996]. \t  -0.6262431675183743 \t -0.4517949441804544\n",
            "2      \t [9.53270461 0.83608685 6.         0.99976104 3.         0.60747265]. \t  -0.5533922160484371 \t -0.4517949441804544\n",
            "3      \t [ 1.15250646  0.51569303  5.          0.69322415 17.          0.16291682]. \t  -0.6917729046487754 \t -0.4517949441804544\n",
            "4      \t [ 6.37447116  9.98862868 14.          0.72259558  4.          0.61899393]. \t  -0.5434760859463281 \t -0.4517949441804544\n",
            "5      \t [ 8.08070558  7.65123431 14.          0.59104644 19.          0.9302005 ]. \t  -0.45873776977159153 \t -0.4517949441804544\n",
            "6      \t [ 6.26901123  2.37239677  5.          0.95063922 11.          0.52876485]. \t  -0.6158458956422692 \t -0.4517949441804544\n",
            "7      \t [1.0907871  9.11150389 5.         0.58744161 9.         0.87439163]. \t  -0.4933476055445382 \t -0.4517949441804544\n",
            "8      \t [ 2.21919917  0.31865167 14.          0.54399684 14.          0.18056849]. \t  -0.6942842535038787 \t -0.4517949441804544\n",
            "9      \t [ 7.34435618  8.39108681  7.          0.81547994 19.          0.62767067]. \t  -0.546522525570801 \t -0.4517949441804544\n",
            "10     \t [ 6.57539862  4.15362663 14.          0.96053826  9.          0.2080482 ]. \t  -0.691194495047777 \t -0.4517949441804544\n",
            "11     \t [6.00529466 8.33461243 6.         0.97581801 4.         0.31582778]. \t  -0.6226835813168086 \t -0.4517949441804544\n",
            "12     \t [0.56405811 0.         5.         0.5        7.860223   0.1       ]. \t  -0.6942634907462908 \t -0.4517949441804544\n",
            "13     \t [0.  0.  5.  0.5 1.  0.1]. \t  -0.6943016112587715 \t -0.4517949441804544\n",
            "14     \t [ 0.84209312  7.86749962 14.          0.8648448   1.          0.1570582 ]. \t  -0.6937301125121068 \t -0.4517949441804544\n",
            "15     \t [ 1.22436035  9.67689861 12.          0.7460835   9.          0.69284382]. \t  -0.5399272231860202 \t -0.4517949441804544\n",
            "16     \t [ 4.4386325   0.37972087 10.          0.61118217  8.          0.15198669]. \t  -0.6912722224579347 \t -0.4517949441804544\n",
            "17     \t [ 7.0496565   9.86456792 14.          0.62416475 11.          0.68843039]. \t  -0.5429606831910581 \t -0.4517949441804544\n",
            "18     \t [ 3.32112913  6.27078367  9.          0.98238085 14.          0.82374888]. \t  -0.5322541144293694 \t -0.4517949441804544\n",
            "19     \t [ 9.66203958  6.20819976 11.          0.71875858  5.          0.43480687]. \t  -0.6129974118264951 \t -0.4517949441804544\n",
            "20     \t [ 0.13684155  3.86104617 14.          0.82673561  9.          0.20283653]. \t  -0.6919355943160154 \t -0.4517949441804544\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.334202643456426"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Po5wImJaU6VC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea8dc7bd-8bc0-44db-9239-baa3b708c484"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 15\n",
        "\n",
        "np.random.seed(run_num_15)\n",
        "surrogate_loser_15 = dGaussianProcess(cov_func)\n",
        "\n",
        "X_train15, X_test15, y_train15, y_test15 = train_test_split(X, y, test_size=test_perc, random_state=run_num_15)\n",
        "\n",
        "def f_syn_polarity15(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_15, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train15, y=y_train15).mean())\n",
        "    return operator * score\n",
        "\n",
        "loser_15 = dGPGO(surrogate_loser_15, Acquisition_new(util_loser), f_syn_polarity15, param, n_jobs = -1) # define BayesOpt\n",
        "loser_15.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_loser_15 = loser_15.getResult()[0]\n",
        "params_loser_15['max_depth'] = int(params_loser_15['max_depth'])\n",
        "params_loser_15['min_child_weight'] = int(params_loser_15['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_loser_train15 = xgb.DMatrix(X_train15, y_train15)\n",
        "dX_loser_test15 = xgb.DMatrix(X_test15, y_test15)\n",
        "model_loser_15 = xgb.train(params_loser_15, dX_loser_train15)\n",
        "pred_loser_15 = model_loser_15.predict(dX_loser_test15)\n",
        "\n",
        "rmse_loser_15 = np.sqrt(mean_squared_error(pred_loser_15, y_test15))\n",
        "rmse_loser_15"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 5.64315424  8.97297685  6.          0.81493635 11.          0.33008564]. \t  -0.5946480910132583 \t -0.4392277798535851\n",
            "init   \t [9.51455383 9.60803032 5.         0.68485728 1.         0.30145081]. \t  -0.5968379804084254 \t -0.4392277798535851\n",
            "init   \t [ 8.11809889  6.32508536  7.          0.61766329 19.          0.51383159]. \t  -0.5092318398204285 \t -0.4392277798535851\n",
            "init   \t [ 7.28571251  5.36071389 10.          0.76240297 12.          0.12035683]. \t  -0.6346120183758519 \t -0.4392277798535851\n",
            "init   \t [ 6.80847768  3.59767194 14.          0.51365585 16.          0.89730673]. \t  -0.4392277798535851 \t -0.4392277798535851\n",
            "1      \t [ 2.28792065  8.58421878 12.          0.67551126  1.          0.37971264]. \t  -0.596161946051267 \t -0.4392277798535851\n",
            "2      \t [ 9.35473751  1.48895841 12.          0.55362474  4.          0.11106606]. \t  -0.6327865627642436 \t -0.4392277798535851\n",
            "3      \t [3.91572323 2.23174129 6.         0.50937407 1.         0.51336364]. \t  -0.5160141940687206 \t -0.4392277798535851\n",
            "4      \t [ 0.74184801  3.17830864 14.          0.90502664  8.          0.31107615]. \t  -0.5905073342805591 \t -0.4392277798535851\n",
            "5      \t [ 1.33182259  0.8125478   9.          0.94558338 18.          0.71777254]. \t  -0.4467705016632985 \t -0.4392277798535851\n",
            "6      \t [ 0.48538268  1.39730148  6.          0.8238236  11.          0.45741795]. \t  -0.5144696373796631 \t -0.4392277798535851\n",
            "7      \t [ 0.57885492  9.35143843  6.          0.69337529 17.          0.15510814]. \t  -0.6331051904120661 \t -0.4392277798535851\n",
            "8      \t [0.24960251 9.45540541 5.         0.62890215 3.         0.73551484]. \t  -0.47327438015853385 \t -0.4392277798535851\n",
            "9      \t [ 8.23428085  9.77240559 13.          0.85977854  8.          0.93876208]. \t  \u001b[92m-0.4281095031928478\u001b[0m \t -0.4281095031928478\n",
            "10     \t [ 1.35750397  9.83931966 14.          0.92698791 15.          0.2272101 ]. \t  -0.6341880539033447 \t -0.4281095031928478\n",
            "11     \t [ 9.85965179  9.04589828 13.          0.95564786 19.          0.1426314 ]. \t  -0.632071954329094 \t -0.4281095031928478\n",
            "12     \t [9.06175259 4.11518452 5.         0.87621161 6.         0.52300377]. \t  -0.5192107686497183 \t -0.4281095031928478\n",
            "13     \t [ 0.15053668  0.43786928 13.          0.62231098  1.          0.88145445]. \t  -0.43523183945633176 \t -0.4281095031928478\n",
            "14     \t [ 7.18775342  0.2062489   5.          0.98239776 17.          0.28043211]. \t  -0.634545784625454 \t -0.4281095031928478\n",
            "15     \t [0.10178544 7.49949902 9.         0.60008085 8.         0.35520416]. \t  -0.5902884239300654 \t -0.4281095031928478\n",
            "16     \t [0.22091899 0.18516021 8.21358087 0.5        5.21358087 0.1       ]. \t  -0.6326798838280104 \t -0.4281095031928478\n",
            "17     \t [ 8.4324691   8.27251391 11.          0.71354715  2.          0.54685464]. \t  -0.498601956775111 \t -0.4281095031928478\n",
            "18     \t [ 2.12400437  9.06769045 14.          0.70585245  9.          0.90846764]. \t  -0.4286407197787245 \t -0.4281095031928478\n",
            "19     \t [9.58390631 0.99765082 6.         0.72021045 1.         0.39555686]. \t  -0.5960083204162412 \t -0.4281095031928478\n",
            "20     \t [ 9.96011127  1.55235537 14.          0.63004724 11.          0.14483121]. \t  -0.6331081546428745 \t -0.4281095031928478\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.189953800077167"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HrAQN-pU9Qo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2816be6-1437-4f0d-b1b1-ba0c98646ad2"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 16\n",
        "\n",
        "np.random.seed(run_num_16)\n",
        "surrogate_loser_16 = dGaussianProcess(cov_func)\n",
        "\n",
        "X_train16, X_test16, y_train16, y_test16 = train_test_split(X, y, test_size=test_perc, random_state=run_num_16)\n",
        "\n",
        "def f_syn_polarity16(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_16, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train16, y=y_train16).mean())\n",
        "    return operator * score\n",
        "\n",
        "loser_16 = dGPGO(surrogate_loser_16, Acquisition_new(util_loser), f_syn_polarity16, param, n_jobs = -1) # define BayesOpt\n",
        "loser_16.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_loser_16 = loser_16.getResult()[0]\n",
        "params_loser_16['max_depth'] = int(params_loser_16['max_depth'])\n",
        "params_loser_16['min_child_weight'] = int(params_loser_16['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_loser_train16 = xgb.DMatrix(X_train16, y_train16)\n",
        "dX_loser_test16 = xgb.DMatrix(X_test16, y_test16)\n",
        "model_loser_16 = xgb.train(params_loser_16, dX_loser_train16)\n",
        "pred_loser_16 = model_loser_16.predict(dX_loser_test16)\n",
        "\n",
        "rmse_loser_16 = np.sqrt(mean_squared_error(pred_loser_16, y_test16))\n",
        "rmse_loser_16"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [8.63948332 3.16366002 7.         0.75395583 1.         0.56999262]. \t  -0.5713220948397237 \t -0.4331621293825035\n",
            "init   \t [ 3.3426621   9.77029134 14.          0.93751218 17.          0.80549151]. \t  -0.4331621293825035 \t -0.4331621293825035\n",
            "init   \t [ 3.36871073  4.8308288  13.          0.91984982 14.          0.48829226]. \t  -0.5561611494046279 \t -0.4331621293825035\n",
            "init   \t [ 8.84540736  1.59826081  8.          0.75772888 14.          0.29029227]. \t  -0.6835926126526815 \t -0.4331621293825035\n",
            "init   \t [ 7.74981136  8.34377636 13.          0.83182409 15.          0.19378622]. \t  -0.7677243885128009 \t -0.4331621293825035\n",
            "1      \t [ 0.60074712  1.1402995   5.          0.75926551 10.          0.30612858]. \t  -0.6844266759595645 \t -0.4331621293825035\n",
            "2      \t [ 3.6191694   8.20490707  5.          0.81280746 16.          0.99306688]. \t  -0.4581574120389309 \t -0.4331621293825035\n",
            "3      \t [ 0.55111138  3.69634376 11.          0.53864409  3.          0.92267876]. \t  -0.43468988385343615 \t -0.4331621293825035\n",
            "4      \t [9.52987381 8.92087345 5.         0.95441282 7.         0.10412548]. \t  -0.7679863416880384 \t -0.4331621293825035\n",
            "5      \t [ 8.31345174  8.15216774 14.          0.81052242  4.          0.5053732 ]. \t  -0.5586554806770512 \t -0.4331621293825035\n",
            "6      \t [ 7.65476591  1.74203379 12.          0.78837413  7.          0.71460419]. \t  -0.4382687724745528 \t -0.4331621293825035\n",
            "7      \t [1.68021993 9.25252958 9.         0.78643474 9.         0.55639556]. \t  -0.5630452959468858 \t -0.4331621293825035\n",
            "8      \t [0.  0.  5.  0.5 1.  0.1]. \t  -0.7670392049994392 \t -0.4331621293825035\n",
            "9      \t [ 2.6903245   0.3560009   7.          0.98255651 19.          0.9851534 ]. \t  -0.4412750880856537 \t -0.4331621293825035\n",
            "10     \t [ 0.4751313   0.18137204 13.          0.70186285 10.          0.37625565]. \t  -0.68582313055112 \t -0.4331621293825035\n",
            "11     \t [ 8.54895835  0.69961229 14.          0.57654595 19.          0.20454627]. \t  -0.7677953049378372 \t -0.4331621293825035\n",
            "12     \t [ 8.76905299  8.15213676  8.          0.5845007  19.          0.27314045]. \t  -0.7678874315811665 \t -0.4331621293825035\n",
            "13     \t [ 1.48533436  9.57311484 14.          0.8193199   3.          0.99329178]. \t  \u001b[92m-0.4185372839160708\u001b[0m \t -0.4185372839160708\n",
            "14     \t [4.49490325 9.22009337 8.         0.71314799 3.         0.58255672]. \t  -0.48560746523878195 \t -0.4185372839160708\n",
            "15     \t [ 0.76287087  0.49621071 13.          0.81150873 18.          0.76762748]. \t  -0.4363142295126112 \t -0.4185372839160708\n",
            "16     \t [0.56061777 6.48476685 5.         0.81826035 5.         0.59167243]. \t  -0.5183862929687044 \t -0.4185372839160708\n",
            "17     \t [8.8152352  0.5898958  5.         0.831707   9.         0.92137997]. \t  -0.45519808186832184 \t -0.4185372839160708\n",
            "18     \t [ 8.76208148  1.79224635  5.          0.52306305 19.          0.78632749]. \t  -0.4871095639687443 \t -0.4185372839160708\n",
            "19     \t [ 9.63332031  8.534049   10.          0.55556169 10.          0.35894848]. \t  -0.6847217417593005 \t -0.4185372839160708\n",
            "20     \t [ 4.52692506  0.27496659 10.          0.6345842   1.          0.46474283]. \t  -0.5631095705887621 \t -0.4185372839160708\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.361483761377183"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXelbcAVVCqO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a108936-09af-461d-ebaf-3026a1086846"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 17\n",
        "\n",
        "np.random.seed(run_num_17)\n",
        "surrogate_loser_17 = dGaussianProcess(cov_func)\n",
        "\n",
        "X_train17, X_test17, y_train17, y_test17 = train_test_split(X, y, test_size=test_perc, random_state=run_num_17)\n",
        "\n",
        "def f_syn_polarity17(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_17, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train17, y=y_train17).mean())\n",
        "    return operator * score\n",
        "\n",
        "loser_17 = dGPGO(surrogate_loser_17, Acquisition_new(util_loser), f_syn_polarity17, param, n_jobs = -1) # define BayesOpt\n",
        "loser_17.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_loser_17 = loser_17.getResult()[0]\n",
        "params_loser_17['max_depth'] = int(params_loser_17['max_depth'])\n",
        "params_loser_17['min_child_weight'] = int(params_loser_17['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_loser_train17 = xgb.DMatrix(X_train17, y_train17)\n",
        "dX_loser_test17 = xgb.DMatrix(X_test17, y_test17)\n",
        "model_loser_17 = xgb.train(params_loser_17, dX_loser_train17)\n",
        "pred_loser_17 = model_loser_17.predict(dX_loser_test17)\n",
        "\n",
        "rmse_loser_17 = np.sqrt(mean_squared_error(pred_loser_17, y_test17))\n",
        "rmse_loser_17"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 1.85575275  3.86599303 13.          0.85645882  2.          0.78699813]. \t  -0.4597056260580034 \t -0.4597056260580034\n",
            "init   \t [1.97780831 2.92527676 8.         0.92268607 9.         0.39017299]. \t  -0.5687207024308397 \t -0.4597056260580034\n",
            "init   \t [ 7.16656007  1.028324    5.          0.96975483 15.          0.90932849]. \t  -0.4688025715067707 \t -0.4597056260580034\n",
            "init   \t [ 2.46007304  9.53596657  7.          0.71704714 11.          0.17659118]. \t  -0.6983036523024195 \t -0.4597056260580034\n",
            "init   \t [3.44161135 9.92427612 8.         0.96331165 2.         0.29160186]. \t  -0.5675253504862205 \t -0.4597056260580034\n",
            "1      \t [ 9.98828606  3.49693094 13.          0.57156882  8.          0.14604355]. \t  -0.6992595730796743 \t -0.4597056260580034\n",
            "2      \t [ 6.95619362  6.95953721 12.          0.92317057 19.          0.34981799]. \t  -0.5628121282495753 \t -0.4597056260580034\n",
            "3      \t [8.58397417 0.63190934 7.         0.80049626 2.         0.98851802]. \t  \u001b[92m-0.453932525933606\u001b[0m \t -0.453932525933606\n",
            "4      \t [ 1.79054908  0.97767001 14.          0.72051649 17.          0.75055942]. \t  -0.4586950539387324 \t -0.453932525933606\n",
            "5      \t [0.  0.  5.  0.5 1.  0.1]. \t  -0.6990710246949499 \t -0.453932525933606\n",
            "6      \t [ 0.19331023  6.0899655   6.          0.97237498 19.          0.59441475]. \t  -0.54171231691412 \t -0.453932525933606\n",
            "7      \t [ 8.95440724  5.31304705 13.          0.73043408  1.          0.56398539]. \t  -0.5373247826064305 \t -0.453932525933606\n",
            "8      \t [ 8.26790477  9.9524702   5.          0.59716297 16.          0.88254342]. \t  -0.47691281251538375 \t -0.453932525933606\n",
            "9      \t [ 0.90443942  7.22834774 14.          0.92580232 12.          0.13465218]. \t  -0.6965996897404226 \t -0.453932525933606\n",
            "10     \t [ 8.52285971  0.31262108 12.          0.8797653  16.          0.34954268]. \t  -0.5627660085466504 \t -0.453932525933606\n",
            "11     \t [ 7.87455923  9.60434668 13.          0.6097695  11.          0.18672661]. \t  -0.6988278637935389 \t -0.453932525933606\n",
            "12     \t [7.77469206 6.81026607 6.         0.84173013 6.         0.4321758 ]. \t  -0.5485988100854867 \t -0.453932525933606\n",
            "13     \t [ 1.78559475  9.94726594 14.          0.84124706  5.          0.41412983]. \t  -0.5643771257102934 \t -0.453932525933606\n",
            "14     \t [ 3.55475249  2.77437165 14.          0.64501595  8.          0.71111614]. \t  -0.5328675498676292 \t -0.453932525933606\n",
            "15     \t [9.7645138  6.96943344 7.         0.64264761 1.         0.5601612 ]. \t  -0.5443333775093777 \t -0.453932525933606\n",
            "16     \t [ 1.13635919  9.59512209 12.          0.5730343  19.          0.36892674]. \t  -0.5697966935770927 \t -0.453932525933606\n",
            "17     \t [6.7007636  0.         5.         0.5        9.02769561 0.1       ]. \t  -0.6997654771340948 \t -0.453932525933606\n",
            "18     \t [2.67758022 4.58792547 5.         0.80193375 3.         0.36129163]. \t  -0.5801383609237367 \t -0.453932525933606\n",
            "19     \t [ 0.          0.11898767  8.66620882  0.5        15.66620882  0.1       ]. \t  -0.6994511860723951 \t -0.453932525933606\n",
            "20     \t [ 4.7028715   4.84232296 11.          0.86477956 14.          0.62900428]. \t  -0.534274537118095 \t -0.453932525933606\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.347883948412589"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJG2fAtAVFDZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3bd47ae-3952-4470-d637-4e538231f1d1"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 18\n",
        "\n",
        "np.random.seed(run_num_18)\n",
        "surrogate_loser_18 = dGaussianProcess(cov_func)\n",
        "\n",
        "X_train18, X_test18, y_train18, y_test18 = train_test_split(X, y, test_size=test_perc, random_state=run_num_18)\n",
        "\n",
        "def f_syn_polarity18(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_11, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train18, y=y_train18).mean())\n",
        "    return operator * score\n",
        "\n",
        "loser_18 = dGPGO(surrogate_loser_18, Acquisition_new(util_loser), f_syn_polarity18, param, n_jobs = -1) # define BayesOpt\n",
        "loser_18.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_loser_18 = loser_18.getResult()[0]\n",
        "params_loser_18['max_depth'] = int(params_loser_18['max_depth'])\n",
        "params_loser_18['min_child_weight'] = int(params_loser_18['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_loser_train18 = xgb.DMatrix(X_train18, y_train18)\n",
        "dX_loser_test18 = xgb.DMatrix(X_test18, y_test18)\n",
        "model_loser_18 = xgb.train(params_loser_18, dX_loser_train18)\n",
        "pred_loser_18 = model_loser_18.predict(dX_loser_test18)\n",
        "\n",
        "rmse_loser_18 = np.sqrt(mean_squared_error(pred_loser_18, y_test18))\n",
        "rmse_loser_18"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 8.73429403  9.68540663 10.          0.68875849  9.          0.48011572]. \t  -0.48312028405718427 \t -0.4337207096533448\n",
            "init   \t [ 6.12033333  7.66062926  8.          0.76133734 13.          0.93379456]. \t  -0.44701743563076113 \t -0.4337207096533448\n",
            "init   \t [ 1.46524679  7.01527914  7.          0.90913299 10.          0.36016753]. \t  -0.6042122061654378 \t -0.4337207096533448\n",
            "init   \t [ 9.73855241  3.33774046 14.          0.53290419  7.          0.7088681 ]. \t  -0.47205818581725156 \t -0.4337207096533448\n",
            "init   \t [ 3.00618018  1.82702795 11.          0.75681389 14.          0.98627449]. \t  -0.4337207096533448 \t -0.4337207096533448\n",
            "1      \t [ 1.46586883  1.13810325 14.          0.90052794  5.          0.16182112]. \t  -0.7143993241707929 \t -0.4337207096533448\n",
            "2      \t [ 9.17850108  5.22481201 14.          0.60105853 18.          0.63442006]. \t  -0.47652759630987357 \t -0.4337207096533448\n",
            "3      \t [6.20612396 4.37552923 6.         0.71941082 3.         0.90943147]. \t  -0.4522652783180721 \t -0.4337207096533448\n",
            "4      \t [ 0.42107725  9.92938812 12.          0.92462675 18.          0.88352347]. \t  \u001b[92m-0.42696094495506925\u001b[0m \t -0.42696094495506925\n",
            "5      \t [ 1.45418057  8.89399    11.          0.93175096  1.          0.52391385]. \t  -0.4748883821619788 \t -0.42696094495506925\n",
            "6      \t [0.         0.         5.21827599 0.5        6.21827599 0.1       ]. \t  -0.7142978598833383 \t -0.42696094495506925\n",
            "7      \t [ 8.67626106  0.1397848   6.          0.55771681 18.          0.89120888]. \t  -0.4678231542313444 \t -0.42696094495506925\n",
            "8      \t [ 7.20807715  7.41236348  5.          0.52425285 19.          0.24651851]. \t  -0.7151531754828971 \t -0.42696094495506925\n",
            "9      \t [7.94412856 1.24068931 7.         0.96649935 9.         0.47928692]. \t  -0.48764145519377255 \t -0.42696094495506925\n",
            "10     \t [ 2.66410938  9.83743919 13.          0.58899688  8.          0.29675269]. \t  -0.6087724199154481 \t -0.42696094495506925\n",
            "11     \t [ 8.62753755  4.44899956 12.          0.98231134  1.          0.88389967]. \t  \u001b[92m-0.42538744453232347\u001b[0m \t -0.42538744453232347\n",
            "12     \t [ 1.2277143   1.05411485  5.          0.50198686 13.          0.89258454]. \t  -0.4690426518871574 \t -0.42538744453232347\n",
            "13     \t [ 3.86727486  0.04582369 10.          0.54629234  1.          0.59406244]. \t  -0.47507759871416455 \t -0.42538744453232347\n",
            "14     \t [9.89654007 7.91459554 5.         0.64703423 6.         0.95875063]. \t  -0.4610204259667869 \t -0.42538744453232347\n",
            "15     \t [ 9.49381141  1.31696272 11.          0.51283555 13.          0.25364349]. \t  -0.7155727045133423 \t -0.42538744453232347\n",
            "16     \t [0.         3.85327138 5.71339306 0.5        1.         0.1       ]. \t  -0.7130688739294866 \t -0.42538744453232347\n",
            "17     \t [ 1.04974938  7.3117691   5.          0.60120794 17.          0.83587336]. \t  -0.49012043280719225 \t -0.42538744453232347\n",
            "18     \t [5.83203328 9.4812958  5.         0.60479193 1.         0.24995758]. \t  -0.7139401129279975 \t -0.42538744453232347\n",
            "19     \t [ 3.86883296  7.31704712 14.          0.75047101 14.          0.80828494]. \t  -0.4565506921595134 \t -0.42538744453232347\n",
            "20     \t [ 4.0233419   3.91847735 10.          0.87935081  7.          0.57035954]. \t  -0.47552190244029113 \t -0.42538744453232347\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3.937438334950126"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHidSEGcVHvG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e0534e2-8252-4e89-db6d-0465bc809c3d"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 19\n",
        "\n",
        "np.random.seed(run_num_19)\n",
        "surrogate_loser_19 = dGaussianProcess(cov_func)\n",
        "\n",
        "X_train19, X_test19, y_train19, y_test19 = train_test_split(X, y, test_size=test_perc, random_state=run_num_19)\n",
        "\n",
        "def f_syn_polarity19(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_19, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train19, y=y_train19).mean())\n",
        "    return operator * score\n",
        "\n",
        "loser_19 = dGPGO(surrogate_loser_19, Acquisition_new(util_loser), f_syn_polarity19, param, n_jobs = -1) # define BayesOpt\n",
        "loser_19.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_loser_19 = loser_19.getResult()[0]\n",
        "params_loser_19['max_depth'] = int(params_loser_19['max_depth'])\n",
        "params_loser_19['min_child_weight'] = int(params_loser_19['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_loser_train19 = xgb.DMatrix(X_train19, y_train19)\n",
        "dX_loser_test19 = xgb.DMatrix(X_test19, y_test19)\n",
        "model_loser_19 = xgb.train(params_loser_19, dX_loser_train19)\n",
        "pred_loser_19 = model_loser_19.predict(dX_loser_test19)\n",
        "\n",
        "rmse_loser_19 = np.sqrt(mean_squared_error(pred_loser_19, y_test19))\n",
        "rmse_loser_19"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 8.24520174  9.84626759 12.          0.81846905 12.          0.27594309]. \t  -0.6624515538441494 \t -0.4321567975765851\n",
            "init   \t [ 8.6021676   8.64300514 14.          0.95067208 18.          0.13363564]. \t  -0.663790961968593 \t -0.4321567975765851\n",
            "init   \t [ 3.02486657  6.06229509 10.          0.50494482  9.          0.10218675]. \t  -0.668875700942164 \t -0.4321567975765851\n",
            "init   \t [ 6.57586195  8.01231848  7.          0.63945407 18.          0.49206406]. \t  -0.5640092362987262 \t -0.4321567975765851\n",
            "init   \t [ 6.05008856  8.15558548 14.          0.64718943  8.          0.90611106]. \t  -0.4321567975765851 \t -0.4321567975765851\n",
            "1      \t [ 0.85655458  0.69294498  9.          0.90992265 19.          0.25605959]. \t  -0.6632879756016564 \t -0.4321567975765851\n",
            "2      \t [ 1.16239725  9.89254371 11.          0.98469188  1.          0.20271996]. \t  -0.6659068700895453 \t -0.4321567975765851\n",
            "3      \t [9.26425961 9.717621   5.         0.8856331  5.         0.65589573]. \t  -0.5256636565447839 \t -0.4321567975765851\n",
            "4      \t [8.38158931 1.39953008 5.         0.6836482  9.         0.46934447]. \t  -0.5657479189361881 \t -0.4321567975765851\n",
            "5      \t [ 9.89729152  2.88709408 11.          0.55121781  3.          0.58381748]. \t  -0.5217676759230677 \t -0.4321567975765851\n",
            "6      \t [0.  0.  5.  0.5 1.  0.1]. \t  -0.6682393310586452 \t -0.4321567975765851\n",
            "7      \t [ 0.52039992  0.56380532 14.          0.70324781  2.          0.77427978]. \t  -0.44119318049659856 \t -0.4321567975765851\n",
            "8      \t [ 9.45607821  0.96438893 14.          0.56821928 16.          0.48995458]. \t  -0.560546633196372 \t -0.4321567975765851\n",
            "9      \t [ 0.56432836  7.79494932 13.          0.55871992 19.          0.10266045]. \t  -0.6660467600439527 \t -0.4321567975765851\n",
            "10     \t [ 3.16676683  1.78770811 13.          0.5650927  13.          0.7417697 ]. \t  -0.4472490143246649 \t -0.4321567975765851\n",
            "11     \t [5.56939336 3.95670769 5.         0.95243816 3.         0.31115527]. \t  -0.6118696715045784 \t -0.4321567975765851\n",
            "12     \t [0.         0.         6.79132524 0.5        9.79132524 0.1       ]. \t  -0.6680142576361826 \t -0.4321567975765851\n",
            "13     \t [ 7.46605559  8.55652968 13.          0.99452706  1.          0.43764631]. \t  -0.5573240867828692 \t -0.4321567975765851\n",
            "14     \t [ 9.00192632  0.12142981  6.          0.72039779 17.          0.26695117]. \t  -0.6635472276398457 \t -0.4321567975765851\n",
            "15     \t [ 1.20113738  5.88016015  6.          0.57839075 15.          0.60752162]. \t  -0.5267223663659426 \t -0.4321567975765851\n",
            "16     \t [1.7958895  9.19371155 5.         0.54448974 5.         0.93868386]. \t  -0.4669880318809955 \t -0.4321567975765851\n",
            "17     \t [ 9.62487731  6.86253449  5.          0.66465947 12.          0.21107909]. \t  -0.6659019290689656 \t -0.4321567975765851\n",
            "18     \t [ 7.01475491  4.53933393 10.          0.82075044 14.          0.83058729]. \t  -0.4517750017757683 \t -0.4321567975765851\n",
            "19     \t [ 5.75277105  1.25995511 14.          0.97274397  8.          0.39753437]. \t  -0.6135363407765908 \t -0.4321567975765851\n",
            "20     \t [ 2.00664037  5.88682269 14.          0.94696775  4.          0.80981462]. \t  -0.4349577325456077 \t -0.4321567975765851\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.2307134915487135"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWGPYRJhVKsO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2de77ae8-b280-4a76-ceb9-eeec2b2cae75"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 20\n",
        "\n",
        "np.random.seed(run_num_20)\n",
        "surrogate_loser_20 = dGaussianProcess(cov_func)\n",
        "\n",
        "X_train20, X_test20, y_train20, y_test20 = train_test_split(X, y, test_size=test_perc, random_state=run_num_20)\n",
        "\n",
        "def f_syn_polarity20(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_20, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train20, y=y_train20).mean())\n",
        "    return operator * score\n",
        "\n",
        "loser_20 = dGPGO(surrogate_loser_20, Acquisition_new(util_loser), f_syn_polarity20, param, n_jobs = -1) # define BayesOpt\n",
        "loser_20.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_loser_20 = loser_20.getResult()[0]\n",
        "params_loser_20['max_depth'] = int(params_loser_20['max_depth'])\n",
        "params_loser_20['min_child_weight'] = int(params_loser_20['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_loser_train20 = xgb.DMatrix(X_train20, y_train20)\n",
        "dX_loser_test20 = xgb.DMatrix(X_test20, y_test20)\n",
        "model_loser_20 = xgb.train(params_loser_20, dX_loser_train20)\n",
        "pred_loser_20 = model_loser_20.predict(dX_loser_test20)\n",
        "\n",
        "rmse_loser_20 = np.sqrt(mean_squared_error(pred_loser_20, y_test20))\n",
        "rmse_loser_20"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 5.70517285  5.64528755 11.          0.51213999 19.          0.57883228]. \t  -0.5510634045248362 \t -0.4667910180825121\n",
            "init   \t [0.68106288 5.8452906  6.         0.58037829 8.         0.87974899]. \t  -0.4667910180825121 \t -0.4667910180825121\n",
            "init   \t [ 4.04121426  7.17971417 11.          0.57603584 10.          0.19954225]. \t  -0.6541702754539586 \t -0.4667910180825121\n",
            "init   \t [ 5.27556761  8.67655329 13.          0.54252273  4.          0.48570796]. \t  -0.5813476654605825 \t -0.4667910180825121\n",
            "init   \t [ 5.30003889  5.73946822 12.          0.52666698 16.          0.77467019]. \t  -0.5078849923055367 \t -0.4667910180825121\n",
            "1      \t [ 9.25190708  0.65043072 10.          0.56676233  5.          0.41289847]. \t  -0.6164858434894066 \t -0.4667910180825121\n",
            "2      \t [8.65689736 8.11684598 5.         0.53907072 1.         0.87574215]. \t  -0.47576906935129115 \t -0.4667910180825121\n",
            "3      \t [6.49728107e-01 7.72862043e-04 7.00000000e+00 5.71905117e-01\n",
            " 2.00000000e+00 1.02014940e-01]. \t  -0.6533990377801203 \t -0.4667910180825121\n",
            "4      \t [ 9.51806562  0.44900923  6.          0.62997227 16.          0.40028974]. \t  -0.6257141325107729 \t -0.4667910180825121\n",
            "5      \t [ 0.48808143  1.97030839  7.          0.79892377 16.          0.98477167]. \t  \u001b[92m-0.4602371335347518\u001b[0m \t -0.4602371335347518\n",
            "6      \t [ 9.98029391  8.23632437  6.          0.63606408 13.          0.22908429]. \t  -0.6532745897130707 \t -0.4602371335347518\n",
            "7      \t [ 3.9245879   0.87058505 14.          0.68269183  1.          0.34048922]. \t  -0.623283664424315 \t -0.4602371335347518\n",
            "8      \t [ 0.45468431  0.13267263 10.          0.667214    9.          0.47071141]. \t  -0.5807933165122651 \t -0.4602371335347518\n",
            "9      \t [ 4.64792903  9.65609988  6.          0.78696753 17.          0.90406502]. \t  -0.47227152594951016 \t -0.4602371335347518\n",
            "10     \t [ 9.73713042  9.77875019 14.          0.8552366  13.          0.6588042 ]. \t  -0.5450528828222597 \t -0.4602371335347518\n",
            "11     \t [ 9.74564452  1.02954501 14.          0.98964301 12.          0.47743205]. \t  -0.5773481317621234 \t -0.4602371335347518\n",
            "12     \t [ 7.76906477  2.6060444   5.          0.53491145 10.          0.41560504]. \t  -0.627993339841767 \t -0.4602371335347518\n",
            "13     \t [6.49958993 1.98622034 6.         0.83635209 1.         0.2931143 ]. \t  -0.6276302072456058 \t -0.4602371335347518\n",
            "14     \t [1.36051555 9.95957797 7.         0.9242116  2.         0.87097462]. \t  \u001b[92m-0.449167771194887\u001b[0m \t -0.449167771194887\n",
            "15     \t [ 0.20978943  8.45447688 14.          0.77351134 17.          0.2656195 ]. \t  -0.6586945947582249 \t -0.449167771194887\n",
            "16     \t [7.55121074 9.86496533 6.         0.88828535 7.         0.3097795 ]. \t  -0.6258969685789838 \t -0.449167771194887\n",
            "17     \t [ 1.88681738  3.46035586 14.          0.68070687  6.          0.9944693 ]. \t  \u001b[92m-0.42905917541157645\u001b[0m \t -0.42905917541157645\n",
            "18     \t [ 1.00455473  4.86452269 10.          0.65242434  1.          0.22603818]. \t  -0.6579989052523156 \t -0.42905917541157645\n",
            "19     \t [ 0.87144588  0.14777143 14.          0.88891044 13.          0.11582186]. \t  -0.6586044677275812 \t -0.42905917541157645\n",
            "20     \t [ 8.17732484  5.22697425 11.          0.61303379  1.          0.32672413]. \t  -0.6191817873624627 \t -0.42905917541157645\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.2900830531756915"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1d_1LyydIfe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27f04669-fe6e-44b0-d5f6-b1aace7df3e0"
      },
      "source": [
        "end_lose = time.time()\n",
        "end_lose\n",
        "\n",
        "time_lose = end_lose - start_lose\n",
        "time_lose\n",
        "\n",
        "start_win = time.time()\n",
        "start_win"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1613632554.344337"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZAyOw7XYVwAf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ad18bcb-29b8-40b8-e349-3e8f7ff5be4a"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 1 \n",
        "\n",
        "np.random.seed(run_num_1)\n",
        "surrogate_winner_1 = dtStudentProcess(d_cov_func, nu = df)\n",
        "\n",
        "X_train1, X_test1, y_train1, y_test1 = train_test_split(X, y, test_size=test_perc, random_state=run_num_1)\n",
        "\n",
        "def f_syn_polarity1(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_1, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train1, y=y_train1).mean())\n",
        "    return operator * score\n",
        "\n",
        "winner_1 = dGPGO_stp(surrogate_winner_1, Acquisition_new(util_winner), f_syn_polarity1, param, n_jobs = -1) # Define BayesOpt\n",
        "winner_1.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_winner_1 = winner_1.getResult()[0]\n",
        "params_winner_1['max_depth'] = int(params_winner_1['max_depth'])\n",
        "params_winner_1['min_child_weight'] = int(params_winner_1['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_winner_train1 = xgb.DMatrix(X_train1, y_train1)\n",
        "dX_winner_test1 = xgb.DMatrix(X_test1, y_test1)\n",
        "model_winner_1 = xgb.train(params_winner_1, dX_winner_train1)\n",
        "pred_winner_1 = model_winner_1.predict(dX_winner_test1)\n",
        "\n",
        "rmse_winner_1 = np.sqrt(mean_squared_error(pred_winner_1, y_test1))\n",
        "rmse_winner_1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 6.12170176  1.69069754 11.          0.92695323  3.          0.36579277]. \t  -0.5875926438135084 \t -0.45245711231879204\n",
            "init   \t [ 1.49162957  0.22478325  6.          0.85613654 15.          0.19363322]. \t  -0.6811472770543097 \t -0.45245711231879204\n",
            "init   \t [ 5.27957805  2.81819356 11.          0.81062146 14.          0.74826845]. \t  -0.46117277972616416 \t -0.45245711231879204\n",
            "init   \t [3.81060005 6.24236569 7.         0.95038709 5.         0.9861745 ]. \t  -0.45245711231879204 \t -0.45245711231879204\n",
            "init   \t [4.77531134 7.87990376 6.         0.78863691 6.         0.47918609]. \t  -0.5612893767574513 \t -0.45245711231879204\n",
            "1      \t [0.  0.  5.  0.5 1.  0.1]. \t  -0.6826322236836013 \t -0.45245711231879204\n",
            "2      \t [ 9.60774341  8.94642175  5.          0.60295867 15.          0.76784922]. \t  -0.5001571385184761 \t -0.45245711231879204\n",
            "3      \t [ 0.4810761   9.31563532  8.          0.92845546 14.          0.70014982]. \t  -0.5056468669267833 \t -0.45245711231879204\n",
            "4      \t [ 9.85849625  0.83439907  7.          0.7705582  19.          0.15487995]. \t  -0.6815452883308839 \t -0.45245711231879204\n",
            "5      \t [ 9.1348132   9.54667443 13.          0.95557113 18.          0.11016711]. \t  -0.6791252728059957 \t -0.45245711231879204\n",
            "6      \t [ 9.97122318  8.75901621 13.          0.9904996   9.          0.60161775]. \t  -0.4895864123043516 \t -0.45245711231879204\n",
            "7      \t [7.19079936 0.87565629 5.         0.54470564 8.         0.4449021 ]. \t  -0.5736147518216788 \t -0.45245711231879204\n",
            "8      \t [ 5.24989998  8.90398408 13.          0.54165135  1.          0.74771929]. \t  -0.46141851807096074 \t -0.45245711231879204\n",
            "9      \t [ 1.7157918   5.89141512 14.          0.5296881  10.          0.1803042 ]. \t  -0.6825568503758982 \t -0.45245711231879204\n",
            "10     \t [ 1.33491429  4.9488224  14.          0.57825982 18.          0.42075517]. \t  -0.5921600742966667 \t -0.45245711231879204\n",
            "11     \t [8.99095022 4.91206514 6.         0.87212346 1.         0.53198502]. \t  -0.5638457176592562 \t -0.45245711231879204\n",
            "12     \t [0.51753753 1.68511981 9.         0.80360039 9.         0.45200858]. \t  -0.5426731188677099 \t -0.45245711231879204\n",
            "13     \t [ 0.47819106  0.69361363 14.          0.7271902   3.          0.48615024]. \t  -0.5388370338312534 \t -0.45245711231879204\n",
            "14     \t [ 3.39786838  7.15897283  5.          0.7138544  19.          0.22047835]. \t  -0.6821252760743777 \t -0.45245711231879204\n",
            "15     \t [0.4939583  9.37390762 8.         0.7641092  1.         0.4917045 ]. \t  -0.5481894484993145 \t -0.45245711231879204\n",
            "16     \t [ 8.70209222  0.21363429 14.          0.98391211 18.          0.50653374]. \t  -0.5341942124085062 \t -0.45245711231879204\n",
            "17     \t [ 5.30693639  9.58195731  9.          0.53796245 11.          0.46099251]. \t  -0.5549909911648074 \t -0.45245711231879204\n",
            "18     \t [ 3.83524595  9.61883757 12.          0.81445794 17.          0.17700713]. \t  -0.6797728963073484 \t -0.45245711231879204\n",
            "19     \t [ 0.25234021  8.87254669 14.          0.7101496   5.          0.12345742]. \t  -0.684133223230854 \t -0.45245711231879204\n",
            "20     \t [9.81917647 9.19103216 5.         0.82619792 9.         0.10869763]. \t  -0.6822789922076578 \t -0.45245711231879204\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.87018129255363"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hrDQbChpZ48F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78c95dc1-6e29-4ee1-8ecf-a299d1b9d270"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 2 \n",
        "\n",
        "np.random.seed(run_num_2)\n",
        "surrogate_winner_2 = dtStudentProcess(d_cov_func, nu = df)\n",
        "\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X, y, test_size=test_perc, random_state=run_num_2)\n",
        "\n",
        "def f_syn_polarity2(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_2, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train2, y=y_train2).mean())\n",
        "    return operator * score\n",
        "\n",
        "winner_2 = dGPGO_stp(surrogate_winner_2, Acquisition_new(util_winner), f_syn_polarity2, param, n_jobs = -1) # Define BayesOpt\n",
        "winner_2.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_winner_2 = winner_2.getResult()[0]\n",
        "params_winner_2['max_depth'] = int(params_winner_2['max_depth'])\n",
        "params_winner_2['min_child_weight'] = int(params_winner_2['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_winner_train2 = xgb.DMatrix(X_train2, y_train2)\n",
        "dX_winner_test2 = xgb.DMatrix(X_test2, y_test2)\n",
        "model_winner_2 = xgb.train(params_winner_2, dX_winner_train2)\n",
        "pred_winner_2 = model_winner_2.predict(dX_winner_test2)\n",
        "\n",
        "rmse_winner_2 = np.sqrt(mean_squared_error(pred_winner_2, y_test2))\n",
        "rmse_winner_2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 8.51985493  0.73903599  7.          0.71824677 17.          0.61827209]. \t  -0.5233872966482411 \t -0.42848969661986275\n",
            "init   \t [ 8.4047092   4.35120551 12.          0.84231905 16.          0.80172484]. \t  -0.42848969661986275 \t -0.42848969661986275\n",
            "init   \t [ 6.42749368  9.61026168 10.          0.73363448 15.          0.84932207]. \t  -0.43419399571353007 \t -0.42848969661986275\n",
            "init   \t [ 2.66008363  8.36685389  6.          0.92082163 12.          0.69999264]. \t  -0.5353675801921922 \t -0.42848969661986275\n",
            "init   \t [ 1.74779478  2.17691512 12.          0.71794562  2.          0.48990646]. \t  -0.5346620615324694 \t -0.42848969661986275\n",
            "1      \t [0.65899411 0.08911785 5.         0.57513718 8.         0.6649358 ]. \t  -0.5435492302872315 \t -0.42848969661986275\n",
            "2      \t [7.50154303 9.49874736 7.         0.83842364 4.         0.5714185 ]. \t  -0.5422231033003287 \t -0.42848969661986275\n",
            "3      \t [9.88263606 1.71181353 7.         0.97275193 1.         0.74825108]. \t  -0.441747506473179 \t -0.42848969661986275\n",
            "4      \t [ 0.38425009  3.26044865 12.          0.89101602 19.          0.732816  ]. \t  \u001b[92m-0.4225097924337229\u001b[0m \t -0.4225097924337229\n",
            "5      \t [ 0.17098677  4.24340552 14.          0.58045842 10.          0.64319766]. \t  -0.5100599626019375 \t -0.4225097924337229\n",
            "6      \t [ 8.24807017  4.40295761 13.          0.99966982  5.          0.29909156]. \t  -0.5674503952477116 \t -0.4225097924337229\n",
            "7      \t [8.17228996 0.99614058 7.         0.60509314 8.         0.9040458 ]. \t  -0.44411068565064227 \t -0.4225097924337229\n",
            "8      \t [0.  0.  5.  0.5 1.  0.1]. \t  -0.6860882831213788 \t -0.4225097924337229\n",
            "9      \t [ 2.26995028  7.26113474  5.          0.89244153 19.          0.72822627]. \t  -0.46107668663747675 \t -0.4225097924337229\n",
            "10     \t [0.32359376 9.19341132 6.         0.74153554 1.         0.25113354]. \t  -0.6853813278526151 \t -0.4225097924337229\n",
            "11     \t [ 2.40520679  9.030061   11.          0.61309393  6.          0.56126501]. \t  -0.5355680728892867 \t -0.4225097924337229\n",
            "12     \t [ 3.05457734  1.34829356  9.          0.5836481  13.          0.61527323]. \t  -0.5184947344115468 \t -0.4225097924337229\n",
            "13     \t [ 8.36253694  0.57899647 14.          0.6792832  11.          0.81681485]. \t  -0.42836400911298467 \t -0.4225097924337229\n",
            "14     \t [ 0.99039506  9.70962894 14.          0.53225067 15.          0.94095862]. \t  -0.43140168430232356 \t -0.4225097924337229\n",
            "15     \t [ 9.59276769  7.37641605  6.          0.89538811 12.          0.94983532]. \t  -0.45071664883790197 \t -0.4225097924337229\n",
            "16     \t [ 1.81325607  0.38056982  5.          0.809705   18.          0.5636356 ]. \t  -0.5668463487185564 \t -0.4225097924337229\n",
            "17     \t [ 8.38595123  0.05803388 12.          0.71922258 19.          0.39814673]. \t  -0.5729508925204103 \t -0.4225097924337229\n",
            "18     \t [ 8.11913722  7.7931672  14.          0.50393458 11.          0.23511725]. \t  -0.6881787024693147 \t -0.4225097924337229\n",
            "19     \t [ 8.06744935  9.40296538  6.          0.60147647 19.          0.17678037]. \t  -0.6864884735391246 \t -0.4225097924337229\n",
            "20     \t [ 8.96311905  9.42507441 14.          0.78456647  1.          0.32673768]. \t  -0.5683371749241675 \t -0.4225097924337229\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.316024710268154"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpUPyXRfZ95Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4d4167c-e587-48d4-83e7-d870e53b190d"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 3 \n",
        "\n",
        "np.random.seed(run_num_3)\n",
        "surrogate_winner_3 = dtStudentProcess(d_cov_func, nu = df)\n",
        "\n",
        "X_train3, X_test3, y_train3, y_test3 = train_test_split(X, y, test_size=test_perc, random_state=run_num_3)\n",
        "\n",
        "def f_syn_polarity3(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_3, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train3, y=y_train3).mean())\n",
        "    return operator * score\n",
        "\n",
        "winner_3 = dGPGO_stp(surrogate_winner_3, Acquisition_new(util_winner), f_syn_polarity3, param, n_jobs = -1) # Define BayesOpt\n",
        "winner_3.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_winner_3 = winner_3.getResult()[0]\n",
        "params_winner_3['max_depth'] = int(params_winner_3['max_depth'])\n",
        "params_winner_3['min_child_weight'] = int(params_winner_3['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_winner_train3 = xgb.DMatrix(X_train3, y_train3)\n",
        "dX_winner_test3 = xgb.DMatrix(X_test3, y_test3)\n",
        "model_winner_3 = xgb.train(params_winner_3, dX_winner_train3)\n",
        "pred_winner_3 = model_winner_3.predict(dX_winner_test3)\n",
        "\n",
        "rmse_winner_3 = np.sqrt(mean_squared_error(pred_winner_3, y_test3))\n",
        "rmse_winner_3"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 7.51575607  1.09251275 13.          0.80377147 16.          0.31229624]. \t  -0.6858957949546655 \t -0.5081673732303724\n",
            "init   \t [6.12794891 1.19652403 5.         0.74822174 8.         0.33695838]. \t  -0.6849895205987827 \t -0.5081673732303724\n",
            "init   \t [ 6.21631223  4.4909976  10.          0.66764529  9.          0.77207323]. \t  -0.5081673732303724 \t -0.5081673732303724\n",
            "init   \t [ 0.58127233  4.79918477 11.          0.64526552 15.          0.62431773]. \t  -0.5106501747238601 \t -0.5081673732303724\n",
            "init   \t [ 5.76228258  0.67219724  7.          0.88228691 13.          0.29867219]. \t  -0.6839256266327581 \t -0.5081673732303724\n",
            "1      \t [9.79994489 9.57810833 8.         0.93763368 1.         0.1772549 ]. \t  -0.7316297465421655 \t -0.5081673732303724\n",
            "2      \t [ 8.09618702  9.87899793 14.          0.76477208 16.          0.92750484]. \t  \u001b[92m-0.4230067277420167\u001b[0m \t -0.4230067277420167\n",
            "3      \t [0.  0.  5.  0.5 1.  0.1]. \t  -0.7323904210578609 \t -0.4230067277420167\n",
            "4      \t [ 0.38905261  9.83674845 14.          0.9364986   7.          0.11108644]. \t  -0.7312980300425612 \t -0.4230067277420167\n",
            "5      \t [ 6.29879634  0.65335492 12.          0.51090918  1.          0.59990728]. \t  -0.516414574482896 \t -0.4230067277420167\n",
            "6      \t [ 8.97980523  8.12851147  5.          0.67381125 19.          0.9346588 ]. \t  -0.46734051592589515 \t -0.4230067277420167\n",
            "7      \t [ 1.1995358   9.87930749  5.          0.84448568 18.          0.26022822]. \t  -0.7297224033867351 \t -0.4230067277420167\n",
            "8      \t [ 1.96525323  0.16048396 14.          0.65133015  9.          0.3003508 ]. \t  -0.6860931128073492 \t -0.4230067277420167\n",
            "9      \t [0.97819006 9.26311586 5.         0.97908914 5.         0.15389027]. \t  -0.7306239589771446 \t -0.4230067277420167\n",
            "10     \t [5.14760079 5.54852798 5.         0.50222966 1.         0.38420497]. \t  -0.6868871017723605 \t -0.4230067277420167\n",
            "11     \t [ 1.74817442  7.77445435 11.          0.97071394  1.          0.19589347]. \t  -0.7309743263045692 \t -0.4230067277420167\n",
            "12     \t [ 7.84223409  9.79539983 14.          0.56374986  7.          0.9106257 ]. \t  -0.43098974880612706 \t -0.4230067277420167\n",
            "13     \t [ 2.80079796  0.61761702  5.          0.62606225 19.          0.71334647]. \t  -0.5274647167409906 \t -0.4230067277420167\n",
            "14     \t [ 7.56317604  9.70544922  7.          0.71515672 13.          0.17880351]. \t  -0.7314841751890333 \t -0.4230067277420167\n",
            "15     \t [ 9.39703158  0.81960902 14.          0.62547886 10.          0.15060928]. \t  -0.7316995199725552 \t -0.4230067277420167\n",
            "16     \t [ 1.84620574  0.32709184 13.          0.76394118 19.          0.90059117]. \t  \u001b[92m-0.42118028750387354\u001b[0m \t -0.42118028750387354\n",
            "17     \t [ 0.          1.72364089  5.          0.5        11.59881694  0.1       ]. \t  -0.7323784626096901 \t -0.42118028750387354\n",
            "18     \t [7.55452431e-03 9.44297730e+00 1.40000000e+01 9.48110213e-01\n",
            " 1.90000000e+01 5.05826889e-01]. \t  -0.6563162692297837 \t -0.42118028750387354\n",
            "19     \t [ 0.63647231  9.18176901  7.          0.66842704 11.          0.85370231]. \t  -0.5109938145846963 \t -0.42118028750387354\n",
            "20     \t [8.6529263  7.8426258  5.         0.6184649  7.         0.44068282]. \t  -0.6534419783960326 \t -0.42118028750387354\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.721916790421568"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKX_nfEaaAwm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53dc9c4e-3bc7-4c78-f9bd-581a66c71a8e"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 4 \n",
        "\n",
        "np.random.seed(run_num_4)\n",
        "surrogate_winner_4 = dtStudentProcess(d_cov_func, nu = df)\n",
        "\n",
        "X_train4, X_test4, y_train4, y_test4 = train_test_split(X, y, test_size=test_perc, random_state=run_num_4)\n",
        "\n",
        "def f_syn_polarity4(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_4, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train4, y=y_train4).mean())\n",
        "    return operator * score\n",
        "\n",
        "winner_4 = dGPGO_stp(surrogate_winner_4, Acquisition_new(util_winner), f_syn_polarity4, param, n_jobs = -1) # Define BayesOpt\n",
        "winner_4.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_winner_4 = winner_4.getResult()[0]\n",
        "params_winner_4['max_depth'] = int(params_winner_4['max_depth'])\n",
        "params_winner_4['min_child_weight'] = int(params_winner_4['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_winner_train4 = xgb.DMatrix(X_train4, y_train4)\n",
        "dX_winner_test4 = xgb.DMatrix(X_test4, y_test4)\n",
        "model_winner_4 = xgb.train(params_winner_4, dX_winner_train4)\n",
        "pred_winner_4 = model_winner_4.predict(dX_winner_test4)\n",
        "\n",
        "rmse_winner_4 = np.sqrt(mean_squared_error(pred_winner_4, y_test4))\n",
        "rmse_winner_4"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 9.42798506  1.23319667 14.          0.81910755  7.          0.7021549 ]. \t  -0.5581090891563811 \t -0.5581090891563811\n",
            "init   \t [ 5.54158317  7.71233962 10.          0.59117644  6.          0.31476458]. \t  -0.6204181120564479 \t -0.5581090891563811\n",
            "init   \t [2.18282634 2.93794127 7.         0.84739327 5.         0.62716229]. \t  -0.5626656925564042 \t -0.5581090891563811\n",
            "init   \t [ 5.66159581  4.68678814 11.          0.54147808  9.          0.58609221]. \t  -0.5595885941852147 \t -0.5581090891563811\n",
            "init   \t [ 3.02160958  6.07432614  6.          0.92038559 14.          0.12778609]. \t  -0.6361772582153176 \t -0.5581090891563811\n",
            "1      \t [ 1.89460697  0.10324856 13.          0.70356349 16.          0.67776197]. \t  \u001b[92m-0.5568243993303088\u001b[0m \t -0.5568243993303088\n",
            "2      \t [ 9.69969     4.32607677 14.          0.87739158 16.          0.66014279]. \t  -0.5573098211239071 \t -0.5568243993303088\n",
            "3      \t [ 2.2210328   8.31054196 13.          0.70317243 19.          0.56953182]. \t  -0.573504425404327 \t -0.5568243993303088\n",
            "4      \t [ 0.14706294  0.75702217 14.          0.56309891  8.          0.29887971]. \t  -0.6254087880261816 \t -0.5568243993303088\n",
            "5      \t [ 6.94218243  0.32144048  6.          0.59286079 19.          0.70636825]. \t  -0.5706574487524134 \t -0.5568243993303088\n",
            "6      \t [ 9.24159749  9.43575685  7.          0.98380459 19.          0.34937346]. \t  -0.6187589419936593 \t -0.5568243993303088\n",
            "7      \t [7.57184449 0.0324327  9.         0.86318723 1.         0.6413689 ]. \t  -0.5611723012436254 \t -0.5568243993303088\n",
            "8      \t [ 1.62755426  6.7921099  14.          0.91903975  1.          0.61791343]. \t  -0.5680865264437001 \t -0.5568243993303088\n",
            "9      \t [ 9.43074486  0.98044049  5.          0.96290856 10.          0.27397556]. \t  -0.6390681992417737 \t -0.5568243993303088\n",
            "10     \t [ 8.73128599  9.22658128 14.          0.98344346  2.          0.13489277]. \t  -0.6368164613179 \t -0.5568243993303088\n",
            "11     \t [ 0.          0.          7.34373854  0.5        12.34373854  0.1       ]. \t  -0.6353530029272869 \t -0.5568243993303088\n",
            "12     \t [8.91398595 7.00476796 6.         0.72156795 1.         0.41673037]. \t  -0.6210807206819127 \t -0.5568243993303088\n",
            "13     \t [ 9.58851274  9.75423983 14.          0.60815214 12.          0.97572229]. \t  \u001b[92m-0.44045254560081915\u001b[0m \t -0.44045254560081915\n",
            "14     \t [ 9.87497198  6.68701692  6.          0.55980787 13.          0.32605223]. \t  -0.6200123103666009 \t -0.44045254560081915\n",
            "15     \t [ 2.16464782  9.35692886 11.          0.97431128 12.          0.43257419]. \t  -0.5747879420555002 \t -0.44045254560081915\n",
            "16     \t [ 6.80688897  8.81405808 12.          0.62335212 16.          0.93781708]. \t  -0.44402607071927225 \t -0.44045254560081915\n",
            "17     \t [0.  0.  5.  0.5 1.  0.1]. \t  -0.6351353738423114 \t -0.44045254560081915\n",
            "18     \t [7.36355448 5.3077281  5.         0.5709887  7.         0.87569884]. \t  -0.467633753337906 \t -0.44045254560081915\n",
            "19     \t [ 7.93069008  0.11830338 11.          0.97110397 13.          0.22887984]. \t  -0.6356672520315936 \t -0.44045254560081915\n",
            "20     \t [ 6.39787933  3.84205635 14.          0.72880814  3.          0.21974221]. \t  -0.6364956155454153 \t -0.44045254560081915\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.469425637772456"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJmI9saAaEG1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b97a77ea-ae6b-4737-c4c6-208d2eb8668b"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 5 \n",
        "\n",
        "np.random.seed(run_num_5)\n",
        "surrogate_winner_5 = dtStudentProcess(d_cov_func, nu = df)\n",
        "\n",
        "X_train5, X_test5, y_train5, y_test5 = train_test_split(X, y, test_size=test_perc, random_state=run_num_5)\n",
        "\n",
        "def f_syn_polarity5(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_5, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train5, y=y_train5).mean())\n",
        "    return operator * score\n",
        "\n",
        "winner_5 = dGPGO_stp(surrogate_winner_5, Acquisition_new(util_winner), f_syn_polarity5, param, n_jobs = -1) # Define BayesOpt\n",
        "winner_5.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_winner_5 = winner_5.getResult()[0]\n",
        "params_winner_5['max_depth'] = int(params_winner_5['max_depth'])\n",
        "params_winner_5['min_child_weight'] = int(params_winner_5['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_winner_train5 = xgb.DMatrix(X_train5, y_train5)\n",
        "dX_winner_test5 = xgb.DMatrix(X_test5, y_test5)\n",
        "model_winner_5 = xgb.train(params_winner_5, dX_winner_train5)\n",
        "pred_winner_5 = model_winner_5.predict(dX_winner_test5)\n",
        "\n",
        "rmse_winner_5 = np.sqrt(mean_squared_error(pred_winner_5, y_test5))\n",
        "rmse_winner_5"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 5.57353274  5.41760627  8.          0.53710825 19.          0.26034376]. \t  -0.6523797617249512 \t -0.466901759574539\n",
            "init   \t [5.17826437 9.78817462 5.         0.84048437 6.         0.79712615]. \t  -0.4886603987288415 \t -0.466901759574539\n",
            "init   \t [ 9.84476201  9.65484856  5.          0.59464714 19.          0.34528747]. \t  -0.564220001422124 \t -0.466901759574539\n",
            "init   \t [ 9.17476842  4.53262239 13.          0.64368565 10.          0.515308  ]. \t  -0.5049042831858346 \t -0.466901759574539\n",
            "init   \t [ 8.66719465  7.99685372 12.          0.79833571 17.          0.72443466]. \t  -0.466901759574539 \t -0.466901759574539\n",
            "1      \t [ 3.31480734  6.81602635 12.          0.95330947  1.          0.27334927]. \t  -0.6531167190113543 \t -0.466901759574539\n",
            "2      \t [2.53140694 0.29740057 9.         0.7505917  7.         0.82287404]. \t  -0.47475119282304395 \t -0.466901759574539\n",
            "3      \t [ 9.84728748  0.28222034 13.          0.93969018  3.          0.22339517]. \t  -0.6522247903981228 \t -0.466901759574539\n",
            "4      \t [ 0.68116737  3.94603776 14.          0.80380753 14.          0.77317996]. \t  \u001b[92m-0.4613270217842209\u001b[0m \t -0.4613270217842209\n",
            "5      \t [ 9.6928838   0.38736829  6.          0.56167539 11.          0.4775038 ]. \t  -0.5239028011959734 \t -0.4613270217842209\n",
            "6      \t [ 1.62699639  9.0444625   7.          0.60549869 13.          0.13190699]. \t  -0.6526723866694732 \t -0.4613270217842209\n",
            "7      \t [ 7.31108123  0.10391064 14.          0.53552151 15.          0.19462546]. \t  -0.6531977730432595 \t -0.4613270217842209\n",
            "8      \t [0.  0.  5.  0.5 1.  0.1]. \t  -0.6542193437616992 \t -0.4613270217842209\n",
            "9      \t [ 1.05461775  1.17661042  5.          0.5648338  17.          0.90649901]. \t  -0.4702337390488589 \t -0.4613270217842209\n",
            "10     \t [ 0.29281635  9.83144868 11.          0.57517626 19.          0.13846438]. \t  -0.6543975191191339 \t -0.4613270217842209\n",
            "11     \t [9.66604659 1.51730123 5.         0.59597224 4.         0.66643448]. \t  -0.5016316198301449 \t -0.4613270217842209\n",
            "12     \t [0.57140531 6.2823043  5.         0.70475062 2.         0.3328708 ]. \t  -0.5594485185267623 \t -0.4613270217842209\n",
            "13     \t [ 2.5240907   6.84169344 14.          0.61623097  8.          0.50182056]. \t  -0.5037582468108619 \t -0.4613270217842209\n",
            "14     \t [0.0858834  4.38043443 5.         0.57114514 9.         0.11102852]. \t  -0.6531285980012218 \t -0.4613270217842209\n",
            "15     \t [ 9.08851157  7.7443441   6.          0.74314397 12.          0.52508293]. \t  -0.5238407378055661 \t -0.4613270217842209\n",
            "16     \t [ 0.62266475  1.9018111  14.          0.63329124  3.          0.94054341]. \t  \u001b[92m-0.42529491633232686\u001b[0m \t -0.42529491633232686\n",
            "17     \t [ 9.23494676  7.58606552 12.          0.78487568  2.          0.96478853]. \t  -0.42939677763278594 \t -0.42529491633232686\n",
            "18     \t [ 5.11817103  4.38089559  9.          0.53776664 13.          0.7209118 ]. \t  -0.4792507830025482 \t -0.42529491633232686\n",
            "19     \t [8.44899175 7.77761284 6.         0.53217155 1.         0.55958377]. \t  -0.5238817589404079 \t -0.42529491633232686\n",
            "20     \t [ 3.67612744  5.42599858 14.          0.93140269 19.          0.80853077]. \t  -0.4626498801811607 \t -0.42529491633232686\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.31722690080873"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulhEolsxaG4k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2f01134-8e9b-4c06-9016-515b28fdec78"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 6 \n",
        "\n",
        "np.random.seed(run_num_6)\n",
        "surrogate_winner_6 = dtStudentProcess(d_cov_func, nu = df)\n",
        "\n",
        "X_train6, X_test6, y_train6, y_test6 = train_test_split(X, y, test_size=test_perc, random_state=run_num_6)\n",
        "\n",
        "def f_syn_polarity6(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=int(min_child_weight),\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_6, objective = 'reg:squarederror', eval_metric = 'rmse')\n",
        "    score = np.array(cross_val_score(reg, X=X_train6, y=y_train6).mean())\n",
        "    return operator * score\n",
        "\n",
        "winner_6 = dGPGO_stp(surrogate_winner_6, Acquisition_new(util_winner), f_syn_polarity6, param, n_jobs = -1) # Define BayesOpt\n",
        "winner_6.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_winner_6 = winner_6.getResult()[0]\n",
        "params_winner_6['max_depth'] = int(params_winner_6['max_depth'])\n",
        "params_winner_6['min_child_weight'] = int(params_winner_6['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_winner_train6 = xgb.DMatrix(X_train6, y_train6)\n",
        "dX_winner_test6 = xgb.DMatrix(X_test6, y_test6)\n",
        "model_winner_6 = xgb.train(params_winner_6, dX_winner_train6)\n",
        "pred_winner_6 = model_winner_6.predict(dX_winner_test6)\n",
        "\n",
        "rmse_winner_6 = np.sqrt(mean_squared_error(pred_winner_6, y_test6))\n",
        "rmse_winner_6"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [8.92860151 3.31979805 5.         0.99251441 2.         0.57683563]. \t  -0.5471181110640053 \t -0.5219867521874206\n",
            "init   \t [4.18807429 3.35407849 9.         0.87750649 3.         0.56623277]. \t  -0.629286701398377 \t -0.5219867521874206\n",
            "init   \t [ 5.788586    6.45355096 14.          0.70660047 12.          0.82154882]. \t  -0.5219867521874206 \t -0.5219867521874206\n",
            "init   \t [4.58184578 6.73834679 5.         0.90108528 3.         0.65482895]. \t  -0.5476166257785141 \t -0.5219867521874206\n",
            "init   \t [ 4.42510505  5.75952352 14.          0.97882365 15.          0.29525604]. \t  -0.6655867879161768 \t -0.5219867521874206\n",
            "1      \t [ 7.09628606  0.82458553  6.          0.52978945 17.          0.78827589]. \t  -0.5368551114917748 \t -0.5219867521874206\n",
            "2      \t [ 1.88885215  8.62524025  6.          0.90916563 14.          0.40379086]. \t  -0.6680072912825885 \t -0.5219867521874206\n",
            "3      \t [ 0.5654966   9.52584762 14.          0.82016688  4.          0.10717254]. \t  -0.7389555129670606 \t -0.5219867521874206\n",
            "4      \t [ 9.63033822  0.03793409 13.          0.68076302  7.          0.91391681]. \t  \u001b[92m-0.4301920669254681\u001b[0m \t -0.4301920669254681\n",
            "5      \t [ 9.97469028  9.15579642 12.          0.66745021 19.          0.28959158]. \t  -0.6668248751122167 \t -0.4301920669254681\n",
            "6      \t [ 9.85880187  8.48475629 13.          0.76006171  2.          0.58751173]. \t  -0.5403277210407594 \t -0.4301920669254681\n",
            "7      \t [ 9.51844     9.62786679  5.          0.56989798 15.          0.51068013]. \t  -0.6345288616058514 \t -0.4301920669254681\n",
            "8      \t [ 0.15151883  3.04875397  5.          0.61495161 19.          0.79420245]. \t  -0.5357132607573185 \t -0.4301920669254681\n",
            "9      \t [ 3.08077333  0.89569015  6.          0.84149914 11.          0.94992477]. \t  -0.4472711154368675 \t -0.4301920669254681\n",
            "10     \t [ 0.19816257  1.95434851 14.          0.59292238  7.          0.17971262]. \t  -0.7383261448869979 \t -0.4301920669254681\n",
            "11     \t [ 0.56219975  9.42928268 11.          0.51903033 19.          0.52214278]. \t  -0.6326946461691906 \t -0.4301920669254681\n",
            "12     \t [9.58176304 9.94093363 6.         0.96280567 8.         0.32855082]. \t  -0.6677341620025121 \t -0.4301920669254681\n",
            "13     \t [ 5.43877488  0.17504551 13.          0.58721411 19.          0.60115325]. \t  -0.5401916158723044 \t -0.4301920669254681\n",
            "14     \t [0.  0.  5.  0.5 1.  0.1]. \t  -0.7382709735940614 \t -0.4301920669254681\n",
            "15     \t [9.18163606 3.89469538 8.         0.59772622 8.         0.18901946]. \t  -0.7380580836797348 \t -0.4301920669254681\n",
            "16     \t [0.52725495 9.82259272 6.         0.62080595 7.         0.68770749]. \t  -0.5497290569747493 \t -0.4301920669254681\n",
            "17     \t [ 6.71573436  6.18076517  8.          0.50736725 18.          0.99692636]. \t  -0.4548470613295324 \t -0.4301920669254681\n",
            "18     \t [ 7.23381367  0.21465732 13.          0.75484136 13.          0.44717401]. \t  -0.6306144837329356 \t -0.4301920669254681\n",
            "19     \t [ 0.17525712  4.19183624 13.          0.93093846  1.          0.77496282]. \t  -0.5254340278392874 \t -0.4301920669254681\n",
            "20     \t [ 0.22896089  2.69806987 14.          0.53297649 18.          0.65056758]. \t  -0.5430966302037052 \t -0.4301920669254681\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.1451215046465775"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYebx3RVaJ1w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "914ca46e-6f34-4bd2-f46f-e69a91a15f3a"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 7 \n",
        "\n",
        "np.random.seed(run_num_7)\n",
        "surrogate_winner_7 = dtStudentProcess(d_cov_func, nu = df)\n",
        "\n",
        "X_train7, X_test7, y_train7, y_test7 = train_test_split(X, y, test_size=test_perc, random_state=run_num_7)\n",
        "\n",
        "def f_syn_polarity7(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_7, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train7, y=y_train7).mean())\n",
        "    return operator * score\n",
        "\n",
        "winner_7 = dGPGO_stp(surrogate_winner_7, Acquisition_new(util_winner), f_syn_polarity7, param, n_jobs = -1) # Define BayesOpt\n",
        "winner_7.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_winner_7 = winner_7.getResult()[0]\n",
        "params_winner_7['max_depth'] = int(params_winner_7['max_depth'])\n",
        "params_winner_7['min_child_weight'] = int(params_winner_7['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_winner_train7 = xgb.DMatrix(X_train7, y_train7)\n",
        "dX_winner_test7 = xgb.DMatrix(X_test7, y_test7)\n",
        "model_winner_7 = xgb.train(params_winner_7, dX_winner_train7)\n",
        "pred_winner_7 = model_winner_7.predict(dX_winner_test7)\n",
        "\n",
        "rmse_winner_7 = np.sqrt(mean_squared_error(pred_winner_7, y_test7))\n",
        "rmse_winner_7"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 8.11505126  6.92324143 13.          0.94046175 12.          0.3650384 ]. \t  -0.612852517751432 \t -0.4284992540085738\n",
            "init   \t [ 5.15357029  9.0066636   6.          0.74104227 15.          0.8966337 ]. \t  -0.45500673865987984 \t -0.4284992540085738\n",
            "init   \t [ 2.63548913  9.6610934  14.          0.64712408  1.          0.80820948]. \t  -0.43730487111474525 \t -0.4284992540085738\n",
            "init   \t [ 1.40821426  1.72758589 12.          0.70495069  5.          0.86632715]. \t  -0.4284992540085738 \t -0.4284992540085738\n",
            "init   \t [2.85126987 2.73434054 7.         0.88394733 8.         0.65545939]. \t  -0.48253390492139586 \t -0.4284992540085738\n",
            "1      \t [1.03697999e-02 2.51695028e+00 1.40000000e+01 6.19497248e-01\n",
            " 1.90000000e+01 7.01827417e-01]. \t  -0.47758423778274467 \t -0.4284992540085738\n",
            "2      \t [9.9089961  7.50235155 7.         0.67925503 5.         0.2471192 ]. \t  -0.6736712023667494 \t -0.4284992540085738\n",
            "3      \t [ 7.21196079  0.11384945  8.          0.67755731 17.          0.74323626]. \t  -0.4647919397201027 \t -0.4284992540085738\n",
            "4      \t [0.35283087 9.2950614  5.         0.93054416 3.         0.27625531]. \t  -0.670508947897148 \t -0.4284992540085738\n",
            "5      \t [ 0.19352731  8.09930152 12.          0.94917595  9.          0.75994611]. \t  -0.4324409379892192 \t -0.4284992540085738\n",
            "6      \t [9.76759527 0.48761621 7.         0.96362191 1.         0.137157  ]. \t  -0.6715547907624335 \t -0.4284992540085738\n",
            "7      \t [ 8.53387117  0.35946342 13.          0.65332712  6.          0.68302293]. \t  -0.4712802345283902 \t -0.4284992540085738\n",
            "8      \t [0.         0.         5.         0.5        1.03607737 0.1       ]. \t  -0.6744375402059073 \t -0.4284992540085738\n",
            "9      \t [ 0.30506512  9.51761383 13.          0.5277377  16.          0.92234588]. \t  -0.4390633386106087 \t -0.4284992540085738\n",
            "10     \t [ 9.21287749  3.65759887  5.          0.85687073 12.          0.88459516]. \t  -0.45867733996321497 \t -0.4284992540085738\n",
            "11     \t [ 7.34212739  6.00942781 13.          0.89177574 19.          0.18238365]. \t  -0.6705926103380092 \t -0.4284992540085738\n",
            "12     \t [ 0.08020415  5.86819932  5.          0.75400569 19.          0.89491758]. \t  -0.46498007849528483 \t -0.4284992540085738\n",
            "13     \t [ 0.28968499  1.98768694 14.          0.97996073 12.          0.15615797]. \t  -0.6725851052423111 \t -0.4284992540085738\n",
            "14     \t [5.4721942  4.70341961 9.         0.675658   1.         0.91263074]. \t  -0.44024324260844283 \t -0.4284992540085738\n",
            "15     \t [ 7.56572451  6.67046024 14.          0.88731871  4.          0.29940932]. \t  -0.6117537685654454 \t -0.4284992540085738\n",
            "16     \t [ 1.45285385  0.40498637  8.          0.93325877 14.          0.82817501]. \t  -0.4515772957447745 \t -0.4284992540085738\n",
            "17     \t [ 9.52566208  5.8884151   6.          0.78376025 19.          0.40148749]. \t  -0.6258103990077785 \t -0.4284992540085738\n",
            "18     \t [ 7.79919254  0.6285715  14.          0.87477175 15.          0.15632002]. \t  -0.6708453498973665 \t -0.4284992540085738\n",
            "19     \t [9.3259212  0.13501662 8.         0.77280133 8.         0.95737716]. \t  -0.44539283788135425 \t -0.4284992540085738\n",
            "20     \t [3.25216408 9.7762424  5.         0.77034466 9.         0.64475858]. \t  -0.4900500764131763 \t -0.4284992540085738\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.492050628419268"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xk0IPTSTbIl3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c152f3b2-929e-4a7c-eb03-1fdabb93b0c3"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 8 \n",
        "\n",
        "np.random.seed(run_num_8)\n",
        "surrogate_winner_8 = dtStudentProcess(d_cov_func, nu = df)\n",
        "\n",
        "X_train8, X_test8, y_train8, y_test8 = train_test_split(X, y, test_size=test_perc, random_state=run_num_8)\n",
        "\n",
        "def f_syn_polarity8(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_8, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train8, y=y_train8).mean())\n",
        "    return operator * score\n",
        "\n",
        "winner_8 = dGPGO_stp(surrogate_winner_8, Acquisition_new(util_winner), f_syn_polarity8, param, n_jobs = -1) # Define BayesOpt\n",
        "winner_8.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_winner_8 = winner_8.getResult()[0]\n",
        "params_winner_8['max_depth'] = int(params_winner_8['max_depth'])\n",
        "params_winner_8['min_child_weight'] = int(params_winner_8['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_winner_train8 = xgb.DMatrix(X_train8, y_train8)\n",
        "dX_winner_test8 = xgb.DMatrix(X_test8, y_test8)\n",
        "model_winner_8 = xgb.train(params_winner_8, dX_winner_train8)\n",
        "pred_winner_8 = model_winner_8.predict(dX_winner_test8)\n",
        "\n",
        "rmse_winner_8 = np.sqrt(mean_squared_error(pred_winner_8, y_test8))\n",
        "rmse_winner_8"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 1.29221489  0.77396898 11.          0.56062468  3.          0.25451879]. \t  -0.6231794271723858 \t -0.4674926150198253\n",
            "init   \t [ 0.10611429  1.37648061  8.          0.6050375  15.          0.790205  ]. \t  -0.4674926150198253 \t -0.4674926150198253\n",
            "init   \t [ 8.35239571  4.16908043  7.          0.67902746 14.          0.16683749]. \t  -0.6176439249709439 \t -0.4674926150198253\n",
            "init   \t [ 6.06301777  7.27127491 12.          0.87295531 11.          0.37694382]. \t  -0.5951520828762356 \t -0.4674926150198253\n",
            "init   \t [8.85209637 7.89269955 6.         0.72444854 3.         0.47766986]. \t  -0.5688519156580504 \t -0.4674926150198253\n",
            "1      \t [ 1.47825581  9.16010046 11.          0.96606546  1.          0.92358292]. \t  \u001b[92m-0.43518063190798095\u001b[0m \t -0.43518063190798095\n",
            "2      \t [ 7.52273592  2.30141271 14.          0.75505047 19.          0.77804046]. \t  -0.45158957529537797 \t -0.43518063190798095\n",
            "3      \t [1.64420908 9.47353882 5.         0.79993338 7.         0.15673599]. \t  -0.6175838614360663 \t -0.43518063190798095\n",
            "4      \t [ 1.02399475  9.65113789  9.          0.53947972 17.          0.8292164 ]. \t  -0.4719352875928771 \t -0.43518063190798095\n",
            "5      \t [8.14212583 0.61314553 6.         0.86488725 6.         0.28574491]. \t  -0.5954481959734638 \t -0.43518063190798095\n",
            "6      \t [ 8.76387285  7.83054608 13.          0.54159783  4.          0.24710422]. \t  -0.6207082177098945 \t -0.43518063190798095\n",
            "7      \t [ 9.0517509   9.45530828 11.          0.85305105 19.          0.79961763]. \t  -0.4589725747508552 \t -0.43518063190798095\n",
            "8      \t [ 7.77762087  0.97608919 14.          0.56979605  8.          0.73436998]. \t  -0.4483480686509709 \t -0.43518063190798095\n",
            "9      \t [2.67676373 3.95611485 5.         0.79739163 2.         0.89987835]. \t  -0.45674643004535687 \t -0.43518063190798095\n",
            "10     \t [ 0.62854908  4.77207773 14.          0.78182185 18.          0.73284936]. \t  -0.4461225555243119 \t -0.43518063190798095\n",
            "11     \t [3.06290827 1.66002331 6.         0.51411222 9.         0.98529356]. \t  -0.461628022517463 \t -0.43518063190798095\n",
            "12     \t [ 0.55545334  6.5757175   9.          0.88529579 12.          0.24203797]. \t  -0.6187020960077592 \t -0.43518063190798095\n",
            "13     \t [ 9.10053278  1.48753729 11.          0.9741735   1.          0.65977303]. \t  -0.5223704878111852 \t -0.43518063190798095\n",
            "14     \t [ 9.48329103  0.2076913   7.          0.76655383 19.          0.88164364]. \t  -0.4616094756962891 \t -0.43518063190798095\n",
            "15     \t [ 0.81390458  8.42248516 12.          0.78729971  7.          0.71328057]. \t  -0.5176932464043367 \t -0.43518063190798095\n",
            "16     \t [ 5.51535579  8.63063044  6.          0.64760098 18.          0.14185561]. \t  -0.6172074564902792 \t -0.43518063190798095\n",
            "17     \t [ 0.81210087  0.8533868  13.          0.66456412 12.          0.37769434]. \t  -0.5971747957799627 \t -0.43518063190798095\n",
            "18     \t [ 8.74769008  9.46041181  5.          0.76888057 12.          0.28611188]. \t  -0.5996998823178632 \t -0.43518063190798095\n",
            "19     \t [ 6.67918913  1.91467598 12.          0.83062216 13.          0.72788563]. \t  -0.45035357506572193 \t -0.43518063190798095\n",
            "20     \t [ 4.53564763  5.14157265 14.          0.52558575  1.          0.72718907]. \t  -0.45119962298136185 \t -0.43518063190798095\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.711175290813375"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9UroEj_RbLSb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee5d046c-a386-4359-efe0-7fb4838bba96"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 9 \n",
        "\n",
        "np.random.seed(run_num_9)\n",
        "surrogate_winner_9 = dtStudentProcess(d_cov_func, nu = df)\n",
        "\n",
        "X_train9, X_test9, y_train9, y_test9 = train_test_split(X, y, test_size=test_perc, random_state=run_num_9)\n",
        "\n",
        "def f_syn_polarity9(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_9, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train9, y=y_train9).mean())\n",
        "    return operator * score\n",
        "\n",
        "winner_9 = dGPGO_stp(surrogate_winner_9, Acquisition_new(util_winner), f_syn_polarity9, param, n_jobs = -1) # Define BayesOpt\n",
        "winner_9.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_winner_9 = winner_9.getResult()[0]\n",
        "params_winner_9['max_depth'] = int(params_winner_9['max_depth'])\n",
        "params_winner_9['min_child_weight'] = int(params_winner_9['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_winner_train9 = xgb.DMatrix(X_train9, y_train9)\n",
        "dX_winner_test9 = xgb.DMatrix(X_test9, y_test9)\n",
        "model_winner_9 = xgb.train(params_winner_9, dX_winner_train9)\n",
        "pred_winner_9 = model_winner_9.predict(dX_winner_test9)\n",
        "\n",
        "rmse_winner_9 = np.sqrt(mean_squared_error(pred_winner_9, y_test9))\n",
        "rmse_winner_9"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 8.0342804   5.27522296 13.          0.60741904 17.          0.15336222]. \t  -0.6890593508433319 \t -0.4886355541929722\n",
            "init   \t [7.56063644 1.79876369 7.         0.84869647 5.         0.55956942]. \t  -0.5532305414240943 \t -0.4886355541929722\n",
            "init   \t [2.43675539 0.20225128 6.         0.51669963 7.         0.4646446 ]. \t  -0.5609155518755553 \t -0.4886355541929722\n",
            "init   \t [ 7.6043547   4.73758376  9.          0.87564624 15.          0.65751286]. \t  -0.4886355541929722 \t -0.4886355541929722\n",
            "init   \t [ 4.70349314  6.71029586  6.          0.97680389 11.          0.41971652]. \t  -0.6548660487332898 \t -0.4886355541929722\n",
            "1      \t [ 0.20910387  7.25545521 14.          0.59407892  3.          0.95410586]. \t  \u001b[92m-0.4312356520914486\u001b[0m \t -0.4312356520914486\n",
            "2      \t [ 2.14892068  9.25577985 14.          0.84828472 11.          0.2793841 ]. \t  -0.6883959263608518 \t -0.4312356520914486\n",
            "3      \t [ 0.37305557  1.02756836 13.          0.57348052 13.          0.68410192]. \t  -0.4836872799099695 \t -0.4312356520914486\n",
            "4      \t [ 7.98252507  8.04957695 12.          0.6367811   3.          0.85775382]. \t  -0.4368210491589304 \t -0.4312356520914486\n",
            "5      \t [ 1.2588195   0.04438454  7.          0.70342864 18.          0.58554745]. \t  -0.4951281025154505 \t -0.4312356520914486\n",
            "6      \t [ 9.17710587  9.71573287  5.          0.9584376  19.          0.78072441]. \t  -0.4682275209845145 \t -0.4312356520914486\n",
            "7      \t [6.11416909 9.54959201 5.         0.91943284 1.         0.10968146]. \t  -0.6901756354808697 \t -0.4312356520914486\n",
            "8      \t [ 0.31270412  9.94818438  8.          0.84397658 19.          0.43974503]. \t  -0.5483333357303245 \t -0.4312356520914486\n",
            "9      \t [ 8.76504058  0.46155195 14.          0.61459938 10.          0.19095511]. \t  -0.6902913672662703 \t -0.4312356520914486\n",
            "10     \t [0.         3.84758969 7.07296986 0.5        1.07296986 0.1       ]. \t  -0.6905762407409385 \t -0.4312356520914486\n",
            "11     \t [ 0.76412088  5.02527282 14.          0.53794557 19.          0.6779987 ]. \t  -0.4862552398836468 \t -0.4312356520914486\n",
            "12     \t [ 8.7007577   8.84182108 11.          0.88074977 10.          0.89589347]. \t  -0.43700927635364534 \t -0.4312356520914486\n",
            "13     \t [ 4.60583171  0.06752    13.          0.94036906 18.          0.58293565]. \t  -0.47784115580365294 \t -0.4312356520914486\n",
            "14     \t [6.09359952e-03 8.65908473e+00 6.00000000e+00 6.67169539e-01\n",
            " 6.00000000e+00 4.18236274e-01]. \t  -0.6570878867409317 \t -0.4312356520914486\n",
            "15     \t [ 2.81530947  0.45687064 12.          0.85013604  4.          0.69362997]. \t  -0.4810465641500857 \t -0.4312356520914486\n",
            "16     \t [ 8.70866564  2.32451773 14.          0.72827616  3.          0.67258674]. \t  -0.4820355845046433 \t -0.4312356520914486\n",
            "17     \t [ 7.73870808  0.45484097  8.          0.78209444 11.          0.78106252]. \t  -0.45412926763907296 \t -0.4312356520914486\n",
            "18     \t [ 3.56621236  4.91536137  7.          0.71255145 19.          0.80604757]. \t  -0.4579607623923458 \t -0.4312356520914486\n",
            "19     \t [ 0.04792473  4.61613328  7.          0.67434301 13.          0.17016755]. \t  -0.6879545105363035 \t -0.4312356520914486\n",
            "20     \t [9.96752263 8.45749074 7.         0.92612294 6.         0.22771617]. \t  -0.6878594070452866 \t -0.4312356520914486\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.130636454660636"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7VgaJOoJbOIE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a56f312a-9370-4ed5-cefc-16194e9ab565"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 10 \n",
        "\n",
        "np.random.seed(run_num_10)\n",
        "surrogate_winner_10 = dtStudentProcess(d_cov_func, nu = df)\n",
        "\n",
        "X_train10, X_test10, y_train10, y_test10 = train_test_split(X, y, test_size=test_perc, random_state=run_num_10)\n",
        "\n",
        "def f_syn_polarity10(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_10, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train10, y=y_train10).mean())\n",
        "    return operator * score\n",
        "\n",
        "winner_10 = dGPGO_stp(surrogate_winner_10, Acquisition_new(util_winner), f_syn_polarity10, param, n_jobs = -1) # Define BayesOpt\n",
        "winner_10.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_winner_10 = winner_10.getResult()[0]\n",
        "params_winner_10['max_depth'] = int(params_winner_10['max_depth'])\n",
        "params_winner_10['min_child_weight'] = int(params_winner_10['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_winner_train10 = xgb.DMatrix(X_train10, y_train10)\n",
        "dX_winner_test10 = xgb.DMatrix(X_test10, y_test10)\n",
        "model_winner_10 = xgb.train(params_winner_10, dX_winner_train10)\n",
        "pred_winner_10 = model_winner_10.predict(dX_winner_test10)\n",
        "\n",
        "rmse_winner_10 = np.sqrt(mean_squared_error(pred_winner_10, y_test10))\n",
        "rmse_winner_10"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 6.53589585  1.15006943  6.          0.93623727 14.          0.13663866]. \t  -0.6929852048256443 \t -0.4483221221388197\n",
            "init   \t [ 3.97194461  2.33132197 14.          0.9533253  19.          0.26403087]. \t  -0.6932702114662359 \t -0.4483221221388197\n",
            "init   \t [ 7.43539415  0.69582081  9.          0.9763222  11.          0.12608349]. \t  -0.6931242497450955 \t -0.4483221221388197\n",
            "init   \t [ 9.82027485  3.39637684  9.          0.78522537 19.          0.87618843]. \t  -0.4483221221388197 \t -0.4483221221388197\n",
            "init   \t [ 0.57576207  5.82646405 12.          0.77704362 18.          0.11865044]. \t  -0.6946096327074635 \t -0.4483221221388197\n",
            "1      \t [ 3.17878299  6.40666671 10.          0.71928645  2.          0.37920433]. \t  -0.6059745513602156 \t -0.4483221221388197\n",
            "2      \t [ 3.37448201  8.10780464 13.          0.50477079 10.          0.914384  ]. \t  \u001b[92m-0.43629727149692393\u001b[0m \t -0.43629727149692393\n",
            "3      \t [ 9.185601    0.61355962 14.          0.74945204  2.          0.47381106]. \t  -0.5791988425691649 \t -0.43629727149692393\n",
            "4      \t [8.35411693 8.91554079 5.         0.73130633 7.         0.67857606]. \t  -0.5771647264709212 \t -0.43629727149692393\n",
            "5      \t [ 6.07704325  9.68005151  5.          0.52932858 19.          0.88896368]. \t  -0.4736105624167554 \t -0.43629727149692393\n",
            "6      \t [ 8.63718494  8.55246455 14.          0.55824531 18.          0.4228152 ]. \t  -0.6085826477793935 \t -0.43629727149692393\n",
            "7      \t [9.97805381 2.33960922 6.         0.72769906 2.         0.16112568]. \t  -0.693543381593374 \t -0.43629727149692393\n",
            "8      \t [0.  0.  5.  0.5 1.  0.1]. \t  -0.6914754878687801 \t -0.43629727149692393\n",
            "9      \t [ 0.03680135  0.50694581 14.          0.91124751  8.          0.87026982]. \t  \u001b[92m-0.4215933675272626\u001b[0m \t -0.4215933675272626\n",
            "10     \t [0.        0.        5.        0.5       9.2646171 0.1      ]. \t  -0.6921994104683225 \t -0.4215933675272626\n",
            "11     \t [ 9.77077394  8.65786606 12.          0.7706968   5.          0.42456779]. \t  -0.6030224397086726 \t -0.4215933675272626\n",
            "12     \t [ 0.05397878  3.75867073  5.          0.81182517 15.          0.65360487]. \t  -0.5751913770301034 \t -0.4215933675272626\n",
            "13     \t [0.50440288 9.75457432 5.         0.8137431  6.         0.15250773]. \t  -0.6941630128319229 \t -0.4215933675272626\n",
            "14     \t [ 0.64964288  9.19466467  8.          0.98075603 13.          0.30250269]. \t  -0.6043055075317849 \t -0.4215933675272626\n",
            "15     \t [2.78752086e+00 7.83961972e-03 1.10000000e+01 9.72670765e-01\n",
            " 2.00000000e+00 4.67980644e-01]. \t  -0.5809464009440812 \t -0.4215933675272626\n",
            "16     \t [3.60168939 5.29718037 7.         0.81976941 9.         0.8455105 ]. \t  -0.5135078695429718 \t -0.4215933675272626\n",
            "17     \t [7.64536232 8.37707893 5.         0.8688534  1.         0.76688319]. \t  -0.5294614889177678 \t -0.4215933675272626\n",
            "18     \t [ 9.76797494  9.255426    9.          0.86149378 12.          0.48283228]. \t  -0.5739073603085145 \t -0.4215933675272626\n",
            "19     \t [ 8.52646717  0.12245704 14.          0.80155281 15.          0.7100538 ]. \t  -0.5709119331337676 \t -0.4215933675272626\n",
            "20     \t [ 5.10066267  3.0910046  13.          0.6245447   6.          0.14935846]. \t  -0.693597235300419 \t -0.4215933675272626\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.436171669561738"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51z87uHWbRGr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52063147-d671-4a87-a408-7fe9cfa8d3aa"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 11 \n",
        "\n",
        "np.random.seed(run_num_11)\n",
        "surrogate_winner_11 = dtStudentProcess(d_cov_func, nu = df)\n",
        "\n",
        "X_train11, X_test11, y_train11, y_test11 = train_test_split(X, y, test_size=test_perc, random_state=run_num_11)\n",
        "\n",
        "def f_syn_polarity11(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_11, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train11, y=y_train11).mean())\n",
        "    return operator * score\n",
        "\n",
        "winner_11 = dGPGO_stp(surrogate_winner_11, Acquisition_new(util_winner), f_syn_polarity11, param, n_jobs = -1) # Define BayesOpt\n",
        "winner_11.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_winner_11 = winner_11.getResult()[0]\n",
        "params_winner_11['max_depth'] = int(params_winner_11['max_depth'])\n",
        "params_winner_11['min_child_weight'] = int(params_winner_11['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_winner_train11 = xgb.DMatrix(X_train11, y_train11)\n",
        "dX_winner_test11 = xgb.DMatrix(X_test11, y_test11)\n",
        "model_winner_11 = xgb.train(params_winner_11, dX_winner_train11)\n",
        "pred_winner_11 = model_winner_11.predict(dX_winner_test11)\n",
        "\n",
        "rmse_winner_11 = np.sqrt(mean_squared_error(pred_winner_11, y_test11))\n",
        "rmse_winner_11"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 9.81035663  8.2124785   9.          0.68487173 15.          0.28207988]. \t  -0.7133787013189323 \t -0.47317045732943896\n",
            "init   \t [ 4.91906771  1.38558332  6.          0.618878   19.          0.10263204]. \t  -0.7112023662943636 \t -0.47317045732943896\n",
            "init   \t [ 5.23426175  1.89971159 13.          0.76313779 17.          0.46597853]. \t  -0.47317045732943896 \t -0.47317045732943896\n",
            "init   \t [ 4.0204056   3.75167577  9.          0.86204818 17.          0.26560056]. \t  -0.7132057194891379 \t -0.47317045732943896\n",
            "init   \t [8.8704459  5.64917815 5.         0.51406171 1.         0.82826731]. \t  -0.48048685248123163 \t -0.47317045732943896\n",
            "1      \t [0.08349406 1.89250335 6.         0.66386636 1.         0.88641723]. \t  \u001b[92m-0.4484239383130805\u001b[0m \t -0.4484239383130805\n",
            "2      \t [ 5.35837408  0.31510164 14.          0.97519815  6.          0.3034681 ]. \t  -0.607905230502132 \t -0.4484239383130805\n",
            "3      \t [ 4.66315557  9.30076713 11.          0.83534598  8.          0.21303816]. \t  -0.7135282416096518 \t -0.4484239383130805\n",
            "4      \t [9.02046973 1.16865322 7.         0.6807515  8.         0.2962635 ]. \t  -0.6068266615066733 \t -0.4484239383130805\n",
            "5      \t [0.79988529 9.85424348 5.         0.5743044  3.         0.67901433]. \t  -0.48209943859810334 \t -0.4484239383130805\n",
            "6      \t [ 0.02057794  7.14089873 13.          0.90855163  1.          0.61823707]. \t  -0.45805892074172583 \t -0.4484239383130805\n",
            "7      \t [ 0.97603689  3.49871064  9.          0.91846837 10.          0.35158251]. \t  -0.6085109103402477 \t -0.4484239383130805\n",
            "8      \t [ 9.27240029  9.57413463 14.          0.58544186  3.          0.73779027]. \t  -0.45267877796490286 \t -0.4484239383130805\n",
            "9      \t [ 0.4027375   9.38447167 12.          0.94595612 15.          0.85310792]. \t  -0.451136347669006 \t -0.4484239383130805\n",
            "10     \t [ 1.3440527   8.97080429  5.          0.80277597 12.          0.63284111]. \t  -0.48349651102568225 \t -0.4484239383130805\n",
            "11     \t [ 7.94225687  3.68352192 13.          0.94902199 11.          0.95471054]. \t  \u001b[92m-0.422771965919715\u001b[0m \t -0.422771965919715\n",
            "12     \t [9.20332934 9.98167579 5.         0.57575366 6.         0.3717594 ]. \t  -0.6046860081027627 \t -0.422771965919715\n",
            "13     \t [5.68758712 2.62033511 9.         0.67614771 3.         0.67632008]. \t  -0.47052528509931824 \t -0.422771965919715\n",
            "14     \t [2.94143112e-03 7.03829535e-01 1.30000000e+01 5.98722168e-01\n",
            " 2.00000000e+00 1.01335355e-01]. \t  -0.7122743790062083 \t -0.422771965919715\n",
            "15     \t [ 0.81561387  9.31451412  5.          0.70594876 19.          0.98284163]. \t  -0.45988817082889427 \t -0.422771965919715\n",
            "16     \t [ 9.8968071   1.89203294 13.          0.7888373   2.          0.82539259]. \t  -0.45294282860990764 \t -0.422771965919715\n",
            "17     \t [ 8.37835592  2.52983026  5.          0.97806207 14.          0.6500905 ]. \t  -0.48098936825483635 \t -0.422771965919715\n",
            "18     \t [0.         0.         5.         0.5        6.80971405 0.1       ]. \t  -0.7118647031184924 \t -0.422771965919715\n",
            "19     \t [ 0.94895422  1.0618759  13.          0.53521104 13.          0.16819311]. \t  -0.713104540231849 \t -0.422771965919715\n",
            "20     \t [ 8.98906297  7.01351387 13.          0.77197065 19.          0.47265223]. \t  -0.475345282554151 \t -0.422771965919715\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.29564873555569"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8jZUeoWbTvn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "399c97e6-336d-44a0-a8f4-fe75e0ebc0f1"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 12\n",
        "\n",
        "np.random.seed(run_num_12)\n",
        "surrogate_winner_12 = dtStudentProcess(d_cov_func, nu = df)\n",
        "\n",
        "X_train12, X_test12, y_train12, y_test12 = train_test_split(X, y, test_size=test_perc, random_state=run_num_12)\n",
        "\n",
        "def f_syn_polarity12(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_12, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train12, y=y_train12).mean())\n",
        "    return operator * score\n",
        "\n",
        "winner_12 = dGPGO_stp(surrogate_winner_12, Acquisition_new(util_winner), f_syn_polarity12, param, n_jobs = -1) # Define BayesOpt\n",
        "winner_12.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_winner_12 = winner_12.getResult()[0]\n",
        "params_winner_12['max_depth'] = int(params_winner_12['max_depth'])\n",
        "params_winner_12['min_child_weight'] = int(params_winner_12['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_winner_train12 = xgb.DMatrix(X_train12, y_train12)\n",
        "dX_winner_test12 = xgb.DMatrix(X_test12, y_test12)\n",
        "model_winner_12 = xgb.train(params_winner_12, dX_winner_train12)\n",
        "pred_winner_12 = model_winner_12.predict(dX_winner_test12)\n",
        "\n",
        "rmse_winner_12 = np.sqrt(mean_squared_error(pred_winner_12, y_test12))\n",
        "rmse_winner_12"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 1.9151945   6.22108771  9.          0.89267929 16.          0.87460279]. \t  -0.4420077321695894 \t -0.4420077321695894\n",
            "init   \t [ 1.50636966  1.98518759 11.          0.67890863 17.          0.5381501 ]. \t  -0.5998539491151165 \t -0.4420077321695894\n",
            "init   \t [ 3.31015428  8.02639569  7.          0.75154158 13.          0.11992952]. \t  -0.7324503600335561 \t -0.4420077321695894\n",
            "init   \t [ 2.90728553  2.46394443  5.          0.94461307 17.          0.43194161]. \t  -0.5995790338913064 \t -0.4420077321695894\n",
            "init   \t [ 9.33140102  6.51378143  8.          0.76907392 18.          0.38515251]. \t  -0.5982523495604211 \t -0.4420077321695894\n",
            "1      \t [5.68098653 8.6912739  6.         0.52027807 1.         0.51631894]. \t  -0.5999713561677879 \t -0.4420077321695894\n",
            "2      \t [ 4.61089026  7.78540446 14.          0.61560224  6.          0.60910898]. \t  -0.5310432089168232 \t -0.4420077321695894\n",
            "3      \t [9.12195032 0.87787126 9.         0.87292886 5.         0.2622847 ]. \t  -0.7326988083055951 \t -0.4420077321695894\n",
            "4      \t [ 0.42901773  1.02682351 10.          0.62582208  7.          0.40519792]. \t  -0.600033100066767 \t -0.4420077321695894\n",
            "5      \t [ 9.48743388  2.23858916 14.          0.84020064 13.          0.41361313]. \t  -0.5921503481987865 \t -0.4420077321695894\n",
            "6      \t [ 6.85696089  9.7830661  14.          0.97897908 13.          0.89524334]. \t  \u001b[92m-0.43162480213383303\u001b[0m \t -0.43162480213383303\n",
            "7      \t [ 7.4487954   0.25501799  6.          0.59353564 11.          0.79320372]. \t  -0.5151049017617342 \t -0.43162480213383303\n",
            "8      \t [0.92483761 5.24951841 9.5523778  0.5        1.         0.1       ]. \t  -0.7336623780663796 \t -0.43162480213383303\n",
            "9      \t [0.  0.  5.  0.5 1.  0.1]. \t  -0.7334076855722197 \t -0.43162480213383303\n",
            "10     \t [9.69324843 9.42428289 7.         0.6384364  6.         0.83589131]. \t  -0.509004724458299 \t -0.43162480213383303\n",
            "11     \t [ 6.57352389  3.08967805 14.          0.73179937  1.          0.49930476]. \t  -0.6024722906486825 \t -0.43162480213383303\n",
            "12     \t [ 0.          1.63414632  5.          0.5        10.91374953  0.1       ]. \t  -0.7335141622603135 \t -0.43162480213383303\n",
            "13     \t [ 8.76460845  0.13227845  6.          0.75995694 17.          0.20887306]. \t  -0.7329728586443053 \t -0.43162480213383303\n",
            "14     \t [0.41721862 8.87551388 5.         0.69088851 6.         0.86973069]. \t  -0.468169334397084 \t -0.43162480213383303\n",
            "15     \t [ 1.07765263  6.20138171 14.          0.59898972 11.          0.65973368]. \t  -0.5325207293005967 \t -0.43162480213383303\n",
            "16     \t [ 4.36197463  9.75001875 13.          0.72270526 18.          0.55588964]. \t  -0.5980036083013376 \t -0.43162480213383303\n",
            "17     \t [ 8.95792842  8.74686238 12.          0.60340754  1.          0.64076061]. \t  -0.5341583767040248 \t -0.43162480213383303\n",
            "18     \t [ 9.82623366  8.33878675  6.          0.79015982 12.          0.42024491]. \t  -0.5979516918871897 \t -0.43162480213383303\n",
            "19     \t [ 5.54847359  0.06497189 14.          0.98018975  7.          0.10144988]. \t  -0.7326985019066897 \t -0.43162480213383303\n",
            "20     \t [ 7.25143535  1.86402411 14.          0.59310957 19.          0.53879904]. \t  -0.5987977809908415 \t -0.43162480213383303\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.07295168965299"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snTrqE2RbWbe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f0f9372-796c-423f-c0e8-750730709c32"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 13 \n",
        "\n",
        "np.random.seed(run_num_13)\n",
        "surrogate_winner_13 = dtStudentProcess(d_cov_func, nu = df)\n",
        "\n",
        "X_train13, X_test13, y_train13, y_test13 = train_test_split(X, y, test_size=test_perc, random_state=run_num_13)\n",
        "\n",
        "def f_syn_polarity13(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_13, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train13, y=y_train13).mean())\n",
        "    return operator * score\n",
        "\n",
        "winner_13 = dGPGO_stp(surrogate_winner_13, Acquisition_new(util_winner), f_syn_polarity13, param, n_jobs = -1) # Define BayesOpt\n",
        "winner_13.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_winner_13 = winner_13.getResult()[0]\n",
        "params_winner_13['max_depth'] = int(params_winner_13['max_depth'])\n",
        "params_winner_13['min_child_weight'] = int(params_winner_13['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_winner_train13 = xgb.DMatrix(X_train13, y_train13)\n",
        "dX_winner_test13 = xgb.DMatrix(X_test13, y_test13)\n",
        "model_winner_13 = xgb.train(params_winner_13, dX_winner_train13)\n",
        "pred_winner_13 = model_winner_13.predict(dX_winner_test13)\n",
        "\n",
        "rmse_winner_13 = np.sqrt(mean_squared_error(pred_winner_13, y_test13))\n",
        "rmse_winner_13"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 0.30967343  2.54282073  6.          0.9850871  17.          0.50848087]. \t  -0.5693403578256062 \t -0.4459810811310791\n",
            "init   \t [ 6.95119021  3.25593423 14.          0.8602375   8.          0.82055713]. \t  -0.5112217679187058 \t -0.4459810811310791\n",
            "init   \t [ 5.09252675  8.19921021 10.          0.78792002 16.          0.382158  ]. \t  -0.581693558072188 \t -0.4459810811310791\n",
            "init   \t [ 5.4465191   3.19841462 14.          0.90654581  5.          0.10359415]. \t  -0.6818818523119694 \t -0.4459810811310791\n",
            "init   \t [ 9.8393864   9.89619708  9.          0.63387763 10.          0.88979174]. \t  -0.4459810811310791 \t -0.4459810811310791\n",
            "1      \t [ 9.27118046  9.4344906  10.          0.71578295  1.          0.5463837 ]. \t  -0.5604994598662556 \t -0.4459810811310791\n",
            "2      \t [0.21910824 0.53051294 5.         0.62754372 4.         0.55299122]. \t  -0.578556985871312 \t -0.4459810811310791\n",
            "3      \t [0.32904954 9.80442032 8.         0.7285653  2.         0.94646442]. \t  \u001b[92m-0.43885135786496293\u001b[0m \t -0.43885135786496293\n",
            "4      \t [ 8.54553338  0.49573541 12.          0.81406546 19.          0.20999807]. \t  -0.6795516887212301 \t -0.43885135786496293\n",
            "5      \t [9.40145549 1.43629395 5.         0.63655396 1.         0.63002732]. \t  -0.5662893508870628 \t -0.43885135786496293\n",
            "6      \t [ 8.7238127   0.85665784  6.          0.75815556 12.          0.87180469]. \t  -0.4644522650882914 \t -0.43885135786496293\n",
            "7      \t [ 0.11400844  9.5774521   6.          0.90472821 11.          0.18513309]. \t  -0.6793968950126906 \t -0.43885135786496293\n",
            "8      \t [ 1.95513735  0.12334702 13.          0.54467307 14.          0.86324917]. \t  \u001b[92m-0.43794830902637205\u001b[0m \t -0.43794830902637205\n",
            "9      \t [ 2.89959158  8.10209185 14.          0.9094105   1.          0.86003598]. \t  \u001b[92m-0.42582758559630707\u001b[0m \t -0.42582758559630707\n",
            "10     \t [ 0.63533588  7.88973169 13.          0.71777205  8.          0.22258282]. \t  -0.6847473450330067 \t -0.42582758559630707\n",
            "11     \t [4.59853484 7.19189873 5.         0.92777191 6.         0.5760847 ]. \t  -0.5604008171822471 \t -0.42582758559630707\n",
            "12     \t [ 9.78022669  6.34740254  5.          0.81941991 15.          0.17866883]. \t  -0.6810440437950237 \t -0.42582758559630707\n",
            "13     \t [ 0.44924058  6.01817024 13.          0.62113095 19.          0.22629481]. \t  -0.6838258323776948 \t -0.42582758559630707\n",
            "14     \t [ 2.87751807  1.16984879  6.          0.80421316 10.          0.54045166]. \t  -0.5677361264662384 \t -0.42582758559630707\n",
            "15     \t [ 8.93459678  5.75013258 14.          0.86506851 14.          0.11026757]. \t  -0.6798789215383618 \t -0.42582758559630707\n",
            "16     \t [ 1.2199304   9.32862427  5.          0.98694875 18.          0.69275841]. \t  -0.5593626212561581 \t -0.42582758559630707\n",
            "17     \t [7.30733696 1.06961871 9.         0.6583612  6.         0.41488291]. \t  -0.5857440675365031 \t -0.42582758559630707\n",
            "18     \t [ 0.89951993  0.         10.92151675  0.5         2.92151675  0.1       ]. \t  -0.6861001150556951 \t -0.42582758559630707\n",
            "19     \t [ 5.54919942  8.87232522  5.          0.84416975 12.          0.31830005]. \t  -0.5998464916577737 \t -0.42582758559630707\n",
            "20     \t [ 9.03439965  9.39442441 14.          0.98025678 19.          0.55921955]. \t  -0.550861248380592 \t -0.42582758559630707\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.159065314760995"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23OXr6XLbY7_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "007f0370-f56c-4249-b40d-cca2e0b7cbec"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 14 \n",
        "\n",
        "np.random.seed(run_num_14)\n",
        "surrogate_winner_14 = dtStudentProcess(d_cov_func, nu = df)\n",
        "\n",
        "X_train14, X_test14, y_train14, y_test14 = train_test_split(X, y, test_size=test_perc, random_state=run_num_14)\n",
        "\n",
        "def f_syn_polarity14(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_14, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train14, y=y_train14).mean())\n",
        "    return operator * score\n",
        "\n",
        "winner_14 = dGPGO_stp(surrogate_winner_14, Acquisition_new(util_winner), f_syn_polarity14, param, n_jobs = -1) # Define BayesOpt\n",
        "winner_14.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_winner_14 = winner_14.getResult()[0]\n",
        "params_winner_14['max_depth'] = int(params_winner_14['max_depth'])\n",
        "params_winner_14['min_child_weight'] = int(params_winner_14['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_winner_train14 = xgb.DMatrix(X_train14, y_train14)\n",
        "dX_winner_test14 = xgb.DMatrix(X_test14, y_test14)\n",
        "model_winner_14 = xgb.train(params_winner_14, dX_winner_train14)\n",
        "pred_winner_14 = model_winner_14.predict(dX_winner_test14)\n",
        "\n",
        "rmse_winner_14 = np.sqrt(mean_squared_error(pred_winner_14, y_test14))\n",
        "rmse_winner_14"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 8.59560606  1.64569495 11.          0.89021195 15.          0.4857008 ]. \t  -0.6102441482204597 \t -0.4517949441804544\n",
            "init   \t [ 0.57460093  9.25007434  5.          0.56647642 17.          0.7533211 ]. \t  -0.5614002279921607 \t -0.4517949441804544\n",
            "init   \t [1.90674396 4.32019547 9.         0.93708225 3.         0.91775905]. \t  -0.4517949441804544 \t -0.4517949441804544\n",
            "init   \t [ 0.36770475  8.43368681 14.          0.6826844  15.          0.6291966 ]. \t  -0.5407924410004543 \t -0.4517949441804544\n",
            "init   \t [ 5.33840324  0.449182   14.          0.52045439  2.          0.48604556]. \t  -0.6242722768101235 \t -0.4517949441804544\n",
            "1      \t [ 9.54492219  9.07631824  8.          0.54579372 12.          0.38295996]. \t  -0.6262431675183743 \t -0.4517949441804544\n",
            "2      \t [9.53270461 0.83608685 6.         0.99976104 3.         0.60747265]. \t  -0.5533922160484371 \t -0.4517949441804544\n",
            "3      \t [ 1.15250646  0.51569303  5.          0.69322415 17.          0.16291682]. \t  -0.6917729046487754 \t -0.4517949441804544\n",
            "4      \t [ 8.50805687  8.26285095 14.          0.73716155  1.          0.12935726]. \t  -0.69096859473492 \t -0.4517949441804544\n",
            "5      \t [ 8.08070558  7.65123431 14.          0.59104644 19.          0.9302005 ]. \t  -0.45873776977159153 \t -0.4517949441804544\n",
            "6      \t [ 6.26901123  2.37239677  5.          0.95063922 11.          0.52876485]. \t  -0.6158458956422692 \t -0.4517949441804544\n",
            "7      \t [1.0907871  9.11150389 5.         0.58744161 9.         0.87439163]. \t  -0.4933476055445382 \t -0.4517949441804544\n",
            "8      \t [ 2.21919917  0.31865167 14.          0.54399684 14.          0.18056849]. \t  -0.6942842535038787 \t -0.4517949441804544\n",
            "9      \t [ 7.34435618  8.39108681  7.          0.81547994 19.          0.62767067]. \t  -0.546522525570801 \t -0.4517949441804544\n",
            "10     \t [ 4.67438649  9.12283668 14.          0.86393974  9.          0.28445597]. \t  -0.6909478110661416 \t -0.4517949441804544\n",
            "11     \t [0.         0.         5.         0.5        8.58005089 0.1       ]. \t  -0.6942441953461963 \t -0.4517949441804544\n",
            "12     \t [9.71394099 7.88076583 5.         0.92469304 3.         0.17093777]. \t  -0.6913576852026504 \t -0.4517949441804544\n",
            "13     \t [ 9.66474341  4.40707718 12.          0.56358802  7.          0.92398436]. \t  -0.4585198583543118 \t -0.4517949441804544\n",
            "14     \t [ 0.84209312  7.86749962 14.          0.8648448   1.          0.1570582 ]. \t  -0.6937301125121068 \t -0.4517949441804544\n",
            "15     \t [0.  0.  5.  0.5 1.  0.1]. \t  -0.6943016112587715 \t -0.4517949441804544\n",
            "16     \t [ 4.4386325   0.37972087 10.          0.61118217  8.          0.15198669]. \t  -0.6912722224579347 \t -0.4517949441804544\n",
            "17     \t [4.68629148 9.76798288 9.         0.83498325 1.         0.13284137]. \t  -0.6892620582991471 \t -0.4517949441804544\n",
            "18     \t [ 3.32112913  6.27078367  9.          0.98238085 14.          0.82374888]. \t  -0.5322541144293694 \t -0.4517949441804544\n",
            "19     \t [ 9.36009081  9.83549826 14.          0.62827914 14.          0.46331936]. \t  -0.6121800312967908 \t -0.4517949441804544\n",
            "20     \t [ 0.13684155  3.86104617 14.          0.82673561  9.          0.20283653]. \t  -0.6919355943160154 \t -0.4517949441804544\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.334202643456426"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgxvE7Irbbj_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f984caf1-b0ba-4d3e-d88b-f1298682850a"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 15 \n",
        "\n",
        "np.random.seed(run_num_15)\n",
        "surrogate_winner_15 = dtStudentProcess(d_cov_func, nu = df)\n",
        "\n",
        "X_train15, X_test15, y_train15, y_test15 = train_test_split(X, y, test_size=test_perc, random_state=run_num_15)\n",
        "\n",
        "def f_syn_polarity15(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_15, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train15, y=y_train15).mean())\n",
        "    return operator * score\n",
        "\n",
        "winner_15 = dGPGO_stp(surrogate_winner_15, Acquisition_new(util_winner), f_syn_polarity15, param, n_jobs = -1) # Define BayesOpt\n",
        "winner_15.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_winner_15 = winner_15.getResult()[0]\n",
        "params_winner_15['max_depth'] = int(params_winner_15['max_depth'])\n",
        "params_winner_15['min_child_weight'] = int(params_winner_15['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_winner_train15 = xgb.DMatrix(X_train15, y_train15)\n",
        "dX_winner_test15 = xgb.DMatrix(X_test15, y_test15)\n",
        "model_winner_15 = xgb.train(params_winner_15, dX_winner_train15)\n",
        "pred_winner_15 = model_winner_15.predict(dX_winner_test15)\n",
        "\n",
        "rmse_winner_15 = np.sqrt(mean_squared_error(pred_winner_15, y_test15))\n",
        "rmse_winner_15"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 5.64315424  8.97297685  6.          0.81493635 11.          0.33008564]. \t  -0.5946480910132583 \t -0.4392277798535851\n",
            "init   \t [9.51455383 9.60803032 5.         0.68485728 1.         0.30145081]. \t  -0.5968379804084254 \t -0.4392277798535851\n",
            "init   \t [ 8.11809889  6.32508536  7.          0.61766329 19.          0.51383159]. \t  -0.5092318398204285 \t -0.4392277798535851\n",
            "init   \t [ 7.28571251  5.36071389 10.          0.76240297 12.          0.12035683]. \t  -0.6346120183758519 \t -0.4392277798535851\n",
            "init   \t [ 6.80847768  3.59767194 14.          0.51365585 16.          0.89730673]. \t  -0.4392277798535851 \t -0.4392277798535851\n",
            "1      \t [ 2.28792065  8.58421878 12.          0.67551126  1.          0.37971264]. \t  -0.596161946051267 \t -0.4392277798535851\n",
            "2      \t [ 9.35473751  1.48895841 12.          0.55362474  4.          0.11106606]. \t  -0.6327865627642436 \t -0.4392277798535851\n",
            "3      \t [ 0.39252895  0.94193855 13.          0.73096739  8.          0.72579132]. \t  \u001b[92m-0.4332783925453615\u001b[0m \t -0.4332783925453615\n",
            "4      \t [1.42366593 3.16676945 5.         0.96590091 2.         0.27912003]. \t  -0.634271692553756 \t -0.4332783925453615\n",
            "5      \t [ 1.33182259  0.8125478   9.          0.94558338 18.          0.71777254]. \t  -0.4467705016632985 \t -0.4332783925453615\n",
            "6      \t [ 0.48538268  1.39730148  6.          0.8238236  11.          0.45741795]. \t  -0.5144696373796631 \t -0.4332783925453615\n",
            "7      \t [ 0.57885492  9.35143843  6.          0.69337529 17.          0.15510814]. \t  -0.6331051904120661 \t -0.4332783925453615\n",
            "8      \t [ 1.48529264  8.62972367 14.          0.64508709  9.          0.77692511]. \t  -0.43545941553430473 \t -0.4332783925453615\n",
            "9      \t [ 8.23428085  9.77240559 13.          0.85977854  8.          0.93876208]. \t  \u001b[92m-0.4281095031928478\u001b[0m \t -0.4281095031928478\n",
            "10     \t [9.60715195 0.38075751 5.         0.53304879 7.         0.99667797]. \t  -0.4680530726728757 \t -0.4281095031928478\n",
            "11     \t [ 9.85965179  9.04589828 13.          0.95564786 19.          0.1426314 ]. \t  -0.632071954329094 \t -0.4281095031928478\n",
            "12     \t [2.01656589 7.5067341  7.         0.97435364 6.         0.63334147]. \t  -0.4626787208905244 \t -0.4281095031928478\n",
            "13     \t [ 0.15053668  0.43786928 13.          0.62231098  1.          0.88145445]. \t  -0.43523183945633176 \t -0.4281095031928478\n",
            "14     \t [ 7.18775342  0.2062489   5.          0.98239776 17.          0.28043211]. \t  -0.634545784625454 \t -0.4281095031928478\n",
            "15     \t [8.27925095 3.51894083 6.         0.51392891 2.         0.22936394]. \t  -0.6317841836587383 \t -0.4281095031928478\n",
            "16     \t [ 2.22607821  9.62798432 12.          0.81069273 19.          0.90626708]. \t  -0.4349388112942007 \t -0.4281095031928478\n",
            "17     \t [ 8.4324691   8.27251391 11.          0.71354715  2.          0.54685464]. \t  -0.498601956775111 \t -0.4281095031928478\n",
            "18     \t [5.1497948  0.         8.13395887 0.5        6.13395887 0.1       ]. \t  -0.6316979120544739 \t -0.4281095031928478\n",
            "19     \t [ 8.20750612  0.08409751 10.          0.75789373 13.          0.95917147]. \t  -0.44101511925014397 \t -0.4281095031928478\n",
            "20     \t [ 1.99999669  4.10746221 13.          0.59478641 13.          0.23668717]. \t  -0.6332580216283775 \t -0.4281095031928478\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.189953800077167"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ye4UEpNabeCL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd960a28-84d9-4238-bb38-91de33e35065"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 16 \n",
        "\n",
        "np.random.seed(run_num_16)\n",
        "surrogate_winner_16 = dtStudentProcess(d_cov_func, nu = df)\n",
        "\n",
        "X_train16, X_test16, y_train16, y_test16 = train_test_split(X, y, test_size=test_perc, random_state=run_num_16)\n",
        "\n",
        "def f_syn_polarity16(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_16, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train16, y=y_train16).mean())\n",
        "    return operator * score\n",
        "\n",
        "winner_16 = dGPGO_stp(surrogate_winner_16, Acquisition_new(util_winner), f_syn_polarity16, param, n_jobs = -1) # Define BayesOpt\n",
        "winner_16.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_winner_16 = winner_16.getResult()[0]\n",
        "params_winner_16['max_depth'] = int(params_winner_16['max_depth'])\n",
        "params_winner_16['min_child_weight'] = int(params_winner_16['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_winner_train16 = xgb.DMatrix(X_train16, y_train16)\n",
        "dX_winner_test16 = xgb.DMatrix(X_test16, y_test16)\n",
        "model_winner_16 = xgb.train(params_winner_16, dX_winner_train16)\n",
        "pred_winner_16 = model_winner_16.predict(dX_winner_test16)\n",
        "\n",
        "rmse_winner_16 = np.sqrt(mean_squared_error(pred_winner_16, y_test16))\n",
        "rmse_winner_16"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [8.63948332 3.16366002 7.         0.75395583 1.         0.56999262]. \t  -0.5713220948397237 \t -0.4331621293825035\n",
            "init   \t [ 3.3426621   9.77029134 14.          0.93751218 17.          0.80549151]. \t  -0.4331621293825035 \t -0.4331621293825035\n",
            "init   \t [ 3.36871073  4.8308288  13.          0.91984982 14.          0.48829226]. \t  -0.5561611494046279 \t -0.4331621293825035\n",
            "init   \t [ 8.84540736  1.59826081  8.          0.75772888 14.          0.29029227]. \t  -0.6835926126526815 \t -0.4331621293825035\n",
            "init   \t [ 7.74981136  8.34377636 13.          0.83182409 15.          0.19378622]. \t  -0.7677243885128009 \t -0.4331621293825035\n",
            "1      \t [0.90186209 9.9119065  9.         0.96247707 9.         0.82473059]. \t  -0.44000666714577197 \t -0.4331621293825035\n",
            "2      \t [ 3.6191694   8.20490707  5.          0.81280746 16.          0.99306688]. \t  -0.4581574120389309 \t -0.4331621293825035\n",
            "3      \t [2.15072525 1.72325427 6.         0.7282133  7.         0.66792915]. \t  -0.510524353390639 \t -0.4331621293825035\n",
            "4      \t [ 4.49483609  7.05102179 14.          0.67776057  3.          0.50727602]. \t  -0.5621481895640063 \t -0.4331621293825035\n",
            "5      \t [ 4.63470447  0.29792913 13.          0.70911522  7.          0.30290504]. \t  -0.6845759859311331 \t -0.4331621293825035\n",
            "6      \t [9.17745543 7.46026312 5.         0.6284814  9.         0.72652494]. \t  -0.47418011145069017 \t -0.4331621293825035\n",
            "7      \t [2.7669192  6.70200041 7.         0.83888371 1.         0.66194747]. \t  -0.49144662381331905 \t -0.4331621293825035\n",
            "8      \t [ 1.86481894  0.99601669  6.          0.66208635 14.          0.74317901]. \t  -0.4660455507597825 \t -0.4331621293825035\n",
            "9      \t [0.  0.  5.  0.5 1.  0.1]. \t  -0.7670392049994392 \t -0.4331621293825035\n",
            "10     \t [ 9.07329721  6.04353062  6.          0.62940057 19.          0.1206801 ]. \t  -0.7674777671186626 \t -0.4331621293825035\n",
            "11     \t [ 8.54895835  0.69961229 14.          0.57654595 19.          0.20454627]. \t  -0.7677953049378372 \t -0.4331621293825035\n",
            "12     \t [8.53167413 9.15372778 5.         0.57519522 2.         0.50414261]. \t  -0.5950876842055789 \t -0.4331621293825035\n",
            "13     \t [ 9.26465723  9.86116065 13.          0.52207502  6.          0.17735657]. \t  -0.7670798777253309 \t -0.4331621293825035\n",
            "14     \t [ 0.3330471   0.01629553 13.          0.91520431 11.          0.22019979]. \t  -0.7679362913445089 \t -0.4331621293825035\n",
            "15     \t [ 0.76287087  0.49621071 13.          0.81150873 18.          0.76762748]. \t  -0.4363142295126112 \t -0.4331621293825035\n",
            "16     \t [9.9506446  0.41258268 8.         0.70130417 7.         0.77986813]. \t  -0.4500722951786756 \t -0.4331621293825035\n",
            "17     \t [ 8.84742705  5.08354098 12.          0.54989851 10.          0.27761013]. \t  -0.7666143119340576 \t -0.4331621293825035\n",
            "18     \t [ 6.47050729  9.70570023  8.          0.55783027 12.          0.88747479]. \t  -0.4508283906225067 \t -0.4331621293825035\n",
            "19     \t [ 9.46331313  2.81887466 13.          0.76076385  3.          0.42087971]. \t  -0.6854869575909893 \t -0.4331621293825035\n",
            "20     \t [ 0.          4.217687   11.64313745  0.5         1.64313745  0.1       ]. \t  -0.7669723054879378 \t -0.4331621293825035\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.501874502356394"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Biq2Uaa5bg3U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9db47f06-a671-4a5e-b28d-30ab0226c68d"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 17 \n",
        "\n",
        "np.random.seed(run_num_17)\n",
        "surrogate_winner_17 = dtStudentProcess(d_cov_func, nu = df)\n",
        "\n",
        "X_train17, X_test17, y_train17, y_test17 = train_test_split(X, y, test_size=test_perc, random_state=run_num_17)\n",
        "\n",
        "def f_syn_polarity17(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_17, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train17, y=y_train17).mean())\n",
        "    return operator * score\n",
        "\n",
        "winner_17 = dGPGO_stp(surrogate_winner_17, Acquisition_new(util_winner), f_syn_polarity17, param, n_jobs = -1) # Define BayesOpt\n",
        "winner_17.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_winner_17 = winner_17.getResult()[0]\n",
        "params_winner_17['max_depth'] = int(params_winner_17['max_depth'])\n",
        "params_winner_17['min_child_weight'] = int(params_winner_17['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_winner_train17 = xgb.DMatrix(X_train17, y_train17)\n",
        "dX_winner_test17 = xgb.DMatrix(X_test17, y_test17)\n",
        "model_winner_17 = xgb.train(params_winner_17, dX_winner_train17)\n",
        "pred_winner_17 = model_winner_17.predict(dX_winner_test17)\n",
        "\n",
        "rmse_winner_17 = np.sqrt(mean_squared_error(pred_winner_17, y_test17))\n",
        "rmse_winner_17"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 1.85575275  3.86599303 13.          0.85645882  2.          0.78699813]. \t  -0.4597056260580034 \t -0.4597056260580034\n",
            "init   \t [1.97780831 2.92527676 8.         0.92268607 9.         0.39017299]. \t  -0.5687207024308397 \t -0.4597056260580034\n",
            "init   \t [ 7.16656007  1.028324    5.          0.96975483 15.          0.90932849]. \t  -0.4688025715067707 \t -0.4597056260580034\n",
            "init   \t [ 2.46007304  9.53596657  7.          0.71704714 11.          0.17659118]. \t  -0.6983036523024195 \t -0.4597056260580034\n",
            "init   \t [3.44161135 9.92427612 8.         0.96331165 2.         0.29160186]. \t  -0.5675253504862205 \t -0.4597056260580034\n",
            "1      \t [ 9.98828606  3.49693094 13.          0.57156882  8.          0.14604355]. \t  -0.6992595730796743 \t -0.4597056260580034\n",
            "2      \t [ 6.95619362  6.95953721 12.          0.92317057 19.          0.34981799]. \t  -0.5628121282495753 \t -0.4597056260580034\n",
            "3      \t [8.58397417 0.63190934 7.         0.80049626 2.         0.98851802]. \t  \u001b[92m-0.453932525933606\u001b[0m \t -0.453932525933606\n",
            "4      \t [ 1.79054908  0.97767001 14.          0.72051649 17.          0.75055942]. \t  -0.4586950539387324 \t -0.453932525933606\n",
            "5      \t [0.  0.  5.  0.5 1.  0.1]. \t  -0.6990710246949499 \t -0.453932525933606\n",
            "6      \t [ 0.19331023  6.0899655   6.          0.97237498 19.          0.59441475]. \t  -0.54171231691412 \t -0.453932525933606\n",
            "7      \t [ 8.95440724  5.31304705 13.          0.73043408  1.          0.56398539]. \t  -0.5373247826064305 \t -0.453932525933606\n",
            "8      \t [ 8.26790477  9.9524702   5.          0.59716297 16.          0.88254342]. \t  -0.47691281251538375 \t -0.453932525933606\n",
            "9      \t [ 0.90443942  7.22834774 14.          0.92580232 12.          0.13465218]. \t  -0.6965996897404226 \t -0.453932525933606\n",
            "10     \t [ 8.52285971  0.31262108 12.          0.8797653  16.          0.34954268]. \t  -0.5627660085466504 \t -0.453932525933606\n",
            "11     \t [ 7.87455923  9.60434668 13.          0.6097695  11.          0.18672661]. \t  -0.6988278637935389 \t -0.453932525933606\n",
            "12     \t [7.77469206 6.81026607 6.         0.84173013 6.         0.4321758 ]. \t  -0.5485988100854867 \t -0.453932525933606\n",
            "13     \t [ 1.78559475  9.94726594 14.          0.84124706  5.          0.41412983]. \t  -0.5643771257102934 \t -0.453932525933606\n",
            "14     \t [ 3.55475249  2.77437165 14.          0.64501595  8.          0.71111614]. \t  -0.5328675498676292 \t -0.453932525933606\n",
            "15     \t [9.7645138  6.96943344 7.         0.64264761 1.         0.5601612 ]. \t  -0.5443333775093777 \t -0.453932525933606\n",
            "16     \t [ 1.13635919  9.59512209 12.          0.5730343  19.          0.36892674]. \t  -0.5697966935770927 \t -0.453932525933606\n",
            "17     \t [6.67904119 0.         5.         0.5        9.0059732  0.1       ]. \t  -0.6997630724078957 \t -0.453932525933606\n",
            "18     \t [2.67758022 4.58792547 5.         0.80193375 3.         0.36129163]. \t  -0.5801383609237367 \t -0.453932525933606\n",
            "19     \t [ 0.          0.07974328  8.62696444  0.5        15.62696444  0.1       ]. \t  -0.6994511860723951 \t -0.453932525933606\n",
            "20     \t [ 4.7028715   4.84232296 11.          0.86477956 14.          0.62900428]. \t  -0.534274537118095 \t -0.453932525933606\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.347883948412589"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5H4MWSXFcZjO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a70f8221-70ea-40b2-d053-c768ea7031a2"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 18 \n",
        "\n",
        "np.random.seed(run_num_18)\n",
        "surrogate_winner_18 = dtStudentProcess(d_cov_func, nu = df)\n",
        "\n",
        "X_train18, X_test18, y_train18, y_test18 = train_test_split(X, y, test_size=test_perc, random_state=run_num_18)\n",
        "\n",
        "def f_syn_polarity18(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_18, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train18, y=y_train18).mean())\n",
        "    return operator * score\n",
        "\n",
        "winner_18 = dGPGO_stp(surrogate_winner_18, Acquisition_new(util_winner), f_syn_polarity18, param, n_jobs = -1) # Define BayesOpt\n",
        "winner_18.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_winner_18 = winner_18.getResult()[0]\n",
        "params_winner_18['max_depth'] = int(params_winner_18['max_depth'])\n",
        "params_winner_18['min_child_weight'] = int(params_winner_18['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_winner_train18 = xgb.DMatrix(X_train18, y_train18)\n",
        "dX_winner_test18 = xgb.DMatrix(X_test18, y_test18)\n",
        "model_winner_18 = xgb.train(params_winner_18, dX_winner_train18)\n",
        "pred_winner_18 = model_winner_18.predict(dX_winner_test18)\n",
        "\n",
        "rmse_winner_18 = np.sqrt(mean_squared_error(pred_winner_18, y_test18))\n",
        "rmse_winner_18"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 8.73429403  9.68540663 10.          0.68875849  9.          0.48011572]. \t  -0.536396330012702 \t -0.4337279026393176\n",
            "init   \t [ 6.12033333  7.66062926  8.          0.76133734 13.          0.93379456]. \t  -0.4433380408289477 \t -0.4337279026393176\n",
            "init   \t [ 1.46524679  7.01527914  7.          0.90913299 10.          0.36016753]. \t  -0.5541638729833303 \t -0.4337279026393176\n",
            "init   \t [ 9.73855241  3.33774046 14.          0.53290419  7.          0.7088681 ]. \t  -0.5062350384654568 \t -0.4337279026393176\n",
            "init   \t [ 3.00618018  1.82702795 11.          0.75681389 14.          0.98627449]. \t  -0.4337279026393176 \t -0.4337279026393176\n",
            "1      \t [ 1.46586883  1.13810325 14.          0.90052794  5.          0.16182112]. \t  -0.6179729230954069 \t -0.4337279026393176\n",
            "2      \t [ 9.17850108  5.22481201 14.          0.60105853 18.          0.63442006]. \t  -0.5052903785308961 \t -0.4337279026393176\n",
            "3      \t [6.20612396 4.37552923 6.         0.71941082 3.         0.90943147]. \t  -0.45137007433293086 \t -0.4337279026393176\n",
            "4      \t [ 0.42107725  9.92938812 12.          0.92462675 18.          0.88352347]. \t  \u001b[92m-0.4255403331751115\u001b[0m \t -0.4255403331751115\n",
            "5      \t [ 1.45418057  8.89399    11.          0.93175096  1.          0.52391385]. \t  -0.5358666580451181 \t -0.4255403331751115\n",
            "6      \t [0.  0.  5.  0.5 1.  0.1]. \t  -0.6230798973009806 \t -0.4255403331751115\n",
            "7      \t [ 8.67626106  0.1397848   6.          0.55771681 18.          0.89120888]. \t  -0.4650266593401632 \t -0.4255403331751115\n",
            "8      \t [ 7.20807715  7.41236348  5.          0.52425285 19.          0.24651851]. \t  -0.6221585758998005 \t -0.4255403331751115\n",
            "9      \t [7.94412856 1.24068931 7.         0.96649935 9.         0.47928692]. \t  -0.5420539933608051 \t -0.4255403331751115\n",
            "10     \t [ 2.66410938  9.83743919 13.          0.58899688  8.          0.29675269]. \t  -0.5550068432497505 \t -0.4255403331751115\n",
            "11     \t [ 8.62753755  4.44899956 12.          0.98231134  1.          0.88389967]. \t  -0.4267991891390327 \t -0.4255403331751115\n",
            "12     \t [ 1.2277143   1.05411485  5.          0.50198686 13.          0.89258454]. \t  -0.470676137340884 \t -0.4255403331751115\n",
            "13     \t [ 3.86727486  0.04582369 10.          0.54629234  1.          0.59406244]. \t  -0.5128905589861811 \t -0.4255403331751115\n",
            "14     \t [9.89654007 7.91459554 5.         0.64703423 6.         0.95875063]. \t  -0.46098420452454364 \t -0.4255403331751115\n",
            "15     \t [ 9.49381141  1.31696272 11.          0.51283555 13.          0.25364349]. \t  -0.6228089112997249 \t -0.4255403331751115\n",
            "16     \t [ 5.63166852  5.17712738 14.          0.50846445 12.          0.90154746]. \t  -0.4392645322335234 \t -0.4255403331751115\n",
            "17     \t [ 1.04974938  7.3117691   5.          0.60120794 17.          0.83587336]. \t  -0.5008428213738166 \t -0.4255403331751115\n",
            "18     \t [0.80615891 4.04370581 8.         0.97632304 4.         0.55920826]. \t  -0.5382235837397811 \t -0.4255403331751115\n",
            "19     \t [5.81345214 9.69898534 7.         0.79242119 1.         0.72012886]. \t  -0.47859716684644626 \t -0.4255403331751115\n",
            "20     \t [3.87981938 0.         5.         0.5        5.45574212 0.1       ]. \t  -0.6240014800170586 \t -0.4255403331751115\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3.9005482179018887"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-EgxA8k4ccL6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2895281e-d5f1-4653-aac1-590534b39e49"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 19 \n",
        "\n",
        "np.random.seed(run_num_19)\n",
        "surrogate_winner_19 = dtStudentProcess(d_cov_func, nu = df)\n",
        "\n",
        "X_train19, X_test19, y_train19, y_test19 = train_test_split(X, y, test_size=test_perc, random_state=run_num_19)\n",
        "\n",
        "def f_syn_polarity19(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_19, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train19, y=y_train19).mean())\n",
        "    return operator * score\n",
        "\n",
        "winner_19 = dGPGO_stp(surrogate_winner_19, Acquisition_new(util_winner), f_syn_polarity19, param, n_jobs = -1) # Define BayesOpt\n",
        "winner_19.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_winner_19 = winner_19.getResult()[0]\n",
        "params_winner_19['max_depth'] = int(params_winner_19['max_depth'])\n",
        "params_winner_19['min_child_weight'] = int(params_winner_19['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_winner_train19 = xgb.DMatrix(X_train19, y_train19)\n",
        "dX_winner_test19 = xgb.DMatrix(X_test19, y_test19)\n",
        "model_winner_19 = xgb.train(params_winner_19, dX_winner_train19)\n",
        "pred_winner_19 = model_winner_19.predict(dX_winner_test19)\n",
        "\n",
        "rmse_winner_19 = np.sqrt(mean_squared_error(pred_winner_19, y_test19))\n",
        "rmse_winner_19"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 8.24520174  9.84626759 12.          0.81846905 12.          0.27594309]. \t  -0.6624515538441494 \t -0.4321567975765851\n",
            "init   \t [ 8.6021676   8.64300514 14.          0.95067208 18.          0.13363564]. \t  -0.663790961968593 \t -0.4321567975765851\n",
            "init   \t [ 3.02486657  6.06229509 10.          0.50494482  9.          0.10218675]. \t  -0.668875700942164 \t -0.4321567975765851\n",
            "init   \t [ 6.57586195  8.01231848  7.          0.63945407 18.          0.49206406]. \t  -0.5640092362987262 \t -0.4321567975765851\n",
            "init   \t [ 6.05008856  8.15558548 14.          0.64718943  8.          0.90611106]. \t  -0.4321567975765851 \t -0.4321567975765851\n",
            "1      \t [ 0.85655458  0.69294498  9.          0.90992265 19.          0.25605959]. \t  -0.6632879756016564 \t -0.4321567975765851\n",
            "2      \t [ 1.16239725  9.89254371 11.          0.98469188  1.          0.20271996]. \t  -0.6659068700895453 \t -0.4321567975765851\n",
            "3      \t [9.26425961 9.717621   5.         0.8856331  5.         0.65589573]. \t  -0.5256636565447839 \t -0.4321567975765851\n",
            "4      \t [8.38158931 1.39953008 5.         0.6836482  9.         0.46934447]. \t  -0.5657479189361881 \t -0.4321567975765851\n",
            "5      \t [ 9.89729152  2.88709408 11.          0.55121781  3.          0.58381748]. \t  -0.5217676759230677 \t -0.4321567975765851\n",
            "6      \t [1.48550425 0.         6.00656105 0.5        1.         0.1       ]. \t  -0.66804117799612 \t -0.4321567975765851\n",
            "7      \t [ 0.52039992  0.56380532 14.          0.70324781  2.          0.77427978]. \t  -0.44119318049659856 \t -0.4321567975765851\n",
            "8      \t [ 9.45607821  0.96438893 14.          0.56821928 16.          0.48995458]. \t  -0.560546633196372 \t -0.4321567975765851\n",
            "9      \t [ 0.56432836  7.79494932 13.          0.55871992 19.          0.10266045]. \t  -0.6660467600439527 \t -0.4321567975765851\n",
            "10     \t [ 3.16676683  1.78770811 13.          0.5650927  13.          0.7417697 ]. \t  -0.4472490143246649 \t -0.4321567975765851\n",
            "11     \t [ 0.64379529  1.54731928  6.          0.82512479 11.          0.6319422 ]. \t  -0.5209201684825245 \t -0.4321567975765851\n",
            "12     \t [ 7.73804622  1.66772104  5.          0.66266078 17.          0.87441258]. \t  -0.4732994211333518 \t -0.4321567975765851\n",
            "13     \t [ 7.46605559  8.55652968 13.          0.99452706  1.          0.43764631]. \t  -0.5573240867828692 \t -0.4321567975765851\n",
            "14     \t [ 0.36172735  9.41325373  5.          0.88367823 18.          0.31289693]. \t  -0.610279476214515 \t -0.4321567975765851\n",
            "15     \t [6.50533986 6.11396147 7.         0.78114466 1.         0.75288314]. \t  -0.45838856364045305 \t -0.4321567975765851\n",
            "16     \t [1.7958895  9.19371155 5.         0.54448974 5.         0.93868386]. \t  -0.4669880318809955 \t -0.4321567975765851\n",
            "17     \t [ 9.62487731  6.86253449  5.          0.66465947 12.          0.21107909]. \t  -0.6659019290689656 \t -0.4321567975765851\n",
            "18     \t [ 1.7944313   9.07405715  5.          0.89658957 12.          0.48452475]. \t  -0.5646254130385107 \t -0.4321567975765851\n",
            "19     \t [ 5.75277105  1.25995511 14.          0.97274397  8.          0.39753437]. \t  -0.6135363407765908 \t -0.4321567975765851\n",
            "20     \t [ 2.00664037  5.88682269 14.          0.94696775  4.          0.80981462]. \t  -0.4349577325456077 \t -0.4321567975765851\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.2307134915487135"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08y-IGpyceje",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6370974f-8550-4a5e-e5fa-af122095454e"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 20 \n",
        "\n",
        "np.random.seed(run_num_20)\n",
        "surrogate_winner_20 = dtStudentProcess(d_cov_func, nu = df)\n",
        "\n",
        "X_train20, X_test20, y_train20, y_test20 = train_test_split(X, y, test_size=test_perc, random_state=run_num_20)\n",
        "\n",
        "def f_syn_polarity20(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_20, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train20, y=y_train20).mean())\n",
        "    return operator * score\n",
        "\n",
        "winner_20 = dGPGO_stp(surrogate_winner_20, Acquisition_new(util_winner), f_syn_polarity20, param, n_jobs = -1) # Define BayesOpt\n",
        "winner_20.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_winner_20 = winner_20.getResult()[0]\n",
        "params_winner_20['max_depth'] = int(params_winner_20['max_depth'])\n",
        "params_winner_20['min_child_weight'] = int(params_winner_20['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_winner_train20 = xgb.DMatrix(X_train20, y_train20)\n",
        "dX_winner_test20 = xgb.DMatrix(X_test20, y_test20)\n",
        "model_winner_20 = xgb.train(params_winner_20, dX_winner_train20)\n",
        "pred_winner_20 = model_winner_20.predict(dX_winner_test20)\n",
        "\n",
        "rmse_winner_20 = np.sqrt(mean_squared_error(pred_winner_20, y_test20))\n",
        "rmse_winner_20"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 5.70517285  5.64528755 11.          0.51213999 19.          0.57883228]. \t  -0.5510634045248362 \t -0.4667910180825121\n",
            "init   \t [0.68106288 5.8452906  6.         0.58037829 8.         0.87974899]. \t  -0.4667910180825121 \t -0.4667910180825121\n",
            "init   \t [ 4.04121426  7.17971417 11.          0.57603584 10.          0.19954225]. \t  -0.6541702754539586 \t -0.4667910180825121\n",
            "init   \t [ 5.27556761  8.67655329 13.          0.54252273  4.          0.48570796]. \t  -0.5813476654605825 \t -0.4667910180825121\n",
            "init   \t [ 5.30003889  5.73946822 12.          0.52666698 16.          0.77467019]. \t  -0.5078849923055367 \t -0.4667910180825121\n",
            "1      \t [ 9.25190708  0.65043072 10.          0.56676233  5.          0.41289847]. \t  -0.6164858434894066 \t -0.4667910180825121\n",
            "2      \t [8.65689736 8.11684598 5.         0.53907072 1.         0.87574215]. \t  -0.47576906935129115 \t -0.4667910180825121\n",
            "3      \t [6.49728107e-01 7.72862043e-04 7.00000000e+00 5.71905117e-01\n",
            " 2.00000000e+00 1.02014940e-01]. \t  -0.6533990377801203 \t -0.4667910180825121\n",
            "4      \t [ 9.51806562  0.44900923  6.          0.62997227 16.          0.40028974]. \t  -0.6257141325107729 \t -0.4667910180825121\n",
            "5      \t [ 0.48808143  1.97030839  7.          0.79892377 16.          0.98477167]. \t  \u001b[92m-0.4602371335347518\u001b[0m \t -0.4602371335347518\n",
            "6      \t [ 9.98029391  8.23632437  6.          0.63606408 13.          0.22908429]. \t  -0.6532745897130707 \t -0.4602371335347518\n",
            "7      \t [ 3.9245879   0.87058505 14.          0.68269183  1.          0.34048922]. \t  -0.623283664424315 \t -0.4602371335347518\n",
            "8      \t [ 0.45468431  0.13267263 10.          0.667214    9.          0.47071141]. \t  -0.5807933165122651 \t -0.4602371335347518\n",
            "9      \t [ 4.64792903  9.65609988  6.          0.78696753 17.          0.90406502]. \t  -0.47227152594951016 \t -0.4602371335347518\n",
            "10     \t [ 9.73713042  9.77875019 14.          0.8552366  13.          0.6588042 ]. \t  -0.5450528828222597 \t -0.4602371335347518\n",
            "11     \t [ 9.74564452  1.02954501 14.          0.98964301 12.          0.47743205]. \t  -0.5773481317621234 \t -0.4602371335347518\n",
            "12     \t [ 7.76906477  2.6060444   5.          0.53491145 10.          0.41560504]. \t  -0.627993339841767 \t -0.4602371335347518\n",
            "13     \t [6.49958993 1.98622034 6.         0.83635209 1.         0.2931143 ]. \t  -0.6276302072456058 \t -0.4602371335347518\n",
            "14     \t [1.36051555 9.95957797 7.         0.9242116  2.         0.87097462]. \t  \u001b[92m-0.449167771194887\u001b[0m \t -0.449167771194887\n",
            "15     \t [ 0.20978943  8.45447688 14.          0.77351134 17.          0.2656195 ]. \t  -0.6586945947582249 \t -0.449167771194887\n",
            "16     \t [7.55121074 9.86496533 6.         0.88828535 7.         0.3097795 ]. \t  -0.6258969685789838 \t -0.449167771194887\n",
            "17     \t [ 1.88681738  3.46035586 14.          0.68070687  6.          0.9944693 ]. \t  \u001b[92m-0.42905917541157645\u001b[0m \t -0.42905917541157645\n",
            "18     \t [ 1.00455473  4.86452269 10.          0.65242434  1.          0.22603818]. \t  -0.6579989052523156 \t -0.42905917541157645\n",
            "19     \t [ 0.87144588  0.14777143 14.          0.88891044 13.          0.11582186]. \t  -0.6586044677275812 \t -0.42905917541157645\n",
            "20     \t [ 8.17732484  5.22697425 11.          0.61303379  1.          0.32672413]. \t  -0.6191817873624627 \t -0.42905917541157645\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.2900830531756915"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78QysHAlchIb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e17e2c35-ae71-4e91-f2c6-ed2469a165d9"
      },
      "source": [
        "end_win = time.time()\n",
        "end_win\n",
        "\n",
        "time_win = end_win - start_win\n",
        "time_win"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1557.7249507904053"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CU2FlhY4vHUk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4cb5ed8-0c82-4f32-8ec7-f3f1b34ce57e"
      },
      "source": [
        "rmse_loser = [rmse_loser_1,\n",
        "rmse_loser_2,\n",
        "rmse_loser_3,\n",
        "rmse_loser_4,\n",
        "rmse_loser_5,\n",
        "rmse_loser_6,\n",
        "rmse_loser_7,\n",
        "rmse_loser_8,\n",
        "rmse_loser_9,\n",
        "rmse_loser_10,\n",
        "rmse_loser_11,\n",
        "rmse_loser_12,\n",
        "rmse_loser_13,\n",
        "rmse_loser_14,\n",
        "rmse_loser_15,\n",
        "rmse_loser_16,\n",
        "rmse_loser_17,\n",
        "rmse_loser_18,\n",
        "rmse_loser_19,\n",
        "rmse_loser_20]\n",
        "\n",
        "np.mean(rmse_loser)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.335494497141253"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQ1lotm7vJi9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56286c87-0e6f-42c1-9e91-ffcd7bfa5e79"
      },
      "source": [
        "rmse_winner = [rmse_winner_1,\n",
        "rmse_winner_2,\n",
        "rmse_winner_3,\n",
        "rmse_winner_4,\n",
        "rmse_winner_5,\n",
        "rmse_winner_6,\n",
        "rmse_winner_7,\n",
        "rmse_winner_8,\n",
        "rmse_winner_9,\n",
        "rmse_winner_10,\n",
        "rmse_winner_11,\n",
        "rmse_winner_12,\n",
        "rmse_winner_13,\n",
        "rmse_winner_14,\n",
        "rmse_winner_15,\n",
        "rmse_winner_16,\n",
        "rmse_winner_17,\n",
        "rmse_winner_18,\n",
        "rmse_winner_19,\n",
        "rmse_winner_20]\n",
        "\n",
        "np.mean(rmse_winner)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.346642813841234"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1yU4s1GRvMdl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2521548-0c75-4de3-9c95-379bc46ce5f2"
      },
      "source": [
        "min_rmse_loser = min_max_array(rmse_loser)\n",
        "min_rmse_loser, len(min_rmse_loser)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([4.87018129255363,\n",
              "  4.343487868410211,\n",
              "  4.343487868410211,\n",
              "  4.343487868410211,\n",
              "  4.31722690080873,\n",
              "  4.1451215046465775,\n",
              "  4.1451215046465775,\n",
              "  4.1451215046465775,\n",
              "  4.130636454660636,\n",
              "  4.130636454660636,\n",
              "  4.130636454660636,\n",
              "  3.9962203731848227,\n",
              "  3.9962203731848227,\n",
              "  3.9962203731848227,\n",
              "  3.9962203731848227,\n",
              "  3.9962203731848227,\n",
              "  3.9962203731848227,\n",
              "  3.937438334950126,\n",
              "  3.937438334950126,\n",
              "  3.937438334950126],\n",
              " 20)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unXOpKHcvO15",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1653241c-05bd-4559-8e82-78b912327bf2"
      },
      "source": [
        "min_rmse_winner = min_max_array(rmse_winner)\n",
        "min_rmse_winner, len(min_rmse_winner)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([4.87018129255363,\n",
              "  4.316024710268154,\n",
              "  4.316024710268154,\n",
              "  4.316024710268154,\n",
              "  4.316024710268154,\n",
              "  4.1451215046465775,\n",
              "  4.1451215046465775,\n",
              "  4.1451215046465775,\n",
              "  4.130636454660636,\n",
              "  4.130636454660636,\n",
              "  4.130636454660636,\n",
              "  4.07295168965299,\n",
              "  4.07295168965299,\n",
              "  4.07295168965299,\n",
              "  4.07295168965299,\n",
              "  4.07295168965299,\n",
              "  4.07295168965299,\n",
              "  3.9005482179018887,\n",
              "  3.9005482179018887,\n",
              "  3.9005482179018887],\n",
              " 20)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yxo85-HEvRPi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "outputId": "d5617d93-d8ab-47ee-d186-dc3244365e84"
      },
      "source": [
        "### Visualise!\n",
        "\n",
        "title = obj_func\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(min_rmse_loser, color = 'Green', label='RMSE: GP dEI ')\n",
        "plt.plot(min_rmse_winner, color = 'Red', label='RMSE: STP dERM ')\n",
        "\n",
        "plt.title(title, weight = 'bold', family = 'Arial')\n",
        "plt.xlabel('Experiment(s)', weight = 'bold', family = 'Arial') # x-axis label\n",
        "plt.ylabel('RMSE ($)', weight = 'bold', family = 'Arial') # y-axis label\n",
        "plt.legend(loc=0) # add plot legend\n",
        "\n",
        "### Make the x-ticks integers, not floats:\n",
        "count = len(min_rmse_loser)\n",
        "plt.xticks(np.arange(count), np.arange(1, count + 1))\n",
        "plt.grid(b=None)\n",
        "plt.show() #visualize!\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwyO7_iZvT7a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8824d10f-f160-45e6-dce7-a2511582a41e"
      },
      "source": [
        "time_lose, time_win"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1527.9233508110046, 1557.7249507904053)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    }
  ]
}