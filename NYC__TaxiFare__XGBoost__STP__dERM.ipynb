{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NYC__TaxiFare__XGBoost__STP__dERM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9wHutsqZUcn"
      },
      "source": [
        "XGBoost Regression - 'real-world' example: NYC Taxi-Fare Predictor\n",
        "\n",
        "GP dEI versus STP nu = 3 dERM (winner)\n",
        "\n",
        "https://www.kaggle.com/c/new-york-city-taxi-fare-prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7PwmXsgZO8D",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "98d3148f-64b9-46fd-cb09-a18be4eaa02f"
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-329a79a8-396e-4407-b7d3-fa1b5d6c1e62\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-329a79a8-396e-4407-b7d3-fa1b5d6c1e62\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"conorc2006\",\"key\":\"c5c5a6382a7d50c022aab991694fc17f\"}'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMwbJ6hjZltI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20e1a4d9-9fe1-4cf2-a659-28c6271c6070"
      },
      "source": [
        "## Ensure the kaggle.json file is present:\n",
        "!ls -lha kaggle.json"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-rw-r--r-- 1 root root 66 Feb 15 17:00 kaggle.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8Pu-UlWZovH"
      },
      "source": [
        "## Next, install the Kaggle API client:\n",
        "!pip install -q kaggle"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUOQ4SE7Zuj3"
      },
      "source": [
        "## The Kaggle API Client expects this file to be ~/.kaggle\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJcEztjCZxOn"
      },
      "source": [
        "## Permissions' change\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-u4Tmj7ZUD3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7dfedd64-bfb2-469f-9872-8f28764c5620"
      },
      "source": [
        "!kaggle competitions download -c new-york-city-taxi-fare-prediction"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.10 / client 1.5.4)\n",
            "Downloading GCP-Coupons-Instructions.rtf to /content\n",
            "  0% 0.00/486 [00:00<?, ?B/s]\n",
            "100% 486/486 [00:00<00:00, 443kB/s]\n",
            "Downloading sample_submission.csv to /content\n",
            "  0% 0.00/335k [00:00<?, ?B/s]\n",
            "100% 335k/335k [00:00<00:00, 47.1MB/s]\n",
            "Downloading train.csv.zip to /content\n",
            " 99% 1.54G/1.56G [00:14<00:00, 124MB/s]\n",
            "100% 1.56G/1.56G [00:14<00:00, 119MB/s]\n",
            "Downloading test.csv to /content\n",
            "  0% 0.00/960k [00:00<?, ?B/s]\n",
            "100% 960k/960k [00:00<00:00, 103MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-0Pe1i4Z2R_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3bed7fb-734b-4e15-9425-c0c9c7a6e864"
      },
      "source": [
        "!pip install pyGPGO"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyGPGO\n",
            "  Downloading https://files.pythonhosted.org/packages/8d/87/a113c91ba014708114f7635d5c0f6a5e5c773480c5f0a537b257a02d180d/pyGPGO-0.4.0.dev1.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pyGPGO) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from pyGPGO) (1.4.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from pyGPGO) (1.0.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from pyGPGO) (0.22.2.post1)\n",
            "Requirement already satisfied: theano in /usr/local/lib/python3.6/dist-packages (from pyGPGO) (1.0.5)\n",
            "Requirement already satisfied: pyMC3 in /usr/local/lib/python3.6/dist-packages (from pyGPGO) (3.7)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from theano->pyGPGO) (1.15.0)\n",
            "Requirement already satisfied: tqdm>=4.8.4 in /usr/local/lib/python3.6/dist-packages (from pyMC3->pyGPGO) (4.41.1)\n",
            "Requirement already satisfied: pandas>=0.18.0 in /usr/local/lib/python3.6/dist-packages (from pyMC3->pyGPGO) (1.1.5)\n",
            "Requirement already satisfied: h5py>=2.7.0 in /usr/local/lib/python3.6/dist-packages (from pyMC3->pyGPGO) (2.10.0)\n",
            "Requirement already satisfied: patsy>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from pyMC3->pyGPGO) (0.5.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.18.0->pyMC3->pyGPGO) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.18.0->pyMC3->pyGPGO) (2.8.1)\n",
            "Building wheels for collected packages: pyGPGO\n",
            "  Building wheel for pyGPGO (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyGPGO: filename=pyGPGO-0.4.0.dev1-cp36-none-any.whl size=19867 sha256=4d5233a3f320100acdaa145684b60f676ce6d929ff8fb0d04a0859948f6bf489\n",
            "  Stored in directory: /root/.cache/pip/wheels/d1/27/04/c4fa3bfe194d36e3cd51542132f43415a6813114a5e8301acb\n",
            "Successfully built pyGPGO\n",
            "Installing collected packages: pyGPGO\n",
            "Successfully installed pyGPGO-0.4.0.dev1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7zDTf1naBsH"
      },
      "source": [
        "# Load some default Python modules:\n",
        "\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "import pandas as pd\n",
        "import pymc3 as pm\n",
        "import matplotlib.pyplot as plt\n",
        "import xgboost as xgb\n",
        "import time\n",
        "\n",
        "from matplotlib.pyplot import rc\n",
        "rc('font',**{'family':'sans-serif','sans-serif':['Helvetica']})\n",
        "rc('text', usetex=False)\n",
        "### % matplotlib inline\n",
        "import seaborn as sns\n",
        "plt.style.use('seaborn-whitegrid')\n",
        "\n",
        "from collections import OrderedDict\n",
        "from joblib import Parallel, delayed\n",
        "from numpy.linalg import slogdet, inv, cholesky, solve\n",
        "from scipy.optimize import minimize\n",
        "from scipy.spatial.distance import cdist\n",
        "from scipy.special import gamma\n",
        "from scipy.stats import norm, t\n",
        "from joblib import Parallel, delayed\n",
        "\n",
        "from pyGPGO.logger import EventLogger\n",
        "from pyGPGO.GPGO import GPGO\n",
        "from pyGPGO.surrogates.GaussianProcess import GaussianProcess\n",
        "from pyGPGO.surrogates.tStudentProcess import tStudentProcess\n",
        "from pyGPGO.surrogates.tStudentProcess import logpdf\n",
        "from pyGPGO.acquisition import Acquisition\n",
        "from pyGPGO.covfunc import squaredExponential\n",
        "from sklearn.model_selection import cross_val_score, train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from xgboost import XGBRegressor\n",
        "from pandas_datareader import data\n",
        "\n",
        "import warnings\n",
        "import random\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXicekJhaE0P"
      },
      "source": [
        "# Read data in pandas dataframe:\n",
        "\n",
        "df_train =  pd.read_csv('/content/train.csv.zip', nrows = 1_000_000, parse_dates=[\"pickup_datetime\"])"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQ0mDzt_cBmw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "53439f97-893f-4046-da48-4d7f10a90774"
      },
      "source": [
        "# List first rows:\n",
        "\n",
        "df_train.head()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>key</th>\n",
              "      <th>fare_amount</th>\n",
              "      <th>pickup_datetime</th>\n",
              "      <th>pickup_longitude</th>\n",
              "      <th>pickup_latitude</th>\n",
              "      <th>dropoff_longitude</th>\n",
              "      <th>dropoff_latitude</th>\n",
              "      <th>passenger_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2009-06-15 17:26:21.0000001</td>\n",
              "      <td>4.5</td>\n",
              "      <td>2009-06-15 17:26:21+00:00</td>\n",
              "      <td>-73.844311</td>\n",
              "      <td>40.721319</td>\n",
              "      <td>-73.841610</td>\n",
              "      <td>40.712278</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2010-01-05 16:52:16.0000002</td>\n",
              "      <td>16.9</td>\n",
              "      <td>2010-01-05 16:52:16+00:00</td>\n",
              "      <td>-74.016048</td>\n",
              "      <td>40.711303</td>\n",
              "      <td>-73.979268</td>\n",
              "      <td>40.782004</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2011-08-18 00:35:00.00000049</td>\n",
              "      <td>5.7</td>\n",
              "      <td>2011-08-18 00:35:00+00:00</td>\n",
              "      <td>-73.982738</td>\n",
              "      <td>40.761270</td>\n",
              "      <td>-73.991242</td>\n",
              "      <td>40.750562</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2012-04-21 04:30:42.0000001</td>\n",
              "      <td>7.7</td>\n",
              "      <td>2012-04-21 04:30:42+00:00</td>\n",
              "      <td>-73.987130</td>\n",
              "      <td>40.733143</td>\n",
              "      <td>-73.991567</td>\n",
              "      <td>40.758092</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2010-03-09 07:51:00.000000135</td>\n",
              "      <td>5.3</td>\n",
              "      <td>2010-03-09 07:51:00+00:00</td>\n",
              "      <td>-73.968095</td>\n",
              "      <td>40.768008</td>\n",
              "      <td>-73.956655</td>\n",
              "      <td>40.783762</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                             key  ...  passenger_count\n",
              "0    2009-06-15 17:26:21.0000001  ...                1\n",
              "1    2010-01-05 16:52:16.0000002  ...                1\n",
              "2   2011-08-18 00:35:00.00000049  ...                2\n",
              "3    2012-04-21 04:30:42.0000001  ...                1\n",
              "4  2010-03-09 07:51:00.000000135  ...                1\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9fZujMycFMo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d3514b6-3bf1-4682-db05-ca42ec0b42bb"
      },
      "source": [
        "# Format 'pickup_datetime' variable:\n",
        "\n",
        "df_train['pickup_datetime'] =  pd.to_datetime(df_train['pickup_datetime'], utc=True, format='%Y-%m-%d %H:%M')\n",
        "df_train['pickup_datetime'].head()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0   2009-06-15 17:26:21+00:00\n",
              "1   2010-01-05 16:52:16+00:00\n",
              "2   2011-08-18 00:35:00+00:00\n",
              "3   2012-04-21 04:30:42+00:00\n",
              "4   2010-03-09 07:51:00+00:00\n",
              "Name: pickup_datetime, dtype: datetime64[ns, UTC]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nReKu62HcVFI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "c0647468-e79b-4bbf-fe52-c98be1f96680"
      },
      "source": [
        "df_train.sort_values(by = 'pickup_datetime').tail() ### June 2015 the final month"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>key</th>\n",
              "      <th>fare_amount</th>\n",
              "      <th>pickup_datetime</th>\n",
              "      <th>pickup_longitude</th>\n",
              "      <th>pickup_latitude</th>\n",
              "      <th>dropoff_longitude</th>\n",
              "      <th>dropoff_latitude</th>\n",
              "      <th>passenger_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>286276</th>\n",
              "      <td>2015-06-30 23:38:21.0000003</td>\n",
              "      <td>26.5</td>\n",
              "      <td>2015-06-30 23:38:21+00:00</td>\n",
              "      <td>-74.008385</td>\n",
              "      <td>40.711571</td>\n",
              "      <td>-73.884071</td>\n",
              "      <td>40.737385</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>955575</th>\n",
              "      <td>2015-06-30 23:45:57.0000003</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2015-06-30 23:45:57+00:00</td>\n",
              "      <td>-74.002342</td>\n",
              "      <td>40.739819</td>\n",
              "      <td>-74.005829</td>\n",
              "      <td>40.745239</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>915826</th>\n",
              "      <td>2015-06-30 23:48:35.0000005</td>\n",
              "      <td>30.5</td>\n",
              "      <td>2015-06-30 23:48:35+00:00</td>\n",
              "      <td>-73.983826</td>\n",
              "      <td>40.729546</td>\n",
              "      <td>-73.927917</td>\n",
              "      <td>40.661186</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>751350</th>\n",
              "      <td>2015-06-30 23:53:23.0000002</td>\n",
              "      <td>3.5</td>\n",
              "      <td>2015-06-30 23:53:23+00:00</td>\n",
              "      <td>-73.978020</td>\n",
              "      <td>40.757439</td>\n",
              "      <td>-73.980705</td>\n",
              "      <td>40.753544</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>785182</th>\n",
              "      <td>2015-06-30 23:53:49.0000003</td>\n",
              "      <td>7.5</td>\n",
              "      <td>2015-06-30 23:53:49+00:00</td>\n",
              "      <td>-73.959969</td>\n",
              "      <td>40.762405</td>\n",
              "      <td>-73.953064</td>\n",
              "      <td>40.782688</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                key  ...  passenger_count\n",
              "286276  2015-06-30 23:38:21.0000003  ...                5\n",
              "955575  2015-06-30 23:45:57.0000003  ...                1\n",
              "915826  2015-06-30 23:48:35.0000005  ...                2\n",
              "751350  2015-06-30 23:53:23.0000002  ...                1\n",
              "785182  2015-06-30 23:53:49.0000003  ...                1\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W9j9LnIfcXcX"
      },
      "source": [
        "# Add time variables:\n",
        "\n",
        "df_train['hour'] = df_train['pickup_datetime'].dt.hour\n",
        "df_train['weekday'] = df_train['pickup_datetime'].dt.weekday\n",
        "df_train['month'] = df_train['pickup_datetime'].dt.month\n",
        "df_train['year'] = df_train['pickup_datetime'].dt.year"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVyFZIVIcaj3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "ea9b1c43-4b70-4de9-fa56-b325646d23f0"
      },
      "source": [
        "df_train = df_train.drop(['pickup_datetime','key'], axis = 1)\n",
        "df_train.head()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>fare_amount</th>\n",
              "      <th>pickup_longitude</th>\n",
              "      <th>pickup_latitude</th>\n",
              "      <th>dropoff_longitude</th>\n",
              "      <th>dropoff_latitude</th>\n",
              "      <th>passenger_count</th>\n",
              "      <th>hour</th>\n",
              "      <th>weekday</th>\n",
              "      <th>month</th>\n",
              "      <th>year</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4.5</td>\n",
              "      <td>-73.844311</td>\n",
              "      <td>40.721319</td>\n",
              "      <td>-73.841610</td>\n",
              "      <td>40.712278</td>\n",
              "      <td>1</td>\n",
              "      <td>17</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>2009</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>16.9</td>\n",
              "      <td>-74.016048</td>\n",
              "      <td>40.711303</td>\n",
              "      <td>-73.979268</td>\n",
              "      <td>40.782004</td>\n",
              "      <td>1</td>\n",
              "      <td>16</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2010</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5.7</td>\n",
              "      <td>-73.982738</td>\n",
              "      <td>40.761270</td>\n",
              "      <td>-73.991242</td>\n",
              "      <td>40.750562</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>2011</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>7.7</td>\n",
              "      <td>-73.987130</td>\n",
              "      <td>40.733143</td>\n",
              "      <td>-73.991567</td>\n",
              "      <td>40.758092</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>2012</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5.3</td>\n",
              "      <td>-73.968095</td>\n",
              "      <td>40.768008</td>\n",
              "      <td>-73.956655</td>\n",
              "      <td>40.783762</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2010</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   fare_amount  pickup_longitude  pickup_latitude  ...  weekday  month  year\n",
              "0          4.5        -73.844311        40.721319  ...        0      6  2009\n",
              "1         16.9        -74.016048        40.711303  ...        1      1  2010\n",
              "2          5.7        -73.982738        40.761270  ...        3      8  2011\n",
              "3          7.7        -73.987130        40.733143  ...        5      4  2012\n",
              "4          5.3        -73.968095        40.768008  ...        1      3  2010\n",
              "\n",
              "[5 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVfm-KSqcdVY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "228b84e6-1c69-4d5d-d478-c9997ab5be72"
      },
      "source": [
        "# Remove negative fares and postive outliers:\n",
        "\n",
        "df_train = df_train[df_train.fare_amount>=0]\n",
        "df_train = df_train[df_train.fare_amount<=60]\n",
        "print('New size: %d' % len(df_train))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "New size: 997297\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTVDAD2KchTv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2b11211-5d24-4ec2-c349-61bd37a187d8"
      },
      "source": [
        "# Remove missing data:\n",
        "\n",
        "df_train = df_train.dropna(how = 'any', axis = 'rows')\n",
        "print('New size: %d' % len(df_train))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "New size: 997288\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUYksJ2cclVQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "736f5246-4f1c-4e4a-922d-c1eb337a69f2"
      },
      "source": [
        "# June 2015 NYC taxi data (Wu et al, 2017):\n",
        "\n",
        "df_train = df_train[df_train.month==6]\n",
        "df_train = df_train[df_train.year==2015]\n",
        "print('New size: %d' % len(df_train))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "New size: 11269\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXgSHPyYcnuv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "outputId": "6029a33f-7f48-4a71-b14e-bc5747d22fee"
      },
      "source": [
        "# Histogram fare plot:\n",
        "\n",
        "df_train[df_train.fare_amount<60].fare_amount.hist(bins=100, figsize=(16,5), color = \"red\")\n",
        "plt.xlabel('$USD')\n",
        "plt.title('NYC Taxi Fares: Dataset');\n",
        "plt.grid(b=None)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1152x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9TMSdAAjcr4o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72f67f6a-edf9-4993-ba39-fa9b0e1e8a26"
      },
      "source": [
        "y = df_train.fare_amount.values + 1e-10\n",
        "y ### for supervised learning: output vector y"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([22.54,  8.  , 34.  , ...,  4.5 ,  6.5 ,  7.  ])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FOeHvi3cu1n",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "fa5d9420-b4cf-419d-e3a8-0fd4f53a8afa"
      },
      "source": [
        "# List first rows (post-cleaning):\n",
        "\n",
        "df_train.head()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>fare_amount</th>\n",
              "      <th>pickup_longitude</th>\n",
              "      <th>pickup_latitude</th>\n",
              "      <th>dropoff_longitude</th>\n",
              "      <th>dropoff_latitude</th>\n",
              "      <th>passenger_count</th>\n",
              "      <th>hour</th>\n",
              "      <th>weekday</th>\n",
              "      <th>month</th>\n",
              "      <th>year</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>22.54</td>\n",
              "      <td>-74.010483</td>\n",
              "      <td>40.717667</td>\n",
              "      <td>-73.985771</td>\n",
              "      <td>40.660366</td>\n",
              "      <td>1</td>\n",
              "      <td>21</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>310</th>\n",
              "      <td>8.00</td>\n",
              "      <td>-74.010727</td>\n",
              "      <td>40.710091</td>\n",
              "      <td>-73.998100</td>\n",
              "      <td>40.722900</td>\n",
              "      <td>1</td>\n",
              "      <td>9</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>314</th>\n",
              "      <td>34.00</td>\n",
              "      <td>-73.974899</td>\n",
              "      <td>40.751095</td>\n",
              "      <td>-73.908546</td>\n",
              "      <td>40.881878</td>\n",
              "      <td>0</td>\n",
              "      <td>23</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>321</th>\n",
              "      <td>8.00</td>\n",
              "      <td>-73.961784</td>\n",
              "      <td>40.759579</td>\n",
              "      <td>-73.978943</td>\n",
              "      <td>40.772606</td>\n",
              "      <td>4</td>\n",
              "      <td>21</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>486</th>\n",
              "      <td>11.50</td>\n",
              "      <td>-73.957443</td>\n",
              "      <td>40.761703</td>\n",
              "      <td>-73.973236</td>\n",
              "      <td>40.787079</td>\n",
              "      <td>1</td>\n",
              "      <td>19</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     fare_amount  pickup_longitude  pickup_latitude  ...  weekday  month  year\n",
              "31         22.54        -74.010483        40.717667  ...        6      6  2015\n",
              "310         8.00        -74.010727        40.710091  ...        5      6  2015\n",
              "314        34.00        -73.974899        40.751095  ...        1      6  2015\n",
              "321         8.00        -73.961784        40.759579  ...        0      6  2015\n",
              "486        11.50        -73.957443        40.761703  ...        0      6  2015\n",
              "\n",
              "[5 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-lT9BBicw4P",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "6450fae9-bc7f-475d-c1a7-d2f1afeada56"
      },
      "source": [
        "X = df_train.drop(['fare_amount', 'month', 'year'], axis = 1)\n",
        "X.head() ### for supervised learning: input matrix X"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pickup_longitude</th>\n",
              "      <th>pickup_latitude</th>\n",
              "      <th>dropoff_longitude</th>\n",
              "      <th>dropoff_latitude</th>\n",
              "      <th>passenger_count</th>\n",
              "      <th>hour</th>\n",
              "      <th>weekday</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>-74.010483</td>\n",
              "      <td>40.717667</td>\n",
              "      <td>-73.985771</td>\n",
              "      <td>40.660366</td>\n",
              "      <td>1</td>\n",
              "      <td>21</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>310</th>\n",
              "      <td>-74.010727</td>\n",
              "      <td>40.710091</td>\n",
              "      <td>-73.998100</td>\n",
              "      <td>40.722900</td>\n",
              "      <td>1</td>\n",
              "      <td>9</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>314</th>\n",
              "      <td>-73.974899</td>\n",
              "      <td>40.751095</td>\n",
              "      <td>-73.908546</td>\n",
              "      <td>40.881878</td>\n",
              "      <td>0</td>\n",
              "      <td>23</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>321</th>\n",
              "      <td>-73.961784</td>\n",
              "      <td>40.759579</td>\n",
              "      <td>-73.978943</td>\n",
              "      <td>40.772606</td>\n",
              "      <td>4</td>\n",
              "      <td>21</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>486</th>\n",
              "      <td>-73.957443</td>\n",
              "      <td>40.761703</td>\n",
              "      <td>-73.973236</td>\n",
              "      <td>40.787079</td>\n",
              "      <td>1</td>\n",
              "      <td>19</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     pickup_longitude  pickup_latitude  ...  hour  weekday\n",
              "31         -74.010483        40.717667  ...    21        6\n",
              "310        -74.010727        40.710091  ...     9        5\n",
              "314        -73.974899        40.751095  ...    23        1\n",
              "321        -73.961784        40.759579  ...    21        0\n",
              "486        -73.957443        40.761703  ...    19        0\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2eC8SDPzczNY"
      },
      "source": [
        "### Optimum rmse: regression model objective function is Root Mean Square Error (RMSE); \n",
        "### Should be minimized (as close to zero as possible):\n",
        "\n",
        "y_global_orig = 0"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GoTmWEhSc1qQ"
      },
      "source": [
        "### Bayesian Optimization - inputs:\n",
        "\n",
        "obj_func = 'XGBoost'\n",
        "n_start_AcqFunc = 100\n",
        "n_test = n_start_AcqFunc # test points\n",
        "df = 3 # nu\n",
        "\n",
        "util_loser = 'dEI_GP'\n",
        "util_winner = 'dERM_STP'\n",
        "n_init = 5 # random initialisations\n",
        "\n",
        "test_perc = 0.15\n",
        "train_perc = 1 - test_perc\n",
        "\n",
        "n_test = int(len(df_train) * test_perc)\n",
        "n_train = int(len(df_train) - n_test)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2ngnRxbc7cg"
      },
      "source": [
        "### Objective function:\n",
        "\n",
        "if obj_func == 'XGBoost': # 6-D\n",
        "            \n",
        "    # Constraints:\n",
        "    param_lb_alpha = 0\n",
        "    param_ub_alpha = 10\n",
        "    \n",
        "    param_lb_gamma = 0\n",
        "    param_ub_gamma = 10\n",
        "    \n",
        "    param_lb_max_depth = 5\n",
        "    param_ub_max_depth = 15\n",
        "    \n",
        "    param_lb_min_child_weight = 1\n",
        "    param_ub_min_child_weight = 20\n",
        "    \n",
        "    param_lb_subsample = .5\n",
        "    param_ub_subsample = 1\n",
        "    \n",
        "    param_lb_colsample = .1\n",
        "    param_ub_colsample = 1\n",
        "    \n",
        "    # 6-D inputs' parameter bounds:\n",
        "    param = { 'alpha':  ('cont', (param_lb_alpha, param_ub_alpha)),\n",
        "         'gamma':  ('cont', (param_lb_gamma, param_ub_gamma)),     \n",
        "         'max_depth':  ('int', (param_lb_max_depth, param_ub_max_depth)),\n",
        "         'subsample':  ('cont', (param_lb_subsample, param_ub_subsample)),\n",
        "          'min_child_weight':  ('int', (param_lb_min_child_weight, param_ub_min_child_weight)),\n",
        "            'colsample': ('cont', (param_lb_colsample, param_ub_colsample))\n",
        "        }\n",
        "       \n",
        "    # True y bounds:\n",
        "    dim = 6\n",
        "    \n",
        "    max_iter = 20  # iterations of Bayesian optimization\n",
        "    \n",
        "    operator = 1 \n",
        "    \n",
        "    n_est = 3"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmJsNX29c_xA"
      },
      "source": [
        "### Derivatives - Squared-exponential covariance function:\n",
        "\n",
        "def l2norm_(X, Xstar):\n",
        "    \n",
        "    return cdist(X, Xstar)\n",
        "\n",
        "def kronDelta(X, Xstar):\n",
        "\n",
        "    return cdist(X, Xstar) < np.finfo(np.float32).eps\n",
        "\n",
        "class squaredExponentialDeriv(squaredExponential):\n",
        "    \n",
        "    def K(self, X, Xstar):\n",
        "        \n",
        "        r = (l2norm_(X, Xstar)/self.l)\n",
        "        K = self.sigmaf * np.exp(-1/2*r **2) + self.sigman * kronDelta(X, Xstar)\n",
        "        return K\n",
        "    \n",
        "    def dK(self, X, Xstar):\n",
        "        \n",
        "        r = (l2norm_(X, Xstar)/self.l)\n",
        "        dK = self.sigmaf/self.l**2 * np.exp(-1/2 * r **2) * l2norm_(X, Xstar)\n",
        "        return dK\n",
        "    \n",
        "        \n",
        "    def d2K(self, X, Xstar):\n",
        "        \n",
        "        r = (l2norm_(X, Xstar)/self.l)\n",
        "        d2K = self.sigmaf/self.l**2 * np.exp(-1/2 * r **2) * (1 - r **2)\n",
        "        return d2K\n",
        "    \n",
        "cov_func = squaredExponentialDeriv()\n",
        "d_cov_func = squaredExponentialDeriv()\n",
        "    "
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9ZuEB2VdE0W"
      },
      "source": [
        "### Set-seeds:\n",
        "\n",
        "run_num_1 = 111\n",
        "run_num_2 = 113\n",
        "run_num_3 = 3333\n",
        "run_num_4 = 4444\n",
        "run_num_5 = 5555\n",
        "run_num_6 = 6\n",
        "run_num_7 = 7777\n",
        "run_num_8 = 8878\n",
        "run_num_9 = 999\n",
        "run_num_10 = 1000\n",
        "run_num_11 = 1113\n",
        "run_num_12 = 1234\n",
        "run_num_13 = 234\n",
        "run_num_14 = 888\n",
        "run_num_15 = 1557\n",
        "run_num_16 = 1666\n",
        "run_num_17 = 71\n",
        "run_num_18 = 8\n",
        "run_num_19 = 1999\n",
        "run_num_20 = 2000"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XgHMFEyPdCk4"
      },
      "source": [
        "### Cumulative Regret Calculator:\n",
        "\n",
        "def min_max_array(x):\n",
        "    new_list = []\n",
        "    for i, num in enumerate(x):\n",
        "            new_list.append(np.min(x[0:i+1]))\n",
        "    return new_list"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJMhL70fdHz_"
      },
      "source": [
        "class Acquisition_new(Acquisition):    \n",
        "    def __init__(self, mode, eps=1e-08, **params):\n",
        "        \n",
        "        self.params = params\n",
        "        self.eps = eps\n",
        "\n",
        "        mode_dict = {\n",
        "            'dEI_GP': self.dEI_GP,\n",
        "            'dERM_STP': self.dERM_STP\n",
        "        }\n",
        "\n",
        "        self.f = mode_dict[mode]\n",
        "    \n",
        "    def dEI_GP(self, tau, mean, std, ds, dm, dvdv, d2v, d2m):\n",
        "        z = -1 * (tau - mean - self.eps) / (std + self.eps)\n",
        "        \n",
        "        dsdx = ds / 2 * (std + self.eps)\n",
        "        d2sdx = -dsdx**2 / ((std + self.eps)) - dvdv / (std + self.eps) - d2v / (std + self.eps)\n",
        "        dmdx = (dm - z * dsdx) / (std + self.eps)\n",
        "        d2mdx = (d2m - (z * d2sdx + 2 * dmdx * dsdx)) / (std + self.eps)\n",
        "        \n",
        "        f = (std + self.eps) * (z * norm.cdf(z) + norm.pdf(z)[0])\n",
        "        df = (f / (std + self.eps) * dsdx + (std + self.eps) * norm.cdf(z) * dmdx)\n",
        "        d2f = (f / (std + self.eps) * d2sdx + dsdx * dmdx * norm.cdf(z) \\\n",
        "            + d2mdx * (std + self.eps) * norm.cdf(z) + dsdx * norm.cdf(z) * dmdx \\\n",
        "            + norm.pdf(z)[0] * (std + self.eps) * dmdx)\n",
        "            \n",
        "        return f, df, d2f\n",
        "\n",
        "    def dERM_STP(self, tau, mean, std, ds, dm, dvdv, d2v, d2m, nu=3.0):\n",
        "    \n",
        "        gamma = -1 * (y_global_orig - mean - self.eps) / (std + self.eps)\n",
        "        dsdx = ds / (std + self.eps)\n",
        "        dmdx = (dm - gamma * dsdx) / (std + self.eps)\n",
        "        f = (std + self.eps) * (gamma * t.cdf(gamma, df=nu) + (nu + gamma ** 2)/(nu - 1) * t.pdf(gamma, df=nu))\n",
        "        df = (gamma * t.cdf(gamma, df=nu) + (nu + gamma ** 2)/(nu - 1) * t.pdf(gamma,df=nu)[0]) * dsdx \\\n",
        "             + (std + self.eps) * (t.cdf(gamma,df=nu) * dmdx + gamma * t.pdf(gamma, df=nu) * \\\n",
        "             (1 - (nu + gamma ** 2)/(nu - 1) + 2/(nu - 1) * dmdx))\n",
        "        return f, df\n",
        "    \n",
        "    def _eval(self, tau, mean, std):\n",
        "    \n",
        "        return self.f(tau, mean, std, **self.params)\n",
        "    \n",
        "    def d_eval(self, tau, mean, std, ds, dm, dvdv, d2v, d2m):\n",
        "    \n",
        "        return self.f(tau, mean, std, ds, dm, dvdv, d2v, d2m, **self.params)\n",
        "\n",
        "    def d_eval_stp(self, tau, mean, std, ds, dm, dvdv, d2v, d2m, nu=3.0):\n",
        "    \n",
        "        return self.f(tau, mean, std, ds, dm, dvdv, d2v, d2m, nu=3.0, **self.params)\n"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "comHkJv9dH_O"
      },
      "source": [
        "### Surrogate derivatives: \n",
        "\n",
        "from scipy.linalg import cholesky, solve\n",
        "\n",
        "class dGaussianProcess(GaussianProcess):\n",
        "    l = 1\n",
        "    sigmaf = 1e-4\n",
        "    sigman = 1e-6\n",
        "\n",
        "    def AcqGrad(self, Xstar, return_std=False):\n",
        "        r_X = l2norm_(self.X, self.X)/self.l\n",
        "        K = self.sigmaf * np.exp(-1/2*r_X **2) + self.sigman * kronDelta(self.X, self.X)\n",
        "        L = cholesky(K).T\n",
        "        alpha = solve(L.T, solve(L, self.y))\n",
        "        Xstar = np.atleast_2d(Xstar)\n",
        "        Kstar = squaredExponentialDeriv.K(self, self.X, Xstar).T\n",
        "        dKstar = squaredExponentialDeriv.dK(self, self.X, Xstar).T\n",
        "        d2Kstar = squaredExponentialDeriv.d2K(self, self.X, Xstar).T\n",
        "        v = solve(self.L, Kstar.T)\n",
        "        dv = solve(self.L, dKstar.T)\n",
        "        d2v = solve(self.L, d2Kstar.T)\n",
        "        \n",
        "        ds = -2 * np.dot(dv.T, v)\n",
        "        dvdv = np.dot(dv.T, dv)\n",
        "        d2s = -2 * (dvdv + d2v)\n",
        "        \n",
        "        dm = np.dot(dKstar, alpha)\n",
        "        d2m = np.dot(d2Kstar, alpha)\n",
        "        return ds, dm, dvdv, d2v, d2m\n",
        "\n",
        "class dtStudentProcess(tStudentProcess):\n",
        "    l = 1\n",
        "    sigmaf = 1e-4\n",
        "    sigman = 1e-6\n",
        "    \n",
        "    def AcqGrad(self, Xstar, return_std=False):\n",
        "        r_X = l2norm_(self.X, self.X)/self.l\n",
        "        K = self.sigmaf * np.exp(-1/2*r_X **2) + self.sigman * kronDelta(self.X, self.X)\n",
        "        L = cholesky(self.K11).T\n",
        "        alpha = solve(L.T, solve(L, self.y))\n",
        "        Xstar = np.atleast_2d(Xstar)\n",
        "        Kstar = squaredExponentialDeriv.K(self, self.X, Xstar).T\n",
        "        dKstar = squaredExponentialDeriv.dK(self, self.X, Xstar).T\n",
        "        d2Kstar = squaredExponentialDeriv.d2K(self, self.X, Xstar).T\n",
        "        v = solve(L, Kstar.T)\n",
        "        dv = solve(L, dKstar.T)\n",
        "        d2v = solve(L, d2Kstar.T)\n",
        "        \n",
        "        ds = -2 * np.dot(dv.T, v)\n",
        "        dvdv = np.dot(dv.T, dv)\n",
        "        d2s = -2 * (dvdv + d2v)\n",
        "        \n",
        "        dm = np.dot(dKstar, alpha)\n",
        "        d2m = np.dot(d2Kstar, alpha)\n",
        "        return ds, dm, dvdv, d2v, d2m"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S422jNLsdIMm"
      },
      "source": [
        "class dGPGO(GPGO):  \n",
        "    n_start = n_start_AcqFunc\n",
        "    eps = 1e-08\n",
        "        \n",
        "    def func(self, xnew):\n",
        "        new_mean, new_var = self.GP.predict(xnew, return_std=True)\n",
        "        new_std = np.sqrt(new_var + 1e-6)\n",
        "        ds, dm, dvdv, d2v, d2m = self.GP.AcqGrad(xnew, return_std=True)\n",
        "        f  = np.empty((self.n_start,))\n",
        "        df = np.empty((self.n_start,))\n",
        "        f  = -self.A.d_eval(self.tau, new_mean, new_std, ds=ds, dm=dm, dvdv=dvdv, d2v=d2v, d2m=d2m)[0]\n",
        "        df = -self.A.d_eval(self.tau, new_mean, new_std, ds=ds, dm=dm, dvdv=dvdv, d2v=d2v, d2m=d2m)[1]\n",
        "        df_array = np.full((len(xnew),),df)\n",
        "        return f, df_array\n",
        "        \n",
        "    def d_optimizeAcq(self, method='L-BFGS-B', n_start=n_start):\n",
        "        start_points_dict = [self._sampleParam() for i in range(n_start)]\n",
        "        start_points_arr = np.array([list(s.values())\n",
        "                                     for s in start_points_dict])\n",
        "        x_best = np.empty((n_start, len(self.parameter_key)))\n",
        "        f_best = np.empty((n_start,))\n",
        "        opt = Parallel(n_jobs=self.n_jobs)(delayed(minimize)(self.func,\n",
        "                                                                 x0=start_point,\n",
        "                                                                 method=method,\n",
        "                                                                 jac = True,\n",
        "                                                                 bounds=self.parameter_range) for start_point in\n",
        "                                               start_points_arr)\n",
        "        x_best = np.array([res.x for res in opt])\n",
        "        f_best = np.array([np.atleast_1d(res.fun)[0] for res in opt])\n",
        "\n",
        "        self.x_best = x_best\n",
        "        self.f_best = f_best\n",
        "        self.best = x_best[np.argmin(f_best)]\n",
        "    \n",
        "    def run(self, max_iter=10, init_evals=3, resume=False):\n",
        "        \n",
        "        if not resume:\n",
        "            self.init_evals = init_evals\n",
        "            self._firstRun(self.init_evals)\n",
        "            self.logger._printInit(self)\n",
        "        for iteration in range(max_iter):\n",
        "            self.d_optimizeAcq()\n",
        "            self.updateGP()\n",
        "            self.logger._printCurrent(self)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwEwZD0qdIPO"
      },
      "source": [
        "## dGPGO_stp - BayesOpt derivatives' class: Student's-t\n",
        "\n",
        "class dGPGO_stp(GPGO):  \n",
        "    n_start = 100\n",
        "        \n",
        "    def func_stp(self, xnew):\n",
        "        new_mean, new_var = self.GP.predict(xnew, return_std=True)\n",
        "        new_std = np.sqrt(new_var + 1e-6)\n",
        "        ds, dm, dvdv, d2v, d2m = self.GP.AcqGrad(xnew, return_std=True)\n",
        "        f  = np.empty((self.n_start,))\n",
        "        df = np.empty((self.n_start,))\n",
        "        f  = -self.A.d_eval_stp(self.tau, new_mean, new_std, ds=ds, dm=dm, dvdv=dvdv, d2v=d2v, d2m=d2m, nu=df)[0]\n",
        "        df = -self.A.d_eval_stp(self.tau, new_mean, new_std, ds=ds, dm=dm, dvdv=dvdv, d2v=d2v, d2m=d2m, nu=df)[1]\n",
        "        df_array = np.full((len(xnew),),df)\n",
        "        return f, df_array\n",
        "        \n",
        "    def d_optimizeAcq_stp(self, method='L-BFGS-B', n_start=n_start):\n",
        "        start_points_dict = [self._sampleParam() for i in range(n_start)]\n",
        "        start_points_arr = np.array([list(s.values())\n",
        "                                     for s in start_points_dict])\n",
        "        x_best = np.empty((n_start, len(self.parameter_key)))\n",
        "        f_best = np.empty((n_start,))\n",
        "        opt = Parallel(n_jobs=self.n_jobs)(delayed(minimize)(self.func_stp,\n",
        "                                                                 x0=start_point,\n",
        "                                                                 method=method,\n",
        "                                                                 jac = True,\n",
        "                                                                 bounds=self.parameter_range) for start_point in\n",
        "                                               start_points_arr)\n",
        "        x_best = np.array([res.x for res in opt])\n",
        "        f_best = np.array([np.atleast_1d(res.fun)[0] for res in opt])\n",
        "\n",
        "        self.x_best = x_best\n",
        "        self.f_best = f_best\n",
        "        self.best = x_best[np.argmin(f_best)]\n",
        "    \n",
        "    def run(self, max_iter=10, init_evals=3, resume=False):\n",
        "        \n",
        "        if not resume:\n",
        "            self.init_evals = init_evals\n",
        "            self._firstRun(self.init_evals)\n",
        "            self.logger._printInit(self)\n",
        "        for iteration in range(max_iter):\n",
        "            self.d_optimizeAcq_stp()\n",
        "            self.updateGP()\n",
        "            self.logger._printCurrent(self)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HlilveEgdIR_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0cb70f7f-25c6-46ba-e24d-9c8edb86b808"
      },
      "source": [
        "start_lose = time.time()\n",
        "start_lose"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1613408577.96063"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wlzDSHbUG-c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11da63f6-1888-487f-da2f-0b749c7ba07d"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 1\n",
        "\n",
        "np.random.seed(run_num_1)\n",
        "surrogate_loser_1 = dGaussianProcess(cov_func)\n",
        "\n",
        "X_train1, X_test1, y_train1, y_test1 = train_test_split(X, y, test_size=test_perc, random_state=run_num_1)\n",
        "\n",
        "def f_syn_polarity1(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_1, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train1, y=y_train1).mean())\n",
        "    return operator * score\n",
        "\n",
        "loser_1 = dGPGO(surrogate_loser_1, Acquisition_new(util_loser), f_syn_polarity1, param, n_jobs = -1) # define BayesOpt\n",
        "loser_1.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_loser_1 = loser_1.getResult()[0]\n",
        "params_loser_1['max_depth'] = int(params_loser_1['max_depth'])\n",
        "params_loser_1['min_child_weight'] = int(params_loser_1['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_loser_train1 = xgb.DMatrix(X_train1, y_train1)\n",
        "dX_loser_test1 = xgb.DMatrix(X_test1, y_test1)\n",
        "model_loser_1 = xgb.train(params_loser_1, dX_loser_train1)\n",
        "pred_loser_1 = model_loser_1.predict(dX_loser_test1)\n",
        "\n",
        "rmse_loser_1 = np.sqrt(mean_squared_error(pred_loser_1, y_test1))\n",
        "rmse_loser_1"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 6.12170176  1.69069754 11.          0.92695323  3.          0.36579277]. \t  -0.5875926438135084 \t -0.45245711231879204\n",
            "init   \t [ 1.49162957  0.22478325  6.          0.85613654 15.          0.19363322]. \t  -0.6811472770543097 \t -0.45245711231879204\n",
            "init   \t [ 5.27957805  2.81819356 11.          0.81062146 14.          0.74826845]. \t  -0.46117277972616416 \t -0.45245711231879204\n",
            "init   \t [3.81060005 6.24236569 7.         0.95038709 5.         0.9861745 ]. \t  -0.45245711231879204 \t -0.45245711231879204\n",
            "init   \t [4.77531134 7.87990376 6.         0.78863691 6.         0.47918609]. \t  -0.5612893767574513 \t -0.45245711231879204\n",
            "1      \t [0.  0.  5.  0.5 1.  0.1]. \t  -0.6826322236836013 \t -0.45245711231879204\n",
            "2      \t [ 9.60774341  8.94642175  5.          0.60295867 15.          0.76784922]. \t  -0.5001571385184761 \t -0.45245711231879204\n",
            "3      \t [ 0.4810761   9.31563532  8.          0.92845546 14.          0.70014982]. \t  -0.5056468669267833 \t -0.45245711231879204\n",
            "4      \t [ 9.85849625  0.83439907  7.          0.7705582  19.          0.15487995]. \t  -0.6815452883308839 \t -0.45245711231879204\n",
            "5      \t [ 9.1348132   9.54667443 13.          0.95557113 18.          0.11016711]. \t  -0.6791252728059957 \t -0.45245711231879204\n",
            "6      \t [ 9.97122318  8.75901621 13.          0.9904996   9.          0.60161775]. \t  -0.4895864123043516 \t -0.45245711231879204\n",
            "7      \t [7.19079936 0.87565629 5.         0.54470564 8.         0.4449021 ]. \t  -0.5736147518216788 \t -0.45245711231879204\n",
            "8      \t [ 5.24989998  8.90398408 13.          0.54165135  1.          0.74771929]. \t  -0.46141851807096074 \t -0.45245711231879204\n",
            "9      \t [ 1.7157918   5.89141512 14.          0.5296881  10.          0.1803042 ]. \t  -0.6825568503758982 \t -0.45245711231879204\n",
            "10     \t [ 1.33491429  4.9488224  14.          0.57825982 18.          0.42075517]. \t  -0.5921600742966667 \t -0.45245711231879204\n",
            "11     \t [8.99095022 4.91206514 6.         0.87212346 1.         0.53198502]. \t  -0.5638457176592562 \t -0.45245711231879204\n",
            "12     \t [0.51753753 1.68511981 9.         0.80360039 9.         0.45200858]. \t  -0.5426731188677099 \t -0.45245711231879204\n",
            "13     \t [ 0.47819106  0.69361363 14.          0.7271902   3.          0.48615024]. \t  -0.5388370338312534 \t -0.45245711231879204\n",
            "14     \t [ 3.39786838  7.15897283  5.          0.7138544  19.          0.22047835]. \t  -0.6821252760743777 \t -0.45245711231879204\n",
            "15     \t [0.4939583  9.37390762 8.         0.7641092  1.         0.4917045 ]. \t  -0.5481894484993145 \t -0.45245711231879204\n",
            "16     \t [ 8.70209222  0.21363429 14.          0.98391211 18.          0.50653374]. \t  -0.5341942124085062 \t -0.45245711231879204\n",
            "17     \t [ 5.30693639  9.58195731  9.          0.53796245 11.          0.46099251]. \t  -0.5549909911648074 \t -0.45245711231879204\n",
            "18     \t [ 3.83524595  9.61883757 12.          0.81445794 17.          0.17700713]. \t  -0.6797728963073484 \t -0.45245711231879204\n",
            "19     \t [ 0.25234021  8.87254669 14.          0.7101496   5.          0.12345742]. \t  -0.684133223230854 \t -0.45245711231879204\n",
            "20     \t [9.81917647 9.19103216 5.         0.82619792 9.         0.10869763]. \t  -0.6822789922076578 \t -0.45245711231879204\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.87018129255363"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ClJ9rN2KUJzy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45ca6696-b079-48c2-dd15-125b53d52229"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 2\n",
        "\n",
        "np.random.seed(run_num_2)\n",
        "surrogate_loser_2 = dGaussianProcess(cov_func)\n",
        "\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X, y, test_size=test_perc, random_state=run_num_2)\n",
        "\n",
        "def f_syn_polarity2(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_2, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train2, y=y_train2).mean())\n",
        "    return operator * score\n",
        "\n",
        "loser_2 = dGPGO(surrogate_loser_2, Acquisition_new(util_loser), f_syn_polarity2, param, n_jobs = -1) # define BayesOpt\n",
        "loser_2.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_loser_2 = loser_2.getResult()[0]\n",
        "params_loser_2['max_depth'] = int(params_loser_2['max_depth'])\n",
        "params_loser_2['min_child_weight'] = int(params_loser_2['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_loser_train2 = xgb.DMatrix(X_train2, y_train2)\n",
        "dX_loser_test2 = xgb.DMatrix(X_test2, y_test2)\n",
        "model_loser_2 = xgb.train(params_loser_2, dX_loser_train2)\n",
        "pred_loser_2 = model_loser_2.predict(dX_loser_test2)\n",
        "\n",
        "rmse_loser_2 = np.sqrt(mean_squared_error(pred_loser_2, y_test2))\n",
        "rmse_loser_2"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 8.51985493  0.73903599  7.          0.71824677 17.          0.61827209]. \t  -0.5233872966482411 \t -0.42848969661986275\n",
            "init   \t [ 8.4047092   4.35120551 12.          0.84231905 16.          0.80172484]. \t  -0.42848969661986275 \t -0.42848969661986275\n",
            "init   \t [ 6.42749368  9.61026168 10.          0.73363448 15.          0.84932207]. \t  -0.43419399571353007 \t -0.42848969661986275\n",
            "init   \t [ 2.66008363  8.36685389  6.          0.92082163 12.          0.69999264]. \t  -0.5353675801921922 \t -0.42848969661986275\n",
            "init   \t [ 1.74779478  2.17691512 12.          0.71794562  2.          0.48990646]. \t  -0.5346620615324694 \t -0.42848969661986275\n",
            "1      \t [0.65899411 0.08911785 5.         0.57513718 8.         0.6649358 ]. \t  -0.5435492302872315 \t -0.42848969661986275\n",
            "2      \t [7.50154303 9.49874736 7.         0.83842364 4.         0.5714185 ]. \t  -0.5422231033003287 \t -0.42848969661986275\n",
            "3      \t [9.88263606 1.71181353 7.         0.97275193 1.         0.74825108]. \t  -0.441747506473179 \t -0.42848969661986275\n",
            "4      \t [ 0.50045611  0.9041258  14.          0.89701685 13.          0.41057236]. \t  -0.5651338432083423 \t -0.42848969661986275\n",
            "5      \t [ 8.92213252  4.56062357 13.          0.93869871  8.          0.53052713]. \t  -0.5289605254310144 \t -0.42848969661986275\n",
            "6      \t [ 3.60181415  8.33097203 14.          0.9882562   5.          0.89449689]. \t  \u001b[92m-0.4137972346428497\u001b[0m \t -0.4137972346428497\n",
            "7      \t [ 0.11546318  7.06183776 14.          0.68323086 19.          0.99255453]. \t  -0.42239160139562504 \t -0.4137972346428497\n",
            "8      \t [0.  0.  5.  0.5 1.  0.1]. \t  -0.6860882831213788 \t -0.4137972346428497\n",
            "9      \t [ 0.10082071  2.20765876  6.          0.79154439 17.          0.83321591]. \t  -0.45448270216379194 \t -0.4137972346428497\n",
            "10     \t [0.32359376 9.19341132 6.         0.74153554 1.         0.25113354]. \t  -0.6853813278526151 \t -0.4137972346428497\n",
            "11     \t [ 9.07851132  6.67468648 14.          0.65555017  1.          0.96610958]. \t  -0.424975755228691 \t -0.4137972346428497\n",
            "12     \t [ 9.80974078  5.67295504  6.          0.91127307 13.          0.44957804]. \t  -0.5591662642332185 \t -0.4137972346428497\n",
            "13     \t [ 2.77198081  0.67245678 14.          0.56883806 19.          0.9553332 ]. \t  -0.4333861030323646 \t -0.4137972346428497\n",
            "14     \t [6.86928577 3.72419603 7.         0.55484489 8.         0.11419186]. \t  -0.6867308079831548 \t -0.4137972346428497\n",
            "15     \t [ 0.83926825  9.31286624 13.          0.83595717 11.          0.92691071]. \t  -0.41668725948491847 \t -0.4137972346428497\n",
            "16     \t [ 2.2599948   8.5777314   7.          0.92101066 18.          0.81278145]. \t  -0.44458847008023056 \t -0.4137972346428497\n",
            "17     \t [ 7.48756016  0.13555104 14.          0.75269236 12.          0.41177876]. \t  -0.5678014017572689 \t -0.4137972346428497\n",
            "18     \t [2.94548912 4.25555055 5.         0.68007425 4.         0.7533029 ]. \t  -0.45906401928062657 \t -0.4137972346428497\n",
            "19     \t [ 8.06744935  9.40296538  6.          0.60147647 19.          0.17678037]. \t  -0.6864884735391246 \t -0.4137972346428497\n",
            "20     \t [ 3.87077387  0.04333662 11.          0.79311966  8.          0.7797114 ]. \t  -0.423652004810958 \t -0.4137972346428497\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.343487868410211"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-45l3NU4UNiI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27e98c7d-bee0-475c-ff25-216d996b3c91"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 3\n",
        "\n",
        "np.random.seed(run_num_3)\n",
        "surrogate_loser_3 = dGaussianProcess(cov_func)\n",
        "\n",
        "X_train3, X_test3, y_train3, y_test3 = train_test_split(X, y, test_size=test_perc, random_state=run_num_3)\n",
        "\n",
        "def f_syn_polarity3(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_3, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train3, y=y_train3).mean())\n",
        "    return operator * score\n",
        "\n",
        "loser_3 = dGPGO(surrogate_loser_3, Acquisition_new(util_loser), f_syn_polarity3, param, n_jobs = -1) # define BayesOpt\n",
        "loser_3.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_loser_3 = loser_3.getResult()[0]\n",
        "params_loser_3['max_depth'] = int(params_loser_3['max_depth'])\n",
        "params_loser_3['min_child_weight'] = int(params_loser_3['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_loser_train3 = xgb.DMatrix(X_train3, y_train3)\n",
        "dX_loser_test3 = xgb.DMatrix(X_test3, y_test3)\n",
        "model_loser_3 = xgb.train(params_loser_3, dX_loser_train3)\n",
        "pred_loser_3 = model_loser_3.predict(dX_loser_test3)\n",
        "\n",
        "rmse_loser_3 = np.sqrt(mean_squared_error(pred_loser_3, y_test3))\n",
        "rmse_loser_3"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 7.51575607  1.09251275 13.          0.80377147 16.          0.31229624]. \t  -0.6858957949546655 \t -0.5081673732303724\n",
            "init   \t [6.12794891 1.19652403 5.         0.74822174 8.         0.33695838]. \t  -0.6849895205987827 \t -0.5081673732303724\n",
            "init   \t [ 6.21631223  4.4909976  10.          0.66764529  9.          0.77207323]. \t  -0.5081673732303724 \t -0.5081673732303724\n",
            "init   \t [ 0.58127233  4.79918477 11.          0.64526552 15.          0.62431773]. \t  -0.5106501747238601 \t -0.5081673732303724\n",
            "init   \t [ 5.76228258  0.67219724  7.          0.88228691 13.          0.29867219]. \t  -0.6839256266327581 \t -0.5081673732303724\n",
            "1      \t [9.79994489 9.57810833 8.         0.93763368 1.         0.1772549 ]. \t  -0.7316297465421655 \t -0.5081673732303724\n",
            "2      \t [ 3.27075931  5.07207678 14.          0.99587284  2.          0.51564135]. \t  -0.6620507374994331 \t -0.5081673732303724\n",
            "3      \t [0.  0.  5.  0.5 1.  0.1]. \t  -0.7323904210578609 \t -0.5081673732303724\n",
            "4      \t [ 8.88346709  9.03427619  8.          0.525814   19.          0.22866902]. \t  -0.732406939137863 \t -0.5081673732303724\n",
            "5      \t [1.21637479 8.34226514 5.         0.57939054 5.         0.55479603]. \t  -0.6531491980452168 \t -0.5081673732303724\n",
            "6      \t [ 2.93278685  9.15897164 14.          0.67343746 10.          0.85376406]. \t  \u001b[92m-0.504811636744922\u001b[0m \t -0.504811636744922\n",
            "7      \t [ 1.1995358   9.87930749  5.          0.84448568 18.          0.26022822]. \t  -0.7297224033867351 \t -0.504811636744922\n",
            "8      \t [ 1.96525323  0.16048396 14.          0.65133015  9.          0.3003508 ]. \t  -0.6860931128073492 \t -0.504811636744922\n",
            "9      \t [6.19566593 0.54575662 9.         0.53247577 1.         0.58149275]. \t  -0.5171483404397559 \t -0.504811636744922\n",
            "10     \t [ 3.97093318  9.58939295  6.          0.5381492  12.          0.74760437]. \t  -0.5217629748454513 \t -0.504811636744922\n",
            "11     \t [ 9.94719165  9.26136843 13.          0.64663072 11.          0.47018827]. \t  -0.6546346492088932 \t -0.504811636744922\n",
            "12     \t [9.55919323 7.66342182 6.         0.63144136 7.         0.82664124]. \t  -0.5195220027397864 \t -0.504811636744922\n",
            "13     \t [ 2.80079796  0.61761702  5.          0.62606225 19.          0.71334647]. \t  -0.5274647167409906 \t -0.504811636744922\n",
            "14     \t [ 9.59599852  3.11576364  5.          0.59256945 17.          0.56621848]. \t  -0.6538676177721936 \t -0.504811636744922\n",
            "15     \t [ 8.91533927  0.16704192 13.          0.66647549  6.          0.25014   ]. \t  -0.7314177593008601 \t -0.504811636744922\n",
            "16     \t [ 4.33253274  9.14516752 14.          0.94398901 19.          0.74581708]. \t  \u001b[92m-0.5007721901039625\u001b[0m \t -0.5007721901039625\n",
            "17     \t [9.75857066 2.84550968 5.         0.7181157  3.         0.67923616]. \t  -0.5274197984747595 \t -0.5007721901039625\n",
            "18     \t [ 0.          0.          5.          0.5        10.44582355  0.1       ]. \t  -0.7323784626096901 \t -0.5007721901039625\n",
            "19     \t [ 9.94497871  9.34276152 14.          0.66308198  5.          0.66061477]. \t  -0.5102482930096841 \t -0.5007721901039625\n",
            "20     \t [ 0.45956381  8.89338741 11.          0.67604128  4.          0.20043998]. \t  -0.7307540586635348 \t -0.5007721901039625\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.6249009343932785"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voPfk1UDUQU0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6670e39a-834f-4dd0-eafd-aabbc700e67a"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 4\n",
        "\n",
        "np.random.seed(run_num_4)\n",
        "surrogate_loser_4 = dGaussianProcess(cov_func)\n",
        "\n",
        "X_train4, X_test4, y_train4, y_test4 = train_test_split(X, y, test_size=test_perc, random_state=run_num_4)\n",
        "\n",
        "def f_syn_polarity4(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_4, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train4, y=y_train4).mean())\n",
        "    return operator * score\n",
        "\n",
        "loser_4 = dGPGO(surrogate_loser_4, Acquisition_new(util_loser), f_syn_polarity4, param, n_jobs = -1) # define BayesOpt\n",
        "loser_4.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_loser_4 = loser_4.getResult()[0]\n",
        "params_loser_4['max_depth'] = int(params_loser_4['max_depth'])\n",
        "params_loser_4['min_child_weight'] = int(params_loser_4['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_loser_train4 = xgb.DMatrix(X_train4, y_train4)\n",
        "dX_loser_test4 = xgb.DMatrix(X_test4, y_test4)\n",
        "model_loser_4 = xgb.train(params_loser_4, dX_loser_train4)\n",
        "pred_loser_4 = model_loser_4.predict(dX_loser_test4)\n",
        "\n",
        "rmse_loser_4 = np.sqrt(mean_squared_error(pred_loser_4, y_test4))\n",
        "rmse_loser_4"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 9.42798506  1.23319667 14.          0.81910755  7.          0.7021549 ]. \t  -0.5581090891563811 \t -0.5581090891563811\n",
            "init   \t [ 5.54158317  7.71233962 10.          0.59117644  6.          0.31476458]. \t  -0.6204181120564479 \t -0.5581090891563811\n",
            "init   \t [2.18282634 2.93794127 7.         0.84739327 5.         0.62716229]. \t  -0.5626656925564042 \t -0.5581090891563811\n",
            "init   \t [ 5.66159581  4.68678814 11.          0.54147808  9.          0.58609221]. \t  -0.5595885941852147 \t -0.5581090891563811\n",
            "init   \t [ 3.02160958  6.07432614  6.          0.92038559 14.          0.12778609]. \t  -0.6361772582153176 \t -0.5581090891563811\n",
            "1      \t [ 1.89460697  0.10324856 13.          0.70356349 16.          0.67776197]. \t  \u001b[92m-0.5568243993303088\u001b[0m \t -0.5568243993303088\n",
            "2      \t [ 9.69969     4.32607677 14.          0.87739158 16.          0.66014279]. \t  -0.5573098211239071 \t -0.5568243993303088\n",
            "3      \t [ 2.2210328   8.31054196 13.          0.70317243 19.          0.56953182]. \t  -0.573504425404327 \t -0.5568243993303088\n",
            "4      \t [ 0.14706294  0.75702217 14.          0.56309891  8.          0.29887971]. \t  -0.6254087880261816 \t -0.5568243993303088\n",
            "5      \t [ 6.94218243  0.32144048  6.          0.59286079 19.          0.70636825]. \t  -0.5706574487524134 \t -0.5568243993303088\n",
            "6      \t [ 9.24159749  9.43575685  7.          0.98380459 19.          0.34937346]. \t  -0.6187589419936593 \t -0.5568243993303088\n",
            "7      \t [7.57184449 0.0324327  9.         0.86318723 1.         0.6413689 ]. \t  -0.5611723012436254 \t -0.5568243993303088\n",
            "8      \t [ 1.62755426  6.7921099  14.          0.91903975  1.          0.61791343]. \t  -0.5680865264437001 \t -0.5568243993303088\n",
            "9      \t [ 9.43074486  0.98044049  5.          0.96290856 10.          0.27397556]. \t  -0.6390681992417737 \t -0.5568243993303088\n",
            "10     \t [ 8.73128599  9.22658128 14.          0.98344346  2.          0.13489277]. \t  -0.6368164613179 \t -0.5568243993303088\n",
            "11     \t [ 0.          0.          7.37720743  0.5        12.37720743  0.1       ]. \t  -0.6353530029272869 \t -0.5568243993303088\n",
            "12     \t [8.91398595 7.00476796 6.         0.72156795 1.         0.41673037]. \t  -0.6210807206819127 \t -0.5568243993303088\n",
            "13     \t [ 9.58851274  9.75423983 14.          0.60815214 12.          0.97572229]. \t  \u001b[92m-0.44045254560081915\u001b[0m \t -0.44045254560081915\n",
            "14     \t [ 9.87497198  6.68701692  6.          0.55980787 13.          0.32605223]. \t  -0.6200123103666009 \t -0.44045254560081915\n",
            "15     \t [ 2.16464782  9.35692886 11.          0.97431128 12.          0.43257419]. \t  -0.5747879420555002 \t -0.44045254560081915\n",
            "16     \t [0.  0.  5.  0.5 1.  0.1]. \t  -0.6351353738423114 \t -0.44045254560081915\n",
            "17     \t [0.94208981 7.15934626 8.47738305 0.5        1.47738305 0.1       ]. \t  -0.6346215545948299 \t -0.44045254560081915\n",
            "18     \t [7.36355448 5.3077281  5.         0.5709887  7.         0.87569884]. \t  -0.467633753337906 \t -0.44045254560081915\n",
            "19     \t [ 7.93069008  0.11830338 11.          0.97110397 13.          0.22887984]. \t  -0.6356672520315936 \t -0.44045254560081915\n",
            "20     \t [ 6.39787933  3.84205635 14.          0.72880814  3.          0.21974221]. \t  -0.6364956155454153 \t -0.44045254560081915\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.469425637772456"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kEnTd7MUdlv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfb5e35e-1eb2-43af-d6b4-120e178c169a"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 5\n",
        "\n",
        "np.random.seed(run_num_5)\n",
        "surrogate_loser_5 = dGaussianProcess(cov_func)\n",
        "\n",
        "X_train5, X_test5, y_train5, y_test5 = train_test_split(X, y, test_size=test_perc, random_state=run_num_5)\n",
        "\n",
        "def f_syn_polarity5(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_5, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train5, y=y_train5).mean())\n",
        "    return operator * score\n",
        "\n",
        "loser_5 = dGPGO(surrogate_loser_5, Acquisition_new(util_loser), f_syn_polarity5, param, n_jobs = -1) # define BayesOpt\n",
        "loser_5.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_loser_5 = loser_5.getResult()[0]\n",
        "params_loser_5['max_depth'] = int(params_loser_5['max_depth'])\n",
        "params_loser_5['min_child_weight'] = int(params_loser_5['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_loser_train5 = xgb.DMatrix(X_train5, y_train5)\n",
        "dX_loser_test5 = xgb.DMatrix(X_test5, y_test5)\n",
        "model_loser_5 = xgb.train(params_loser_5, dX_loser_train5)\n",
        "pred_loser_5 = model_loser_5.predict(dX_loser_test5)\n",
        "\n",
        "rmse_loser_5 = np.sqrt(mean_squared_error(pred_loser_5, y_test5))\n",
        "rmse_loser_5"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 5.57353274  5.41760627  8.          0.53710825 19.          0.26034376]. \t  -0.6523797617249512 \t -0.466901759574539\n",
            "init   \t [5.17826437 9.78817462 5.         0.84048437 6.         0.79712615]. \t  -0.4886603987288415 \t -0.466901759574539\n",
            "init   \t [ 9.84476201  9.65484856  5.          0.59464714 19.          0.34528747]. \t  -0.564220001422124 \t -0.466901759574539\n",
            "init   \t [ 9.17476842  4.53262239 13.          0.64368565 10.          0.515308  ]. \t  -0.5049042831858346 \t -0.466901759574539\n",
            "init   \t [ 8.66719465  7.99685372 12.          0.79833571 17.          0.72443466]. \t  -0.466901759574539 \t -0.466901759574539\n",
            "1      \t [ 3.31480734  6.81602635 12.          0.95330947  1.          0.27334927]. \t  -0.6531167190113543 \t -0.466901759574539\n",
            "2      \t [2.53140694 0.29740057 9.         0.7505917  7.         0.82287404]. \t  -0.47475119282304395 \t -0.466901759574539\n",
            "3      \t [ 9.84728748  0.28222034 13.          0.93969018  3.          0.22339517]. \t  -0.6522247903981228 \t -0.466901759574539\n",
            "4      \t [ 0.68116737  3.94603776 14.          0.80380753 14.          0.77317996]. \t  \u001b[92m-0.4613270217842209\u001b[0m \t -0.4613270217842209\n",
            "5      \t [ 9.6928838   0.38736829  6.          0.56167539 11.          0.4775038 ]. \t  -0.5239028011959734 \t -0.4613270217842209\n",
            "6      \t [0.         2.96817906 5.         0.5        1.         0.1       ]. \t  -0.6542193437616992 \t -0.4613270217842209\n",
            "7      \t [ 0.96088206  9.84240251  9.          0.79592881 15.          0.83460449]. \t  -0.47215351911272185 \t -0.4613270217842209\n",
            "8      \t [ 9.56204849  0.80573312 12.          0.60476955 16.          0.23612747]. \t  -0.6524022297735614 \t -0.4613270217842209\n",
            "9      \t [ 1.05461775  1.17661042  5.          0.5648338  17.          0.90649901]. \t  -0.4702337390488589 \t -0.4613270217842209\n",
            "10     \t [ 1.28998978  8.90442798 10.          0.78625314  8.          0.13448748]. \t  -0.6545570207229267 \t -0.4613270217842209\n",
            "11     \t [9.66604659 1.51730123 5.         0.59597224 4.         0.66643448]. \t  -0.5016316198301449 \t -0.4613270217842209\n",
            "12     \t [ 2.6635752   5.58712964  6.          0.93254798 12.          0.28064449]. \t  -0.6521391482286414 \t -0.4613270217842209\n",
            "13     \t [ 2.76670882  9.36479927 14.          0.77451901 19.          0.10649887]. \t  -0.6539377811460048 \t -0.4613270217842209\n",
            "14     \t [0.02835933 9.74346527 7.         0.54472382 1.         0.83619676]. \t  -0.48645443650554226 \t -0.4613270217842209\n",
            "15     \t [ 9.08851157  7.7443441   6.          0.74314397 12.          0.52508293]. \t  -0.5238407378055661 \t -0.4613270217842209\n",
            "16     \t [ 0.62266475  1.9018111  14.          0.63329124  3.          0.94054341]. \t  \u001b[92m-0.42529491633232686\u001b[0m \t -0.42529491633232686\n",
            "17     \t [ 9.23494676  7.58606552 12.          0.78487568  2.          0.96478853]. \t  -0.42939677763278594 \t -0.42529491633232686\n",
            "18     \t [6.8698644  5.44523052 7.         0.65534714 1.         0.29639138]. \t  -0.5598703662523957 \t -0.42529491633232686\n",
            "19     \t [ 0.          0.          5.          0.5        10.21001817  0.1       ]. \t  -0.6545380212108272 \t -0.42529491633232686\n",
            "20     \t [ 8.32286546  0.75756777  6.          0.69885376 18.          0.98511179]. \t  -0.46068165397024996 \t -0.42529491633232686\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.31722690080873"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjVSH6caUgyy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "997868a1-45de-48da-e840-3dbe94a84e1a"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 6\n",
        "\n",
        "np.random.seed(run_num_6)\n",
        "surrogate_loser_6 = dGaussianProcess(cov_func)\n",
        "\n",
        "X_train6, X_test6, y_train6, y_test6 = train_test_split(X, y, test_size=test_perc, random_state=run_num_6)\n",
        "\n",
        "def f_syn_polarity6(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_6, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train6, y=y_train6).mean())\n",
        "    return operator * score\n",
        "\n",
        "loser_6 = dGPGO(surrogate_loser_6, Acquisition_new(util_loser), f_syn_polarity6, param, n_jobs = -1) # define BayesOpt\n",
        "loser_6.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_loser_6 = loser_6.getResult()[0]\n",
        "params_loser_6['max_depth'] = int(params_loser_6['max_depth'])\n",
        "params_loser_6['min_child_weight'] = int(params_loser_6['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_loser_train6 = xgb.DMatrix(X_train6, y_train6)\n",
        "dX_loser_test6 = xgb.DMatrix(X_test6, y_test6)\n",
        "model_loser_6 = xgb.train(params_loser_6, dX_loser_train6)\n",
        "pred_loser_6 = model_loser_6.predict(dX_loser_test6)\n",
        "\n",
        "rmse_loser_6 = np.sqrt(mean_squared_error(pred_loser_6, y_test6))\n",
        "rmse_loser_6"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [8.92860151 3.31979805 5.         0.99251441 2.         0.57683563]. \t  -0.5471181110640053 \t -0.5219867521874206\n",
            "init   \t [4.18807429 3.35407849 9.         0.87750649 3.         0.56623277]. \t  -0.629286701398377 \t -0.5219867521874206\n",
            "init   \t [ 5.788586    6.45355096 14.          0.70660047 12.          0.82154882]. \t  -0.5219867521874206 \t -0.5219867521874206\n",
            "init   \t [4.58184578 6.73834679 5.         0.90108528 3.         0.65482895]. \t  -0.5476166257785141 \t -0.5219867521874206\n",
            "init   \t [ 4.42510505  5.75952352 14.          0.97882365 15.          0.29525604]. \t  -0.6655867879161768 \t -0.5219867521874206\n",
            "1      \t [ 7.09628606  0.82458553  6.          0.52978945 17.          0.78827589]. \t  -0.5368551114917748 \t -0.5219867521874206\n",
            "2      \t [ 1.88885215  8.62524025  6.          0.90916563 14.          0.40379086]. \t  -0.6680072912825885 \t -0.5219867521874206\n",
            "3      \t [ 0.5654966   9.52584762 14.          0.82016688  4.          0.10717254]. \t  -0.7389555129670606 \t -0.5219867521874206\n",
            "4      \t [ 9.63033822  0.03793409 13.          0.68076302  7.          0.91391681]. \t  \u001b[92m-0.4301920669254681\u001b[0m \t -0.4301920669254681\n",
            "5      \t [ 9.97469028  9.15579642 12.          0.66745021 19.          0.28959158]. \t  -0.6668248751122167 \t -0.4301920669254681\n",
            "6      \t [ 9.85880187  8.48475629 13.          0.76006171  2.          0.58751173]. \t  -0.5403277210407594 \t -0.4301920669254681\n",
            "7      \t [ 9.51844     9.62786679  5.          0.56989798 15.          0.51068013]. \t  -0.6345288616058514 \t -0.4301920669254681\n",
            "8      \t [ 0.15151883  3.04875397  5.          0.61495161 19.          0.79420245]. \t  -0.5357132607573185 \t -0.4301920669254681\n",
            "9      \t [ 3.08077333  0.89569015  6.          0.84149914 11.          0.94992477]. \t  -0.4472711154368675 \t -0.4301920669254681\n",
            "10     \t [ 0.19816257  1.95434851 14.          0.59292238  7.          0.17971262]. \t  -0.7383261448869979 \t -0.4301920669254681\n",
            "11     \t [ 0.56219975  9.42928268 11.          0.51903033 19.          0.52214278]. \t  -0.6326946461691906 \t -0.4301920669254681\n",
            "12     \t [9.58176304 9.94093363 6.         0.96280567 8.         0.32855082]. \t  -0.6677341620025121 \t -0.4301920669254681\n",
            "13     \t [ 5.43877488  0.17504551 13.          0.58721411 19.          0.60115325]. \t  -0.5401916158723044 \t -0.4301920669254681\n",
            "14     \t [0.  0.  5.  0.5 1.  0.1]. \t  -0.7382709735940614 \t -0.4301920669254681\n",
            "15     \t [9.18163606 3.89469538 8.         0.59772622 8.         0.18901946]. \t  -0.7380580836797348 \t -0.4301920669254681\n",
            "16     \t [0.52725495 9.82259272 6.         0.62080595 7.         0.68770749]. \t  -0.5497290569747493 \t -0.4301920669254681\n",
            "17     \t [ 6.71573436  6.18076517  8.          0.50736725 18.          0.99692636]. \t  -0.4548470613295324 \t -0.4301920669254681\n",
            "18     \t [ 7.23381367  0.21465732 13.          0.75484136 13.          0.44717401]. \t  -0.6306144837329356 \t -0.4301920669254681\n",
            "19     \t [ 0.17525712  4.19183624 13.          0.93093846  1.          0.77496282]. \t  -0.5254340278392874 \t -0.4301920669254681\n",
            "20     \t [ 0.22896089  2.69806987 14.          0.53297649 18.          0.65056758]. \t  -0.5430966302037052 \t -0.4301920669254681\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.1451215046465775"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1WsphKSUj19",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7df9e964-0d68-46a2-c1ec-2ff39597ffa8"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 7\n",
        "\n",
        "np.random.seed(run_num_7)\n",
        "surrogate_loser_7 = dGaussianProcess(cov_func)\n",
        "\n",
        "X_train7, X_test7, y_train7, y_test7 = train_test_split(X, y, test_size=test_perc, random_state=run_num_7)\n",
        "\n",
        "def f_syn_polarity7(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_7, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train7, y=y_train7).mean())\n",
        "    return operator * score\n",
        "\n",
        "loser_7 = dGPGO(surrogate_loser_7, Acquisition_new(util_loser), f_syn_polarity7, param, n_jobs = -1) # define BayesOpt\n",
        "loser_7.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_loser_7 = loser_7.getResult()[0]\n",
        "params_loser_7['max_depth'] = int(params_loser_7['max_depth'])\n",
        "params_loser_7['min_child_weight'] = int(params_loser_7['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_loser_train7 = xgb.DMatrix(X_train7, y_train7)\n",
        "dX_loser_test7 = xgb.DMatrix(X_test7, y_test7)\n",
        "model_loser_7 = xgb.train(params_loser_7, dX_loser_train7)\n",
        "pred_loser_7 = model_loser_7.predict(dX_loser_test7)\n",
        "\n",
        "rmse_loser_7 = np.sqrt(mean_squared_error(pred_loser_7, y_test7))\n",
        "rmse_loser_7"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 8.11505126  6.92324143 13.          0.94046175 12.          0.3650384 ]. \t  -0.612852517751432 \t -0.4284992540085738\n",
            "init   \t [ 5.15357029  9.0066636   6.          0.74104227 15.          0.8966337 ]. \t  -0.45500673865987984 \t -0.4284992540085738\n",
            "init   \t [ 2.63548913  9.6610934  14.          0.64712408  1.          0.80820948]. \t  -0.43730487111474525 \t -0.4284992540085738\n",
            "init   \t [ 1.40821426  1.72758589 12.          0.70495069  5.          0.86632715]. \t  -0.4284992540085738 \t -0.4284992540085738\n",
            "init   \t [2.85126987 2.73434054 7.         0.88394733 8.         0.65545939]. \t  -0.48253390492139586 \t -0.4284992540085738\n",
            "1      \t [1.03697999e-02 2.51695028e+00 1.40000000e+01 6.19497248e-01\n",
            " 1.90000000e+01 7.01827417e-01]. \t  -0.47758423778274467 \t -0.4284992540085738\n",
            "2      \t [9.9089961  7.50235155 7.         0.67925503 5.         0.2471192 ]. \t  -0.6736712023667494 \t -0.4284992540085738\n",
            "3      \t [ 7.21196079  0.11384945  8.          0.67755731 17.          0.74323626]. \t  -0.4647919397201027 \t -0.4284992540085738\n",
            "4      \t [0.35283087 9.2950614  5.         0.93054416 3.         0.27625531]. \t  -0.670508947897148 \t -0.4284992540085738\n",
            "5      \t [0.  0.  5.  0.5 1.  0.1]. \t  -0.6745888440506937 \t -0.4284992540085738\n",
            "6      \t [9.76759527 0.48761621 7.         0.96362191 1.         0.137157  ]. \t  -0.6715547907624335 \t -0.4284992540085738\n",
            "7      \t [ 8.53387117  0.35946342 13.          0.65332712  6.          0.68302293]. \t  -0.4712802345283902 \t -0.4284992540085738\n",
            "8      \t [ 0.45155781  2.33209808  5.          0.73660734 17.          0.83890491]. \t  -0.4804814871680856 \t -0.4284992540085738\n",
            "9      \t [ 0.30506512  9.51761383 13.          0.5277377  16.          0.92234588]. \t  -0.4390633386106087 \t -0.4284992540085738\n",
            "10     \t [ 9.21287749  3.65759887  5.          0.85687073 12.          0.88459516]. \t  -0.45867733996321497 \t -0.4284992540085738\n",
            "11     \t [ 1.14642054  8.54710172 12.          0.83923566  8.          0.25317439]. \t  -0.6716592254658202 \t -0.4284992540085738\n",
            "12     \t [ 6.43879816  6.53341304 14.          0.70290258 19.          0.43578856]. \t  -0.5879212739277196 \t -0.4284992540085738\n",
            "13     \t [ 0.28968499  1.98768694 14.          0.97996073 12.          0.15615797]. \t  -0.6725851052423111 \t -0.4284992540085738\n",
            "14     \t [5.4721942  4.70341961 9.         0.675658   1.         0.91263074]. \t  -0.44024324260844283 \t -0.4284992540085738\n",
            "15     \t [ 7.56572451  6.67046024 14.          0.88731871  4.          0.29940932]. \t  -0.6117537685654454 \t -0.4284992540085738\n",
            "16     \t [4.54036829 9.23672573 7.         0.78071654 7.         0.925943  ]. \t  -0.44422074369565967 \t -0.4284992540085738\n",
            "17     \t [ 9.52566208  5.8884151   6.          0.78376025 19.          0.40148749]. \t  -0.6258103990077785 \t -0.4284992540085738\n",
            "18     \t [ 7.79919254  0.6285715  14.          0.87477175 15.          0.15632002]. \t  -0.6708453498973665 \t -0.4284992540085738\n",
            "19     \t [ 0.          0.          8.43853294  0.5        11.43853294  0.1       ]. \t  -0.6744471853974358 \t -0.4284992540085738\n",
            "20     \t [ 3.1237334   5.58211355  9.          0.8525378  19.          0.1292964 ]. \t  -0.6703702828267247 \t -0.4284992540085738\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.492050628419268"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hI8sFP4ZUmOs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58552781-ccda-4567-c134-d38b50fefccd"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 8\n",
        "\n",
        "np.random.seed(run_num_8)\n",
        "surrogate_loser_8 = dGaussianProcess(cov_func)\n",
        "\n",
        "X_train8, X_test8, y_train8, y_test8 = train_test_split(X, y, test_size=test_perc, random_state=run_num_8)\n",
        "\n",
        "def f_syn_polarity8(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_8, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train8, y=y_train8).mean())\n",
        "    return operator * score\n",
        "\n",
        "loser_8 = dGPGO(surrogate_loser_8, Acquisition_new(util_loser), f_syn_polarity8, param, n_jobs = -1) # define BayesOpt\n",
        "loser_8.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_loser_8 = loser_8.getResult()[0]\n",
        "params_loser_8['max_depth'] = int(params_loser_8['max_depth'])\n",
        "params_loser_8['min_child_weight'] = int(params_loser_8['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_loser_train8 = xgb.DMatrix(X_train8, y_train8)\n",
        "dX_loser_test8 = xgb.DMatrix(X_test8, y_test8)\n",
        "model_loser_8 = xgb.train(params_loser_8, dX_loser_train8)\n",
        "pred_loser_8 = model_loser_8.predict(dX_loser_test8)\n",
        "\n",
        "rmse_loser_8 = np.sqrt(mean_squared_error(pred_loser_8, y_test8))\n",
        "rmse_loser_8"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 1.29221489  0.77396898 11.          0.56062468  3.          0.25451879]. \t  -0.6231794271723858 \t -0.4674926150198253\n",
            "init   \t [ 0.10611429  1.37648061  8.          0.6050375  15.          0.790205  ]. \t  -0.4674926150198253 \t -0.4674926150198253\n",
            "init   \t [ 8.35239571  4.16908043  7.          0.67902746 14.          0.16683749]. \t  -0.6176439249709439 \t -0.4674926150198253\n",
            "init   \t [ 6.06301777  7.27127491 12.          0.87295531 11.          0.37694382]. \t  -0.5951520828762356 \t -0.4674926150198253\n",
            "init   \t [8.85209637 7.89269955 6.         0.72444854 3.         0.47766986]. \t  -0.5688519156580504 \t -0.4674926150198253\n",
            "1      \t [ 1.47825581  9.16010046 11.          0.96606546  1.          0.92358292]. \t  \u001b[92m-0.43518063190798095\u001b[0m \t -0.43518063190798095\n",
            "2      \t [ 7.52273592  2.30141271 14.          0.75505047 19.          0.77804046]. \t  -0.45158957529537797 \t -0.43518063190798095\n",
            "3      \t [1.64420908 9.47353882 5.         0.79993338 7.         0.15673599]. \t  -0.6175838614360663 \t -0.43518063190798095\n",
            "4      \t [ 1.02399475  9.65113789  9.          0.53947972 17.          0.8292164 ]. \t  -0.4719352875928771 \t -0.43518063190798095\n",
            "5      \t [8.14212583 0.61314553 6.         0.86488725 6.         0.28574491]. \t  -0.5954481959734638 \t -0.43518063190798095\n",
            "6      \t [ 8.76387285  7.83054608 13.          0.54159783  4.          0.24710422]. \t  -0.6207082177098945 \t -0.43518063190798095\n",
            "7      \t [ 9.0517509   9.45530828 11.          0.85305105 19.          0.79961763]. \t  -0.4589725747508552 \t -0.43518063190798095\n",
            "8      \t [ 7.77762087  0.97608919 14.          0.56979605  8.          0.73436998]. \t  -0.4483480686509709 \t -0.43518063190798095\n",
            "9      \t [2.67676373 3.95611485 5.         0.79739163 2.         0.89987835]. \t  -0.45674643004535687 \t -0.43518063190798095\n",
            "10     \t [ 0.62854908  4.77207773 14.          0.78182185 18.          0.73284936]. \t  -0.4461225555243119 \t -0.43518063190798095\n",
            "11     \t [3.06290827 1.66002331 6.         0.51411222 9.         0.98529356]. \t  -0.461628022517463 \t -0.43518063190798095\n",
            "12     \t [ 0.55545334  6.5757175   9.          0.88529579 12.          0.24203797]. \t  -0.6187020960077592 \t -0.43518063190798095\n",
            "13     \t [ 9.10053278  1.48753729 11.          0.9741735   1.          0.65977303]. \t  -0.5223704878111852 \t -0.43518063190798095\n",
            "14     \t [ 9.48329103  0.2076913   7.          0.76655383 19.          0.88164364]. \t  -0.4616094756962891 \t -0.43518063190798095\n",
            "15     \t [ 0.81390458  8.42248516 12.          0.78729971  7.          0.71328057]. \t  -0.5176932464043367 \t -0.43518063190798095\n",
            "16     \t [ 5.51535579  8.63063044  6.          0.64760098 18.          0.14185561]. \t  -0.6172074564902792 \t -0.43518063190798095\n",
            "17     \t [ 0.81210087  0.8533868  13.          0.66456412 12.          0.37769434]. \t  -0.5971747957799627 \t -0.43518063190798095\n",
            "18     \t [ 8.74769008  9.46041181  5.          0.76888057 12.          0.28611188]. \t  -0.5996998823178632 \t -0.43518063190798095\n",
            "19     \t [ 6.67918913  1.91467598 12.          0.83062216 13.          0.72788563]. \t  -0.45035357506572193 \t -0.43518063190798095\n",
            "20     \t [ 4.53564763  5.14157265 14.          0.52558575  1.          0.72718907]. \t  -0.45119962298136185 \t -0.43518063190798095\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.711175290813375"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vw5IYus6UpAn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb32f367-70e8-4897-97aa-b77229413650"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 9\n",
        "\n",
        "np.random.seed(run_num_9)\n",
        "surrogate_loser_9 = dGaussianProcess(cov_func)\n",
        "\n",
        "X_train9, X_test9, y_train9, y_test9 = train_test_split(X, y, test_size=test_perc, random_state=run_num_9)\n",
        "\n",
        "def f_syn_polarity9(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_9, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train9, y=y_train9).mean())\n",
        "    return operator * score\n",
        "\n",
        "loser_9 = dGPGO(surrogate_loser_9, Acquisition_new(util_loser), f_syn_polarity9, param, n_jobs = -1) # define BayesOpt\n",
        "loser_9.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_loser_9 = loser_9.getResult()[0]\n",
        "params_loser_9['max_depth'] = int(params_loser_9['max_depth'])\n",
        "params_loser_9['min_child_weight'] = int(params_loser_9['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_loser_train9 = xgb.DMatrix(X_train9, y_train9)\n",
        "dX_loser_test9 = xgb.DMatrix(X_test9, y_test9)\n",
        "model_loser_9 = xgb.train(params_loser_9, dX_loser_train9)\n",
        "pred_loser_9 = model_loser_9.predict(dX_loser_test9)\n",
        "\n",
        "rmse_loser_9 = np.sqrt(mean_squared_error(pred_loser_9, y_test9))\n",
        "rmse_loser_9"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 8.0342804   5.27522296 13.          0.60741904 17.          0.15336222]. \t  -0.6890593508433319 \t -0.4886355541929722\n",
            "init   \t [7.56063644 1.79876369 7.         0.84869647 5.         0.55956942]. \t  -0.5532305414240943 \t -0.4886355541929722\n",
            "init   \t [2.43675539 0.20225128 6.         0.51669963 7.         0.4646446 ]. \t  -0.5609155518755553 \t -0.4886355541929722\n",
            "init   \t [ 7.6043547   4.73758376  9.          0.87564624 15.          0.65751286]. \t  -0.4886355541929722 \t -0.4886355541929722\n",
            "init   \t [ 4.70349314  6.71029586  6.          0.97680389 11.          0.41971652]. \t  -0.6548660487332898 \t -0.4886355541929722\n",
            "1      \t [ 0.20910387  7.25545521 14.          0.59407892  3.          0.95410586]. \t  \u001b[92m-0.4312356520914486\u001b[0m \t -0.4312356520914486\n",
            "2      \t [ 2.14892068  9.25577985 14.          0.84828472 11.          0.2793841 ]. \t  -0.6883959263608518 \t -0.4312356520914486\n",
            "3      \t [ 0.37305557  1.02756836 13.          0.57348052 13.          0.68410192]. \t  -0.4836872799099695 \t -0.4312356520914486\n",
            "4      \t [ 7.98252507  8.04957695 12.          0.6367811   3.          0.85775382]. \t  -0.4368210491589304 \t -0.4312356520914486\n",
            "5      \t [ 1.2588195   0.04438454  7.          0.70342864 18.          0.58554745]. \t  -0.4951281025154505 \t -0.4312356520914486\n",
            "6      \t [ 9.17710587  9.71573287  5.          0.9584376  19.          0.78072441]. \t  -0.4682275209845145 \t -0.4312356520914486\n",
            "7      \t [6.11416909 9.54959201 5.         0.91943284 1.         0.10968146]. \t  -0.6901756354808697 \t -0.4312356520914486\n",
            "8      \t [ 0.31270412  9.94818438  8.          0.84397658 19.          0.43974503]. \t  -0.5483333357303245 \t -0.4312356520914486\n",
            "9      \t [ 8.76504058  0.46155195 14.          0.61459938 10.          0.19095511]. \t  -0.6902913672662703 \t -0.4312356520914486\n",
            "10     \t [ 0.77001066  0.48822065 12.          0.89190754  5.          0.37116504]. \t  -0.6599680642282422 \t -0.4312356520914486\n",
            "11     \t [ 0.76412088  5.02527282 14.          0.53794557 19.          0.6779987 ]. \t  -0.4862552398836468 \t -0.4312356520914486\n",
            "12     \t [1.11781249 4.65858905 8.         0.79271199 1.         0.71342306]. \t  -0.4899393997261333 \t -0.4312356520914486\n",
            "13     \t [ 4.60583171  0.06752    13.          0.94036906 18.          0.58293565]. \t  -0.47784115580365294 \t -0.4312356520914486\n",
            "14     \t [6.09359952e-03 8.65908473e+00 6.00000000e+00 6.67169539e-01\n",
            " 6.00000000e+00 4.18236274e-01]. \t  -0.6570878867409317 \t -0.4312356520914486\n",
            "15     \t [ 7.69267031  0.52709121  5.          0.97712342 11.          0.47111142]. \t  -0.5671609865381374 \t -0.4312356520914486\n",
            "16     \t [9.94406911 8.10389604 7.         0.8844164  7.         0.60247845]. \t  -0.4916963401121463 \t -0.4312356520914486\n",
            "17     \t [ 4.9384323   0.37731064 14.          0.9492051   1.          0.8118423 ]. \t  -0.4371623420164621 \t -0.4312356520914486\n",
            "18     \t [0.  0.  5.  0.5 1.  0.1]. \t  -0.6893514282293631 \t -0.4312356520914486\n",
            "19     \t [ 9.68084543  9.58294825 13.          0.63062284  9.          0.21596151]. \t  -0.689113084484204 \t -0.4312356520914486\n",
            "20     \t [ 5.24998729  5.27861916  5.          0.87944065 19.          0.38196712]. \t  -0.656916604582956 \t -0.4312356520914486\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.130636454660636"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YD494io_Ur7V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db2bb97a-cda6-4a2b-e3dd-bf0f7480fe72"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 10\n",
        "\n",
        "np.random.seed(run_num_10)\n",
        "surrogate_loser_10 = dGaussianProcess(cov_func)\n",
        "\n",
        "X_train10, X_test10, y_train10, y_test10 = train_test_split(X, y, test_size=test_perc, random_state=run_num_10)\n",
        "\n",
        "def f_syn_polarity10(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_10, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train10, y=y_train10).mean())\n",
        "    return operator * score\n",
        "\n",
        "loser_10 = dGPGO(surrogate_loser_10, Acquisition_new(util_loser), f_syn_polarity10, param, n_jobs = -1) # define BayesOpt\n",
        "loser_10.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_loser_10 = loser_10.getResult()[0]\n",
        "params_loser_10['max_depth'] = int(params_loser_10['max_depth'])\n",
        "params_loser_10['min_child_weight'] = int(params_loser_10['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_loser_train10 = xgb.DMatrix(X_train10, y_train10)\n",
        "dX_loser_test10 = xgb.DMatrix(X_test10, y_test10)\n",
        "model_loser_10 = xgb.train(params_loser_10, dX_loser_train10)\n",
        "pred_loser_10 = model_loser_10.predict(dX_loser_test10)\n",
        "\n",
        "rmse_loser_10 = np.sqrt(mean_squared_error(pred_loser_10, y_test10))\n",
        "rmse_loser_10"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 6.53589585  1.15006943  6.          0.93623727 14.          0.13663866]. \t  -0.6929852048256443 \t -0.4483221221388197\n",
            "init   \t [ 3.97194461  2.33132197 14.          0.9533253  19.          0.26403087]. \t  -0.6932702114662359 \t -0.4483221221388197\n",
            "init   \t [ 7.43539415  0.69582081  9.          0.9763222  11.          0.12608349]. \t  -0.6931242497450955 \t -0.4483221221388197\n",
            "init   \t [ 9.82027485  3.39637684  9.          0.78522537 19.          0.87618843]. \t  -0.4483221221388197 \t -0.4483221221388197\n",
            "init   \t [ 0.57576207  5.82646405 12.          0.77704362 18.          0.11865044]. \t  -0.6946096327074635 \t -0.4483221221388197\n",
            "1      \t [ 3.17878299  6.40666671 10.          0.71928645  2.          0.37920433]. \t  -0.6059745513602156 \t -0.4483221221388197\n",
            "2      \t [ 3.37448201  8.10780464 13.          0.50477079 10.          0.914384  ]. \t  \u001b[92m-0.43629727149692393\u001b[0m \t -0.43629727149692393\n",
            "3      \t [0.         0.         5.         0.5        6.20440214 0.1       ]. \t  -0.6921234195705354 \t -0.43629727149692393\n",
            "4      \t [8.35411693 8.91554079 5.         0.73130633 7.         0.67857606]. \t  -0.5771647264709212 \t -0.43629727149692393\n",
            "5      \t [9.65375532 1.28029677 7.         0.79386133 1.         0.88548903]. \t  -0.44750949856169625 \t -0.43629727149692393\n",
            "6      \t [ 5.33344337  9.63007578  7.          0.55704064 18.          0.25018677]. \t  -0.6934805730034364 \t -0.43629727149692393\n",
            "7      \t [ 9.463998    9.23285451 14.          0.59406232  3.          0.52567113]. \t  -0.580720745359468 \t -0.43629727149692393\n",
            "8      \t [ 0.72612997  5.71374346  5.          0.7962252  12.          0.3276676 ]. \t  -0.6111050009653851 \t -0.43629727149692393\n",
            "9      \t [ 0.03680135  0.50694581 14.          0.91124751  8.          0.87026982]. \t  \u001b[92m-0.4215933675272626\u001b[0m \t -0.4215933675272626\n",
            "10     \t [ 9.9457377   8.37767281 13.          0.92820573 13.          0.22203611]. \t  -0.6925765145348073 \t -0.4215933675272626\n",
            "11     \t [ 6.39500048  0.03099442 14.          0.5358351   2.          0.80194525]. \t  -0.5166569861309996 \t -0.4215933675272626\n",
            "12     \t [6.5189698  8.43001454 5.         0.6517671  1.         0.9994397 ]. \t  -0.45630664213640165 \t -0.4215933675272626\n",
            "13     \t [0.50440288 9.75457432 5.         0.8137431  6.         0.15250773]. \t  -0.6941630128319229 \t -0.4215933675272626\n",
            "14     \t [2.58869992 0.21309666 9.         0.79352016 1.         0.74644401]. \t  -0.5123069056709456 \t -0.4215933675272626\n",
            "15     \t [ 0.61215987  1.50253986 10.92723111  0.5        12.92723111  0.1       ]. \t  -0.6934503902545976 \t -0.4215933675272626\n",
            "16     \t [ 6.3510504   9.80953833 14.          0.77195312 18.          0.95584163]. \t  -0.432776560658424 \t -0.4215933675272626\n",
            "17     \t [ 8.82570869  3.65595981 13.          0.7057174   6.          0.34013494]. \t  -0.6037775379099287 \t -0.4215933675272626\n",
            "18     \t [ 5.46843147  5.816847   11.          0.55561922 15.          0.42343459]. \t  -0.6089133208955124 \t -0.4215933675272626\n",
            "19     \t [ 8.52646717  0.12245704 14.          0.80155281 15.          0.7100538 ]. \t  -0.5709119331337676 \t -0.4215933675272626\n",
            "20     \t [5.51569281 2.48923068 5.         0.97468437 6.         0.88188042]. \t  -0.4524796762835521 \t -0.4215933675272626\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.436171669561738"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N03Sq0TvUuhp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02bb5fed-83dc-4914-d770-93e750903a18"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 11\n",
        "\n",
        "np.random.seed(run_num_11)\n",
        "surrogate_loser_11 = dGaussianProcess(cov_func)\n",
        "\n",
        "X_train11, X_test11, y_train11, y_test11 = train_test_split(X, y, test_size=test_perc, random_state=run_num_11)\n",
        "\n",
        "def f_syn_polarity11(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_11, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train11, y=y_train11).mean())\n",
        "    return operator * score\n",
        "\n",
        "loser_11 = dGPGO(surrogate_loser_11, Acquisition_new(util_loser), f_syn_polarity11, param, n_jobs = -1) # define BayesOpt\n",
        "loser_11.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_loser_11 = loser_11.getResult()[0]\n",
        "params_loser_11['max_depth'] = int(params_loser_11['max_depth'])\n",
        "params_loser_11['min_child_weight'] = int(params_loser_11['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_loser_train11 = xgb.DMatrix(X_train11, y_train11)\n",
        "dX_loser_test11 = xgb.DMatrix(X_test11, y_test11)\n",
        "model_loser_11 = xgb.train(params_loser_11, dX_loser_train11)\n",
        "pred_loser_11 = model_loser_11.predict(dX_loser_test11)\n",
        "\n",
        "rmse_loser_11 = np.sqrt(mean_squared_error(pred_loser_11, y_test11))\n",
        "rmse_loser_11"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 9.81035663  8.2124785   9.          0.68487173 15.          0.28207988]. \t  -0.7133787013189323 \t -0.47317045732943896\n",
            "init   \t [ 4.91906771  1.38558332  6.          0.618878   19.          0.10263204]. \t  -0.7112023662943636 \t -0.47317045732943896\n",
            "init   \t [ 5.23426175  1.89971159 13.          0.76313779 17.          0.46597853]. \t  -0.47317045732943896 \t -0.47317045732943896\n",
            "init   \t [ 4.0204056   3.75167577  9.          0.86204818 17.          0.26560056]. \t  -0.7132057194891379 \t -0.47317045732943896\n",
            "init   \t [8.8704459  5.64917815 5.         0.51406171 1.         0.82826731]. \t  -0.48048685248123163 \t -0.47317045732943896\n",
            "1      \t [0.08349406 1.89250335 6.         0.66386636 1.         0.88641723]. \t  \u001b[92m-0.4484239383130805\u001b[0m \t -0.4484239383130805\n",
            "2      \t [ 5.35837408  0.31510164 14.          0.97519815  6.          0.3034681 ]. \t  -0.607905230502132 \t -0.4484239383130805\n",
            "3      \t [ 4.66315557  9.30076713 11.          0.83534598  8.          0.21303816]. \t  -0.7135282416096518 \t -0.4484239383130805\n",
            "4      \t [9.02046973 1.16865322 7.         0.6807515  8.         0.2962635 ]. \t  -0.6068266615066733 \t -0.4484239383130805\n",
            "5      \t [0.79988529 9.85424348 5.         0.5743044  3.         0.67901433]. \t  -0.48209943859810334 \t -0.4484239383130805\n",
            "6      \t [ 0.02057794  7.14089873 13.          0.90855163  1.          0.61823707]. \t  -0.45805892074172583 \t -0.4484239383130805\n",
            "7      \t [ 0.97603689  3.49871064  9.          0.91846837 10.          0.35158251]. \t  -0.6085109103402477 \t -0.4484239383130805\n",
            "8      \t [ 9.27240029  9.57413463 14.          0.58544186  3.          0.73779027]. \t  -0.45267877796490286 \t -0.4484239383130805\n",
            "9      \t [ 0.4027375   9.38447167 12.          0.94595612 15.          0.85310792]. \t  -0.451136347669006 \t -0.4484239383130805\n",
            "10     \t [ 1.3440527   8.97080429  5.          0.80277597 12.          0.63284111]. \t  -0.48349651102568225 \t -0.4484239383130805\n",
            "11     \t [ 7.94225687  3.68352192 13.          0.94902199 11.          0.95471054]. \t  \u001b[92m-0.422771965919715\u001b[0m \t -0.422771965919715\n",
            "12     \t [9.20332934 9.98167579 5.         0.57575366 6.         0.3717594 ]. \t  -0.6046860081027627 \t -0.422771965919715\n",
            "13     \t [5.68758712 2.62033511 9.         0.67614771 3.         0.67632008]. \t  -0.47052528509931824 \t -0.422771965919715\n",
            "14     \t [2.94143112e-03 7.03829535e-01 1.30000000e+01 5.98722168e-01\n",
            " 2.00000000e+00 1.01335355e-01]. \t  -0.7122743790062083 \t -0.422771965919715\n",
            "15     \t [ 0.81561387  9.31451412  5.          0.70594876 19.          0.98284163]. \t  -0.45988817082889427 \t -0.422771965919715\n",
            "16     \t [ 9.8968071   1.89203294 13.          0.7888373   2.          0.82539259]. \t  -0.45294282860990764 \t -0.422771965919715\n",
            "17     \t [ 8.37835592  2.52983026  5.          0.97806207 14.          0.6500905 ]. \t  -0.48098936825483635 \t -0.422771965919715\n",
            "18     \t [1.94243031 0.         5.         0.5        7.86661322 0.1       ]. \t  -0.7118264597599575 \t -0.422771965919715\n",
            "19     \t [ 0.         0.         8.2483413  0.5       14.2483413  0.1      ]. \t  -0.7120633980051373 \t -0.422771965919715\n",
            "20     \t [ 8.98906297  7.01351387 13.          0.77197065 19.          0.47265223]. \t  -0.475345282554151 \t -0.422771965919715\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.29564873555569"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_nP9lQjUztV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5c72cd7-7e0e-4fc9-fe4d-3c63ea994080"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 12\n",
        "\n",
        "np.random.seed(run_num_12)\n",
        "surrogate_loser_12 = dGaussianProcess(cov_func)\n",
        "\n",
        "X_train12, X_test12, y_train12, y_test12 = train_test_split(X, y, test_size=test_perc, random_state=run_num_12)\n",
        "\n",
        "def f_syn_polarity12(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_12, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train12, y=y_train12).mean())\n",
        "    return operator * score\n",
        "\n",
        "loser_12 = dGPGO(surrogate_loser_12, Acquisition_new(util_loser), f_syn_polarity12, param, n_jobs = -1) # define BayesOpt\n",
        "loser_12.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_loser_12 = loser_12.getResult()[0]\n",
        "params_loser_12['max_depth'] = int(params_loser_12['max_depth'])\n",
        "params_loser_12['min_child_weight'] = int(params_loser_12['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_loser_train12 = xgb.DMatrix(X_train12, y_train12)\n",
        "dX_loser_test12 = xgb.DMatrix(X_test12, y_test12)\n",
        "model_loser_12 = xgb.train(params_loser_12, dX_loser_train12)\n",
        "pred_loser_12 = model_loser_12.predict(dX_loser_test12)\n",
        "\n",
        "rmse_loser_12 = np.sqrt(mean_squared_error(pred_loser_12, y_test12))\n",
        "rmse_loser_12"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 1.9151945   6.22108771  9.          0.89267929 16.          0.87460279]. \t  -0.4420077321695894 \t -0.4420077321695894\n",
            "init   \t [ 1.50636966  1.98518759 11.          0.67890863 17.          0.5381501 ]. \t  -0.5998539491151165 \t -0.4420077321695894\n",
            "init   \t [ 3.31015428  8.02639569  7.          0.75154158 13.          0.11992952]. \t  -0.7324503600335561 \t -0.4420077321695894\n",
            "init   \t [ 2.90728553  2.46394443  5.          0.94461307 17.          0.43194161]. \t  -0.5995790338913064 \t -0.4420077321695894\n",
            "init   \t [ 9.33140102  6.51378143  8.          0.76907392 18.          0.38515251]. \t  -0.5982523495604211 \t -0.4420077321695894\n",
            "1      \t [5.68098653 8.6912739  6.         0.52027807 1.         0.51631894]. \t  -0.5999713561677879 \t -0.4420077321695894\n",
            "2      \t [ 4.61089026  7.78540446 14.          0.61560224  6.          0.60910898]. \t  -0.5310432089168232 \t -0.4420077321695894\n",
            "3      \t [ 7.71510638  1.71782806 11.          0.9748513   2.          0.33442931]. \t  -0.5954035506799399 \t -0.4420077321695894\n",
            "4      \t [ 0.42901773  1.02682351 10.          0.62582208  7.          0.40519792]. \t  -0.600033100066767 \t -0.4420077321695894\n",
            "5      \t [ 9.48743388  2.23858916 14.          0.84020064 13.          0.41361313]. \t  -0.5921503481987865 \t -0.4420077321695894\n",
            "6      \t [ 7.09099339  0.08964775  6.          0.68605854 11.          0.16899055]. \t  -0.7330665763811677 \t -0.4420077321695894\n",
            "7      \t [ 5.1543633   9.69004682 14.          0.74319768 17.          0.13447035]. \t  -0.7322160475319931 \t -0.4420077321695894\n",
            "8      \t [0.99238486 5.31706566 9.61992505 0.5        1.         0.1       ]. \t  -0.7336693179826522 \t -0.4420077321695894\n",
            "9      \t [0.  0.  5.  0.5 1.  0.1]. \t  -0.7334076855722197 \t -0.4420077321695894\n",
            "10     \t [ 9.28455241  7.99305791  9.          0.7151018  11.          0.47357317]. \t  -0.5981360899216244 \t -0.4420077321695894\n",
            "11     \t [0.28778391 6.56278858 6.         0.98668711 8.         0.21832977]. \t  -0.7330268911586586 \t -0.4420077321695894\n",
            "12     \t [8.71347715 2.52417421 5.         0.81450435 4.         0.19261726]. \t  -0.7322521890009479 \t -0.4420077321695894\n",
            "13     \t [ 8.76460845  0.13227845  6.          0.75995694 17.          0.20887306]. \t  -0.7329728586443053 \t -0.4420077321695894\n",
            "14     \t [ 1.31628683  1.0257918   5.          0.7820955  10.          0.83995059]. \t  -0.5114296694528737 \t -0.4420077321695894\n",
            "15     \t [ 1.07765263  6.20138171 14.          0.59898972 11.          0.65973368]. \t  -0.5325207293005967 \t -0.4420077321695894\n",
            "16     \t [6.28736248 5.81565128 5.         0.62918024 8.         0.71228537]. \t  -0.5479129205332877 \t -0.4420077321695894\n",
            "17     \t [ 8.95792842  8.74686238 12.          0.60340754  1.          0.64076061]. \t  -0.5341583767040248 \t -0.4420077321695894\n",
            "18     \t [ 5.72640258  1.49259688 14.          0.92448749  7.          0.73129851]. \t  -0.4929336813081906 \t -0.4420077321695894\n",
            "19     \t [ 0.60473619  9.7310361   5.          0.764657   19.          0.44899982]. \t  -0.602200593591361 \t -0.4420077321695894\n",
            "20     \t [ 7.25143535  1.86402411 14.          0.59310957 19.          0.53879904]. \t  -0.5987977809908415 \t -0.4420077321695894\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.301967512402236"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDI2Bi9vU05U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "844726c4-33da-4c5f-baca-ae12781f2721"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 13\n",
        "\n",
        "np.random.seed(run_num_13)\n",
        "surrogate_loser_13 = dGaussianProcess(cov_func)\n",
        "\n",
        "X_train13, X_test13, y_train13, y_test13 = train_test_split(X, y, test_size=test_perc, random_state=run_num_13)\n",
        "\n",
        "def f_syn_polarity13(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_13, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train13, y=y_train13).mean())\n",
        "    return operator * score\n",
        "\n",
        "loser_13 = dGPGO(surrogate_loser_13, Acquisition_new(util_loser), f_syn_polarity13, param, n_jobs = -1) # define BayesOpt\n",
        "loser_13.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_loser_13 = loser_13.getResult()[0]\n",
        "params_loser_13['max_depth'] = int(params_loser_13['max_depth'])\n",
        "params_loser_13['min_child_weight'] = int(params_loser_13['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_loser_train13 = xgb.DMatrix(X_train13, y_train13)\n",
        "dX_loser_test13 = xgb.DMatrix(X_test13, y_test13)\n",
        "model_loser_13 = xgb.train(params_loser_13, dX_loser_train13)\n",
        "pred_loser_13 = model_loser_13.predict(dX_loser_test13)\n",
        "\n",
        "rmse_loser_13 = np.sqrt(mean_squared_error(pred_loser_13, y_test13))\n",
        "rmse_loser_13"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 0.30967343  2.54282073  6.          0.9850871  17.          0.50848087]. \t  -0.5693403578256062 \t -0.4459810811310791\n",
            "init   \t [ 6.95119021  3.25593423 14.          0.8602375   8.          0.82055713]. \t  -0.5112217679187058 \t -0.4459810811310791\n",
            "init   \t [ 5.09252675  8.19921021 10.          0.78792002 16.          0.382158  ]. \t  -0.581693558072188 \t -0.4459810811310791\n",
            "init   \t [ 5.4465191   3.19841462 14.          0.90654581  5.          0.10359415]. \t  -0.6818818523119694 \t -0.4459810811310791\n",
            "init   \t [ 9.8393864   9.89619708  9.          0.63387763 10.          0.88979174]. \t  -0.4459810811310791 \t -0.4459810811310791\n",
            "1      \t [ 9.27118046  9.4344906  10.          0.71578295  1.          0.5463837 ]. \t  -0.5604994598662556 \t -0.4459810811310791\n",
            "2      \t [0.21910824 0.53051294 5.         0.62754372 4.         0.55299122]. \t  -0.578556985871312 \t -0.4459810811310791\n",
            "3      \t [0.32904954 9.80442032 8.         0.7285653  2.         0.94646442]. \t  \u001b[92m-0.43885135786496293\u001b[0m \t -0.43885135786496293\n",
            "4      \t [ 8.54553338  0.49573541 12.          0.81406546 19.          0.20999807]. \t  -0.6795516887212301 \t -0.43885135786496293\n",
            "5      \t [9.40145549 1.43629395 5.         0.63655396 1.         0.63002732]. \t  -0.5662893508870628 \t -0.43885135786496293\n",
            "6      \t [ 8.7238127   0.85665784  6.          0.75815556 12.          0.87180469]. \t  -0.4644522650882914 \t -0.43885135786496293\n",
            "7      \t [ 0.11400844  9.5774521   6.          0.90472821 11.          0.18513309]. \t  -0.6793968950126906 \t -0.43885135786496293\n",
            "8      \t [ 1.95513735  0.12334702 13.          0.54467307 14.          0.86324917]. \t  \u001b[92m-0.43794830902637205\u001b[0m \t -0.43794830902637205\n",
            "9      \t [ 2.89959158  8.10209185 14.          0.9094105   1.          0.86003598]. \t  \u001b[92m-0.42582758559630707\u001b[0m \t -0.42582758559630707\n",
            "10     \t [ 0.63533588  7.88973169 13.          0.71777205  8.          0.22258282]. \t  -0.6847473450330067 \t -0.42582758559630707\n",
            "11     \t [4.59853484 7.19189873 5.         0.92777191 6.         0.5760847 ]. \t  -0.5604008171822471 \t -0.42582758559630707\n",
            "12     \t [ 9.78022669  6.34740254  5.          0.81941991 15.          0.17866883]. \t  -0.6810440437950237 \t -0.42582758559630707\n",
            "13     \t [ 0.44924058  6.01817024 13.          0.62113095 19.          0.22629481]. \t  -0.6838258323776948 \t -0.42582758559630707\n",
            "14     \t [2.08063721 0.20431355 5.85195635 0.5        9.85195635 0.1       ]. \t  -0.6829528263861964 \t -0.42582758559630707\n",
            "15     \t [ 8.93459678  5.75013258 14.          0.86506851 14.          0.11026757]. \t  -0.6798789215383618 \t -0.42582758559630707\n",
            "16     \t [ 1.2199304   9.32862427  5.          0.98694875 18.          0.69275841]. \t  -0.5593626212561581 \t -0.42582758559630707\n",
            "17     \t [7.30733696 1.06961871 9.         0.6583612  6.         0.41488291]. \t  -0.5857440675365031 \t -0.42582758559630707\n",
            "18     \t [ 0.25964514  2.37009305 11.          0.85577938  4.          0.42985865]. \t  -0.556481072440868 \t -0.42582758559630707\n",
            "19     \t [ 5.54919942  8.87232522  5.          0.84416975 12.          0.31830005]. \t  -0.5998464916577737 \t -0.42582758559630707\n",
            "20     \t [ 9.03439965  9.39442441 14.          0.98025678 19.          0.55921955]. \t  -0.550861248380592 \t -0.42582758559630707\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.159065314760995"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z2F_Q194U3uu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad97dbf6-2623-4480-fa03-5ef0a355205e"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 14\n",
        "\n",
        "np.random.seed(run_num_14)\n",
        "surrogate_loser_14 = dGaussianProcess(cov_func)\n",
        "\n",
        "X_train14, X_test14, y_train14, y_test14 = train_test_split(X, y, test_size=test_perc, random_state=run_num_14)\n",
        "\n",
        "def f_syn_polarity14(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_14, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train14, y=y_train14).mean())\n",
        "    return operator * score\n",
        "\n",
        "loser_14 = dGPGO(surrogate_loser_14, Acquisition_new(util_loser), f_syn_polarity14, param, n_jobs = -1) # define BayesOpt\n",
        "loser_14.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_loser_14 = loser_14.getResult()[0]\n",
        "params_loser_14['max_depth'] = int(params_loser_14['max_depth'])\n",
        "params_loser_14['min_child_weight'] = int(params_loser_14['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_loser_train14 = xgb.DMatrix(X_train14, y_train14)\n",
        "dX_loser_test14 = xgb.DMatrix(X_test14, y_test14)\n",
        "model_loser_14 = xgb.train(params_loser_14, dX_loser_train14)\n",
        "pred_loser_14 = model_loser_14.predict(dX_loser_test14)\n",
        "\n",
        "rmse_loser_14 = np.sqrt(mean_squared_error(pred_loser_14, y_test14))\n",
        "rmse_loser_14"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 8.59560606  1.64569495 11.          0.89021195 15.          0.4857008 ]. \t  -0.6102441482204597 \t -0.4517949441804544\n",
            "init   \t [ 0.57460093  9.25007434  5.          0.56647642 17.          0.7533211 ]. \t  -0.5614002279921607 \t -0.4517949441804544\n",
            "init   \t [1.90674396 4.32019547 9.         0.93708225 3.         0.91775905]. \t  -0.4517949441804544 \t -0.4517949441804544\n",
            "init   \t [ 0.36770475  8.43368681 14.          0.6826844  15.          0.6291966 ]. \t  -0.5407924410004543 \t -0.4517949441804544\n",
            "init   \t [ 5.33840324  0.449182   14.          0.52045439  2.          0.48604556]. \t  -0.6242722768101235 \t -0.4517949441804544\n",
            "1      \t [ 9.54492219  9.07631824  8.          0.54579372 12.          0.38295996]. \t  -0.6262431675183743 \t -0.4517949441804544\n",
            "2      \t [9.53270461 0.83608685 6.         0.99976104 3.         0.60747265]. \t  -0.5533922160484371 \t -0.4517949441804544\n",
            "3      \t [ 1.15250646  0.51569303  5.          0.69322415 17.          0.16291682]. \t  -0.6917729046487754 \t -0.4517949441804544\n",
            "4      \t [ 6.37447116  9.98862868 14.          0.72259558  4.          0.61899393]. \t  -0.5434760859463281 \t -0.4517949441804544\n",
            "5      \t [ 8.08070558  7.65123431 14.          0.59104644 19.          0.9302005 ]. \t  -0.45873776977159153 \t -0.4517949441804544\n",
            "6      \t [0.         0.         5.         0.5        8.91797943 0.1       ]. \t  -0.6942441953461963 \t -0.4517949441804544\n",
            "7      \t [1.0907871  9.11150389 5.         0.58744161 9.         0.87439163]. \t  -0.4933476055445382 \t -0.4517949441804544\n",
            "8      \t [ 2.21919917  0.31865167 14.          0.54399684 14.          0.18056849]. \t  -0.6942842535038787 \t -0.4517949441804544\n",
            "9      \t [ 7.34435618  8.39108681  7.          0.81547994 19.          0.62767067]. \t  -0.546522525570801 \t -0.4517949441804544\n",
            "10     \t [ 6.57539862  4.15362663 14.          0.96053826  9.          0.2080482 ]. \t  -0.691194495047777 \t -0.4517949441804544\n",
            "11     \t [6.00529466 8.33461243 6.         0.97581801 4.         0.31582778]. \t  -0.6226835813168086 \t -0.4517949441804544\n",
            "12     \t [ 7.68230622  0.80909901  5.          0.61509794 11.          0.73230761]. \t  -0.5592301279615596 \t -0.4517949441804544\n",
            "13     \t [0.  0.  5.  0.5 1.  0.1]. \t  -0.6943016112587715 \t -0.4517949441804544\n",
            "14     \t [ 0.84209312  7.86749962 14.          0.8648448   1.          0.1570582 ]. \t  -0.6937301125121068 \t -0.4517949441804544\n",
            "15     \t [ 1.22436035  9.67689861 12.          0.7460835   9.          0.69284382]. \t  -0.5399272231860202 \t -0.4517949441804544\n",
            "16     \t [ 4.4386325   0.37972087 10.          0.61118217  8.          0.15198669]. \t  -0.6912722224579347 \t -0.4517949441804544\n",
            "17     \t [ 7.0496565   9.86456792 14.          0.62416475 11.          0.68843039]. \t  -0.5429606831910581 \t -0.4517949441804544\n",
            "18     \t [ 1.55101374  4.15023981  9.          0.60341671 12.          0.53785307]. \t  -0.6138556386215619 \t -0.4517949441804544\n",
            "19     \t [ 9.66203958  6.20819976 11.          0.71875858  5.          0.43480687]. \t  -0.6129974118264951 \t -0.4517949441804544\n",
            "20     \t [ 0.13684155  3.86104617 14.          0.82673561  9.          0.20283653]. \t  -0.6919355943160154 \t -0.4517949441804544\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.334202643456426"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Po5wImJaU6VC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "406e8c80-1079-4c6e-dd2c-0206b659bbfe"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 15\n",
        "\n",
        "np.random.seed(run_num_15)\n",
        "surrogate_loser_15 = dGaussianProcess(cov_func)\n",
        "\n",
        "X_train15, X_test15, y_train15, y_test15 = train_test_split(X, y, test_size=test_perc, random_state=run_num_15)\n",
        "\n",
        "def f_syn_polarity15(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_15, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train15, y=y_train15).mean())\n",
        "    return operator * score\n",
        "\n",
        "loser_15 = dGPGO(surrogate_loser_15, Acquisition_new(util_loser), f_syn_polarity15, param, n_jobs = -1) # define BayesOpt\n",
        "loser_15.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_loser_15 = loser_15.getResult()[0]\n",
        "params_loser_15['max_depth'] = int(params_loser_15['max_depth'])\n",
        "params_loser_15['min_child_weight'] = int(params_loser_15['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_loser_train15 = xgb.DMatrix(X_train15, y_train15)\n",
        "dX_loser_test15 = xgb.DMatrix(X_test15, y_test15)\n",
        "model_loser_15 = xgb.train(params_loser_15, dX_loser_train15)\n",
        "pred_loser_15 = model_loser_15.predict(dX_loser_test15)\n",
        "\n",
        "rmse_loser_15 = np.sqrt(mean_squared_error(pred_loser_15, y_test15))\n",
        "rmse_loser_15"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 5.64315424  8.97297685  6.          0.81493635 11.          0.33008564]. \t  -0.5946480910132583 \t -0.4392277798535851\n",
            "init   \t [9.51455383 9.60803032 5.         0.68485728 1.         0.30145081]. \t  -0.5968379804084254 \t -0.4392277798535851\n",
            "init   \t [ 8.11809889  6.32508536  7.          0.61766329 19.          0.51383159]. \t  -0.5092318398204285 \t -0.4392277798535851\n",
            "init   \t [ 7.28571251  5.36071389 10.          0.76240297 12.          0.12035683]. \t  -0.6346120183758519 \t -0.4392277798535851\n",
            "init   \t [ 6.80847768  3.59767194 14.          0.51365585 16.          0.89730673]. \t  -0.4392277798535851 \t -0.4392277798535851\n",
            "1      \t [ 2.28792065  8.58421878 12.          0.67551126  1.          0.37971264]. \t  -0.596161946051267 \t -0.4392277798535851\n",
            "2      \t [ 9.35473751  1.48895841 12.          0.55362474  4.          0.11106606]. \t  -0.6327865627642436 \t -0.4392277798535851\n",
            "3      \t [3.91572323 2.23174129 6.         0.50937407 1.         0.51336364]. \t  -0.5160141940687206 \t -0.4392277798535851\n",
            "4      \t [ 0.74184801  3.17830864 14.          0.90502664  8.          0.31107615]. \t  -0.5905073342805591 \t -0.4392277798535851\n",
            "5      \t [ 1.33182259  0.8125478   9.          0.94558338 18.          0.71777254]. \t  -0.4467705016632985 \t -0.4392277798535851\n",
            "6      \t [ 0.48538268  1.39730148  6.          0.8238236  11.          0.45741795]. \t  -0.5144696373796631 \t -0.4392277798535851\n",
            "7      \t [ 0.57885492  9.35143843  6.          0.69337529 17.          0.15510814]. \t  -0.6331051904120661 \t -0.4392277798535851\n",
            "8      \t [0.24960251 9.45540541 5.         0.62890215 3.         0.73551484]. \t  -0.47327438015853385 \t -0.4392277798535851\n",
            "9      \t [ 8.23428085  9.77240559 13.          0.85977854  8.          0.93876208]. \t  \u001b[92m-0.4281095031928478\u001b[0m \t -0.4281095031928478\n",
            "10     \t [ 1.35750397  9.83931966 14.          0.92698791 15.          0.2272101 ]. \t  -0.6341880539033447 \t -0.4281095031928478\n",
            "11     \t [ 9.85965179  9.04589828 13.          0.95564786 19.          0.1426314 ]. \t  -0.632071954329094 \t -0.4281095031928478\n",
            "12     \t [9.06175259 4.11518452 5.         0.87621161 6.         0.52300377]. \t  -0.5192107686497183 \t -0.4281095031928478\n",
            "13     \t [ 0.15053668  0.43786928 13.          0.62231098  1.          0.88145445]. \t  -0.43523183945633176 \t -0.4281095031928478\n",
            "14     \t [ 7.18775342  0.2062489   5.          0.98239776 17.          0.28043211]. \t  -0.634545784625454 \t -0.4281095031928478\n",
            "15     \t [0.10178544 7.49949902 9.         0.60008085 8.         0.35520416]. \t  -0.5902884239300654 \t -0.4281095031928478\n",
            "16     \t [0.24880199 0.21304322 8.24146387 0.5        5.24146387 0.1       ]. \t  -0.6326902614705249 \t -0.4281095031928478\n",
            "17     \t [ 8.4324691   8.27251391 11.          0.71354715  2.          0.54685464]. \t  -0.498601956775111 \t -0.4281095031928478\n",
            "18     \t [ 2.12400437  9.06769045 14.          0.70585245  9.          0.90846764]. \t  -0.4286407197787245 \t -0.4281095031928478\n",
            "19     \t [9.58390631 0.99765082 6.         0.72021045 1.         0.39555686]. \t  -0.5960083204162412 \t -0.4281095031928478\n",
            "20     \t [ 9.96011127  1.55235537 14.          0.63004724 11.          0.14483121]. \t  -0.6331081546428745 \t -0.4281095031928478\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.189953800077167"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HrAQN-pU9Qo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e64095c6-4980-46ff-dda9-ffabe43cfb7b"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 16\n",
        "\n",
        "np.random.seed(run_num_16)\n",
        "surrogate_loser_16 = dGaussianProcess(cov_func)\n",
        "\n",
        "X_train16, X_test16, y_train16, y_test16 = train_test_split(X, y, test_size=test_perc, random_state=run_num_16)\n",
        "\n",
        "def f_syn_polarity16(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_16, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train16, y=y_train16).mean())\n",
        "    return operator * score\n",
        "\n",
        "loser_16 = dGPGO(surrogate_loser_16, Acquisition_new(util_loser), f_syn_polarity16, param, n_jobs = -1) # define BayesOpt\n",
        "loser_16.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_loser_16 = loser_16.getResult()[0]\n",
        "params_loser_16['max_depth'] = int(params_loser_16['max_depth'])\n",
        "params_loser_16['min_child_weight'] = int(params_loser_16['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_loser_train16 = xgb.DMatrix(X_train16, y_train16)\n",
        "dX_loser_test16 = xgb.DMatrix(X_test16, y_test16)\n",
        "model_loser_16 = xgb.train(params_loser_16, dX_loser_train16)\n",
        "pred_loser_16 = model_loser_16.predict(dX_loser_test16)\n",
        "\n",
        "rmse_loser_16 = np.sqrt(mean_squared_error(pred_loser_16, y_test16))\n",
        "rmse_loser_16"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [8.63948332 3.16366002 7.         0.75395583 1.         0.56999262]. \t  -0.5713220948397237 \t -0.4331621293825035\n",
            "init   \t [ 3.3426621   9.77029134 14.          0.93751218 17.          0.80549151]. \t  -0.4331621293825035 \t -0.4331621293825035\n",
            "init   \t [ 3.36871073  4.8308288  13.          0.91984982 14.          0.48829226]. \t  -0.5561611494046279 \t -0.4331621293825035\n",
            "init   \t [ 8.84540736  1.59826081  8.          0.75772888 14.          0.29029227]. \t  -0.6835926126526815 \t -0.4331621293825035\n",
            "init   \t [ 7.74981136  8.34377636 13.          0.83182409 15.          0.19378622]. \t  -0.7677243885128009 \t -0.4331621293825035\n",
            "1      \t [ 0.60074712  1.1402995   5.          0.75926551 10.          0.30612858]. \t  -0.6844266759595645 \t -0.4331621293825035\n",
            "2      \t [ 3.6191694   8.20490707  5.          0.81280746 16.          0.99306688]. \t  -0.4581574120389309 \t -0.4331621293825035\n",
            "3      \t [ 0.55111138  3.69634376 11.          0.53864409  3.          0.92267876]. \t  -0.43468988385343615 \t -0.4331621293825035\n",
            "4      \t [9.52987381 8.92087345 5.         0.95441282 7.         0.10412548]. \t  -0.7679863416880384 \t -0.4331621293825035\n",
            "5      \t [ 8.31345174  8.15216774 14.          0.81052242  4.          0.5053732 ]. \t  -0.5586554806770512 \t -0.4331621293825035\n",
            "6      \t [ 7.65476591  1.74203379 12.          0.78837413  7.          0.71460419]. \t  -0.4382687724745528 \t -0.4331621293825035\n",
            "7      \t [1.68021993 9.25252958 9.         0.78643474 9.         0.55639556]. \t  -0.5630452959468858 \t -0.4331621293825035\n",
            "8      \t [0.  0.  5.  0.5 1.  0.1]. \t  -0.7670392049994392 \t -0.4331621293825035\n",
            "9      \t [ 2.6903245   0.3560009   7.          0.98255651 19.          0.9851534 ]. \t  -0.4412750880856537 \t -0.4331621293825035\n",
            "10     \t [ 0.4751313   0.18137204 13.          0.70186285 10.          0.37625565]. \t  -0.68582313055112 \t -0.4331621293825035\n",
            "11     \t [ 8.54895835  0.69961229 14.          0.57654595 19.          0.20454627]. \t  -0.7677953049378372 \t -0.4331621293825035\n",
            "12     \t [ 8.76905299  8.15213676  8.          0.5845007  19.          0.27314045]. \t  -0.7678874315811665 \t -0.4331621293825035\n",
            "13     \t [ 1.48533436  9.57311484 14.          0.8193199   3.          0.99329178]. \t  \u001b[92m-0.4185372839160708\u001b[0m \t -0.4185372839160708\n",
            "14     \t [4.49490325 9.22009337 8.         0.71314799 3.         0.58255672]. \t  -0.48560746523878195 \t -0.4185372839160708\n",
            "15     \t [ 0.76287087  0.49621071 13.          0.81150873 18.          0.76762748]. \t  -0.4363142295126112 \t -0.4185372839160708\n",
            "16     \t [0.56061777 6.48476685 5.         0.81826035 5.         0.59167243]. \t  -0.5183862929687044 \t -0.4185372839160708\n",
            "17     \t [8.8152352  0.5898958  5.         0.831707   9.         0.92137997]. \t  -0.45519808186832184 \t -0.4185372839160708\n",
            "18     \t [ 8.76208148  1.79224635  5.          0.52306305 19.          0.78632749]. \t  -0.4871095639687443 \t -0.4185372839160708\n",
            "19     \t [ 9.63332031  8.534049   10.          0.55556169 10.          0.35894848]. \t  -0.6847217417593005 \t -0.4185372839160708\n",
            "20     \t [ 4.52692506  0.27496659 10.          0.6345842   1.          0.46474283]. \t  -0.5631095705887621 \t -0.4185372839160708\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.361483761377183"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXelbcAVVCqO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ab1ab53-f3eb-4138-b175-2329be5d0c49"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 17\n",
        "\n",
        "np.random.seed(run_num_17)\n",
        "surrogate_loser_17 = dGaussianProcess(cov_func)\n",
        "\n",
        "X_train17, X_test17, y_train17, y_test17 = train_test_split(X, y, test_size=test_perc, random_state=run_num_17)\n",
        "\n",
        "def f_syn_polarity17(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_17, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train17, y=y_train17).mean())\n",
        "    return operator * score\n",
        "\n",
        "loser_17 = dGPGO(surrogate_loser_17, Acquisition_new(util_loser), f_syn_polarity17, param, n_jobs = -1) # define BayesOpt\n",
        "loser_17.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_loser_17 = loser_17.getResult()[0]\n",
        "params_loser_17['max_depth'] = int(params_loser_17['max_depth'])\n",
        "params_loser_17['min_child_weight'] = int(params_loser_17['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_loser_train17 = xgb.DMatrix(X_train17, y_train17)\n",
        "dX_loser_test17 = xgb.DMatrix(X_test17, y_test17)\n",
        "model_loser_17 = xgb.train(params_loser_17, dX_loser_train17)\n",
        "pred_loser_17 = model_loser_17.predict(dX_loser_test17)\n",
        "\n",
        "rmse_loser_17 = np.sqrt(mean_squared_error(pred_loser_17, y_test17))\n",
        "rmse_loser_17"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 1.85575275  3.86599303 13.          0.85645882  2.          0.78699813]. \t  -0.4597056260580034 \t -0.4597056260580034\n",
            "init   \t [1.97780831 2.92527676 8.         0.92268607 9.         0.39017299]. \t  -0.5687207024308397 \t -0.4597056260580034\n",
            "init   \t [ 7.16656007  1.028324    5.          0.96975483 15.          0.90932849]. \t  -0.4688025715067707 \t -0.4597056260580034\n",
            "init   \t [ 2.46007304  9.53596657  7.          0.71704714 11.          0.17659118]. \t  -0.6983036523024195 \t -0.4597056260580034\n",
            "init   \t [3.44161135 9.92427612 8.         0.96331165 2.         0.29160186]. \t  -0.5675253504862205 \t -0.4597056260580034\n",
            "1      \t [ 9.98828606  3.49693094 13.          0.57156882  8.          0.14604355]. \t  -0.6992595730796743 \t -0.4597056260580034\n",
            "2      \t [ 6.95619362  6.95953721 12.          0.92317057 19.          0.34981799]. \t  -0.5628121282495753 \t -0.4597056260580034\n",
            "3      \t [0.  0.  5.  0.5 1.  0.1]. \t  -0.6990710246949499 \t -0.4597056260580034\n",
            "4      \t [ 1.79054908  0.97767001 14.          0.72051649 17.          0.75055942]. \t  \u001b[92m-0.4586950539387324\u001b[0m \t -0.4586950539387324\n",
            "5      \t [7.85364213 1.7463908  7.         0.80931735 2.         0.57315987]. \t  -0.5442127017186587 \t -0.4586950539387324\n",
            "6      \t [ 1.77308987  6.92082513  5.          0.52059222 19.          0.18788647]. \t  -0.69931956875667 \t -0.4586950539387324\n",
            "7      \t [ 8.86839023  7.16332242  5.          0.6476276  11.          0.20780931]. \t  -0.6987603050812756 \t -0.4586950539387324\n",
            "8      \t [ 9.85681439  0.57206659 14.          0.79465027 19.          0.59553205]. \t  -0.534073864099878 \t -0.4586950539387324\n",
            "9      \t [ 0.90443942  7.22834774 14.          0.92580232 12.          0.13465218]. \t  -0.6965996897404226 \t -0.4586950539387324\n",
            "10     \t [ 9.20219266  7.71975834 14.          0.72531692  2.          0.43032755]. \t  -0.5354718257765292 \t -0.4586950539387324\n",
            "11     \t [ 7.87455923  9.60434668 13.          0.6097695  11.          0.18672661]. \t  -0.6988278637935389 \t -0.4586950539387324\n",
            "12     \t [ 1.83063475  0.7056051   5.          0.62427541 19.          0.60134826]. \t  -0.5515025691718822 \t -0.4586950539387324\n",
            "13     \t [ 1.78559475  9.94726594 14.          0.84124706  5.          0.41412983]. \t  -0.5643771257102934 \t -0.4586950539387324\n",
            "14     \t [ 7.45649622  0.49339183 14.          0.58756242 13.          0.5079427 ]. \t  -0.5393389988159936 \t -0.4586950539387324\n",
            "15     \t [9.52955352 8.15137211 8.         0.76863454 2.         0.57854032]. \t  -0.5395532171313935 \t -0.4586950539387324\n",
            "16     \t [ 1.13635919  9.59512209 12.          0.5730343  19.          0.36892674]. \t  -0.5697966935770927 \t -0.4586950539387324\n",
            "17     \t [6.77242326 0.         5.         0.5        9.09935527 0.1       ]. \t  -0.6997733788661372 \t -0.4586950539387324\n",
            "18     \t [ 5.89593673  0.5459184  14.          0.8691618   4.          0.72423884]. \t  \u001b[92m-0.45644440788193374\u001b[0m \t -0.45644440788193374\n",
            "19     \t [ 7.6390113   9.95449177  6.          0.57337797 19.          0.29431206]. \t  -0.5822507255377387 \t -0.45644440788193374\n",
            "20     \t [ 6.89682696  4.76907936 10.          0.67407985 14.          0.88827919]. \t  \u001b[92m-0.44318669556749074\u001b[0m \t -0.44318669556749074\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.0138942791645995"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJG2fAtAVFDZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5b40d42-7638-42cd-8987-845891ced517"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 18\n",
        "\n",
        "np.random.seed(run_num_18)\n",
        "surrogate_loser_18 = dGaussianProcess(cov_func)\n",
        "\n",
        "X_train18, X_test18, y_train18, y_test18 = train_test_split(X, y, test_size=test_perc, random_state=run_num_18)\n",
        "\n",
        "def f_syn_polarity18(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_11, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train18, y=y_train18).mean())\n",
        "    return operator * score\n",
        "\n",
        "loser_18 = dGPGO(surrogate_loser_18, Acquisition_new(util_loser), f_syn_polarity18, param, n_jobs = -1) # define BayesOpt\n",
        "loser_18.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_loser_18 = loser_18.getResult()[0]\n",
        "params_loser_18['max_depth'] = int(params_loser_18['max_depth'])\n",
        "params_loser_18['min_child_weight'] = int(params_loser_18['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_loser_train18 = xgb.DMatrix(X_train18, y_train18)\n",
        "dX_loser_test18 = xgb.DMatrix(X_test18, y_test18)\n",
        "model_loser_18 = xgb.train(params_loser_18, dX_loser_train18)\n",
        "pred_loser_18 = model_loser_18.predict(dX_loser_test18)\n",
        "\n",
        "rmse_loser_18 = np.sqrt(mean_squared_error(pred_loser_18, y_test18))\n",
        "rmse_loser_18"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 8.73429403  9.68540663 10.          0.68875849  9.          0.48011572]. \t  -0.48312028405718427 \t -0.4337207096533448\n",
            "init   \t [ 6.12033333  7.66062926  8.          0.76133734 13.          0.93379456]. \t  -0.44701743563076113 \t -0.4337207096533448\n",
            "init   \t [ 1.46524679  7.01527914  7.          0.90913299 10.          0.36016753]. \t  -0.6042122061654378 \t -0.4337207096533448\n",
            "init   \t [ 9.73855241  3.33774046 14.          0.53290419  7.          0.7088681 ]. \t  -0.47205818581725156 \t -0.4337207096533448\n",
            "init   \t [ 3.00618018  1.82702795 11.          0.75681389 14.          0.98627449]. \t  -0.4337207096533448 \t -0.4337207096533448\n",
            "1      \t [ 1.46586883  1.13810325 14.          0.90052794  5.          0.16182112]. \t  -0.7143993241707929 \t -0.4337207096533448\n",
            "2      \t [ 9.17850108  5.22481201 14.          0.60105853 18.          0.63442006]. \t  -0.47652759630987357 \t -0.4337207096533448\n",
            "3      \t [6.20612396 4.37552923 6.         0.71941082 3.         0.90943147]. \t  -0.4522652783180721 \t -0.4337207096533448\n",
            "4      \t [ 0.42107725  9.92938812 12.          0.92462675 18.          0.88352347]. \t  \u001b[92m-0.42696094495506925\u001b[0m \t -0.42696094495506925\n",
            "5      \t [ 1.45418057  8.89399    11.          0.93175096  1.          0.52391385]. \t  -0.4748883821619788 \t -0.42696094495506925\n",
            "6      \t [0.         0.         5.         0.5        5.86208378 0.1       ]. \t  -0.7141834012604253 \t -0.42696094495506925\n",
            "7      \t [ 8.67626106  0.1397848   6.          0.55771681 18.          0.89120888]. \t  -0.4678231542313444 \t -0.42696094495506925\n",
            "8      \t [ 7.20807715  7.41236348  5.          0.52425285 19.          0.24651851]. \t  -0.7151531754828971 \t -0.42696094495506925\n",
            "9      \t [7.94412856 1.24068931 7.         0.96649935 9.         0.47928692]. \t  -0.48764145519377255 \t -0.42696094495506925\n",
            "10     \t [ 2.66410938  9.83743919 13.          0.58899688  8.          0.29675269]. \t  -0.6087724199154481 \t -0.42696094495506925\n",
            "11     \t [ 8.62753755  4.44899956 12.          0.98231134  1.          0.88389967]. \t  \u001b[92m-0.42538744453232347\u001b[0m \t -0.42538744453232347\n",
            "12     \t [ 1.2277143   1.05411485  5.          0.50198686 13.          0.89258454]. \t  -0.4690426518871574 \t -0.42538744453232347\n",
            "13     \t [ 3.86727486  0.04582369 10.          0.54629234  1.          0.59406244]. \t  -0.47507759871416455 \t -0.42538744453232347\n",
            "14     \t [9.89654007 7.91459554 5.         0.64703423 6.         0.95875063]. \t  -0.4610204259667869 \t -0.42538744453232347\n",
            "15     \t [ 9.49381141  1.31696272 11.          0.51283555 13.          0.25364349]. \t  -0.7155727045133423 \t -0.42538744453232347\n",
            "16     \t [0.         3.84904456 5.70916624 0.5        1.         0.1       ]. \t  -0.7130688739294866 \t -0.42538744453232347\n",
            "17     \t [ 1.04974938  7.3117691   5.          0.60120794 17.          0.83587336]. \t  -0.49012043280719225 \t -0.42538744453232347\n",
            "18     \t [5.83203328 9.4812958  5.         0.60479193 1.         0.24995758]. \t  -0.7139401129279975 \t -0.42538744453232347\n",
            "19     \t [ 3.86883296  7.31704712 14.          0.75047101 14.          0.80828494]. \t  -0.4565506921595134 \t -0.42538744453232347\n",
            "20     \t [ 4.0233419   3.91847735 10.          0.87935081  7.          0.57035954]. \t  -0.47552190244029113 \t -0.42538744453232347\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3.937438334950126"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHidSEGcVHvG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1281434-99d1-4312-f81f-94bc09215c5b"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 19\n",
        "\n",
        "np.random.seed(run_num_19)\n",
        "surrogate_loser_19 = dGaussianProcess(cov_func)\n",
        "\n",
        "X_train19, X_test19, y_train19, y_test19 = train_test_split(X, y, test_size=test_perc, random_state=run_num_19)\n",
        "\n",
        "def f_syn_polarity19(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_19, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train19, y=y_train19).mean())\n",
        "    return operator * score\n",
        "\n",
        "loser_19 = dGPGO(surrogate_loser_19, Acquisition_new(util_loser), f_syn_polarity19, param, n_jobs = -1) # define BayesOpt\n",
        "loser_19.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_loser_19 = loser_19.getResult()[0]\n",
        "params_loser_19['max_depth'] = int(params_loser_19['max_depth'])\n",
        "params_loser_19['min_child_weight'] = int(params_loser_19['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_loser_train19 = xgb.DMatrix(X_train19, y_train19)\n",
        "dX_loser_test19 = xgb.DMatrix(X_test19, y_test19)\n",
        "model_loser_19 = xgb.train(params_loser_19, dX_loser_train19)\n",
        "pred_loser_19 = model_loser_19.predict(dX_loser_test19)\n",
        "\n",
        "rmse_loser_19 = np.sqrt(mean_squared_error(pred_loser_19, y_test19))\n",
        "rmse_loser_19"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 8.24520174  9.84626759 12.          0.81846905 12.          0.27594309]. \t  -0.6624515538441494 \t -0.4321567975765851\n",
            "init   \t [ 8.6021676   8.64300514 14.          0.95067208 18.          0.13363564]. \t  -0.663790961968593 \t -0.4321567975765851\n",
            "init   \t [ 3.02486657  6.06229509 10.          0.50494482  9.          0.10218675]. \t  -0.668875700942164 \t -0.4321567975765851\n",
            "init   \t [ 6.57586195  8.01231848  7.          0.63945407 18.          0.49206406]. \t  -0.5640092362987262 \t -0.4321567975765851\n",
            "init   \t [ 6.05008856  8.15558548 14.          0.64718943  8.          0.90611106]. \t  -0.4321567975765851 \t -0.4321567975765851\n",
            "1      \t [ 0.85655458  0.69294498  9.          0.90992265 19.          0.25605959]. \t  -0.6632879756016564 \t -0.4321567975765851\n",
            "2      \t [ 1.16239725  9.89254371 11.          0.98469188  1.          0.20271996]. \t  -0.6659068700895453 \t -0.4321567975765851\n",
            "3      \t [9.26425961 9.717621   5.         0.8856331  5.         0.65589573]. \t  -0.5256636565447839 \t -0.4321567975765851\n",
            "4      \t [8.38158931 1.39953008 5.         0.6836482  9.         0.46934447]. \t  -0.5657479189361881 \t -0.4321567975765851\n",
            "5      \t [ 9.89729152  2.88709408 11.          0.55121781  3.          0.58381748]. \t  -0.5217676759230677 \t -0.4321567975765851\n",
            "6      \t [0.  0.  5.  0.5 1.  0.1]. \t  -0.6682393310586452 \t -0.4321567975765851\n",
            "7      \t [ 0.52039992  0.56380532 14.          0.70324781  2.          0.77427978]. \t  -0.44119318049659856 \t -0.4321567975765851\n",
            "8      \t [ 9.45607821  0.96438893 14.          0.56821928 16.          0.48995458]. \t  -0.560546633196372 \t -0.4321567975765851\n",
            "9      \t [ 0.56432836  7.79494932 13.          0.55871992 19.          0.10266045]. \t  -0.6660467600439527 \t -0.4321567975765851\n",
            "10     \t [ 3.16676683  1.78770811 13.          0.5650927  13.          0.7417697 ]. \t  -0.4472490143246649 \t -0.4321567975765851\n",
            "11     \t [5.56939336 3.95670769 5.         0.95243816 3.         0.31115527]. \t  -0.6118696715045784 \t -0.4321567975765851\n",
            "12     \t [0.         0.         6.69414791 0.5        9.69414791 0.1       ]. \t  -0.6680142576361826 \t -0.4321567975765851\n",
            "13     \t [ 7.46605559  8.55652968 13.          0.99452706  1.          0.43764631]. \t  -0.5573240867828692 \t -0.4321567975765851\n",
            "14     \t [ 9.00192632  0.12142981  6.          0.72039779 17.          0.26695117]. \t  -0.6635472276398457 \t -0.4321567975765851\n",
            "15     \t [ 1.20113738  5.88016015  6.          0.57839075 15.          0.60752162]. \t  -0.5267223663659426 \t -0.4321567975765851\n",
            "16     \t [1.7958895  9.19371155 5.         0.54448974 5.         0.93868386]. \t  -0.4669880318809955 \t -0.4321567975765851\n",
            "17     \t [ 9.62487731  6.86253449  5.          0.66465947 12.          0.21107909]. \t  -0.6659019290689656 \t -0.4321567975765851\n",
            "18     \t [ 7.01475491  4.53933393 10.          0.82075044 14.          0.83058729]. \t  -0.4517750017757683 \t -0.4321567975765851\n",
            "19     \t [ 5.75277105  1.25995511 14.          0.97274397  8.          0.39753437]. \t  -0.6135363407765908 \t -0.4321567975765851\n",
            "20     \t [ 2.00664037  5.88682269 14.          0.94696775  4.          0.80981462]. \t  -0.4349577325456077 \t -0.4321567975765851\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.2307134915487135"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWGPYRJhVKsO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbf84c5f-7cc5-4eb6-d0a2-c13460dd0c39"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 20\n",
        "\n",
        "np.random.seed(run_num_20)\n",
        "surrogate_loser_20 = dGaussianProcess(cov_func)\n",
        "\n",
        "X_train20, X_test20, y_train20, y_test20 = train_test_split(X, y, test_size=test_perc, random_state=run_num_20)\n",
        "\n",
        "def f_syn_polarity20(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_20, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train20, y=y_train20).mean())\n",
        "    return operator * score\n",
        "\n",
        "loser_20 = dGPGO(surrogate_loser_20, Acquisition_new(util_loser), f_syn_polarity20, param, n_jobs = -1) # define BayesOpt\n",
        "loser_20.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_loser_20 = loser_20.getResult()[0]\n",
        "params_loser_20['max_depth'] = int(params_loser_20['max_depth'])\n",
        "params_loser_20['min_child_weight'] = int(params_loser_20['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_loser_train20 = xgb.DMatrix(X_train20, y_train20)\n",
        "dX_loser_test20 = xgb.DMatrix(X_test20, y_test20)\n",
        "model_loser_20 = xgb.train(params_loser_20, dX_loser_train20)\n",
        "pred_loser_20 = model_loser_20.predict(dX_loser_test20)\n",
        "\n",
        "rmse_loser_20 = np.sqrt(mean_squared_error(pred_loser_20, y_test20))\n",
        "rmse_loser_20"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 5.70517285  5.64528755 11.          0.51213999 19.          0.57883228]. \t  -0.5510634045248362 \t -0.4667910180825121\n",
            "init   \t [0.68106288 5.8452906  6.         0.58037829 8.         0.87974899]. \t  -0.4667910180825121 \t -0.4667910180825121\n",
            "init   \t [ 4.04121426  7.17971417 11.          0.57603584 10.          0.19954225]. \t  -0.6541702754539586 \t -0.4667910180825121\n",
            "init   \t [ 5.27556761  8.67655329 13.          0.54252273  4.          0.48570796]. \t  -0.5813476654605825 \t -0.4667910180825121\n",
            "init   \t [ 5.30003889  5.73946822 12.          0.52666698 16.          0.77467019]. \t  -0.5078849923055367 \t -0.4667910180825121\n",
            "1      \t [ 9.25190708  0.65043072 10.          0.56676233  5.          0.41289847]. \t  -0.6164858434894066 \t -0.4667910180825121\n",
            "2      \t [8.65689736 8.11684598 5.         0.53907072 1.         0.87574215]. \t  -0.47576906935129115 \t -0.4667910180825121\n",
            "3      \t [6.49728107e-01 7.72862043e-04 7.00000000e+00 5.71905117e-01\n",
            " 2.00000000e+00 1.02014940e-01]. \t  -0.6533990377801203 \t -0.4667910180825121\n",
            "4      \t [ 9.51806562  0.44900923  6.          0.62997227 16.          0.40028974]. \t  -0.6257141325107729 \t -0.4667910180825121\n",
            "5      \t [ 0.48808143  1.97030839  7.          0.79892377 16.          0.98477167]. \t  \u001b[92m-0.4602371335347518\u001b[0m \t -0.4602371335347518\n",
            "6      \t [ 9.98029391  8.23632437  6.          0.63606408 13.          0.22908429]. \t  -0.6532745897130707 \t -0.4602371335347518\n",
            "7      \t [ 3.9245879   0.87058505 14.          0.68269183  1.          0.34048922]. \t  -0.623283664424315 \t -0.4602371335347518\n",
            "8      \t [ 0.45468431  0.13267263 10.          0.667214    9.          0.47071141]. \t  -0.5807933165122651 \t -0.4602371335347518\n",
            "9      \t [ 4.64792903  9.65609988  6.          0.78696753 17.          0.90406502]. \t  -0.47227152594951016 \t -0.4602371335347518\n",
            "10     \t [ 9.73713042  9.77875019 14.          0.8552366  13.          0.6588042 ]. \t  -0.5450528828222597 \t -0.4602371335347518\n",
            "11     \t [ 9.74564452  1.02954501 14.          0.98964301 12.          0.47743205]. \t  -0.5773481317621234 \t -0.4602371335347518\n",
            "12     \t [ 7.76906477  2.6060444   5.          0.53491145 10.          0.41560504]. \t  -0.627993339841767 \t -0.4602371335347518\n",
            "13     \t [6.49958993 1.98622034 6.         0.83635209 1.         0.2931143 ]. \t  -0.6276302072456058 \t -0.4602371335347518\n",
            "14     \t [1.36051555 9.95957797 7.         0.9242116  2.         0.87097462]. \t  \u001b[92m-0.449167771194887\u001b[0m \t -0.449167771194887\n",
            "15     \t [ 0.20978943  8.45447688 14.          0.77351134 17.          0.2656195 ]. \t  -0.6586945947582249 \t -0.449167771194887\n",
            "16     \t [7.55121074 9.86496533 6.         0.88828535 7.         0.3097795 ]. \t  -0.6258969685789838 \t -0.449167771194887\n",
            "17     \t [ 1.88681738  3.46035586 14.          0.68070687  6.          0.9944693 ]. \t  \u001b[92m-0.42905917541157645\u001b[0m \t -0.42905917541157645\n",
            "18     \t [ 1.00455473  4.86452269 10.          0.65242434  1.          0.22603818]. \t  -0.6579989052523156 \t -0.42905917541157645\n",
            "19     \t [ 0.87144588  0.14777143 14.          0.88891044 13.          0.11582186]. \t  -0.6586044677275812 \t -0.42905917541157645\n",
            "20     \t [ 8.17732484  5.22697425 11.          0.61303379  1.          0.32672413]. \t  -0.6191817873624627 \t -0.42905917541157645\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.2900830531756915"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1d_1LyydIfe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1837e3f1-5075-4863-a94c-ac61bb10aec8"
      },
      "source": [
        "end_lose = time.time()\n",
        "end_lose\n",
        "\n",
        "time_lose = end_lose - start_lose\n",
        "time_lose\n",
        "\n",
        "start_win = time.time()\n",
        "start_win"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1613410098.1491902"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZAyOw7XYVwAf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58d96ab1-674f-4800-9923-0cd35a493e2f"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 1 \n",
        "\n",
        "np.random.seed(run_num_1)\n",
        "surrogate_winner_1 = dtStudentProcess(d_cov_func, nu = df)\n",
        "\n",
        "X_train1, X_test1, y_train1, y_test1 = train_test_split(X, y, test_size=test_perc, random_state=run_num_1)\n",
        "\n",
        "def f_syn_polarity1(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_1, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train1, y=y_train1).mean())\n",
        "    return operator * score\n",
        "\n",
        "winner_1 = dGPGO_stp(surrogate_winner_1, Acquisition_new(util_winner), f_syn_polarity1, param, n_jobs = -1) # Define BayesOpt\n",
        "winner_1.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_winner_1 = winner_1.getResult()[0]\n",
        "params_winner_1['max_depth'] = int(params_winner_1['max_depth'])\n",
        "params_winner_1['min_child_weight'] = int(params_winner_1['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_winner_train1 = xgb.DMatrix(X_train1, y_train1)\n",
        "dX_winner_test1 = xgb.DMatrix(X_test1, y_test1)\n",
        "model_winner_1 = xgb.train(params_winner_1, dX_winner_train1)\n",
        "pred_winner_1 = model_winner_1.predict(dX_winner_test1)\n",
        "\n",
        "rmse_winner_1 = np.sqrt(mean_squared_error(pred_winner_1, y_test1))\n",
        "rmse_winner_1"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 6.12170176  1.69069754 11.          0.92695323  3.          0.36579277]. \t  -0.5875926438135084 \t -0.45245711231879204\n",
            "init   \t [ 1.49162957  0.22478325  6.          0.85613654 15.          0.19363322]. \t  -0.6811472770543097 \t -0.45245711231879204\n",
            "init   \t [ 5.27957805  2.81819356 11.          0.81062146 14.          0.74826845]. \t  -0.46117277972616416 \t -0.45245711231879204\n",
            "init   \t [3.81060005 6.24236569 7.         0.95038709 5.         0.9861745 ]. \t  -0.45245711231879204 \t -0.45245711231879204\n",
            "init   \t [4.77531134 7.87990376 6.         0.78863691 6.         0.47918609]. \t  -0.5612893767574513 \t -0.45245711231879204\n",
            "1      \t [ 8.28495996  8.75496601  5.          0.95020974 18.          0.2862647 ]. \t  -0.5992477418063687 \t -0.45245711231879204\n",
            "2      \t [ 9.20334792 10.         15.          1.          8.89162955  1.        ]. \t  \u001b[92m-0.4228087190098053\u001b[0m \t -0.4228087190098053\n",
            "3      \t [ 0.4810761   9.31563532  8.          0.92845546 14.          0.70014982]. \t  -0.5056468669267833 \t -0.4228087190098053\n",
            "4      \t [ 9.85849625  0.83439907  7.          0.7705582  19.          0.15487995]. \t  -0.6815452883308839 \t -0.4228087190098053\n",
            "5      \t [10. 10. 15.  1. 20.  1.]. \t  -0.43194128453244485 \t -0.4228087190098053\n",
            "6      \t [ 5.73880714  9.81972246 13.          0.64554848  1.          0.32222329]. \t  -0.5944311123016245 \t -0.4228087190098053\n",
            "7      \t [7.19079936 0.87565629 5.         0.54470564 8.         0.4449021 ]. \t  -0.5736147518216788 \t -0.4228087190098053\n",
            "8      \t [ 1.22746497  1.93712297 14.          0.64861019 19.          0.28591653]. \t  -0.5920393184822867 \t -0.4228087190098053\n",
            "9      \t [ 1.7157918   5.89141512 14.          0.5296881  10.          0.1803042 ]. \t  -0.6825568503758982 \t -0.4228087190098053\n",
            "10     \t [ 2.68338077  8.90838203 14.          0.93255127 19.          0.31183316]. \t  -0.5868351024513 \t -0.4228087190098053\n",
            "11     \t [ 9.61234666  7.37157367  8.          0.74297257 11.          0.37046864]. \t  -0.5932122018757634 \t -0.4228087190098053\n",
            "12     \t [0.51753753 1.68511981 9.         0.80360039 9.         0.45200858]. \t  -0.5426731188677099 \t -0.4228087190098053\n",
            "13     \t [ 1.40203833  5.5343008  14.          0.98219868  3.          0.57566959]. \t  -0.4926728365998095 \t -0.4228087190098053\n",
            "14     \t [1.07854341 0.8489827  5.         0.60705022 4.         0.31148037]. \t  -0.5991411579830566 \t -0.4228087190098053\n",
            "15     \t [0.4939583  9.37390762 8.         0.7641092  1.         0.4917045 ]. \t  -0.5481894484993145 \t -0.4228087190098053\n",
            "16     \t [ 8.70209222  0.21363429 14.          0.98391211 18.          0.50653374]. \t  -0.5341942124085062 \t -0.4228087190098053\n",
            "17     \t [ 1.54696229  6.56164521  5.          0.89731423 19.          0.6022514 ]. \t  -0.5303528134937734 \t -0.4228087190098053\n",
            "18     \t [8.42183872 2.7124637  5.         0.83754722 2.         0.34952499]. \t  -0.599759594599271 \t -0.4228087190098053\n",
            "19     \t [ 8.91539388  7.01297237 14.          0.92866918 14.          0.42638658]. \t  -0.5873587847670173 \t -0.4228087190098053\n",
            "20     \t [ 9.43889426  2.79120084 13.          0.70777937  7.          0.12485816]. \t  -0.68010278922638 \t -0.4228087190098053\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.350022527863896"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hrDQbChpZ48F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d860acc1-cb22-4c3c-94c5-cb8801b6f251"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 2 \n",
        "\n",
        "np.random.seed(run_num_2)\n",
        "surrogate_winner_2 = dtStudentProcess(d_cov_func, nu = df)\n",
        "\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X, y, test_size=test_perc, random_state=run_num_2)\n",
        "\n",
        "def f_syn_polarity2(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_2, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train2, y=y_train2).mean())\n",
        "    return operator * score\n",
        "\n",
        "winner_2 = dGPGO_stp(surrogate_winner_2, Acquisition_new(util_winner), f_syn_polarity2, param, n_jobs = -1) # Define BayesOpt\n",
        "winner_2.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_winner_2 = winner_2.getResult()[0]\n",
        "params_winner_2['max_depth'] = int(params_winner_2['max_depth'])\n",
        "params_winner_2['min_child_weight'] = int(params_winner_2['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_winner_train2 = xgb.DMatrix(X_train2, y_train2)\n",
        "dX_winner_test2 = xgb.DMatrix(X_test2, y_test2)\n",
        "model_winner_2 = xgb.train(params_winner_2, dX_winner_train2)\n",
        "pred_winner_2 = model_winner_2.predict(dX_winner_test2)\n",
        "\n",
        "rmse_winner_2 = np.sqrt(mean_squared_error(pred_winner_2, y_test2))\n",
        "rmse_winner_2"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 8.51985493  0.73903599  7.          0.71824677 17.          0.61827209]. \t  -0.5233872966482411 \t -0.42848969661986275\n",
            "init   \t [ 8.4047092   4.35120551 12.          0.84231905 16.          0.80172484]. \t  -0.42848969661986275 \t -0.42848969661986275\n",
            "init   \t [ 6.42749368  9.61026168 10.          0.73363448 15.          0.84932207]. \t  -0.43419399571353007 \t -0.42848969661986275\n",
            "init   \t [ 2.66008363  8.36685389  6.          0.92082163 12.          0.69999264]. \t  -0.5353675801921922 \t -0.42848969661986275\n",
            "init   \t [ 1.74779478  2.17691512 12.          0.71794562  2.          0.48990646]. \t  -0.5346620615324694 \t -0.42848969661986275\n",
            "1      \t [0.65899411 0.08911785 5.         0.57513718 8.         0.6649358 ]. \t  -0.5435492302872315 \t -0.42848969661986275\n",
            "2      \t [7.50154303 9.49874736 7.         0.83842364 4.         0.5714185 ]. \t  -0.5422231033003287 \t -0.42848969661986275\n",
            "3      \t [9.88263606 1.71181353 7.         0.97275193 1.         0.74825108]. \t  -0.441747506473179 \t -0.42848969661986275\n",
            "4      \t [ 0.38425009  3.26044865 12.          0.89101602 19.          0.732816  ]. \t  \u001b[92m-0.4225097924337229\u001b[0m \t -0.4225097924337229\n",
            "5      \t [ 0.17098677  4.24340552 14.          0.58045842 10.          0.64319766]. \t  -0.5100599626019375 \t -0.4225097924337229\n",
            "6      \t [10. 10. 15.  1. 20.  1.]. \t  \u001b[92m-0.4092462984541993\u001b[0m \t -0.4092462984541993\n",
            "7      \t [ 8.95026207  5.07641093 12.          0.72801114  7.          0.88502118]. \t  -0.42293252450416274 \t -0.4092462984541993\n",
            "8      \t [ 5.83065813  7.71178418 13.          0.80523658  1.          0.13411915]. \t  -0.6867319410305901 \t -0.4092462984541993\n",
            "9      \t [ 2.26995028  7.26113474  5.          0.89244153 19.          0.72822627]. \t  -0.46107668663747675 \t -0.4092462984541993\n",
            "10     \t [0.15708054 7.33475875 5.         0.73448573 4.         0.78960732]. \t  -0.45554380802299876 \t -0.4092462984541993\n",
            "11     \t [ 7.24091666  2.95408097  5.          0.99315126 11.          0.45511329]. \t  -0.561729127666591 \t -0.4092462984541993\n",
            "12     \t [10.         10.         15.          1.         10.15397168  1.        ]. \t  \u001b[92m-0.40697986014038\u001b[0m \t -0.40697986014038\n",
            "13     \t [ 1.232075    0.38392574  6.          0.82892322 17.          0.89031195]. \t  -0.4492334705885589 \t -0.40697986014038\n",
            "14     \t [ 0.99039506  9.70962894 14.          0.53225067 15.          0.94095862]. \t  -0.43140168430232356 \t -0.40697986014038\n",
            "15     \t [ 1.41040227  9.95893488 11.          0.74362398  6.          0.81255241]. \t  -0.42504011485331744 \t -0.40697986014038\n",
            "16     \t [ 3.73257763  0.5989952  11.          0.5927604   8.          0.12999526]. \t  -0.6870220318894334 \t -0.40697986014038\n",
            "17     \t [ 7.48756016  0.13555104 14.          0.75269236 12.          0.41177876]. \t  -0.5678014017572689 \t -0.40697986014038\n",
            "18     \t [4.3103888  3.16672722 7.         0.53544567 4.         0.24969096]. \t  -0.6863394906633806 \t -0.40697986014038\n",
            "19     \t [ 8.06744935  9.40296538  6.          0.60147647 19.          0.17678037]. \t  -0.6864884735391246 \t -0.40697986014038\n",
            "20     \t [ 6.74476943  0.62418174 14.          0.53389504 19.          0.56154391]. \t  -0.5390022450760578 \t -0.40697986014038\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.334191828429232"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpUPyXRfZ95Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25830cc9-d7f9-4909-b95b-d520a6fc27c0"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 3 \n",
        "\n",
        "np.random.seed(run_num_3)\n",
        "surrogate_winner_3 = dtStudentProcess(d_cov_func, nu = df)\n",
        "\n",
        "X_train3, X_test3, y_train3, y_test3 = train_test_split(X, y, test_size=test_perc, random_state=run_num_3)\n",
        "\n",
        "def f_syn_polarity3(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_3, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train3, y=y_train3).mean())\n",
        "    return operator * score\n",
        "\n",
        "winner_3 = dGPGO_stp(surrogate_winner_3, Acquisition_new(util_winner), f_syn_polarity3, param, n_jobs = -1) # Define BayesOpt\n",
        "winner_3.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_winner_3 = winner_3.getResult()[0]\n",
        "params_winner_3['max_depth'] = int(params_winner_3['max_depth'])\n",
        "params_winner_3['min_child_weight'] = int(params_winner_3['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_winner_train3 = xgb.DMatrix(X_train3, y_train3)\n",
        "dX_winner_test3 = xgb.DMatrix(X_test3, y_test3)\n",
        "model_winner_3 = xgb.train(params_winner_3, dX_winner_train3)\n",
        "pred_winner_3 = model_winner_3.predict(dX_winner_test3)\n",
        "\n",
        "rmse_winner_3 = np.sqrt(mean_squared_error(pred_winner_3, y_test3))\n",
        "rmse_winner_3"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 7.51575607  1.09251275 13.          0.80377147 16.          0.31229624]. \t  -0.6858957949546655 \t -0.5081673732303724\n",
            "init   \t [6.12794891 1.19652403 5.         0.74822174 8.         0.33695838]. \t  -0.6849895205987827 \t -0.5081673732303724\n",
            "init   \t [ 6.21631223  4.4909976  10.          0.66764529  9.          0.77207323]. \t  -0.5081673732303724 \t -0.5081673732303724\n",
            "init   \t [ 0.58127233  4.79918477 11.          0.64526552 15.          0.62431773]. \t  -0.5106501747238601 \t -0.5081673732303724\n",
            "init   \t [ 5.76228258  0.67219724  7.          0.88228691 13.          0.29867219]. \t  -0.6839256266327581 \t -0.5081673732303724\n",
            "1      \t [9.79994489 9.57810833 8.         0.93763368 1.         0.1772549 ]. \t  -0.7316297465421655 \t -0.5081673732303724\n",
            "2      \t [ 8.09618702  9.87899793 14.          0.76477208 16.          0.92750484]. \t  \u001b[92m-0.4230067277420167\u001b[0m \t -0.4230067277420167\n",
            "3      \t [1.12049804 7.19863393 6.         0.79566554 3.         0.23444312]. \t  -0.730351254834763 \t -0.4230067277420167\n",
            "4      \t [ 0.38905261  9.83674845 14.          0.9364986   7.          0.11108644]. \t  -0.7312980300425612 \t -0.4230067277420167\n",
            "5      \t [ 6.29879634  0.65335492 12.          0.51090918  1.          0.59990728]. \t  -0.516414574482896 \t -0.4230067277420167\n",
            "6      \t [ 8.97980523  8.12851147  5.          0.67381125 19.          0.9346588 ]. \t  -0.46734051592589515 \t -0.4230067277420167\n",
            "7      \t [ 1.1995358   9.87930749  5.          0.84448568 18.          0.26022822]. \t  -0.7297224033867351 \t -0.4230067277420167\n",
            "8      \t [ 1.96525323  0.16048396 14.          0.65133015  9.          0.3003508 ]. \t  -0.6860931128073492 \t -0.4230067277420167\n",
            "9      \t [ 4.68364996  9.04822869 13.          0.61511374  1.          0.33064501]. \t  -0.6882356938464944 \t -0.4230067277420167\n",
            "10     \t [ 3.97093318  9.58939295  6.          0.5381492  12.          0.74760437]. \t  -0.5217629748454513 \t -0.4230067277420167\n",
            "11     \t [6.21132346 3.82637656 5.         0.86845687 1.         0.94056335]. \t  -0.4599738148031657 \t -0.4230067277420167\n",
            "12     \t [ 7.84223409  9.79539983 14.          0.56374986  7.          0.9106257 ]. \t  -0.43098974880612706 \t -0.4230067277420167\n",
            "13     \t [ 2.80079796  0.61761702  5.          0.62606225 19.          0.71334647]. \t  -0.5274647167409906 \t -0.4230067277420167\n",
            "14     \t [ 9.95414993  6.14621258  5.          0.623951   13.          0.8780972 ]. \t  -0.4686213887278212 \t -0.4230067277420167\n",
            "15     \t [ 9.39703158  0.81960902 14.          0.62547886 10.          0.15060928]. \t  -0.7316995199725552 \t -0.4230067277420167\n",
            "16     \t [ 1.84620574  0.32709184 13.          0.76394118 19.          0.90059117]. \t  \u001b[92m-0.42118028750387354\u001b[0m \t -0.42118028750387354\n",
            "17     \t [9.10557665 9.72122456 6.         0.65220947 8.         0.87970761]. \t  -0.45674457934706547 \t -0.42118028750387354\n",
            "18     \t [7.55452431e-03 9.44297730e+00 1.40000000e+01 9.48110213e-01\n",
            " 1.90000000e+01 5.05826889e-01]. \t  -0.6563162692297837 \t -0.42118028750387354\n",
            "19     \t [2.30278515 0.35786968 7.         0.96844433 4.         0.61825933]. \t  -0.514535704890589 \t -0.42118028750387354\n",
            "20     \t [ 0.02395946  0.93834865  5.          0.57112302 11.          0.52619361]. \t  -0.6527489525450815 \t -0.42118028750387354\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.721916790421568"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKX_nfEaaAwm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efec3c49-90f9-4663-cece-5de02694a2bb"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 4 \n",
        "\n",
        "np.random.seed(run_num_4)\n",
        "surrogate_winner_4 = dtStudentProcess(d_cov_func, nu = df)\n",
        "\n",
        "X_train4, X_test4, y_train4, y_test4 = train_test_split(X, y, test_size=test_perc, random_state=run_num_4)\n",
        "\n",
        "def f_syn_polarity4(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_4, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train4, y=y_train4).mean())\n",
        "    return operator * score\n",
        "\n",
        "winner_4 = dGPGO_stp(surrogate_winner_4, Acquisition_new(util_winner), f_syn_polarity4, param, n_jobs = -1) # Define BayesOpt\n",
        "winner_4.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_winner_4 = winner_4.getResult()[0]\n",
        "params_winner_4['max_depth'] = int(params_winner_4['max_depth'])\n",
        "params_winner_4['min_child_weight'] = int(params_winner_4['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_winner_train4 = xgb.DMatrix(X_train4, y_train4)\n",
        "dX_winner_test4 = xgb.DMatrix(X_test4, y_test4)\n",
        "model_winner_4 = xgb.train(params_winner_4, dX_winner_train4)\n",
        "pred_winner_4 = model_winner_4.predict(dX_winner_test4)\n",
        "\n",
        "rmse_winner_4 = np.sqrt(mean_squared_error(pred_winner_4, y_test4))\n",
        "rmse_winner_4"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 9.42798506  1.23319667 14.          0.81910755  7.          0.7021549 ]. \t  -0.5581090891563811 \t -0.5581090891563811\n",
            "init   \t [ 5.54158317  7.71233962 10.          0.59117644  6.          0.31476458]. \t  -0.6204181120564479 \t -0.5581090891563811\n",
            "init   \t [2.18282634 2.93794127 7.         0.84739327 5.         0.62716229]. \t  -0.5626656925564042 \t -0.5581090891563811\n",
            "init   \t [ 5.66159581  4.68678814 11.          0.54147808  9.          0.58609221]. \t  -0.5595885941852147 \t -0.5581090891563811\n",
            "init   \t [ 3.02160958  6.07432614  6.          0.92038559 14.          0.12778609]. \t  -0.6361772582153176 \t -0.5581090891563811\n",
            "1      \t [ 1.89460697  0.10324856 13.          0.70356349 16.          0.67776197]. \t  \u001b[92m-0.5568243993303088\u001b[0m \t -0.5568243993303088\n",
            "2      \t [ 9.69969     4.32607677 14.          0.87739158 16.          0.66014279]. \t  -0.5573098211239071 \t -0.5568243993303088\n",
            "3      \t [ 2.2210328   8.31054196 13.          0.70317243 19.          0.56953182]. \t  -0.573504425404327 \t -0.5568243993303088\n",
            "4      \t [ 0.14706294  0.75702217 14.          0.56309891  8.          0.29887971]. \t  -0.6254087880261816 \t -0.5568243993303088\n",
            "5      \t [ 6.94218243  0.32144048  6.          0.59286079 19.          0.70636825]. \t  -0.5706574487524134 \t -0.5568243993303088\n",
            "6      \t [ 9.24159749  9.43575685  7.          0.98380459 19.          0.34937346]. \t  -0.6187589419936593 \t -0.5568243993303088\n",
            "7      \t [7.57184449 0.0324327  9.         0.86318723 1.         0.6413689 ]. \t  -0.5611723012436254 \t -0.5568243993303088\n",
            "8      \t [ 1.62755426  6.7921099  14.          0.91903975  1.          0.61791343]. \t  -0.5680865264437001 \t -0.5568243993303088\n",
            "9      \t [ 9.43074486  0.98044049  5.          0.96290856 10.          0.27397556]. \t  -0.6390681992417737 \t -0.5568243993303088\n",
            "10     \t [10. 10. 15.  1. 20.  1.]. \t  \u001b[92m-0.42247189494363785\u001b[0m \t -0.42247189494363785\n",
            "11     \t [1.55248233 9.95757959 7.         0.54989079 2.         0.51763002]. \t  -0.582693852300668 \t -0.42247189494363785\n",
            "12     \t [8.91398595 7.00476796 6.         0.72156795 1.         0.41673037]. \t  -0.6210807206819127 \t -0.42247189494363785\n",
            "13     \t [10.        10.        15.         1.        11.4226546  1.       ]. \t  \u001b[92m-0.41780774758556855\u001b[0m \t -0.41780774758556855\n",
            "14     \t [ 9.87497198  6.68701692  6.          0.55980787 13.          0.32605223]. \t  -0.6200123103666009 \t -0.41780774758556855\n",
            "15     \t [ 9.8282182   6.06561373 14.          0.66125576  2.          0.46035506]. \t  -0.5775422657514083 \t -0.41780774758556855\n",
            "16     \t [ 3.24618617  0.72358282  6.          0.95248328 10.          0.4713404 ]. \t  -0.58635886876776 \t -0.41780774758556855\n",
            "17     \t [ 0.88735882  8.87463363 12.          0.57232639 10.          0.28510214]. \t  -0.6398573174482565 \t -0.41780774758556855\n",
            "18     \t [7.36355448 5.3077281  5.         0.5709887  7.         0.87569884]. \t  -0.467633753337906 \t -0.41780774758556855\n",
            "19     \t [ 7.93069008  0.11830338 11.          0.97110397 13.          0.22887984]. \t  -0.6356672520315936 \t -0.41780774758556855\n",
            "20     \t [ 1.52748847  0.19736792  5.          0.54225494 16.          0.82015445]. \t  -0.5008608278036402 \t -0.41780774758556855\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.299486064824159"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJmI9saAaEG1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a554314-cc2b-47ba-b516-74067abee5f9"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 5 \n",
        "\n",
        "np.random.seed(run_num_5)\n",
        "surrogate_winner_5 = dtStudentProcess(d_cov_func, nu = df)\n",
        "\n",
        "X_train5, X_test5, y_train5, y_test5 = train_test_split(X, y, test_size=test_perc, random_state=run_num_5)\n",
        "\n",
        "def f_syn_polarity5(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_5, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train5, y=y_train5).mean())\n",
        "    return operator * score\n",
        "\n",
        "winner_5 = dGPGO_stp(surrogate_winner_5, Acquisition_new(util_winner), f_syn_polarity5, param, n_jobs = -1) # Define BayesOpt\n",
        "winner_5.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_winner_5 = winner_5.getResult()[0]\n",
        "params_winner_5['max_depth'] = int(params_winner_5['max_depth'])\n",
        "params_winner_5['min_child_weight'] = int(params_winner_5['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_winner_train5 = xgb.DMatrix(X_train5, y_train5)\n",
        "dX_winner_test5 = xgb.DMatrix(X_test5, y_test5)\n",
        "model_winner_5 = xgb.train(params_winner_5, dX_winner_train5)\n",
        "pred_winner_5 = model_winner_5.predict(dX_winner_test5)\n",
        "\n",
        "rmse_winner_5 = np.sqrt(mean_squared_error(pred_winner_5, y_test5))\n",
        "rmse_winner_5"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 5.57353274  5.41760627  8.          0.53710825 19.          0.26034376]. \t  -0.6523797617249512 \t -0.466901759574539\n",
            "init   \t [5.17826437 9.78817462 5.         0.84048437 6.         0.79712615]. \t  -0.4886603987288415 \t -0.466901759574539\n",
            "init   \t [ 9.84476201  9.65484856  5.          0.59464714 19.          0.34528747]. \t  -0.564220001422124 \t -0.466901759574539\n",
            "init   \t [ 9.17476842  4.53262239 13.          0.64368565 10.          0.515308  ]. \t  -0.5049042831858346 \t -0.466901759574539\n",
            "init   \t [ 8.66719465  7.99685372 12.          0.79833571 17.          0.72443466]. \t  -0.466901759574539 \t -0.466901759574539\n",
            "1      \t [ 3.31480734  6.81602635 12.          0.95330947  1.          0.27334927]. \t  -0.6531167190113543 \t -0.466901759574539\n",
            "2      \t [2.53140694 0.29740057 9.         0.7505917  7.         0.82287404]. \t  -0.47475119282304395 \t -0.466901759574539\n",
            "3      \t [ 9.84728748  0.28222034 13.          0.93969018  3.          0.22339517]. \t  -0.6522247903981228 \t -0.466901759574539\n",
            "4      \t [ 0.68116737  3.94603776 14.          0.80380753 14.          0.77317996]. \t  \u001b[92m-0.4613270217842209\u001b[0m \t -0.4613270217842209\n",
            "5      \t [ 9.6928838   0.38736829  6.          0.56167539 11.          0.4775038 ]. \t  -0.5239028011959734 \t -0.4613270217842209\n",
            "6      \t [ 1.62699639  9.0444625   7.          0.60549869 13.          0.13190699]. \t  -0.6526723866694732 \t -0.4613270217842209\n",
            "7      \t [ 7.31108123  0.10391064 14.          0.53552151 15.          0.19462546]. \t  -0.6531977730432595 \t -0.4613270217842209\n",
            "8      \t [6.05825818 3.15263959 6.         0.61034148 2.         0.17375174]. \t  -0.6517203832791882 \t -0.4613270217842209\n",
            "9      \t [ 1.05461775  1.17661042  5.          0.5648338  17.          0.90649901]. \t  -0.4702337390488589 \t -0.4613270217842209\n",
            "10     \t [ 0.29281635  9.83144868 11.          0.57517626 19.          0.13846438]. \t  -0.6543975191191339 \t -0.4613270217842209\n",
            "11     \t [ 5.65874855  9.21132898 12.          0.83730466  7.          0.74405843]. \t  -0.4656120007342167 \t -0.4613270217842209\n",
            "12     \t [0.57140531 6.2823043  5.         0.70475062 2.         0.3328708 ]. \t  -0.5594485185267623 \t -0.4613270217842209\n",
            "13     \t [ 8.98585505  0.98183268  5.          0.79383712 19.          0.37621609]. \t  -0.561222767289322 \t -0.4613270217842209\n",
            "14     \t [0.0858834  4.38043443 5.         0.57114514 9.         0.11102852]. \t  -0.6531285980012218 \t -0.4613270217842209\n",
            "15     \t [ 9.08851157  7.7443441   6.          0.74314397 12.          0.52508293]. \t  -0.5238407378055661 \t -0.4613270217842209\n",
            "16     \t [ 0.62266475  1.9018111  14.          0.63329124  3.          0.94054341]. \t  \u001b[92m-0.42529491633232686\u001b[0m \t -0.42529491633232686\n",
            "17     \t [ 9.23494676  7.58606552 12.          0.78487568  2.          0.96478853]. \t  -0.42939677763278594 \t -0.42529491633232686\n",
            "18     \t [10.         10.         15.          1.         11.08920293  1.        ]. \t  \u001b[92m-0.3984474514638114\u001b[0m \t -0.3984474514638114\n",
            "19     \t [9.45154406 4.72237798 8.         0.64828056 7.         0.51309131]. \t  -0.5126682572696766 \t -0.3984474514638114\n",
            "20     \t [ 3.67612744  5.42599858 14.          0.93140269 19.          0.80853077]. \t  -0.4626498801811607 \t -0.3984474514638114\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.185955330442662"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulhEolsxaG4k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4bd460f2-ee32-4993-8c7a-0205644a4bfd"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 6 \n",
        "\n",
        "np.random.seed(run_num_6)\n",
        "surrogate_winner_6 = dtStudentProcess(d_cov_func, nu = df)\n",
        "\n",
        "X_train6, X_test6, y_train6, y_test6 = train_test_split(X, y, test_size=test_perc, random_state=run_num_6)\n",
        "\n",
        "def f_syn_polarity6(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=int(min_child_weight),\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_6, objective = 'reg:squarederror', eval_metric = 'rmse')\n",
        "    score = np.array(cross_val_score(reg, X=X_train6, y=y_train6).mean())\n",
        "    return operator * score\n",
        "\n",
        "winner_6 = dGPGO_stp(surrogate_winner_6, Acquisition_new(util_winner), f_syn_polarity6, param, n_jobs = -1) # Define BayesOpt\n",
        "winner_6.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_winner_6 = winner_6.getResult()[0]\n",
        "params_winner_6['max_depth'] = int(params_winner_6['max_depth'])\n",
        "params_winner_6['min_child_weight'] = int(params_winner_6['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_winner_train6 = xgb.DMatrix(X_train6, y_train6)\n",
        "dX_winner_test6 = xgb.DMatrix(X_test6, y_test6)\n",
        "model_winner_6 = xgb.train(params_winner_6, dX_winner_train6)\n",
        "pred_winner_6 = model_winner_6.predict(dX_winner_test6)\n",
        "\n",
        "rmse_winner_6 = np.sqrt(mean_squared_error(pred_winner_6, y_test6))\n",
        "rmse_winner_6"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [8.92860151 3.31979805 5.         0.99251441 2.         0.57683563]. \t  -0.5471181110640053 \t -0.5219867521874206\n",
            "init   \t [4.18807429 3.35407849 9.         0.87750649 3.         0.56623277]. \t  -0.629286701398377 \t -0.5219867521874206\n",
            "init   \t [ 5.788586    6.45355096 14.          0.70660047 12.          0.82154882]. \t  -0.5219867521874206 \t -0.5219867521874206\n",
            "init   \t [4.58184578 6.73834679 5.         0.90108528 3.         0.65482895]. \t  -0.5476166257785141 \t -0.5219867521874206\n",
            "init   \t [ 4.42510505  5.75952352 14.          0.97882365 15.          0.29525604]. \t  -0.6655867879161768 \t -0.5219867521874206\n",
            "1      \t [ 7.09628606  0.82458553  6.          0.52978945 17.          0.78827589]. \t  -0.5368551114917748 \t -0.5219867521874206\n",
            "2      \t [ 1.88885215  8.62524025  6.          0.90916563 14.          0.40379086]. \t  -0.6680072912825885 \t -0.5219867521874206\n",
            "3      \t [ 0.5654966   9.52584762 14.          0.82016688  4.          0.10717254]. \t  -0.7389555129670606 \t -0.5219867521874206\n",
            "4      \t [ 9.63033822  0.03793409 13.          0.68076302  7.          0.91391681]. \t  \u001b[92m-0.4301920669254681\u001b[0m \t -0.4301920669254681\n",
            "5      \t [ 9.97469028  9.15579642 12.          0.66745021 19.          0.28959158]. \t  -0.6668248751122167 \t -0.4301920669254681\n",
            "6      \t [ 9.85880187  8.48475629 13.          0.76006171  2.          0.58751173]. \t  -0.5403277210407594 \t -0.4301920669254681\n",
            "7      \t [ 9.51844     9.62786679  5.          0.56989798 15.          0.51068013]. \t  -0.6345288616058514 \t -0.4301920669254681\n",
            "8      \t [ 0.15151883  3.04875397  5.          0.61495161 19.          0.79420245]. \t  -0.5357132607573185 \t -0.4301920669254681\n",
            "9      \t [ 3.08077333  0.89569015  6.          0.84149914 11.          0.94992477]. \t  -0.4472711154368675 \t -0.4301920669254681\n",
            "10     \t [ 0.19816257  1.95434851 14.          0.59292238  7.          0.17971262]. \t  -0.7383261448869979 \t -0.4301920669254681\n",
            "11     \t [ 0.56219975  9.42928268 11.          0.51903033 19.          0.52214278]. \t  -0.6326946461691906 \t -0.4301920669254681\n",
            "12     \t [9.58176304 9.94093363 6.         0.96280567 8.         0.32855082]. \t  -0.6677341620025121 \t -0.4301920669254681\n",
            "13     \t [ 5.43877488  0.17504551 13.          0.58721411 19.          0.60115325]. \t  -0.5401916158723044 \t -0.4301920669254681\n",
            "14     \t [ 0.47139237  0.24747681 14.          0.53291734 14.          0.24490793]. \t  -0.7380982118151482 \t -0.4301920669254681\n",
            "15     \t [9.18163606 3.89469538 8.         0.59772622 8.         0.18901946]. \t  -0.7380580836797348 \t -0.4301920669254681\n",
            "16     \t [0.52725495 9.82259272 6.         0.62080595 7.         0.68770749]. \t  -0.5497290569747493 \t -0.4301920669254681\n",
            "17     \t [ 6.71573436  6.18076517  8.          0.50736725 18.          0.99692636]. \t  -0.4548470613295324 \t -0.4301920669254681\n",
            "18     \t [ 7.23381367  0.21465732 13.          0.75484136 13.          0.44717401]. \t  -0.6306144837329356 \t -0.4301920669254681\n",
            "19     \t [ 0.17525712  4.19183624 13.          0.93093846  1.          0.77496282]. \t  -0.5254340278392874 \t -0.4301920669254681\n",
            "20     \t [ 0.40153964  8.1115111  14.          0.63943096 11.          0.91959984]. \t  \u001b[92m-0.4282877219858168\u001b[0m \t -0.4282877219858168\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.151672275228334"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYebx3RVaJ1w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9299205b-c5ac-46c4-d606-2b9f55eb95a1"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 7 \n",
        "\n",
        "np.random.seed(run_num_7)\n",
        "surrogate_winner_7 = dtStudentProcess(d_cov_func, nu = df)\n",
        "\n",
        "X_train7, X_test7, y_train7, y_test7 = train_test_split(X, y, test_size=test_perc, random_state=run_num_7)\n",
        "\n",
        "def f_syn_polarity7(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_7, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train7, y=y_train7).mean())\n",
        "    return operator * score\n",
        "\n",
        "winner_7 = dGPGO_stp(surrogate_winner_7, Acquisition_new(util_winner), f_syn_polarity7, param, n_jobs = -1) # Define BayesOpt\n",
        "winner_7.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_winner_7 = winner_7.getResult()[0]\n",
        "params_winner_7['max_depth'] = int(params_winner_7['max_depth'])\n",
        "params_winner_7['min_child_weight'] = int(params_winner_7['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_winner_train7 = xgb.DMatrix(X_train7, y_train7)\n",
        "dX_winner_test7 = xgb.DMatrix(X_test7, y_test7)\n",
        "model_winner_7 = xgb.train(params_winner_7, dX_winner_train7)\n",
        "pred_winner_7 = model_winner_7.predict(dX_winner_test7)\n",
        "\n",
        "rmse_winner_7 = np.sqrt(mean_squared_error(pred_winner_7, y_test7))\n",
        "rmse_winner_7"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 8.11505126  6.92324143 13.          0.94046175 12.          0.3650384 ]. \t  -0.612852517751432 \t -0.4284992540085738\n",
            "init   \t [ 5.15357029  9.0066636   6.          0.74104227 15.          0.8966337 ]. \t  -0.45500673865987984 \t -0.4284992540085738\n",
            "init   \t [ 2.63548913  9.6610934  14.          0.64712408  1.          0.80820948]. \t  -0.43730487111474525 \t -0.4284992540085738\n",
            "init   \t [ 1.40821426  1.72758589 12.          0.70495069  5.          0.86632715]. \t  -0.4284992540085738 \t -0.4284992540085738\n",
            "init   \t [2.85126987 2.73434054 7.         0.88394733 8.         0.65545939]. \t  -0.48253390492139586 \t -0.4284992540085738\n",
            "1      \t [1.03697999e-02 2.51695028e+00 1.40000000e+01 6.19497248e-01\n",
            " 1.90000000e+01 7.01827417e-01]. \t  -0.47758423778274467 \t -0.4284992540085738\n",
            "2      \t [9.9089961  7.50235155 7.         0.67925503 5.         0.2471192 ]. \t  -0.6736712023667494 \t -0.4284992540085738\n",
            "3      \t [ 7.21196079  0.11384945  8.          0.67755731 17.          0.74323626]. \t  -0.4647919397201027 \t -0.4284992540085738\n",
            "4      \t [0.35283087 9.2950614  5.         0.93054416 3.         0.27625531]. \t  -0.670508947897148 \t -0.4284992540085738\n",
            "5      \t [ 0.19352731  8.09930152 12.          0.94917595  9.          0.75994611]. \t  -0.4324409379892192 \t -0.4284992540085738\n",
            "6      \t [9.76759527 0.48761621 7.         0.96362191 1.         0.137157  ]. \t  -0.6715547907624335 \t -0.4284992540085738\n",
            "7      \t [ 8.53387117  0.35946342 13.          0.65332712  6.          0.68302293]. \t  -0.4712802345283902 \t -0.4284992540085738\n",
            "8      \t [ 0.45155781  2.33209808  5.          0.73660734 17.          0.83890491]. \t  -0.4804814871680856 \t -0.4284992540085738\n",
            "9      \t [10.         10.         15.          1.         19.63472119  1.        ]. \t  \u001b[92m-0.41715223091475917\u001b[0m \t -0.41715223091475917\n",
            "10     \t [ 9.21287749  3.65759887  5.          0.85687073 12.          0.88459516]. \t  -0.45867733996321497 \t -0.41715223091475917\n",
            "11     \t [ 9.61655408  3.12016195 13.          0.74848363 18.          0.37867603]. \t  -0.6145824134186163 \t -0.41715223091475917\n",
            "12     \t [ 7.8364028   6.92382441 14.          0.64439058  5.          0.16362829]. \t  -0.6733895956434546 \t -0.41715223091475917\n",
            "13     \t [ 0.28968499  1.98768694 14.          0.97996073 12.          0.15615797]. \t  -0.6725851052423111 \t -0.41715223091475917\n",
            "14     \t [5.4721942  4.70341961 9.         0.675658   1.         0.91263074]. \t  -0.44024324260844283 \t -0.41715223091475917\n",
            "15     \t [ 0.08229595  9.56473656 14.          0.58820989 18.          0.78710652]. \t  -0.4450538726450125 \t -0.41715223091475917\n",
            "16     \t [4.54036829 9.23672573 7.         0.78071654 7.         0.925943  ]. \t  -0.44422074369565967 \t -0.41715223091475917\n",
            "17     \t [ 9.52566208  5.8884151   6.          0.78376025 19.          0.40148749]. \t  -0.6258103990077785 \t -0.41715223091475917\n",
            "18     \t [ 3.62239804  5.58545969 12.          0.9655744  16.          0.14680769]. \t  -0.6715538120326373 \t -0.41715223091475917\n",
            "19     \t [ 5.41500927 10.         10.3172085   1.         19.3172085   1.        ]. \t  -0.4269019351536672 \t -0.41715223091475917\n",
            "20     \t [0.61689986 3.10950502 5.         0.69212439 2.         0.89132788]. \t  -0.455614405666575 \t -0.41715223091475917\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.322061389518115"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xk0IPTSTbIl3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7bcb0517-d58a-4813-ceb4-91b9da4f3331"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 8 \n",
        "\n",
        "np.random.seed(run_num_8)\n",
        "surrogate_winner_8 = dtStudentProcess(d_cov_func, nu = df)\n",
        "\n",
        "X_train8, X_test8, y_train8, y_test8 = train_test_split(X, y, test_size=test_perc, random_state=run_num_8)\n",
        "\n",
        "def f_syn_polarity8(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_8, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train8, y=y_train8).mean())\n",
        "    return operator * score\n",
        "\n",
        "winner_8 = dGPGO_stp(surrogate_winner_8, Acquisition_new(util_winner), f_syn_polarity8, param, n_jobs = -1) # Define BayesOpt\n",
        "winner_8.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_winner_8 = winner_8.getResult()[0]\n",
        "params_winner_8['max_depth'] = int(params_winner_8['max_depth'])\n",
        "params_winner_8['min_child_weight'] = int(params_winner_8['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_winner_train8 = xgb.DMatrix(X_train8, y_train8)\n",
        "dX_winner_test8 = xgb.DMatrix(X_test8, y_test8)\n",
        "model_winner_8 = xgb.train(params_winner_8, dX_winner_train8)\n",
        "pred_winner_8 = model_winner_8.predict(dX_winner_test8)\n",
        "\n",
        "rmse_winner_8 = np.sqrt(mean_squared_error(pred_winner_8, y_test8))\n",
        "rmse_winner_8"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 1.29221489  0.77396898 11.          0.56062468  3.          0.25451879]. \t  -0.6231794271723858 \t -0.4674926150198253\n",
            "init   \t [ 0.10611429  1.37648061  8.          0.6050375  15.          0.790205  ]. \t  -0.4674926150198253 \t -0.4674926150198253\n",
            "init   \t [ 8.35239571  4.16908043  7.          0.67902746 14.          0.16683749]. \t  -0.6176439249709439 \t -0.4674926150198253\n",
            "init   \t [ 6.06301777  7.27127491 12.          0.87295531 11.          0.37694382]. \t  -0.5951520828762356 \t -0.4674926150198253\n",
            "init   \t [8.85209637 7.89269955 6.         0.72444854 3.         0.47766986]. \t  -0.5688519156580504 \t -0.4674926150198253\n",
            "1      \t [ 1.47825581  9.16010046 11.          0.96606546  1.          0.92358292]. \t  \u001b[92m-0.43518063190798095\u001b[0m \t -0.43518063190798095\n",
            "2      \t [ 7.52273592  2.30141271 14.          0.75505047 19.          0.77804046]. \t  -0.45158957529537797 \t -0.43518063190798095\n",
            "3      \t [1.64420908 9.47353882 5.         0.79993338 7.         0.15673599]. \t  -0.6175838614360663 \t -0.43518063190798095\n",
            "4      \t [ 1.02399475  9.65113789  9.          0.53947972 17.          0.8292164 ]. \t  -0.4719352875928771 \t -0.43518063190798095\n",
            "5      \t [8.14212583 0.61314553 6.         0.86488725 6.         0.28574491]. \t  -0.5954481959734638 \t -0.43518063190798095\n",
            "6      \t [10. 10. 15.  1. 20.  1.]. \t  \u001b[92m-0.4294116218364737\u001b[0m \t -0.4294116218364737\n",
            "7      \t [ 8.89360526  2.1140719  14.          0.72842074  5.          0.8531299 ]. \t  -0.4410569328948747 \t -0.4294116218364737\n",
            "8      \t [ 3.67598904  0.33397948 13.          0.67802127 12.          0.26996506]. \t  -0.6182367076254358 \t -0.4294116218364737\n",
            "9      \t [2.67676373 3.95611485 5.         0.79739163 2.         0.89987835]. \t  -0.45674643004535687 \t -0.4294116218364737\n",
            "10     \t [ 0.62854908  4.77207773 14.          0.78182185 18.          0.73284936]. \t  -0.4461225555243119 \t -0.4294116218364737\n",
            "11     \t [ 9.2410685   7.18065382 13.          0.79376037  1.          0.89955829]. \t  -0.43589894991964684 \t -0.4294116218364737\n",
            "12     \t [2.36382825 1.46562105 7.         0.95741638 8.         0.88392976]. \t  -0.44589385792483893 \t -0.4294116218364737\n",
            "13     \t [ 5.04566499  2.23244325  6.          0.98899894 19.          0.3759382 ]. \t  -0.5938655513158236 \t -0.4294116218364737\n",
            "14     \t [ 7.14005926  9.92191207 10.          0.67024483 18.          0.75079055]. \t  -0.46424037774667515 \t -0.4294116218364737\n",
            "15     \t [ 0.81390458  8.42248516 12.          0.78729971  7.          0.71328057]. \t  -0.5176932464043367 \t -0.4294116218364737\n",
            "16     \t [ 9.54383706  1.9311562  14.          0.97343577 12.          0.77846223]. \t  -0.4406155942586145 \t -0.4294116218364737\n",
            "17     \t [ 3.94960632  4.45719214 14.          0.97265653  1.          0.25596646]. \t  -0.6186725847674387 \t -0.4294116218364737\n",
            "18     \t [10.         10.         15.          1.         13.20266019  1.        ]. \t  \u001b[92m-0.4241615830586962\u001b[0m \t -0.4241615830586962\n",
            "19     \t [ 4.79284559  7.56138678  5.          0.91110861 17.          0.56819156]. \t  -0.5722633281332518 \t -0.4241615830586962\n",
            "20     \t [ 0.49912916  8.65454214 14.          0.6041034  13.          0.78070099]. \t  -0.4490076307787912 \t -0.4241615830586962\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.350880990767371"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9UroEj_RbLSb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9f5e42f-70a8-4499-ae95-4c6b4715ee27"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 9 \n",
        "\n",
        "np.random.seed(run_num_9)\n",
        "surrogate_winner_9 = dtStudentProcess(d_cov_func, nu = df)\n",
        "\n",
        "X_train9, X_test9, y_train9, y_test9 = train_test_split(X, y, test_size=test_perc, random_state=run_num_9)\n",
        "\n",
        "def f_syn_polarity9(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_9, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train9, y=y_train9).mean())\n",
        "    return operator * score\n",
        "\n",
        "winner_9 = dGPGO_stp(surrogate_winner_9, Acquisition_new(util_winner), f_syn_polarity9, param, n_jobs = -1) # Define BayesOpt\n",
        "winner_9.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_winner_9 = winner_9.getResult()[0]\n",
        "params_winner_9['max_depth'] = int(params_winner_9['max_depth'])\n",
        "params_winner_9['min_child_weight'] = int(params_winner_9['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_winner_train9 = xgb.DMatrix(X_train9, y_train9)\n",
        "dX_winner_test9 = xgb.DMatrix(X_test9, y_test9)\n",
        "model_winner_9 = xgb.train(params_winner_9, dX_winner_train9)\n",
        "pred_winner_9 = model_winner_9.predict(dX_winner_test9)\n",
        "\n",
        "rmse_winner_9 = np.sqrt(mean_squared_error(pred_winner_9, y_test9))\n",
        "rmse_winner_9"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 8.0342804   5.27522296 13.          0.60741904 17.          0.15336222]. \t  -0.6890593508433319 \t -0.4886355541929722\n",
            "init   \t [7.56063644 1.79876369 7.         0.84869647 5.         0.55956942]. \t  -0.5532305414240943 \t -0.4886355541929722\n",
            "init   \t [2.43675539 0.20225128 6.         0.51669963 7.         0.4646446 ]. \t  -0.5609155518755553 \t -0.4886355541929722\n",
            "init   \t [ 7.6043547   4.73758376  9.          0.87564624 15.          0.65751286]. \t  -0.4886355541929722 \t -0.4886355541929722\n",
            "init   \t [ 4.70349314  6.71029586  6.          0.97680389 11.          0.41971652]. \t  -0.6548660487332898 \t -0.4886355541929722\n",
            "1      \t [ 0.20910387  7.25545521 14.          0.59407892  3.          0.95410586]. \t  \u001b[92m-0.4312356520914486\u001b[0m \t -0.4312356520914486\n",
            "2      \t [ 2.14892068  9.25577985 14.          0.84828472 11.          0.2793841 ]. \t  -0.6883959263608518 \t -0.4312356520914486\n",
            "3      \t [ 0.37305557  1.02756836 13.          0.57348052 13.          0.68410192]. \t  -0.4836872799099695 \t -0.4312356520914486\n",
            "4      \t [ 7.98252507  8.04957695 12.          0.6367811   3.          0.85775382]. \t  -0.4368210491589304 \t -0.4312356520914486\n",
            "5      \t [ 1.2588195   0.04438454  7.          0.70342864 18.          0.58554745]. \t  -0.4951281025154505 \t -0.4312356520914486\n",
            "6      \t [ 9.17710587  9.71573287  5.          0.9584376  19.          0.78072441]. \t  -0.4682275209845145 \t -0.4312356520914486\n",
            "7      \t [6.11416909 9.54959201 5.         0.91943284 1.         0.10968146]. \t  -0.6901756354808697 \t -0.4312356520914486\n",
            "8      \t [ 0.31270412  9.94818438  8.          0.84397658 19.          0.43974503]. \t  -0.5483333357303245 \t -0.4312356520914486\n",
            "9      \t [ 8.76504058  0.46155195 14.          0.61459938 10.          0.19095511]. \t  -0.6902913672662703 \t -0.4312356520914486\n",
            "10     \t [ 0.77001066  0.48822065 12.          0.89190754  5.          0.37116504]. \t  -0.6599680642282422 \t -0.4312356520914486\n",
            "11     \t [ 0.76412088  5.02527282 14.          0.53794557 19.          0.6779987 ]. \t  -0.4862552398836468 \t -0.4312356520914486\n",
            "12     \t [1.11781249 4.65858905 8.         0.79271199 1.         0.71342306]. \t  -0.4899393997261333 \t -0.4312356520914486\n",
            "13     \t [ 4.60583171  0.06752    13.          0.94036906 18.          0.58293565]. \t  -0.47784115580365294 \t -0.4312356520914486\n",
            "14     \t [6.09359952e-03 8.65908473e+00 6.00000000e+00 6.67169539e-01\n",
            " 6.00000000e+00 4.18236274e-01]. \t  -0.6570878867409317 \t -0.4312356520914486\n",
            "15     \t [ 7.69267031  0.52709121  5.          0.97712342 11.          0.47111142]. \t  -0.5671609865381374 \t -0.4312356520914486\n",
            "16     \t [9.94406911 8.10389604 7.         0.8844164  7.         0.60247845]. \t  -0.4916963401121463 \t -0.4312356520914486\n",
            "17     \t [10.         10.         15.          1.         11.67284541  1.        ]. \t  \u001b[92m-0.41138486175626204\u001b[0m \t -0.41138486175626204\n",
            "18     \t [10. 10. 15.  1. 20.  1.]. \t  -0.4130968171998596 \t -0.41138486175626204\n",
            "19     \t [ 4.49239805  5.09496001 15.          1.          7.00448012  1.        ]. \t  \u001b[92m-0.4072189554503979\u001b[0m \t -0.4072189554503979\n",
            "20     \t [ 5.24998729  5.27861916  5.          0.87944065 19.          0.38196712]. \t  -0.656916604582956 \t -0.4072189554503979\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.062815633546083"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7VgaJOoJbOIE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ac82e93-fe97-4ba3-e52f-f5d2f19b122a"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 10 \n",
        "\n",
        "np.random.seed(run_num_10)\n",
        "surrogate_winner_10 = dtStudentProcess(d_cov_func, nu = df)\n",
        "\n",
        "X_train10, X_test10, y_train10, y_test10 = train_test_split(X, y, test_size=test_perc, random_state=run_num_10)\n",
        "\n",
        "def f_syn_polarity10(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_10, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train10, y=y_train10).mean())\n",
        "    return operator * score\n",
        "\n",
        "winner_10 = dGPGO_stp(surrogate_winner_10, Acquisition_new(util_winner), f_syn_polarity10, param, n_jobs = -1) # Define BayesOpt\n",
        "winner_10.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_winner_10 = winner_10.getResult()[0]\n",
        "params_winner_10['max_depth'] = int(params_winner_10['max_depth'])\n",
        "params_winner_10['min_child_weight'] = int(params_winner_10['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_winner_train10 = xgb.DMatrix(X_train10, y_train10)\n",
        "dX_winner_test10 = xgb.DMatrix(X_test10, y_test10)\n",
        "model_winner_10 = xgb.train(params_winner_10, dX_winner_train10)\n",
        "pred_winner_10 = model_winner_10.predict(dX_winner_test10)\n",
        "\n",
        "rmse_winner_10 = np.sqrt(mean_squared_error(pred_winner_10, y_test10))\n",
        "rmse_winner_10"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 6.53589585  1.15006943  6.          0.93623727 14.          0.13663866]. \t  -0.6929852048256443 \t -0.4483221221388197\n",
            "init   \t [ 3.97194461  2.33132197 14.          0.9533253  19.          0.26403087]. \t  -0.6932702114662359 \t -0.4483221221388197\n",
            "init   \t [ 7.43539415  0.69582081  9.          0.9763222  11.          0.12608349]. \t  -0.6931242497450955 \t -0.4483221221388197\n",
            "init   \t [ 9.82027485  3.39637684  9.          0.78522537 19.          0.87618843]. \t  -0.4483221221388197 \t -0.4483221221388197\n",
            "init   \t [ 0.57576207  5.82646405 12.          0.77704362 18.          0.11865044]. \t  -0.6946096327074635 \t -0.4483221221388197\n",
            "1      \t [ 3.17878299  6.40666671 10.          0.71928645  2.          0.37920433]. \t  -0.6059745513602156 \t -0.4483221221388197\n",
            "2      \t [ 3.37448201  8.10780464 13.          0.50477079 10.          0.914384  ]. \t  \u001b[92m-0.43629727149692393\u001b[0m \t -0.43629727149692393\n",
            "3      \t [10. 10. 15.  1. 20.  1.]. \t  \u001b[92m-0.4187162481836483\u001b[0m \t -0.4187162481836483\n",
            "4      \t [8.35411693 8.91554079 5.         0.73130633 7.         0.67857606]. \t  -0.5771647264709212 \t -0.4187162481836483\n",
            "5      \t [9.65375532 1.28029677 7.         0.79386133 1.         0.88548903]. \t  -0.44750949856169625 \t -0.4187162481836483\n",
            "6      \t [ 5.33344337  9.63007578  7.          0.55704064 18.          0.25018677]. \t  -0.6934805730034364 \t -0.4187162481836483\n",
            "7      \t [ 9.463998    9.23285451 14.          0.59406232  3.          0.52567113]. \t  -0.580720745359468 \t -0.4187162481836483\n",
            "8      \t [ 0.72612997  5.71374346  5.          0.7962252  12.          0.3276676 ]. \t  -0.6111050009653851 \t -0.4187162481836483\n",
            "9      \t [ 0.03680135  0.50694581 14.          0.91124751  8.          0.87026982]. \t  -0.4215933675272626 \t -0.4187162481836483\n",
            "10     \t [0.6894011  0.55915825 5.         0.80570877 2.         0.73174198]. \t  -0.5275474548655611 \t -0.4187162481836483\n",
            "11     \t [ 6.39500048  0.03099442 14.          0.5358351   2.          0.80194525]. \t  -0.5166569861309996 \t -0.4187162481836483\n",
            "12     \t [ 8.27988725  8.34955489 14.          0.91794427 14.          0.9969463 ]. \t  -0.427119317510358 \t -0.4187162481836483\n",
            "13     \t [0.50440288 9.75457432 5.         0.8137431  6.         0.15250773]. \t  -0.6941630128319229 \t -0.4187162481836483\n",
            "14     \t [ 9.17801514  5.33609724 14.          0.8363229   8.          0.98255322]. \t  -0.425397647901391 \t -0.4187162481836483\n",
            "15     \t [ 1.45697712  1.01618206  9.          0.54406919 12.          0.6612956 ]. \t  -0.5763826681592955 \t -0.4187162481836483\n",
            "16     \t [ 9.02754061  6.35570829  7.          0.96617525 12.          0.89651924]. \t  -0.44515621171268266 \t -0.4187162481836483\n",
            "17     \t [7.64536232 8.37707893 5.         0.8688534  1.         0.76688319]. \t  -0.5294614889177678 \t -0.4187162481836483\n",
            "18     \t [ 0.3061182   0.40074511 11.          0.73952333  2.          0.55304308]. \t  -0.5825889814267183 \t -0.4187162481836483\n",
            "19     \t [ 8.52646717  0.12245704 14.          0.80155281 15.          0.7100538 ]. \t  -0.5709119331337676 \t -0.4187162481836483\n",
            "20     \t [5.51569281 2.48923068 5.         0.97468437 6.         0.88188042]. \t  -0.4524796762835521 \t -0.4187162481836483\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.518394209029499"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51z87uHWbRGr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3dd221bf-9a02-4c17-c20b-f93c341c743d"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 11 \n",
        "\n",
        "np.random.seed(run_num_11)\n",
        "surrogate_winner_11 = dtStudentProcess(d_cov_func, nu = df)\n",
        "\n",
        "X_train11, X_test11, y_train11, y_test11 = train_test_split(X, y, test_size=test_perc, random_state=run_num_11)\n",
        "\n",
        "def f_syn_polarity11(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_11, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train11, y=y_train11).mean())\n",
        "    return operator * score\n",
        "\n",
        "winner_11 = dGPGO_stp(surrogate_winner_11, Acquisition_new(util_winner), f_syn_polarity11, param, n_jobs = -1) # Define BayesOpt\n",
        "winner_11.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_winner_11 = winner_11.getResult()[0]\n",
        "params_winner_11['max_depth'] = int(params_winner_11['max_depth'])\n",
        "params_winner_11['min_child_weight'] = int(params_winner_11['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_winner_train11 = xgb.DMatrix(X_train11, y_train11)\n",
        "dX_winner_test11 = xgb.DMatrix(X_test11, y_test11)\n",
        "model_winner_11 = xgb.train(params_winner_11, dX_winner_train11)\n",
        "pred_winner_11 = model_winner_11.predict(dX_winner_test11)\n",
        "\n",
        "rmse_winner_11 = np.sqrt(mean_squared_error(pred_winner_11, y_test11))\n",
        "rmse_winner_11"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 9.81035663  8.2124785   9.          0.68487173 15.          0.28207988]. \t  -0.7133787013189323 \t -0.47317045732943896\n",
            "init   \t [ 4.91906771  1.38558332  6.          0.618878   19.          0.10263204]. \t  -0.7112023662943636 \t -0.47317045732943896\n",
            "init   \t [ 5.23426175  1.89971159 13.          0.76313779 17.          0.46597853]. \t  -0.47317045732943896 \t -0.47317045732943896\n",
            "init   \t [ 4.0204056   3.75167577  9.          0.86204818 17.          0.26560056]. \t  -0.7132057194891379 \t -0.47317045732943896\n",
            "init   \t [8.8704459  5.64917815 5.         0.51406171 1.         0.82826731]. \t  -0.48048685248123163 \t -0.47317045732943896\n",
            "1      \t [0.08349406 1.89250335 6.         0.66386636 1.         0.88641723]. \t  \u001b[92m-0.4484239383130805\u001b[0m \t -0.4484239383130805\n",
            "2      \t [ 5.35837408  0.31510164 14.          0.97519815  6.          0.3034681 ]. \t  -0.607905230502132 \t -0.4484239383130805\n",
            "3      \t [ 4.66315557  9.30076713 11.          0.83534598  8.          0.21303816]. \t  -0.7135282416096518 \t -0.4484239383130805\n",
            "4      \t [9.02046973 1.16865322 7.         0.6807515  8.         0.2962635 ]. \t  -0.6068266615066733 \t -0.4484239383130805\n",
            "5      \t [0.79988529 9.85424348 5.         0.5743044  3.         0.67901433]. \t  -0.48209943859810334 \t -0.4484239383130805\n",
            "6      \t [ 0.02057794  7.14089873 13.          0.90855163  1.          0.61823707]. \t  -0.45805892074172583 \t -0.4484239383130805\n",
            "7      \t [ 0.97603689  3.49871064  9.          0.91846837 10.          0.35158251]. \t  -0.6085109103402477 \t -0.4484239383130805\n",
            "8      \t [ 9.27240029  9.57413463 14.          0.58544186  3.          0.73779027]. \t  -0.45267877796490286 \t -0.4484239383130805\n",
            "9      \t [10. 10. 15.  1. 20.  1.]. \t  \u001b[92m-0.41338167185309727\u001b[0m \t -0.41338167185309727\n",
            "10     \t [ 1.32444499  9.05736905 13.          0.98472613 14.          0.38435845]. \t  -0.609928547757409 \t -0.41338167185309727\n",
            "11     \t [ 7.94225687  3.68352192 13.          0.94902199 11.          0.95471054]. \t  -0.422771965919715 \t -0.41338167185309727\n",
            "12     \t [ 3.54989702  8.22459405  5.          0.88913852 14.          0.94693594]. \t  -0.4552688780746853 \t -0.41338167185309727\n",
            "13     \t [8.31268495 9.91059357 5.         0.56157362 8.         0.25141967]. \t  -0.7119859457220532 \t -0.41338167185309727\n",
            "14     \t [2.94143112e-03 7.03829535e-01 1.30000000e+01 5.98722168e-01\n",
            " 2.00000000e+00 1.01335355e-01]. \t  -0.7122743790062083 \t -0.41338167185309727\n",
            "15     \t [ 0.30621463  8.51475454  8.          0.50000134 18.          0.29148602]. \t  -0.6051050541602242 \t -0.41338167185309727\n",
            "16     \t [ 9.8968071   1.89203294 13.          0.7888373   2.          0.82539259]. \t  -0.45294282860990764 \t -0.41338167185309727\n",
            "17     \t [ 8.37835592  2.52983026  5.          0.97806207 14.          0.6500905 ]. \t  -0.48098936825483635 \t -0.41338167185309727\n",
            "18     \t [10.         10.         15.          1.         12.45110301  1.        ]. \t  \u001b[92m-0.4061197165621551\u001b[0m \t -0.4061197165621551\n",
            "19     \t [3.46089449 0.93150791 5.         0.7650872  8.         0.73086442]. \t  -0.47774620815037083 \t -0.4061197165621551\n",
            "20     \t [3.61978968 6.1067744  8.         0.7197336  1.         0.67044941]. \t  -0.4714124633271707 \t -0.4061197165621551\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.272012302762555"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8jZUeoWbTvn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe3d289b-7c12-4985-c9b9-58bf62f6252b"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 12\n",
        "\n",
        "np.random.seed(run_num_12)\n",
        "surrogate_winner_12 = dtStudentProcess(d_cov_func, nu = df)\n",
        "\n",
        "X_train12, X_test12, y_train12, y_test12 = train_test_split(X, y, test_size=test_perc, random_state=run_num_12)\n",
        "\n",
        "def f_syn_polarity12(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_12, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train12, y=y_train12).mean())\n",
        "    return operator * score\n",
        "\n",
        "winner_12 = dGPGO_stp(surrogate_winner_12, Acquisition_new(util_winner), f_syn_polarity12, param, n_jobs = -1) # Define BayesOpt\n",
        "winner_12.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_winner_12 = winner_12.getResult()[0]\n",
        "params_winner_12['max_depth'] = int(params_winner_12['max_depth'])\n",
        "params_winner_12['min_child_weight'] = int(params_winner_12['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_winner_train12 = xgb.DMatrix(X_train12, y_train12)\n",
        "dX_winner_test12 = xgb.DMatrix(X_test12, y_test12)\n",
        "model_winner_12 = xgb.train(params_winner_12, dX_winner_train12)\n",
        "pred_winner_12 = model_winner_12.predict(dX_winner_test12)\n",
        "\n",
        "rmse_winner_12 = np.sqrt(mean_squared_error(pred_winner_12, y_test12))\n",
        "rmse_winner_12"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 1.9151945   6.22108771  9.          0.89267929 16.          0.87460279]. \t  -0.4420077321695894 \t -0.4420077321695894\n",
            "init   \t [ 1.50636966  1.98518759 11.          0.67890863 17.          0.5381501 ]. \t  -0.5998539491151165 \t -0.4420077321695894\n",
            "init   \t [ 3.31015428  8.02639569  7.          0.75154158 13.          0.11992952]. \t  -0.7324503600335561 \t -0.4420077321695894\n",
            "init   \t [ 2.90728553  2.46394443  5.          0.94461307 17.          0.43194161]. \t  -0.5995790338913064 \t -0.4420077321695894\n",
            "init   \t [ 9.33140102  6.51378143  8.          0.76907392 18.          0.38515251]. \t  -0.5982523495604211 \t -0.4420077321695894\n",
            "1      \t [5.68098653 8.6912739  6.         0.52027807 1.         0.51631894]. \t  -0.5999713561677879 \t -0.4420077321695894\n",
            "2      \t [ 4.61089026  7.78540446 14.          0.61560224  6.          0.60910898]. \t  -0.5310432089168232 \t -0.4420077321695894\n",
            "3      \t [9.12195032 0.87787126 9.         0.87292886 5.         0.2622847 ]. \t  -0.7326988083055951 \t -0.4420077321695894\n",
            "4      \t [ 0.42901773  1.02682351 10.          0.62582208  7.          0.40519792]. \t  -0.600033100066767 \t -0.4420077321695894\n",
            "5      \t [ 9.48743388  2.23858916 14.          0.84020064 13.          0.41361313]. \t  -0.5921503481987865 \t -0.4420077321695894\n",
            "6      \t [ 6.85696089  9.7830661  14.          0.97897908 13.          0.89524334]. \t  \u001b[92m-0.43162480213383303\u001b[0m \t -0.43162480213383303\n",
            "7      \t [ 7.4487954   0.25501799  6.          0.59353564 11.          0.79320372]. \t  -0.5151049017617342 \t -0.43162480213383303\n",
            "8      \t [10. 10. 15.  1. 20.  1.]. \t  \u001b[92m-0.4187495885724168\u001b[0m \t -0.4187495885724168\n",
            "9      \t [3.58522218 1.88383687 9.         0.7776694  1.         0.4642546 ]. \t  -0.6003337797165678 \t -0.4187495885724168\n",
            "10     \t [9.69324843 9.42428289 7.         0.6384364  6.         0.83589131]. \t  -0.509004724458299 \t -0.4187495885724168\n",
            "11     \t [ 8.88364352  5.38800449 14.          0.86748322  1.          0.95098406]. \t  -0.43530550198971935 \t -0.4187495885724168\n",
            "12     \t [ 6.90090899  5.34717256 14.          0.74509198 18.          0.53929795]. \t  -0.5970890940506649 \t -0.4187495885724168\n",
            "13     \t [ 8.76460845  0.13227845  6.          0.75995694 17.          0.20887306]. \t  -0.7329728586443053 \t -0.4187495885724168\n",
            "14     \t [0.41721862 8.87551388 5.         0.69088851 6.         0.86973069]. \t  -0.468169334397084 \t -0.4187495885724168\n",
            "15     \t [ 1.07765263  6.20138171 14.          0.59898972 11.          0.65973368]. \t  -0.5325207293005967 \t -0.4187495885724168\n",
            "16     \t [ 1.09889115  8.6390199  10.          0.96730813  2.          0.79300245]. \t  -0.5022099845805148 \t -0.4187495885724168\n",
            "17     \t [ 9.26428304  9.74981947  6.          0.90684616 13.          0.82404302]. \t  -0.5075512306015831 \t -0.4187495885724168\n",
            "18     \t [ 5.72640258  1.49259688 14.          0.92448749  7.          0.73129851]. \t  -0.4929336813081906 \t -0.4187495885724168\n",
            "19     \t [ 0.60473619  9.7310361   5.          0.764657   19.          0.44899982]. \t  -0.602200593591361 \t -0.4187495885724168\n",
            "20     \t [5.52914614 4.85073957 5.         0.57128305 5.         0.32528222]. \t  -0.6020568820840394 \t -0.4187495885724168\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3.954907840073592"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snTrqE2RbWbe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d67b3ec-1dea-484a-a887-2498d05ddee1"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 13 \n",
        "\n",
        "np.random.seed(run_num_13)\n",
        "surrogate_winner_13 = dtStudentProcess(d_cov_func, nu = df)\n",
        "\n",
        "X_train13, X_test13, y_train13, y_test13 = train_test_split(X, y, test_size=test_perc, random_state=run_num_13)\n",
        "\n",
        "def f_syn_polarity13(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_13, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train13, y=y_train13).mean())\n",
        "    return operator * score\n",
        "\n",
        "winner_13 = dGPGO_stp(surrogate_winner_13, Acquisition_new(util_winner), f_syn_polarity13, param, n_jobs = -1) # Define BayesOpt\n",
        "winner_13.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_winner_13 = winner_13.getResult()[0]\n",
        "params_winner_13['max_depth'] = int(params_winner_13['max_depth'])\n",
        "params_winner_13['min_child_weight'] = int(params_winner_13['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_winner_train13 = xgb.DMatrix(X_train13, y_train13)\n",
        "dX_winner_test13 = xgb.DMatrix(X_test13, y_test13)\n",
        "model_winner_13 = xgb.train(params_winner_13, dX_winner_train13)\n",
        "pred_winner_13 = model_winner_13.predict(dX_winner_test13)\n",
        "\n",
        "rmse_winner_13 = np.sqrt(mean_squared_error(pred_winner_13, y_test13))\n",
        "rmse_winner_13"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 0.30967343  2.54282073  6.          0.9850871  17.          0.50848087]. \t  -0.5693403578256062 \t -0.4459810811310791\n",
            "init   \t [ 6.95119021  3.25593423 14.          0.8602375   8.          0.82055713]. \t  -0.5112217679187058 \t -0.4459810811310791\n",
            "init   \t [ 5.09252675  8.19921021 10.          0.78792002 16.          0.382158  ]. \t  -0.581693558072188 \t -0.4459810811310791\n",
            "init   \t [ 5.4465191   3.19841462 14.          0.90654581  5.          0.10359415]. \t  -0.6818818523119694 \t -0.4459810811310791\n",
            "init   \t [ 9.8393864   9.89619708  9.          0.63387763 10.          0.88979174]. \t  -0.4459810811310791 \t -0.4459810811310791\n",
            "1      \t [ 9.27118046  9.4344906  10.          0.71578295  1.          0.5463837 ]. \t  -0.5604994598662556 \t -0.4459810811310791\n",
            "2      \t [0.21910824 0.53051294 5.         0.62754372 4.         0.55299122]. \t  -0.578556985871312 \t -0.4459810811310791\n",
            "3      \t [0.32904954 9.80442032 8.         0.7285653  2.         0.94646442]. \t  \u001b[92m-0.43885135786496293\u001b[0m \t -0.43885135786496293\n",
            "4      \t [ 8.54553338  0.49573541 12.          0.81406546 19.          0.20999807]. \t  -0.6795516887212301 \t -0.43885135786496293\n",
            "5      \t [9.40145549 1.43629395 5.         0.63655396 1.         0.63002732]. \t  -0.5662893508870628 \t -0.43885135786496293\n",
            "6      \t [ 8.7238127   0.85665784  6.          0.75815556 12.          0.87180469]. \t  -0.4644522650882914 \t -0.43885135786496293\n",
            "7      \t [ 0.11400844  9.5774521   6.          0.90472821 11.          0.18513309]. \t  -0.6793968950126906 \t -0.43885135786496293\n",
            "8      \t [ 1.95513735  0.12334702 13.          0.54467307 14.          0.86324917]. \t  \u001b[92m-0.43794830902637205\u001b[0m \t -0.43794830902637205\n",
            "9      \t [ 2.89959158  8.10209185 14.          0.9094105   1.          0.86003598]. \t  \u001b[92m-0.42582758559630707\u001b[0m \t -0.42582758559630707\n",
            "10     \t [10.         10.         15.          1.         19.59649119  1.        ]. \t  \u001b[92m-0.4093163757211637\u001b[0m \t -0.4093163757211637\n",
            "11     \t [4.59853484 7.19189873 5.         0.92777191 6.         0.5760847 ]. \t  -0.5604008171822471 \t -0.4093163757211637\n",
            "12     \t [ 9.78022669  6.34740254  5.          0.81941991 15.          0.17866883]. \t  -0.6810440437950237 \t -0.4093163757211637\n",
            "13     \t [ 0.44924058  6.01817024 13.          0.62113095 19.          0.22629481]. \t  -0.6838258323776948 \t -0.4093163757211637\n",
            "14     \t [ 5.26264554  9.4282438  11.          0.85490999  6.          0.84689742]. \t  -0.5124229417642387 \t -0.4093163757211637\n",
            "15     \t [ 8.93459678  5.75013258 14.          0.86506851 14.          0.11026757]. \t  -0.6798789215383618 \t -0.4093163757211637\n",
            "16     \t [ 1.4145323   8.43775333 14.          0.85641993 12.          0.79067577]. \t  -0.5095348758688919 \t -0.4093163757211637\n",
            "17     \t [ 3.21925588  4.54321703  9.          0.67985163 11.          0.15521889]. \t  -0.6826511950016465 \t -0.4093163757211637\n",
            "18     \t [ 8.00654359  0.19101679 11.          0.82217983  1.          0.71399633]. \t  -0.5462514962984499 \t -0.4093163757211637\n",
            "19     \t [10.          8.52131642  8.28683161  1.         20.          1.        ]. \t  -0.4277023414700382 \t -0.4093163757211637\n",
            "20     \t [ 0.34200713  1.23697723 14.          0.93564476  1.          0.15494802]. \t  -0.6870601637786752 \t -0.4093163757211637\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.119606888589725"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23OXr6XLbY7_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c56a61b3-4dc9-44dd-bc03-932aaf4c5c63"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 14 \n",
        "\n",
        "np.random.seed(run_num_14)\n",
        "surrogate_winner_14 = dtStudentProcess(d_cov_func, nu = df)\n",
        "\n",
        "X_train14, X_test14, y_train14, y_test14 = train_test_split(X, y, test_size=test_perc, random_state=run_num_14)\n",
        "\n",
        "def f_syn_polarity14(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_14, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train14, y=y_train14).mean())\n",
        "    return operator * score\n",
        "\n",
        "winner_14 = dGPGO_stp(surrogate_winner_14, Acquisition_new(util_winner), f_syn_polarity14, param, n_jobs = -1) # Define BayesOpt\n",
        "winner_14.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_winner_14 = winner_14.getResult()[0]\n",
        "params_winner_14['max_depth'] = int(params_winner_14['max_depth'])\n",
        "params_winner_14['min_child_weight'] = int(params_winner_14['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_winner_train14 = xgb.DMatrix(X_train14, y_train14)\n",
        "dX_winner_test14 = xgb.DMatrix(X_test14, y_test14)\n",
        "model_winner_14 = xgb.train(params_winner_14, dX_winner_train14)\n",
        "pred_winner_14 = model_winner_14.predict(dX_winner_test14)\n",
        "\n",
        "rmse_winner_14 = np.sqrt(mean_squared_error(pred_winner_14, y_test14))\n",
        "rmse_winner_14"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 8.59560606  1.64569495 11.          0.89021195 15.          0.4857008 ]. \t  -0.6102441482204597 \t -0.4517949441804544\n",
            "init   \t [ 0.57460093  9.25007434  5.          0.56647642 17.          0.7533211 ]. \t  -0.5614002279921607 \t -0.4517949441804544\n",
            "init   \t [1.90674396 4.32019547 9.         0.93708225 3.         0.91775905]. \t  -0.4517949441804544 \t -0.4517949441804544\n",
            "init   \t [ 0.36770475  8.43368681 14.          0.6826844  15.          0.6291966 ]. \t  -0.5407924410004543 \t -0.4517949441804544\n",
            "init   \t [ 5.33840324  0.449182   14.          0.52045439  2.          0.48604556]. \t  -0.6242722768101235 \t -0.4517949441804544\n",
            "1      \t [ 9.54492219  9.07631824  8.          0.54579372 12.          0.38295996]. \t  -0.6262431675183743 \t -0.4517949441804544\n",
            "2      \t [9.53270461 0.83608685 6.         0.99976104 3.         0.60747265]. \t  -0.5533922160484371 \t -0.4517949441804544\n",
            "3      \t [ 1.15250646  0.51569303  5.          0.69322415 17.          0.16291682]. \t  -0.6917729046487754 \t -0.4517949441804544\n",
            "4      \t [ 8.50805687  8.26285095 14.          0.73716155  1.          0.12935726]. \t  -0.69096859473492 \t -0.4517949441804544\n",
            "5      \t [ 8.08070558  7.65123431 14.          0.59104644 19.          0.9302005 ]. \t  -0.45873776977159153 \t -0.4517949441804544\n",
            "6      \t [ 6.26901123  2.37239677  5.          0.95063922 11.          0.52876485]. \t  -0.6158458956422692 \t -0.4517949441804544\n",
            "7      \t [1.0907871  9.11150389 5.         0.58744161 9.         0.87439163]. \t  -0.4933476055445382 \t -0.4517949441804544\n",
            "8      \t [10.         10.         15.          1.          9.65395669  1.        ]. \t  \u001b[92m-0.417800200312621\u001b[0m \t -0.417800200312621\n",
            "9      \t [ 0.22934559  0.79834242 10.          0.74457361 11.          0.73245968]. \t  -0.535127850290114 \t -0.417800200312621\n",
            "10     \t [ 9.60452882  0.58561918 13.          0.64723541  8.          0.10946567]. \t  -0.6908109709249333 \t -0.417800200312621\n",
            "11     \t [ 0.83447886  9.83982164 14.          0.72332714  4.          0.89152733]. \t  -0.4432538639385051 \t -0.417800200312621\n",
            "12     \t [9.71394099 7.88076583 5.         0.92469304 3.         0.17093777]. \t  -0.6913576852026504 \t -0.417800200312621\n",
            "13     \t [ 8.89707903  3.19640798  5.          0.97710578 18.          0.42067076]. \t  -0.6248477910885939 \t -0.417800200312621\n",
            "14     \t [ 4.86477579  6.64166987 12.          0.895035   10.          0.33157919]. \t  -0.6249515980187641 \t -0.417800200312621\n",
            "15     \t [ 3.38906858  1.42319802 13.          0.65161286 18.          0.80112527]. \t  -0.5362080851699149 \t -0.417800200312621\n",
            "16     \t [5.14948858 9.9293377  8.         0.76473643 3.         0.51134466]. \t  -0.6119547835147783 \t -0.417800200312621\n",
            "17     \t [ 3.80871859  9.05347056 10.          0.61874692 19.          0.16519437]. \t  -0.6906798230345881 \t -0.417800200312621\n",
            "18     \t [ 1.62393567  3.63845704 14.          0.98063234  6.          0.73158162]. \t  -0.5372514711827497 \t -0.417800200312621\n",
            "19     \t [2.5291087  0.53223519 5.         0.77619135 7.         0.13944647]. \t  -0.6904892328550212 \t -0.417800200312621\n",
            "20     \t [ 3.64328712  8.01214891  7.          0.98259382 13.          0.15905312]. \t  -0.6906225126896343 \t -0.417800200312621\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3.9948295184675255"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgxvE7Irbbj_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c44206c-7435-430a-8f9b-21d3de7f2e1d"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 15 \n",
        "\n",
        "np.random.seed(run_num_15)\n",
        "surrogate_winner_15 = dtStudentProcess(d_cov_func, nu = df)\n",
        "\n",
        "X_train15, X_test15, y_train15, y_test15 = train_test_split(X, y, test_size=test_perc, random_state=run_num_15)\n",
        "\n",
        "def f_syn_polarity15(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_15, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train15, y=y_train15).mean())\n",
        "    return operator * score\n",
        "\n",
        "winner_15 = dGPGO_stp(surrogate_winner_15, Acquisition_new(util_winner), f_syn_polarity15, param, n_jobs = -1) # Define BayesOpt\n",
        "winner_15.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_winner_15 = winner_15.getResult()[0]\n",
        "params_winner_15['max_depth'] = int(params_winner_15['max_depth'])\n",
        "params_winner_15['min_child_weight'] = int(params_winner_15['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_winner_train15 = xgb.DMatrix(X_train15, y_train15)\n",
        "dX_winner_test15 = xgb.DMatrix(X_test15, y_test15)\n",
        "model_winner_15 = xgb.train(params_winner_15, dX_winner_train15)\n",
        "pred_winner_15 = model_winner_15.predict(dX_winner_test15)\n",
        "\n",
        "rmse_winner_15 = np.sqrt(mean_squared_error(pred_winner_15, y_test15))\n",
        "rmse_winner_15"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 5.64315424  8.97297685  6.          0.81493635 11.          0.33008564]. \t  -0.5946480910132583 \t -0.4392277798535851\n",
            "init   \t [9.51455383 9.60803032 5.         0.68485728 1.         0.30145081]. \t  -0.5968379804084254 \t -0.4392277798535851\n",
            "init   \t [ 8.11809889  6.32508536  7.          0.61766329 19.          0.51383159]. \t  -0.5092318398204285 \t -0.4392277798535851\n",
            "init   \t [ 7.28571251  5.36071389 10.          0.76240297 12.          0.12035683]. \t  -0.6346120183758519 \t -0.4392277798535851\n",
            "init   \t [ 6.80847768  3.59767194 14.          0.51365585 16.          0.89730673]. \t  -0.4392277798535851 \t -0.4392277798535851\n",
            "1      \t [ 2.28792065  8.58421878 12.          0.67551126  1.          0.37971264]. \t  -0.596161946051267 \t -0.4392277798535851\n",
            "2      \t [ 9.35473751  1.48895841 12.          0.55362474  4.          0.11106606]. \t  -0.6327865627642436 \t -0.4392277798535851\n",
            "3      \t [ 0.39252895  0.94193855 13.          0.73096739  8.          0.72579132]. \t  \u001b[92m-0.4332783925453615\u001b[0m \t -0.4332783925453615\n",
            "4      \t [1.42366593 3.16676945 5.         0.96590091 2.         0.27912003]. \t  -0.634271692553756 \t -0.4332783925453615\n",
            "5      \t [ 1.33182259  0.8125478   9.          0.94558338 18.          0.71777254]. \t  -0.4467705016632985 \t -0.4332783925453615\n",
            "6      \t [ 0.48538268  1.39730148  6.          0.8238236  11.          0.45741795]. \t  -0.5144696373796631 \t -0.4332783925453615\n",
            "7      \t [10. 10. 15.  1. 20.  1.]. \t  \u001b[92m-0.41305725689196027\u001b[0m \t -0.41305725689196027\n",
            "8      \t [ 0.44805719  9.61010592  6.          0.85006975 19.          0.34483998]. \t  -0.5950356207167496 \t -0.41305725689196027\n",
            "9      \t [ 1.00999897  7.89585019 13.          0.93919651 11.          0.15510744]. \t  -0.6336720356006029 \t -0.41305725689196027\n",
            "10     \t [9.60715195 0.38075751 5.         0.53304879 7.         0.99667797]. \t  -0.4680530726728757 \t -0.41305725689196027\n",
            "11     \t [ 7.53509705  8.39585344 14.          0.61737131  6.          0.28569362]. \t  -0.633889533661373 \t -0.41305725689196027\n",
            "12     \t [2.01656589 7.5067341  7.         0.97435364 6.         0.63334147]. \t  -0.4626787208905244 \t -0.41305725689196027\n",
            "13     \t [ 0.15053668  0.43786928 13.          0.62231098  1.          0.88145445]. \t  -0.43523183945633176 \t -0.41305725689196027\n",
            "14     \t [ 7.18775342  0.2062489   5.          0.98239776 17.          0.28043211]. \t  -0.634545784625454 \t -0.41305725689196027\n",
            "15     \t [8.27925095 3.51894083 6.         0.51392891 2.         0.22936394]. \t  -0.6317841836587383 \t -0.41305725689196027\n",
            "16     \t [ 2.22607821  9.62798432 12.          0.81069273 19.          0.90626708]. \t  -0.4349388112942007 \t -0.41305725689196027\n",
            "17     \t [ 1.09736761  4.76943581 14.          0.59193291 18.          0.87199551]. \t  -0.4341779923618919 \t -0.41305725689196027\n",
            "18     \t [ 6.61762334 10.         13.55340194  1.         15.55340194  1.        ]. \t  \u001b[92m-0.4115270057827871\u001b[0m \t -0.4115270057827871\n",
            "19     \t [4.54273681 0.75449129 8.         0.77985583 6.         0.29980964]. \t  -0.592289728869178 \t -0.4115270057827871\n",
            "20     \t [ 9.96011127  1.55235537 14.          0.63004724 11.          0.14483121]. \t  -0.6331081546428745 \t -0.4115270057827871\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.206768987537493"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ye4UEpNabeCL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c470d340-5d2f-4ef5-c33f-edc27afaf9d7"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 16 \n",
        "\n",
        "np.random.seed(run_num_16)\n",
        "surrogate_winner_16 = dtStudentProcess(d_cov_func, nu = df)\n",
        "\n",
        "X_train16, X_test16, y_train16, y_test16 = train_test_split(X, y, test_size=test_perc, random_state=run_num_16)\n",
        "\n",
        "def f_syn_polarity16(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_16, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train16, y=y_train16).mean())\n",
        "    return operator * score\n",
        "\n",
        "winner_16 = dGPGO_stp(surrogate_winner_16, Acquisition_new(util_winner), f_syn_polarity16, param, n_jobs = -1) # Define BayesOpt\n",
        "winner_16.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_winner_16 = winner_16.getResult()[0]\n",
        "params_winner_16['max_depth'] = int(params_winner_16['max_depth'])\n",
        "params_winner_16['min_child_weight'] = int(params_winner_16['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_winner_train16 = xgb.DMatrix(X_train16, y_train16)\n",
        "dX_winner_test16 = xgb.DMatrix(X_test16, y_test16)\n",
        "model_winner_16 = xgb.train(params_winner_16, dX_winner_train16)\n",
        "pred_winner_16 = model_winner_16.predict(dX_winner_test16)\n",
        "\n",
        "rmse_winner_16 = np.sqrt(mean_squared_error(pred_winner_16, y_test16))\n",
        "rmse_winner_16"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [8.63948332 3.16366002 7.         0.75395583 1.         0.56999262]. \t  -0.5713220948397237 \t -0.4331621293825035\n",
            "init   \t [ 3.3426621   9.77029134 14.          0.93751218 17.          0.80549151]. \t  -0.4331621293825035 \t -0.4331621293825035\n",
            "init   \t [ 3.36871073  4.8308288  13.          0.91984982 14.          0.48829226]. \t  -0.5561611494046279 \t -0.4331621293825035\n",
            "init   \t [ 8.84540736  1.59826081  8.          0.75772888 14.          0.29029227]. \t  -0.6835926126526815 \t -0.4331621293825035\n",
            "init   \t [ 7.74981136  8.34377636 13.          0.83182409 15.          0.19378622]. \t  -0.7677243885128009 \t -0.4331621293825035\n",
            "1      \t [0.90186209 9.9119065  9.         0.96247707 9.         0.82473059]. \t  -0.44000666714577197 \t -0.4331621293825035\n",
            "2      \t [ 3.6191694   8.20490707  5.          0.81280746 16.          0.99306688]. \t  -0.4581574120389309 \t -0.4331621293825035\n",
            "3      \t [2.15072525 1.72325427 6.         0.7282133  7.         0.66792915]. \t  -0.510524353390639 \t -0.4331621293825035\n",
            "4      \t [ 4.49483609  7.05102179 14.          0.67776057  3.          0.50727602]. \t  -0.5621481895640063 \t -0.4331621293825035\n",
            "5      \t [ 4.63470447  0.29792913 13.          0.70911522  7.          0.30290504]. \t  -0.6845759859311331 \t -0.4331621293825035\n",
            "6      \t [9.17745543 7.46026312 5.         0.6284814  9.         0.72652494]. \t  -0.47418011145069017 \t -0.4331621293825035\n",
            "7      \t [2.7669192  6.70200041 7.         0.83888371 1.         0.66194747]. \t  -0.49144662381331905 \t -0.4331621293825035\n",
            "8      \t [ 1.86481894  0.99601669  6.          0.66208635 14.          0.74317901]. \t  -0.4660455507597825 \t -0.4331621293825035\n",
            "9      \t [9.8048903  0.9736784  9.         0.6852998  7.         0.71530424]. \t  -0.4471579166365055 \t -0.4331621293825035\n",
            "10     \t [ 9.07329721  6.04353062  6.          0.62940057 19.          0.1206801 ]. \t  -0.7674777671186626 \t -0.4331621293825035\n",
            "11     \t [ 8.54895835  0.69961229 14.          0.57654595 19.          0.20454627]. \t  -0.7677953049378372 \t -0.4331621293825035\n",
            "12     \t [ 3.29229675  0.53359978 10.          0.77148242  1.          0.91273629]. \t  -0.43363477169774267 \t -0.4331621293825035\n",
            "13     \t [ 9.26465723  9.86116065 13.          0.52207502  6.          0.17735657]. \t  -0.7670798777253309 \t -0.4331621293825035\n",
            "14     \t [10. 10. 15.  1. 20.  1.]. \t  \u001b[92m-0.4257136692064744\u001b[0m \t -0.4257136692064744\n",
            "15     \t [10.          4.24397838 15.          1.          9.62180255  1.        ]. \t  \u001b[92m-0.41967897618084693\u001b[0m \t -0.41967897618084693\n",
            "16     \t [ 3.17441475  4.25643907  9.          0.52998446 19.          0.85238195]. \t  -0.4605274006131442 \t -0.41967897618084693\n",
            "17     \t [8.89959759 9.80294812 5.         0.91938908 4.         0.22865578]. \t  -0.7682181679189742 \t -0.41967897618084693\n",
            "18     \t [ 6.47050729  9.70570023  8.          0.55783027 12.          0.88747479]. \t  -0.4508283906225067 \t -0.41967897618084693\n",
            "19     \t [ 9.46331313  2.81887466 13.          0.76076385  3.          0.42087971]. \t  -0.6854869575909893 \t -0.41967897618084693\n",
            "20     \t [ 0.16073405  5.35738719  5.          0.92408258 11.          0.99253058]. \t  -0.4512471509211318 \t -0.41967897618084693\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.490348483726153"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Biq2Uaa5bg3U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "616369e8-1eda-44cc-9fd5-4880192cabb2"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 17 \n",
        "\n",
        "np.random.seed(run_num_17)\n",
        "surrogate_winner_17 = dtStudentProcess(d_cov_func, nu = df)\n",
        "\n",
        "X_train17, X_test17, y_train17, y_test17 = train_test_split(X, y, test_size=test_perc, random_state=run_num_17)\n",
        "\n",
        "def f_syn_polarity17(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_17, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train17, y=y_train17).mean())\n",
        "    return operator * score\n",
        "\n",
        "winner_17 = dGPGO_stp(surrogate_winner_17, Acquisition_new(util_winner), f_syn_polarity17, param, n_jobs = -1) # Define BayesOpt\n",
        "winner_17.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_winner_17 = winner_17.getResult()[0]\n",
        "params_winner_17['max_depth'] = int(params_winner_17['max_depth'])\n",
        "params_winner_17['min_child_weight'] = int(params_winner_17['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_winner_train17 = xgb.DMatrix(X_train17, y_train17)\n",
        "dX_winner_test17 = xgb.DMatrix(X_test17, y_test17)\n",
        "model_winner_17 = xgb.train(params_winner_17, dX_winner_train17)\n",
        "pred_winner_17 = model_winner_17.predict(dX_winner_test17)\n",
        "\n",
        "rmse_winner_17 = np.sqrt(mean_squared_error(pred_winner_17, y_test17))\n",
        "rmse_winner_17"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 1.85575275  3.86599303 13.          0.85645882  2.          0.78699813]. \t  -0.4597056260580034 \t -0.4597056260580034\n",
            "init   \t [1.97780831 2.92527676 8.         0.92268607 9.         0.39017299]. \t  -0.5687207024308397 \t -0.4597056260580034\n",
            "init   \t [ 7.16656007  1.028324    5.          0.96975483 15.          0.90932849]. \t  -0.4688025715067707 \t -0.4597056260580034\n",
            "init   \t [ 2.46007304  9.53596657  7.          0.71704714 11.          0.17659118]. \t  -0.6983036523024195 \t -0.4597056260580034\n",
            "init   \t [3.44161135 9.92427612 8.         0.96331165 2.         0.29160186]. \t  -0.5675253504862205 \t -0.4597056260580034\n",
            "1      \t [ 9.98828606  3.49693094 13.          0.57156882  8.          0.14604355]. \t  -0.6992595730796743 \t -0.4597056260580034\n",
            "2      \t [ 6.95619362  6.95953721 12.          0.92317057 19.          0.34981799]. \t  -0.5628121282495753 \t -0.4597056260580034\n",
            "3      \t [8.58397417 0.63190934 7.         0.80049626 2.         0.98851802]. \t  \u001b[92m-0.453932525933606\u001b[0m \t -0.453932525933606\n",
            "4      \t [ 1.79054908  0.97767001 14.          0.72051649 17.          0.75055942]. \t  -0.4586950539387324 \t -0.453932525933606\n",
            "5      \t [ 6.18141449  9.10978044 14.          0.68104739 11.          0.12476021]. \t  -0.6984618188236655 \t -0.453932525933606\n",
            "6      \t [ 1.77308987  6.92082513  5.          0.52059222 19.          0.18788647]. \t  -0.69931956875667 \t -0.453932525933606\n",
            "7      \t [0.84757532 0.06275289 6.         0.97600366 2.         0.14520225]. \t  -0.6945632868954219 \t -0.453932525933606\n",
            "8      \t [ 8.26790477  9.9524702   5.          0.59716297 16.          0.88254342]. \t  -0.47691281251538375 \t -0.453932525933606\n",
            "9      \t [ 0.68822521  0.26060833 14.          0.54090138 10.          0.57859892]. \t  -0.5381294165153002 \t -0.453932525933606\n",
            "10     \t [ 9.20219266  7.71975834 14.          0.72531692  2.          0.43032755]. \t  -0.5354718257765292 \t -0.453932525933606\n",
            "11     \t [7.50153168 8.37519947 6.         0.6544959  7.         0.90121291]. \t  -0.46115116499634456 \t -0.453932525933606\n",
            "12     \t [ 8.50144324  2.01154388 13.          0.5042928  15.          0.75507006]. \t  -0.47211437873776746 \t -0.453932525933606\n",
            "13     \t [ 0.46952053  9.52164833 13.          0.77701641  6.          0.10604221]. \t  -0.6987462532393639 \t -0.453932525933606\n",
            "14     \t [ 0.62078996  7.1751753  14.          0.71073778 18.          0.41690682]. \t  -0.5657510908419002 \t -0.453932525933606\n",
            "15     \t [9.7645138  6.96943344 7.         0.64264761 1.         0.5601612 ]. \t  -0.5443333775093777 \t -0.453932525933606\n",
            "16     \t [ 0.10112756  0.62078406  5.          0.64383562 17.          0.30963718]. \t  -0.5819290414426197 \t -0.453932525933606\n",
            "17     \t [ 9.98963147  1.02205463  9.          0.92297191 19.          0.88236881]. \t  \u001b[92m-0.4452794423705894\u001b[0m \t -0.4452794423705894\n",
            "18     \t [8.22220428 1.33077182 6.         0.83043869 8.         0.41083758]. \t  -0.5748657724182161 \t -0.4452794423705894\n",
            "19     \t [ 9.93454453  6.44909115  5.          0.8340519  12.          0.49403801]. \t  -0.5555415050171015 \t -0.4452794423705894\n",
            "20     \t [ 3.87116535  3.46167946  9.          0.68739534 15.          0.44731298]. \t  -0.5417289076052462 \t -0.4452794423705894\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.227992464584259"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5H4MWSXFcZjO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5031af8b-f83c-4b8e-a104-74800a8eb01d"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 18 \n",
        "\n",
        "np.random.seed(run_num_18)\n",
        "surrogate_winner_18 = dtStudentProcess(d_cov_func, nu = df)\n",
        "\n",
        "X_train18, X_test18, y_train18, y_test18 = train_test_split(X, y, test_size=test_perc, random_state=run_num_18)\n",
        "\n",
        "def f_syn_polarity18(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_18, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train18, y=y_train18).mean())\n",
        "    return operator * score\n",
        "\n",
        "winner_18 = dGPGO_stp(surrogate_winner_18, Acquisition_new(util_winner), f_syn_polarity18, param, n_jobs = -1) # Define BayesOpt\n",
        "winner_18.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_winner_18 = winner_18.getResult()[0]\n",
        "params_winner_18['max_depth'] = int(params_winner_18['max_depth'])\n",
        "params_winner_18['min_child_weight'] = int(params_winner_18['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_winner_train18 = xgb.DMatrix(X_train18, y_train18)\n",
        "dX_winner_test18 = xgb.DMatrix(X_test18, y_test18)\n",
        "model_winner_18 = xgb.train(params_winner_18, dX_winner_train18)\n",
        "pred_winner_18 = model_winner_18.predict(dX_winner_test18)\n",
        "\n",
        "rmse_winner_18 = np.sqrt(mean_squared_error(pred_winner_18, y_test18))\n",
        "rmse_winner_18"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 8.73429403  9.68540663 10.          0.68875849  9.          0.48011572]. \t  -0.536396330012702 \t -0.4337279026393176\n",
            "init   \t [ 6.12033333  7.66062926  8.          0.76133734 13.          0.93379456]. \t  -0.4433380408289477 \t -0.4337279026393176\n",
            "init   \t [ 1.46524679  7.01527914  7.          0.90913299 10.          0.36016753]. \t  -0.5541638729833303 \t -0.4337279026393176\n",
            "init   \t [ 9.73855241  3.33774046 14.          0.53290419  7.          0.7088681 ]. \t  -0.5062350384654568 \t -0.4337279026393176\n",
            "init   \t [ 3.00618018  1.82702795 11.          0.75681389 14.          0.98627449]. \t  -0.4337279026393176 \t -0.4337279026393176\n",
            "1      \t [ 1.46586883  1.13810325 14.          0.90052794  5.          0.16182112]. \t  -0.6179729230954069 \t -0.4337279026393176\n",
            "2      \t [ 9.17850108  5.22481201 14.          0.60105853 18.          0.63442006]. \t  -0.5052903785308961 \t -0.4337279026393176\n",
            "3      \t [6.20612396 4.37552923 6.         0.71941082 3.         0.90943147]. \t  -0.45137007433293086 \t -0.4337279026393176\n",
            "4      \t [ 0.42107725  9.92938812 12.          0.92462675 18.          0.88352347]. \t  \u001b[92m-0.4255403331751115\u001b[0m \t -0.4255403331751115\n",
            "5      \t [ 1.45418057  8.89399    11.          0.93175096  1.          0.52391385]. \t  -0.5358666580451181 \t -0.4255403331751115\n",
            "6      \t [ 6.37698126  1.83270742  5.          0.89203056 17.          0.10435121]. \t  -0.6174428103417341 \t -0.4255403331751115\n",
            "7      \t [ 3.76328522  7.15740213 14.          0.83667606 10.          0.79012696]. \t  -0.4458786451533837 \t -0.4255403331751115\n",
            "8      \t [0.18032167 1.73628428 7.         0.68558335 5.         0.33274978]. \t  -0.5593106163222252 \t -0.4255403331751115\n",
            "9      \t [7.94412856 1.24068931 7.         0.96649935 9.         0.47928692]. \t  -0.5420539933608051 \t -0.4255403331751115\n",
            "10     \t [ 7.40163369  4.10160591 13.          0.74748073  1.          0.65130888]. \t  -0.5011183921005669 \t -0.4255403331751115\n",
            "11     \t [0.63929161 7.70424849 5.         0.99128679 2.         0.4073203 ]. \t  -0.5610229078873404 \t -0.4255403331751115\n",
            "12     \t [ 3.63739627  8.35435528  5.          0.58995076 19.          0.32125273]. \t  -0.5635646610193202 \t -0.4255403331751115\n",
            "13     \t [ 3.86727486  0.04582369 10.          0.54629234  1.          0.59406244]. \t  -0.5128905589861811 \t -0.4255403331751115\n",
            "14     \t [ 0.24957112  3.26102415  5.          0.59065773 17.          0.91920206]. \t  -0.4670777155018547 \t -0.4255403331751115\n",
            "15     \t [ 7.58250402 10.         15.          1.         14.17240312  1.        ]. \t  \u001b[92m-0.40998495308070115\u001b[0m \t -0.40998495308070115\n",
            "16     \t [ 8.87573689  8.22117832  7.48361367  1.         20.          1.        ]. \t  -0.43077053897041306 \t -0.40998495308070115\n",
            "17     \t [ 8.2099779   9.6979058  11.          0.98471671  3.          0.97310901]. \t  -0.42791308267011485 \t -0.40998495308070115\n",
            "18     \t [9.72504206 0.87233101 9.         0.64831595 2.         0.17558231]. \t  -0.6187685246436907 \t -0.40998495308070115\n",
            "19     \t [10.         10.         15.          1.          7.47919063  1.        ]. \t  \u001b[92m-0.40866103242446866\u001b[0m \t -0.40866103242446866\n",
            "20     \t [ 8.86269845  1.61788611 11.          0.72867371 13.          0.49923805]. \t  -0.5331316926415038 \t -0.40866103242446866\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3.6820909591607354"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-EgxA8k4ccL6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6392fe0e-e675-4bc0-8e4e-b33d5142f942"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 19 \n",
        "\n",
        "np.random.seed(run_num_19)\n",
        "surrogate_winner_19 = dtStudentProcess(d_cov_func, nu = df)\n",
        "\n",
        "X_train19, X_test19, y_train19, y_test19 = train_test_split(X, y, test_size=test_perc, random_state=run_num_19)\n",
        "\n",
        "def f_syn_polarity19(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_19, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train19, y=y_train19).mean())\n",
        "    return operator * score\n",
        "\n",
        "winner_19 = dGPGO_stp(surrogate_winner_19, Acquisition_new(util_winner), f_syn_polarity19, param, n_jobs = -1) # Define BayesOpt\n",
        "winner_19.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_winner_19 = winner_19.getResult()[0]\n",
        "params_winner_19['max_depth'] = int(params_winner_19['max_depth'])\n",
        "params_winner_19['min_child_weight'] = int(params_winner_19['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_winner_train19 = xgb.DMatrix(X_train19, y_train19)\n",
        "dX_winner_test19 = xgb.DMatrix(X_test19, y_test19)\n",
        "model_winner_19 = xgb.train(params_winner_19, dX_winner_train19)\n",
        "pred_winner_19 = model_winner_19.predict(dX_winner_test19)\n",
        "\n",
        "rmse_winner_19 = np.sqrt(mean_squared_error(pred_winner_19, y_test19))\n",
        "rmse_winner_19"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 8.24520174  9.84626759 12.          0.81846905 12.          0.27594309]. \t  -0.6624515538441494 \t -0.4321567975765851\n",
            "init   \t [ 8.6021676   8.64300514 14.          0.95067208 18.          0.13363564]. \t  -0.663790961968593 \t -0.4321567975765851\n",
            "init   \t [ 3.02486657  6.06229509 10.          0.50494482  9.          0.10218675]. \t  -0.668875700942164 \t -0.4321567975765851\n",
            "init   \t [ 6.57586195  8.01231848  7.          0.63945407 18.          0.49206406]. \t  -0.5640092362987262 \t -0.4321567975765851\n",
            "init   \t [ 6.05008856  8.15558548 14.          0.64718943  8.          0.90611106]. \t  -0.4321567975765851 \t -0.4321567975765851\n",
            "1      \t [ 0.85655458  0.69294498  9.          0.90992265 19.          0.25605959]. \t  -0.6632879756016564 \t -0.4321567975765851\n",
            "2      \t [ 1.16239725  9.89254371 11.          0.98469188  1.          0.20271996]. \t  -0.6659068700895453 \t -0.4321567975765851\n",
            "3      \t [9.26425961 9.717621   5.         0.8856331  5.         0.65589573]. \t  -0.5256636565447839 \t -0.4321567975765851\n",
            "4      \t [8.38158931 1.39953008 5.         0.6836482  9.         0.46934447]. \t  -0.5657479189361881 \t -0.4321567975765851\n",
            "5      \t [ 9.89729152  2.88709408 11.          0.55121781  3.          0.58381748]. \t  -0.5217676759230677 \t -0.4321567975765851\n",
            "6      \t [0.69851878 2.66567472 6.         0.67563338 1.         0.71568127]. \t  -0.47007581785399866 \t -0.4321567975765851\n",
            "7      \t [ 0.52039992  0.56380532 14.          0.70324781  2.          0.77427978]. \t  -0.44119318049659856 \t -0.4321567975765851\n",
            "8      \t [ 9.45607821  0.96438893 14.          0.56821928 16.          0.48995458]. \t  -0.560546633196372 \t -0.4321567975765851\n",
            "9      \t [ 0.56432836  7.79494932 13.          0.55871992 19.          0.10266045]. \t  -0.6660467600439527 \t -0.4321567975765851\n",
            "10     \t [ 3.16676683  1.78770811 13.          0.5650927  13.          0.7417697 ]. \t  -0.4472490143246649 \t -0.4321567975765851\n",
            "11     \t [ 0.64379529  1.54731928  6.          0.82512479 11.          0.6319422 ]. \t  -0.5209201684825245 \t -0.4321567975765851\n",
            "12     \t [ 7.73804622  1.66772104  5.          0.66266078 17.          0.87441258]. \t  -0.4732994211333518 \t -0.4321567975765851\n",
            "13     \t [ 7.46605559  8.55652968 13.          0.99452706  1.          0.43764631]. \t  -0.5573240867828692 \t -0.4321567975765851\n",
            "14     \t [ 0.36172735  9.41325373  5.          0.88367823 18.          0.31289693]. \t  -0.610279476214515 \t -0.4321567975765851\n",
            "15     \t [6.50533986 6.11396147 7.         0.78114466 1.         0.75288314]. \t  -0.45838856364045305 \t -0.4321567975765851\n",
            "16     \t [1.7958895  9.19371155 5.         0.54448974 5.         0.93868386]. \t  -0.4669880318809955 \t -0.4321567975765851\n",
            "17     \t [ 9.62487731  6.86253449  5.          0.66465947 12.          0.21107909]. \t  -0.6659019290689656 \t -0.4321567975765851\n",
            "18     \t [ 1.7944313   9.07405715  5.          0.89658957 12.          0.48452475]. \t  -0.5646254130385107 \t -0.4321567975765851\n",
            "19     \t [4.43453509 0.79656434 9.         0.94541308 4.         0.56406861]. \t  -0.5574336760699456 \t -0.4321567975765851\n",
            "20     \t [ 8.84762988  1.25680743 14.          0.66615751  9.          0.97141347]. \t  -0.4335880467055418 \t -0.4321567975765851\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.2307134915487135"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08y-IGpyceje",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b92de58-b119-4047-d742-2ab7c6f43827"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 20 \n",
        "\n",
        "np.random.seed(run_num_20)\n",
        "surrogate_winner_20 = dtStudentProcess(d_cov_func, nu = df)\n",
        "\n",
        "X_train20, X_test20, y_train20, y_test20 = train_test_split(X, y, test_size=test_perc, random_state=run_num_20)\n",
        "\n",
        "def f_syn_polarity20(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_20, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train20, y=y_train20).mean())\n",
        "    return operator * score\n",
        "\n",
        "winner_20 = dGPGO_stp(surrogate_winner_20, Acquisition_new(util_winner), f_syn_polarity20, param, n_jobs = -1) # Define BayesOpt\n",
        "winner_20.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_winner_20 = winner_20.getResult()[0]\n",
        "params_winner_20['max_depth'] = int(params_winner_20['max_depth'])\n",
        "params_winner_20['min_child_weight'] = int(params_winner_20['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_winner_train20 = xgb.DMatrix(X_train20, y_train20)\n",
        "dX_winner_test20 = xgb.DMatrix(X_test20, y_test20)\n",
        "model_winner_20 = xgb.train(params_winner_20, dX_winner_train20)\n",
        "pred_winner_20 = model_winner_20.predict(dX_winner_test20)\n",
        "\n",
        "rmse_winner_20 = np.sqrt(mean_squared_error(pred_winner_20, y_test20))\n",
        "rmse_winner_20"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 5.70517285  5.64528755 11.          0.51213999 19.          0.57883228]. \t  -0.5510634045248362 \t -0.4667910180825121\n",
            "init   \t [0.68106288 5.8452906  6.         0.58037829 8.         0.87974899]. \t  -0.4667910180825121 \t -0.4667910180825121\n",
            "init   \t [ 4.04121426  7.17971417 11.          0.57603584 10.          0.19954225]. \t  -0.6541702754539586 \t -0.4667910180825121\n",
            "init   \t [ 5.27556761  8.67655329 13.          0.54252273  4.          0.48570796]. \t  -0.5813476654605825 \t -0.4667910180825121\n",
            "init   \t [ 5.30003889  5.73946822 12.          0.52666698 16.          0.77467019]. \t  -0.5078849923055367 \t -0.4667910180825121\n",
            "1      \t [ 9.25190708  0.65043072 10.          0.56676233  5.          0.41289847]. \t  -0.6164858434894066 \t -0.4667910180825121\n",
            "2      \t [8.65689736 8.11684598 5.         0.53907072 1.         0.87574215]. \t  -0.47576906935129115 \t -0.4667910180825121\n",
            "3      \t [6.49728107e-01 7.72862043e-04 7.00000000e+00 5.71905117e-01\n",
            " 2.00000000e+00 1.02014940e-01]. \t  -0.6533990377801203 \t -0.4667910180825121\n",
            "4      \t [ 9.51806562  0.44900923  6.          0.62997227 16.          0.40028974]. \t  -0.6257141325107729 \t -0.4667910180825121\n",
            "5      \t [ 0.48808143  1.97030839  7.          0.79892377 16.          0.98477167]. \t  \u001b[92m-0.4602371335347518\u001b[0m \t -0.4602371335347518\n",
            "6      \t [ 9.98029391  8.23632437  6.          0.63606408 13.          0.22908429]. \t  -0.6532745897130707 \t -0.4602371335347518\n",
            "7      \t [ 3.9245879   0.87058505 14.          0.68269183  1.          0.34048922]. \t  -0.623283664424315 \t -0.4602371335347518\n",
            "8      \t [ 0.45468431  0.13267263 10.          0.667214    9.          0.47071141]. \t  -0.5807933165122651 \t -0.4602371335347518\n",
            "9      \t [10. 10. 15.  1. 20.  1.]. \t  \u001b[92m-0.41456787473760653\u001b[0m \t -0.41456787473760653\n",
            "10     \t [ 0.26650768  8.81821255  6.          0.54118329 18.          0.20228777]. \t  -0.6522760722738631 \t -0.41456787473760653\n",
            "11     \t [ 9.74564452  1.02954501 14.          0.98964301 12.          0.47743205]. \t  -0.5773481317621234 \t -0.41456787473760653\n",
            "12     \t [ 7.76906477  2.6060444   5.          0.53491145 10.          0.41560504]. \t  -0.627993339841767 \t -0.41456787473760653\n",
            "13     \t [ 9.81648656  7.54014251 14.          0.94404013 10.          0.32595359]. \t  -0.6133782645290285 \t -0.41456787473760653\n",
            "14     \t [1.36051555 9.95957797 7.         0.9242116  2.         0.87097462]. \t  -0.449167771194887 \t -0.41456787473760653\n",
            "15     \t [ 0.20978943  8.45447688 14.          0.77351134 17.          0.2656195 ]. \t  -0.6586945947582249 \t -0.41456787473760653\n",
            "16     \t [7.55121074 9.86496533 6.         0.88828535 7.         0.3097795 ]. \t  -0.6258969685789838 \t -0.41456787473760653\n",
            "17     \t [ 1.88681738  3.46035586 14.          0.68070687  6.          0.9944693 ]. \t  -0.42905917541157645 \t -0.41456787473760653\n",
            "18     \t [ 1.00455473  4.86452269 10.          0.65242434  1.          0.22603818]. \t  -0.6579989052523156 \t -0.41456787473760653\n",
            "19     \t [ 6.53384033  5.82485681  5.          0.72422044 17.          0.61312031]. \t  -0.5669534875260422 \t -0.41456787473760653\n",
            "20     \t [ 3.12070887  1.85937087 14.          0.99870119 12.          0.10116826]. \t  -0.6581225237799492 \t -0.41456787473760653\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.289968249274355"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78QysHAlchIb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c99a13b4-f16d-4c84-8cdc-47d72a31b2c3"
      },
      "source": [
        "end_win = time.time()\n",
        "end_win\n",
        "\n",
        "time_win = end_win - start_win\n",
        "time_win"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1098.747090101242"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CU2FlhY4vHUk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99f09f4d-7913-44f5-b756-c7fb0a78a3d2"
      },
      "source": [
        "rmse_loser = [rmse_loser_1,\n",
        "rmse_loser_2,\n",
        "rmse_loser_3,\n",
        "rmse_loser_4,\n",
        "rmse_loser_5,\n",
        "rmse_loser_6,\n",
        "rmse_loser_7,\n",
        "rmse_loser_8,\n",
        "rmse_loser_9,\n",
        "rmse_loser_10,\n",
        "rmse_loser_11,\n",
        "rmse_loser_12,\n",
        "rmse_loser_13,\n",
        "rmse_loser_14,\n",
        "rmse_loser_15,\n",
        "rmse_loser_16,\n",
        "rmse_loser_17,\n",
        "rmse_loser_18,\n",
        "rmse_loser_19,\n",
        "rmse_loser_20]\n",
        "\n",
        "np.mean(rmse_loser)"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.332741455425436"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQ1lotm7vJi9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf070f2a-d51a-4016-e025-31a36728c458"
      },
      "source": [
        "rmse_winner = [rmse_winner_1,\n",
        "rmse_winner_2,\n",
        "rmse_winner_3,\n",
        "rmse_winner_4,\n",
        "rmse_winner_5,\n",
        "rmse_winner_6,\n",
        "rmse_winner_7,\n",
        "rmse_winner_8,\n",
        "rmse_winner_9,\n",
        "rmse_winner_10,\n",
        "rmse_winner_11,\n",
        "rmse_winner_12,\n",
        "rmse_winner_13,\n",
        "rmse_winner_14,\n",
        "rmse_winner_15,\n",
        "rmse_winner_16,\n",
        "rmse_winner_17,\n",
        "rmse_winner_18,\n",
        "rmse_winner_19,\n",
        "rmse_winner_20]\n",
        "\n",
        "np.mean(rmse_winner)"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.238331811289802"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1yU4s1GRvMdl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74647d36-ff32-422a-d9fd-bf542ae317f7"
      },
      "source": [
        "min_rmse_loser = min_max_array(rmse_loser)\n",
        "min_rmse_loser, len(min_rmse_loser)"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([4.87018129255363,\n",
              "  4.343487868410211,\n",
              "  4.343487868410211,\n",
              "  4.343487868410211,\n",
              "  4.31722690080873,\n",
              "  4.1451215046465775,\n",
              "  4.1451215046465775,\n",
              "  4.1451215046465775,\n",
              "  4.130636454660636,\n",
              "  4.130636454660636,\n",
              "  4.130636454660636,\n",
              "  4.130636454660636,\n",
              "  4.130636454660636,\n",
              "  4.130636454660636,\n",
              "  4.130636454660636,\n",
              "  4.130636454660636,\n",
              "  4.0138942791645995,\n",
              "  3.937438334950126,\n",
              "  3.937438334950126,\n",
              "  3.937438334950126],\n",
              " 20)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unXOpKHcvO15",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a027a72-7283-4848-9a5f-7afb78526a44"
      },
      "source": [
        "min_rmse_winner = min_max_array(rmse_winner)\n",
        "min_rmse_winner, len(min_rmse_winner)"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([4.350022527863896,\n",
              "  4.334191828429232,\n",
              "  4.334191828429232,\n",
              "  4.299486064824159,\n",
              "  4.185955330442662,\n",
              "  4.151672275228334,\n",
              "  4.151672275228334,\n",
              "  4.151672275228334,\n",
              "  4.062815633546083,\n",
              "  4.062815633546083,\n",
              "  4.062815633546083,\n",
              "  3.954907840073592,\n",
              "  3.954907840073592,\n",
              "  3.954907840073592,\n",
              "  3.954907840073592,\n",
              "  3.954907840073592,\n",
              "  3.954907840073592,\n",
              "  3.6820909591607354,\n",
              "  3.6820909591607354,\n",
              "  3.6820909591607354],\n",
              " 20)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yxo85-HEvRPi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "outputId": "9b73c43b-29b7-44cc-f772-b3f8b03970b9"
      },
      "source": [
        "### Visualise!\n",
        "\n",
        "title = obj_func\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(min_rmse_loser, color = 'Green', label='RMSE: GP dEI ')\n",
        "plt.plot(min_rmse_winner, color = 'Red', label='RMSE: STP dERM ')\n",
        "\n",
        "plt.title(title, weight = 'bold', family = 'Arial')\n",
        "plt.xlabel('Experiment(s)', weight = 'bold', family = 'Arial') # x-axis label\n",
        "plt.ylabel('RMSE', weight = 'bold', family = 'Arial') # y-axis label\n",
        "plt.legend(loc=0) # add plot legend\n",
        "\n",
        "### Make the x-ticks integers, not floats:\n",
        "count = len(min_rmse_loser)\n",
        "plt.xticks(np.arange(count), np.arange(1, count + 1))\n",
        "plt.grid(b=None)\n",
        "plt.show() #visualize!\n"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwyO7_iZvT7a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38dadb9c-87b2-4fbf-d678-d31a381c9fe4"
      },
      "source": [
        "time_lose, time_win"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1520.1884331703186, 1098.747090101242)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    }
  ]
}