{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NYC__TaxiFare__XGBoost__STP__dCBM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9wHutsqZUcn"
      },
      "source": [
        "XGBoost Regression - 'real-world' example: NYC Taxi-Fare Predictor\n",
        "\n",
        "GP dEI versus STP nu = 3 dCBM (winner)\n",
        "\n",
        "https://www.kaggle.com/c/new-york-city-taxi-fare-prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7PwmXsgZO8D",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "533e8d81-27bf-42ee-8df1-6def1b3eaa65"
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-7e3e6b0a-3c0b-47de-8cce-7023fdad00cc\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-7e3e6b0a-3c0b-47de-8cce-7023fdad00cc\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"conorc2006\",\"key\":\"c5c5a6382a7d50c022aab991694fc17f\"}'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMwbJ6hjZltI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7635944-07d2-4348-bace-d1ef67b960a6"
      },
      "source": [
        "## Ensure the kaggle.json file is present:\n",
        "!ls -lha kaggle.json"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-rw-r--r-- 1 root root 66 Feb 16 11:23 kaggle.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8Pu-UlWZovH"
      },
      "source": [
        "## Next, install the Kaggle API client:\n",
        "!pip install -q kaggle"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUOQ4SE7Zuj3"
      },
      "source": [
        "## The Kaggle API Client expects this file to be ~/.kaggle\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJcEztjCZxOn"
      },
      "source": [
        "## Permissions' change\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-u4Tmj7ZUD3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f82d7396-fa3e-4d33-dffc-9842384bbd8e"
      },
      "source": [
        "!kaggle competitions download -c new-york-city-taxi-fare-prediction"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.10 / client 1.5.4)\n",
            "Downloading test.csv to /content\n",
            "  0% 0.00/960k [00:00<?, ?B/s]\n",
            "100% 960k/960k [00:00<00:00, 65.1MB/s]\n",
            "Downloading train.csv.zip to /content\n",
            "100% 1.56G/1.56G [00:15<00:00, 119MB/s]\n",
            "\n",
            "Downloading sample_submission.csv to /content\n",
            "  0% 0.00/335k [00:00<?, ?B/s]\n",
            "100% 335k/335k [00:00<00:00, 87.7MB/s]\n",
            "Downloading GCP-Coupons-Instructions.rtf to /content\n",
            "  0% 0.00/486 [00:00<?, ?B/s]\n",
            "100% 486/486 [00:00<00:00, 473kB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-0Pe1i4Z2R_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c094623-78bf-497c-8c8c-b1f69e0be149"
      },
      "source": [
        "!pip install pyGPGO"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyGPGO\n",
            "  Downloading https://files.pythonhosted.org/packages/8d/87/a113c91ba014708114f7635d5c0f6a5e5c773480c5f0a537b257a02d180d/pyGPGO-0.4.0.dev1.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pyGPGO) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from pyGPGO) (1.4.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from pyGPGO) (1.0.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from pyGPGO) (0.22.2.post1)\n",
            "Requirement already satisfied: theano in /usr/local/lib/python3.6/dist-packages (from pyGPGO) (1.0.5)\n",
            "Requirement already satisfied: pyMC3 in /usr/local/lib/python3.6/dist-packages (from pyGPGO) (3.7)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from theano->pyGPGO) (1.15.0)\n",
            "Requirement already satisfied: tqdm>=4.8.4 in /usr/local/lib/python3.6/dist-packages (from pyMC3->pyGPGO) (4.41.1)\n",
            "Requirement already satisfied: h5py>=2.7.0 in /usr/local/lib/python3.6/dist-packages (from pyMC3->pyGPGO) (2.10.0)\n",
            "Requirement already satisfied: pandas>=0.18.0 in /usr/local/lib/python3.6/dist-packages (from pyMC3->pyGPGO) (1.1.5)\n",
            "Requirement already satisfied: patsy>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from pyMC3->pyGPGO) (0.5.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.18.0->pyMC3->pyGPGO) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.18.0->pyMC3->pyGPGO) (2018.9)\n",
            "Building wheels for collected packages: pyGPGO\n",
            "  Building wheel for pyGPGO (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyGPGO: filename=pyGPGO-0.4.0.dev1-cp36-none-any.whl size=19867 sha256=35e16ae51f13afa976a10d1f85e3e3ccd06e35ed14192422b54460c5fb46cef9\n",
            "  Stored in directory: /root/.cache/pip/wheels/d1/27/04/c4fa3bfe194d36e3cd51542132f43415a6813114a5e8301acb\n",
            "Successfully built pyGPGO\n",
            "Installing collected packages: pyGPGO\n",
            "Successfully installed pyGPGO-0.4.0.dev1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7zDTf1naBsH"
      },
      "source": [
        "# Load some default Python modules:\n",
        "\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "import pandas as pd\n",
        "import pymc3 as pm\n",
        "import matplotlib.pyplot as plt\n",
        "import xgboost as xgb\n",
        "import time\n",
        "\n",
        "from matplotlib.pyplot import rc\n",
        "rc('font',**{'family':'sans-serif','sans-serif':['Helvetica']})\n",
        "rc('text', usetex=False)\n",
        "### % matplotlib inline\n",
        "import seaborn as sns\n",
        "plt.style.use('seaborn-whitegrid')\n",
        "\n",
        "from collections import OrderedDict\n",
        "from joblib import Parallel, delayed\n",
        "from numpy.linalg import slogdet, inv, cholesky, solve\n",
        "from scipy.optimize import minimize\n",
        "from scipy.spatial.distance import cdist\n",
        "from scipy.special import gamma\n",
        "from scipy.stats import norm, t\n",
        "from joblib import Parallel, delayed\n",
        "\n",
        "from pyGPGO.logger import EventLogger\n",
        "from pyGPGO.GPGO import GPGO\n",
        "from pyGPGO.surrogates.GaussianProcess import GaussianProcess\n",
        "from pyGPGO.surrogates.tStudentProcess import tStudentProcess\n",
        "from pyGPGO.surrogates.tStudentProcess import logpdf\n",
        "from pyGPGO.acquisition import Acquisition\n",
        "from pyGPGO.covfunc import squaredExponential\n",
        "from sklearn.model_selection import cross_val_score, train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from xgboost import XGBRegressor\n",
        "from pandas_datareader import data\n",
        "\n",
        "import warnings\n",
        "import random\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXicekJhaE0P"
      },
      "source": [
        "# Read data in pandas dataframe:\n",
        "\n",
        "df_train =  pd.read_csv('/content/train.csv.zip', nrows = 1_000_000, parse_dates=[\"pickup_datetime\"])"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQ0mDzt_cBmw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "028bc15f-e90e-4fbc-c3d5-ff3b81fd78fe"
      },
      "source": [
        "# List first rows:\n",
        "\n",
        "df_train.head()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>key</th>\n",
              "      <th>fare_amount</th>\n",
              "      <th>pickup_datetime</th>\n",
              "      <th>pickup_longitude</th>\n",
              "      <th>pickup_latitude</th>\n",
              "      <th>dropoff_longitude</th>\n",
              "      <th>dropoff_latitude</th>\n",
              "      <th>passenger_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2009-06-15 17:26:21.0000001</td>\n",
              "      <td>4.5</td>\n",
              "      <td>2009-06-15 17:26:21+00:00</td>\n",
              "      <td>-73.844311</td>\n",
              "      <td>40.721319</td>\n",
              "      <td>-73.841610</td>\n",
              "      <td>40.712278</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2010-01-05 16:52:16.0000002</td>\n",
              "      <td>16.9</td>\n",
              "      <td>2010-01-05 16:52:16+00:00</td>\n",
              "      <td>-74.016048</td>\n",
              "      <td>40.711303</td>\n",
              "      <td>-73.979268</td>\n",
              "      <td>40.782004</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2011-08-18 00:35:00.00000049</td>\n",
              "      <td>5.7</td>\n",
              "      <td>2011-08-18 00:35:00+00:00</td>\n",
              "      <td>-73.982738</td>\n",
              "      <td>40.761270</td>\n",
              "      <td>-73.991242</td>\n",
              "      <td>40.750562</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2012-04-21 04:30:42.0000001</td>\n",
              "      <td>7.7</td>\n",
              "      <td>2012-04-21 04:30:42+00:00</td>\n",
              "      <td>-73.987130</td>\n",
              "      <td>40.733143</td>\n",
              "      <td>-73.991567</td>\n",
              "      <td>40.758092</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2010-03-09 07:51:00.000000135</td>\n",
              "      <td>5.3</td>\n",
              "      <td>2010-03-09 07:51:00+00:00</td>\n",
              "      <td>-73.968095</td>\n",
              "      <td>40.768008</td>\n",
              "      <td>-73.956655</td>\n",
              "      <td>40.783762</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                             key  ...  passenger_count\n",
              "0    2009-06-15 17:26:21.0000001  ...                1\n",
              "1    2010-01-05 16:52:16.0000002  ...                1\n",
              "2   2011-08-18 00:35:00.00000049  ...                2\n",
              "3    2012-04-21 04:30:42.0000001  ...                1\n",
              "4  2010-03-09 07:51:00.000000135  ...                1\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9fZujMycFMo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "839acdb9-16a7-4492-91dd-c503022645f8"
      },
      "source": [
        "# Format 'pickup_datetime' variable:\n",
        "\n",
        "df_train['pickup_datetime'] =  pd.to_datetime(df_train['pickup_datetime'], utc=True, format='%Y-%m-%d %H:%M')\n",
        "df_train['pickup_datetime'].head()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0   2009-06-15 17:26:21+00:00\n",
              "1   2010-01-05 16:52:16+00:00\n",
              "2   2011-08-18 00:35:00+00:00\n",
              "3   2012-04-21 04:30:42+00:00\n",
              "4   2010-03-09 07:51:00+00:00\n",
              "Name: pickup_datetime, dtype: datetime64[ns, UTC]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nReKu62HcVFI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "abe76645-978f-49bc-f02c-1bdaba70ba77"
      },
      "source": [
        "df_train.sort_values(by = 'pickup_datetime').tail() ### June 2015 the final month"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>key</th>\n",
              "      <th>fare_amount</th>\n",
              "      <th>pickup_datetime</th>\n",
              "      <th>pickup_longitude</th>\n",
              "      <th>pickup_latitude</th>\n",
              "      <th>dropoff_longitude</th>\n",
              "      <th>dropoff_latitude</th>\n",
              "      <th>passenger_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>286276</th>\n",
              "      <td>2015-06-30 23:38:21.0000003</td>\n",
              "      <td>26.5</td>\n",
              "      <td>2015-06-30 23:38:21+00:00</td>\n",
              "      <td>-74.008385</td>\n",
              "      <td>40.711571</td>\n",
              "      <td>-73.884071</td>\n",
              "      <td>40.737385</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>955575</th>\n",
              "      <td>2015-06-30 23:45:57.0000003</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2015-06-30 23:45:57+00:00</td>\n",
              "      <td>-74.002342</td>\n",
              "      <td>40.739819</td>\n",
              "      <td>-74.005829</td>\n",
              "      <td>40.745239</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>915826</th>\n",
              "      <td>2015-06-30 23:48:35.0000005</td>\n",
              "      <td>30.5</td>\n",
              "      <td>2015-06-30 23:48:35+00:00</td>\n",
              "      <td>-73.983826</td>\n",
              "      <td>40.729546</td>\n",
              "      <td>-73.927917</td>\n",
              "      <td>40.661186</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>751350</th>\n",
              "      <td>2015-06-30 23:53:23.0000002</td>\n",
              "      <td>3.5</td>\n",
              "      <td>2015-06-30 23:53:23+00:00</td>\n",
              "      <td>-73.978020</td>\n",
              "      <td>40.757439</td>\n",
              "      <td>-73.980705</td>\n",
              "      <td>40.753544</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>785182</th>\n",
              "      <td>2015-06-30 23:53:49.0000003</td>\n",
              "      <td>7.5</td>\n",
              "      <td>2015-06-30 23:53:49+00:00</td>\n",
              "      <td>-73.959969</td>\n",
              "      <td>40.762405</td>\n",
              "      <td>-73.953064</td>\n",
              "      <td>40.782688</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                key  ...  passenger_count\n",
              "286276  2015-06-30 23:38:21.0000003  ...                5\n",
              "955575  2015-06-30 23:45:57.0000003  ...                1\n",
              "915826  2015-06-30 23:48:35.0000005  ...                2\n",
              "751350  2015-06-30 23:53:23.0000002  ...                1\n",
              "785182  2015-06-30 23:53:49.0000003  ...                1\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W9j9LnIfcXcX"
      },
      "source": [
        "# Add time variables:\n",
        "\n",
        "df_train['hour'] = df_train['pickup_datetime'].dt.hour\n",
        "df_train['weekday'] = df_train['pickup_datetime'].dt.weekday\n",
        "df_train['month'] = df_train['pickup_datetime'].dt.month\n",
        "df_train['year'] = df_train['pickup_datetime'].dt.year"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVyFZIVIcaj3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "69307e78-76c0-4222-f087-707ba137cfb4"
      },
      "source": [
        "df_train = df_train.drop(['pickup_datetime','key'], axis = 1)\n",
        "df_train.head()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>fare_amount</th>\n",
              "      <th>pickup_longitude</th>\n",
              "      <th>pickup_latitude</th>\n",
              "      <th>dropoff_longitude</th>\n",
              "      <th>dropoff_latitude</th>\n",
              "      <th>passenger_count</th>\n",
              "      <th>hour</th>\n",
              "      <th>weekday</th>\n",
              "      <th>month</th>\n",
              "      <th>year</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4.5</td>\n",
              "      <td>-73.844311</td>\n",
              "      <td>40.721319</td>\n",
              "      <td>-73.841610</td>\n",
              "      <td>40.712278</td>\n",
              "      <td>1</td>\n",
              "      <td>17</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>2009</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>16.9</td>\n",
              "      <td>-74.016048</td>\n",
              "      <td>40.711303</td>\n",
              "      <td>-73.979268</td>\n",
              "      <td>40.782004</td>\n",
              "      <td>1</td>\n",
              "      <td>16</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2010</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5.7</td>\n",
              "      <td>-73.982738</td>\n",
              "      <td>40.761270</td>\n",
              "      <td>-73.991242</td>\n",
              "      <td>40.750562</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>2011</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>7.7</td>\n",
              "      <td>-73.987130</td>\n",
              "      <td>40.733143</td>\n",
              "      <td>-73.991567</td>\n",
              "      <td>40.758092</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>2012</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5.3</td>\n",
              "      <td>-73.968095</td>\n",
              "      <td>40.768008</td>\n",
              "      <td>-73.956655</td>\n",
              "      <td>40.783762</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2010</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   fare_amount  pickup_longitude  pickup_latitude  ...  weekday  month  year\n",
              "0          4.5        -73.844311        40.721319  ...        0      6  2009\n",
              "1         16.9        -74.016048        40.711303  ...        1      1  2010\n",
              "2          5.7        -73.982738        40.761270  ...        3      8  2011\n",
              "3          7.7        -73.987130        40.733143  ...        5      4  2012\n",
              "4          5.3        -73.968095        40.768008  ...        1      3  2010\n",
              "\n",
              "[5 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVfm-KSqcdVY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4f02561-4e14-4ec3-ff67-117386bf1be1"
      },
      "source": [
        "# Remove negative fares and postive outliers:\n",
        "\n",
        "df_train = df_train[df_train.fare_amount>=0]\n",
        "df_train = df_train[df_train.fare_amount<=60]\n",
        "print('New size: %d' % len(df_train))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "New size: 997297\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTVDAD2KchTv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f24c925-9207-4d52-d77e-1fbfd72cab70"
      },
      "source": [
        "# Remove missing data:\n",
        "\n",
        "df_train = df_train.dropna(how = 'any', axis = 'rows')\n",
        "print('New size: %d' % len(df_train))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "New size: 997288\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUYksJ2cclVQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01a6a562-cd55-44cc-c598-a5b83228ffd1"
      },
      "source": [
        "# June 2015 NYC taxi data (Wu et al, 2017):\n",
        "\n",
        "df_train = df_train[df_train.month==6]\n",
        "df_train = df_train[df_train.year==2015]\n",
        "print('New size: %d' % len(df_train))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "New size: 11269\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXgSHPyYcnuv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "outputId": "586bb532-1e30-4bfa-d995-3e4140951f96"
      },
      "source": [
        "# Histogram fare plot:\n",
        "\n",
        "df_train[df_train.fare_amount<60].fare_amount.hist(bins=100, figsize=(16,5), color = \"red\")\n",
        "plt.xlabel('$USD')\n",
        "plt.title('NYC Taxi Fares: Dataset');\n",
        "plt.grid(b=None)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1152x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9TMSdAAjcr4o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1d7880b-988a-4fd1-dee4-df1b28ecc009"
      },
      "source": [
        "y = df_train.fare_amount.values + 1e-10\n",
        "y ### for supervised learning: output vector y"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([22.54,  8.  , 34.  , ...,  4.5 ,  6.5 ,  7.  ])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FOeHvi3cu1n",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "c0bd993b-ae65-4fd5-fdc2-15ad7a9f8293"
      },
      "source": [
        "# List first rows (post-cleaning):\n",
        "\n",
        "df_train.head()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>fare_amount</th>\n",
              "      <th>pickup_longitude</th>\n",
              "      <th>pickup_latitude</th>\n",
              "      <th>dropoff_longitude</th>\n",
              "      <th>dropoff_latitude</th>\n",
              "      <th>passenger_count</th>\n",
              "      <th>hour</th>\n",
              "      <th>weekday</th>\n",
              "      <th>month</th>\n",
              "      <th>year</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>22.54</td>\n",
              "      <td>-74.010483</td>\n",
              "      <td>40.717667</td>\n",
              "      <td>-73.985771</td>\n",
              "      <td>40.660366</td>\n",
              "      <td>1</td>\n",
              "      <td>21</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>310</th>\n",
              "      <td>8.00</td>\n",
              "      <td>-74.010727</td>\n",
              "      <td>40.710091</td>\n",
              "      <td>-73.998100</td>\n",
              "      <td>40.722900</td>\n",
              "      <td>1</td>\n",
              "      <td>9</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>314</th>\n",
              "      <td>34.00</td>\n",
              "      <td>-73.974899</td>\n",
              "      <td>40.751095</td>\n",
              "      <td>-73.908546</td>\n",
              "      <td>40.881878</td>\n",
              "      <td>0</td>\n",
              "      <td>23</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>321</th>\n",
              "      <td>8.00</td>\n",
              "      <td>-73.961784</td>\n",
              "      <td>40.759579</td>\n",
              "      <td>-73.978943</td>\n",
              "      <td>40.772606</td>\n",
              "      <td>4</td>\n",
              "      <td>21</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>486</th>\n",
              "      <td>11.50</td>\n",
              "      <td>-73.957443</td>\n",
              "      <td>40.761703</td>\n",
              "      <td>-73.973236</td>\n",
              "      <td>40.787079</td>\n",
              "      <td>1</td>\n",
              "      <td>19</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     fare_amount  pickup_longitude  pickup_latitude  ...  weekday  month  year\n",
              "31         22.54        -74.010483        40.717667  ...        6      6  2015\n",
              "310         8.00        -74.010727        40.710091  ...        5      6  2015\n",
              "314        34.00        -73.974899        40.751095  ...        1      6  2015\n",
              "321         8.00        -73.961784        40.759579  ...        0      6  2015\n",
              "486        11.50        -73.957443        40.761703  ...        0      6  2015\n",
              "\n",
              "[5 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-lT9BBicw4P",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "a0678444-45d7-4303-cc29-8ce93593d7f8"
      },
      "source": [
        "X = df_train.drop(['fare_amount', 'month', 'year'], axis = 1)\n",
        "X.head() ### for supervised learning: input matrix X"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pickup_longitude</th>\n",
              "      <th>pickup_latitude</th>\n",
              "      <th>dropoff_longitude</th>\n",
              "      <th>dropoff_latitude</th>\n",
              "      <th>passenger_count</th>\n",
              "      <th>hour</th>\n",
              "      <th>weekday</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>-74.010483</td>\n",
              "      <td>40.717667</td>\n",
              "      <td>-73.985771</td>\n",
              "      <td>40.660366</td>\n",
              "      <td>1</td>\n",
              "      <td>21</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>310</th>\n",
              "      <td>-74.010727</td>\n",
              "      <td>40.710091</td>\n",
              "      <td>-73.998100</td>\n",
              "      <td>40.722900</td>\n",
              "      <td>1</td>\n",
              "      <td>9</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>314</th>\n",
              "      <td>-73.974899</td>\n",
              "      <td>40.751095</td>\n",
              "      <td>-73.908546</td>\n",
              "      <td>40.881878</td>\n",
              "      <td>0</td>\n",
              "      <td>23</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>321</th>\n",
              "      <td>-73.961784</td>\n",
              "      <td>40.759579</td>\n",
              "      <td>-73.978943</td>\n",
              "      <td>40.772606</td>\n",
              "      <td>4</td>\n",
              "      <td>21</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>486</th>\n",
              "      <td>-73.957443</td>\n",
              "      <td>40.761703</td>\n",
              "      <td>-73.973236</td>\n",
              "      <td>40.787079</td>\n",
              "      <td>1</td>\n",
              "      <td>19</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     pickup_longitude  pickup_latitude  ...  hour  weekday\n",
              "31         -74.010483        40.717667  ...    21        6\n",
              "310        -74.010727        40.710091  ...     9        5\n",
              "314        -73.974899        40.751095  ...    23        1\n",
              "321        -73.961784        40.759579  ...    21        0\n",
              "486        -73.957443        40.761703  ...    19        0\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2eC8SDPzczNY"
      },
      "source": [
        "### Optimum rmse: regression model objective function is Root Mean Square Error (RMSE); \n",
        "### Should be minimized (as close to zero as possible):\n",
        "\n",
        "y_global_orig = 0"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2ngnRxbc7cg"
      },
      "source": [
        "### Objective function:\n",
        "\n",
        "if obj_func == 'XGBoost': # 6-D\n",
        "            \n",
        "    # Constraints:\n",
        "    param_lb_alpha = 0\n",
        "    param_ub_alpha = 10\n",
        "    \n",
        "    param_lb_gamma = 0\n",
        "    param_ub_gamma = 10\n",
        "    \n",
        "    param_lb_max_depth = 5\n",
        "    param_ub_max_depth = 15\n",
        "    \n",
        "    param_lb_min_child_weight = 1\n",
        "    param_ub_min_child_weight = 20\n",
        "    \n",
        "    param_lb_subsample = .5\n",
        "    param_ub_subsample = 1\n",
        "    \n",
        "    param_lb_colsample = .1\n",
        "    param_ub_colsample = 1\n",
        "    \n",
        "    # 6-D inputs' parameter bounds:\n",
        "    param = { 'alpha':  ('cont', (param_lb_alpha, param_ub_alpha)),\n",
        "         'gamma':  ('cont', (param_lb_gamma, param_ub_gamma)),     \n",
        "         'max_depth':  ('int', (param_lb_max_depth, param_ub_max_depth)),\n",
        "         'subsample':  ('cont', (param_lb_subsample, param_ub_subsample)),\n",
        "          'min_child_weight':  ('int', (param_lb_min_child_weight, param_ub_min_child_weight)),\n",
        "            'colsample': ('cont', (param_lb_colsample, param_ub_colsample))\n",
        "        }\n",
        "       \n",
        "    # True y bounds:\n",
        "    dim = 6\n",
        "    \n",
        "    max_iter = 20  # iterations of Bayesian optimization\n",
        "    \n",
        "    operator = 1 \n",
        "    \n",
        "    n_est = 3"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GoTmWEhSc1qQ"
      },
      "source": [
        "### Bayesian Optimization - inputs:\n",
        "\n",
        "obj_func = 'XGBoost'\n",
        "n_start_AcqFunc = 100\n",
        "n_test = n_start_AcqFunc # test points\n",
        "df = 3 # nu\n",
        "\n",
        "util_loser = 'dEI_GP'\n",
        "util_winner = 'dCBM_STP'\n",
        "\n",
        "v=1 \n",
        "delta=.1\n",
        "Beta_CBM = v * (2 * np.log((max_iter**(dim/2 + 2) * (np.pi**2))/(3 * delta)))\n",
        "\n",
        "n_init = 5 # random initialisations\n",
        "\n",
        "test_perc = 0.15\n",
        "train_perc = 1 - test_perc\n",
        "\n",
        "n_test = int(len(df_train) * test_perc)\n",
        "n_train = int(len(df_train) - n_test)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmJsNX29c_xA"
      },
      "source": [
        "### Derivatives - Squared-exponential covariance function:\n",
        "\n",
        "def l2norm_(X, Xstar):\n",
        "    \n",
        "    return cdist(X, Xstar)\n",
        "\n",
        "def kronDelta(X, Xstar):\n",
        "\n",
        "    return cdist(X, Xstar) < np.finfo(np.float32).eps\n",
        "\n",
        "class squaredExponentialDeriv(squaredExponential):\n",
        "    \n",
        "    def K(self, X, Xstar):\n",
        "        \n",
        "        r = (l2norm_(X, Xstar)/self.l)\n",
        "        K = self.sigmaf * np.exp(-1/2*r **2) + self.sigman * kronDelta(X, Xstar)\n",
        "        return K\n",
        "    \n",
        "    def dK(self, X, Xstar):\n",
        "        \n",
        "        r = (l2norm_(X, Xstar)/self.l)\n",
        "        dK = self.sigmaf/self.l**2 * np.exp(-1/2 * r **2) * l2norm_(X, Xstar)\n",
        "        return dK\n",
        "    \n",
        "        \n",
        "    def d2K(self, X, Xstar):\n",
        "        \n",
        "        r = (l2norm_(X, Xstar)/self.l)\n",
        "        d2K = self.sigmaf/self.l**2 * np.exp(-1/2 * r **2) * (1 - r **2)\n",
        "        return d2K\n",
        "    \n",
        "cov_func = squaredExponentialDeriv()\n",
        "d_cov_func = squaredExponentialDeriv()\n",
        "    "
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9ZuEB2VdE0W"
      },
      "source": [
        "### Set-seeds:\n",
        "\n",
        "run_num_1 = 111\n",
        "run_num_2 = 113\n",
        "run_num_3 = 3333\n",
        "run_num_4 = 4444\n",
        "run_num_5 = 5555\n",
        "run_num_6 = 6\n",
        "run_num_7 = 7777\n",
        "run_num_8 = 8878\n",
        "run_num_9 = 999\n",
        "run_num_10 = 1000\n",
        "run_num_11 = 1113\n",
        "run_num_12 = 1234\n",
        "run_num_13 = 234\n",
        "run_num_14 = 888\n",
        "run_num_15 = 1557\n",
        "run_num_16 = 1666\n",
        "run_num_17 = 71\n",
        "run_num_18 = 8\n",
        "run_num_19 = 1999\n",
        "run_num_20 = 2000"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XgHMFEyPdCk4"
      },
      "source": [
        "### Cumulative Regret Calculator:\n",
        "\n",
        "def min_max_array(x):\n",
        "    new_list = []\n",
        "    for i, num in enumerate(x):\n",
        "            new_list.append(np.min(x[0:i+1]))\n",
        "    return new_list"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJMhL70fdHz_"
      },
      "source": [
        "class Acquisition_new(Acquisition):    \n",
        "    def __init__(self, mode, eps=1e-08, **params):\n",
        "        \n",
        "        self.params = params\n",
        "        self.eps = eps\n",
        "\n",
        "        mode_dict = {\n",
        "            'dEI_GP': self.dEI_GP,\n",
        "            'dCBM_STP': self.dCBM_STP\n",
        "        }\n",
        "\n",
        "        self.f = mode_dict[mode]\n",
        "    \n",
        "    def dEI_GP(self, tau, mean, std, ds, dm, dvdv, d2v, d2m):\n",
        "        z = -1 * (tau - mean - self.eps) / (std + self.eps)\n",
        "        \n",
        "        dsdx = ds / 2 * (std + self.eps)\n",
        "        d2sdx = -dsdx**2 / ((std + self.eps)) - dvdv / (std + self.eps) - d2v / (std + self.eps)\n",
        "        dmdx = (dm - z * dsdx) / (std + self.eps)\n",
        "        d2mdx = (d2m - (z * d2sdx + 2 * dmdx * dsdx)) / (std + self.eps)\n",
        "        \n",
        "        f = (std + self.eps) * (z * norm.cdf(z) + norm.pdf(z)[0])\n",
        "        df = (f / (std + self.eps) * dsdx + (std + self.eps) * norm.cdf(z) * dmdx)\n",
        "        d2f = (f / (std + self.eps) * d2sdx + dsdx * dmdx * norm.cdf(z) \\\n",
        "            + d2mdx * (std + self.eps) * norm.cdf(z) + dsdx * norm.cdf(z) * dmdx \\\n",
        "            + norm.pdf(z)[0] * (std + self.eps) * dmdx)\n",
        "            \n",
        "        return f, df, d2f\n",
        "\n",
        "    def dCBM_STP(self, tau, mean, std, ds, dm, dvdv, d2v, d2m, nu=3.0):\n",
        "    \n",
        "        gamma = -1 * (y_global_orig - mean - self.eps) / (std + self.eps)\n",
        "        dsdx = ds / 2 * (std + self.eps)\n",
        "        dmdx = (dm - gamma * dsdx) / (std + self.eps)\n",
        "\n",
        "        f = (std + self.eps) * (gamma + np.sqrt(Beta_CBM))\n",
        "        df = dsdx * (gamma + np.sqrt(Beta_CBM)) + (std + self.eps) * (dmdx + np.sqrt(Beta_CBM))\n",
        "        return f, df\n",
        "    \n",
        "    def _eval(self, tau, mean, std):\n",
        "    \n",
        "        return self.f(tau, mean, std, **self.params)\n",
        "    \n",
        "    def d_eval(self, tau, mean, std, ds, dm, dvdv, d2v, d2m):\n",
        "    \n",
        "        return self.f(tau, mean, std, ds, dm, dvdv, d2v, d2m, **self.params)\n",
        "\n",
        "    def d_eval_stp(self, tau, mean, std, ds, dm, dvdv, d2v, d2m, nu=3.0):\n",
        "    \n",
        "        return self.f(tau, mean, std, ds, dm, dvdv, d2v, d2m, nu=3.0, **self.params)\n"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "comHkJv9dH_O"
      },
      "source": [
        "### Surrogate derivatives: \n",
        "\n",
        "from scipy.linalg import cholesky, solve\n",
        "\n",
        "class dGaussianProcess(GaussianProcess):\n",
        "    l = 1\n",
        "    sigmaf = 1e-4\n",
        "    sigman = 1e-6\n",
        "\n",
        "    def AcqGrad(self, Xstar, return_std=False):\n",
        "        r_X = l2norm_(self.X, self.X)/self.l\n",
        "        K = self.sigmaf * np.exp(-1/2*r_X **2) + self.sigman * kronDelta(self.X, self.X)\n",
        "        L = cholesky(K).T\n",
        "        alpha = solve(L.T, solve(L, self.y))\n",
        "        Xstar = np.atleast_2d(Xstar)\n",
        "        Kstar = squaredExponentialDeriv.K(self, self.X, Xstar).T\n",
        "        dKstar = squaredExponentialDeriv.dK(self, self.X, Xstar).T\n",
        "        d2Kstar = squaredExponentialDeriv.d2K(self, self.X, Xstar).T\n",
        "        v = solve(self.L, Kstar.T)\n",
        "        dv = solve(self.L, dKstar.T)\n",
        "        d2v = solve(self.L, d2Kstar.T)\n",
        "        \n",
        "        ds = -2 * np.dot(dv.T, v)\n",
        "        dvdv = np.dot(dv.T, dv)\n",
        "        d2s = -2 * (dvdv + d2v)\n",
        "        \n",
        "        dm = np.dot(dKstar, alpha)\n",
        "        d2m = np.dot(d2Kstar, alpha)\n",
        "        return ds, dm, dvdv, d2v, d2m\n",
        "\n",
        "class dtStudentProcess(tStudentProcess):\n",
        "    l = 1\n",
        "    sigmaf = 1e-4\n",
        "    sigman = 1e-6\n",
        "    \n",
        "    def AcqGrad(self, Xstar, return_std=False):\n",
        "        r_X = l2norm_(self.X, self.X)/self.l\n",
        "        K = self.sigmaf * np.exp(-1/2*r_X **2) + self.sigman * kronDelta(self.X, self.X)\n",
        "        L = cholesky(self.K11).T\n",
        "        alpha = solve(L.T, solve(L, self.y))\n",
        "        Xstar = np.atleast_2d(Xstar)\n",
        "        Kstar = squaredExponentialDeriv.K(self, self.X, Xstar).T\n",
        "        dKstar = squaredExponentialDeriv.dK(self, self.X, Xstar).T\n",
        "        d2Kstar = squaredExponentialDeriv.d2K(self, self.X, Xstar).T\n",
        "        v = solve(L, Kstar.T)\n",
        "        dv = solve(L, dKstar.T)\n",
        "        d2v = solve(L, d2Kstar.T)\n",
        "        \n",
        "        ds = -2 * np.dot(dv.T, v)\n",
        "        dvdv = np.dot(dv.T, dv)\n",
        "        d2s = -2 * (dvdv + d2v)\n",
        "        \n",
        "        dm = np.dot(dKstar, alpha)\n",
        "        d2m = np.dot(d2Kstar, alpha)\n",
        "        return ds, dm, dvdv, d2v, d2m"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S422jNLsdIMm"
      },
      "source": [
        "class dGPGO(GPGO):  \n",
        "    n_start = n_start_AcqFunc\n",
        "    eps = 1e-08\n",
        "        \n",
        "    def func(self, xnew):\n",
        "        new_mean, new_var = self.GP.predict(xnew, return_std=True)\n",
        "        new_std = np.sqrt(new_var + 1e-6)\n",
        "        ds, dm, dvdv, d2v, d2m = self.GP.AcqGrad(xnew, return_std=True)\n",
        "        f  = np.empty((self.n_start,))\n",
        "        df = np.empty((self.n_start,))\n",
        "        f  = -self.A.d_eval(self.tau, new_mean, new_std, ds=ds, dm=dm, dvdv=dvdv, d2v=d2v, d2m=d2m)[0]\n",
        "        df = -self.A.d_eval(self.tau, new_mean, new_std, ds=ds, dm=dm, dvdv=dvdv, d2v=d2v, d2m=d2m)[1]\n",
        "        df_array = np.full((len(xnew),),df)\n",
        "        return f, df_array\n",
        "        \n",
        "    def d_optimizeAcq(self, method='L-BFGS-B', n_start=n_start):\n",
        "        start_points_dict = [self._sampleParam() for i in range(n_start)]\n",
        "        start_points_arr = np.array([list(s.values())\n",
        "                                     for s in start_points_dict])\n",
        "        x_best = np.empty((n_start, len(self.parameter_key)))\n",
        "        f_best = np.empty((n_start,))\n",
        "        opt = Parallel(n_jobs=self.n_jobs)(delayed(minimize)(self.func,\n",
        "                                                                 x0=start_point,\n",
        "                                                                 method=method,\n",
        "                                                                 jac = True,\n",
        "                                                                 bounds=self.parameter_range) for start_point in\n",
        "                                               start_points_arr)\n",
        "        x_best = np.array([res.x for res in opt])\n",
        "        f_best = np.array([np.atleast_1d(res.fun)[0] for res in opt])\n",
        "\n",
        "        self.x_best = x_best\n",
        "        self.f_best = f_best\n",
        "        self.best = x_best[np.argmin(f_best)]\n",
        "    \n",
        "    def run(self, max_iter=10, init_evals=3, resume=False):\n",
        "        \n",
        "        if not resume:\n",
        "            self.init_evals = init_evals\n",
        "            self._firstRun(self.init_evals)\n",
        "            self.logger._printInit(self)\n",
        "        for iteration in range(max_iter):\n",
        "            self.d_optimizeAcq()\n",
        "            self.updateGP()\n",
        "            self.logger._printCurrent(self)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwEwZD0qdIPO"
      },
      "source": [
        "## dGPGO_stp - BayesOpt derivatives' class: Student's-t\n",
        "\n",
        "class dGPGO_stp(GPGO):  \n",
        "    n_start = 100\n",
        "        \n",
        "    def func_stp(self, xnew):\n",
        "        new_mean, new_var = self.GP.predict(xnew, return_std=True)\n",
        "        new_std = np.sqrt(new_var + 1e-6)\n",
        "        ds, dm, dvdv, d2v, d2m = self.GP.AcqGrad(xnew, return_std=True)\n",
        "        f  = np.empty((self.n_start,))\n",
        "        df = np.empty((self.n_start,))\n",
        "        f  = -self.A.d_eval_stp(self.tau, new_mean, new_std, ds=ds, dm=dm, dvdv=dvdv, d2v=d2v, d2m=d2m, nu=df)[0]\n",
        "        df = -self.A.d_eval_stp(self.tau, new_mean, new_std, ds=ds, dm=dm, dvdv=dvdv, d2v=d2v, d2m=d2m, nu=df)[1]\n",
        "        df_array = np.full((len(xnew),),df)\n",
        "        return f, df_array\n",
        "        \n",
        "    def d_optimizeAcq_stp(self, method='L-BFGS-B', n_start=n_start):\n",
        "        start_points_dict = [self._sampleParam() for i in range(n_start)]\n",
        "        start_points_arr = np.array([list(s.values())\n",
        "                                     for s in start_points_dict])\n",
        "        x_best = np.empty((n_start, len(self.parameter_key)))\n",
        "        f_best = np.empty((n_start,))\n",
        "        opt = Parallel(n_jobs=self.n_jobs)(delayed(minimize)(self.func_stp,\n",
        "                                                                 x0=start_point,\n",
        "                                                                 method=method,\n",
        "                                                                 jac = True,\n",
        "                                                                 bounds=self.parameter_range) for start_point in\n",
        "                                               start_points_arr)\n",
        "        x_best = np.array([res.x for res in opt])\n",
        "        f_best = np.array([np.atleast_1d(res.fun)[0] for res in opt])\n",
        "\n",
        "        self.x_best = x_best\n",
        "        self.f_best = f_best\n",
        "        self.best = x_best[np.argmin(f_best)]\n",
        "    \n",
        "    def run(self, max_iter=10, init_evals=3, resume=False):\n",
        "        \n",
        "        if not resume:\n",
        "            self.init_evals = init_evals\n",
        "            self._firstRun(self.init_evals)\n",
        "            self.logger._printInit(self)\n",
        "        for iteration in range(max_iter):\n",
        "            self.d_optimizeAcq_stp()\n",
        "            self.updateGP()\n",
        "            self.logger._printCurrent(self)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HlilveEgdIR_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffa31d4f-48e9-4d30-9d2d-1d5f769a0fe5"
      },
      "source": [
        "start_lose = time.time()\n",
        "start_lose"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1613475048.4325247"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wlzDSHbUG-c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60b27f1a-f604-4020-f0af-d8d873ddaac1"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 1\n",
        "\n",
        "np.random.seed(run_num_1)\n",
        "surrogate_loser_1 = dGaussianProcess(cov_func)\n",
        "\n",
        "X_train1, X_test1, y_train1, y_test1 = train_test_split(X, y, test_size=test_perc, random_state=run_num_1)\n",
        "\n",
        "def f_syn_polarity1(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_1, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train1, y=y_train1).mean())\n",
        "    return operator * score\n",
        "\n",
        "loser_1 = dGPGO(surrogate_loser_1, Acquisition_new(util_loser), f_syn_polarity1, param, n_jobs = -1) # define BayesOpt\n",
        "loser_1.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_loser_1 = loser_1.getResult()[0]\n",
        "params_loser_1['max_depth'] = int(params_loser_1['max_depth'])\n",
        "params_loser_1['min_child_weight'] = int(params_loser_1['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_loser_train1 = xgb.DMatrix(X_train1, y_train1)\n",
        "dX_loser_test1 = xgb.DMatrix(X_test1, y_test1)\n",
        "model_loser_1 = xgb.train(params_loser_1, dX_loser_train1)\n",
        "pred_loser_1 = model_loser_1.predict(dX_loser_test1)\n",
        "\n",
        "rmse_loser_1 = np.sqrt(mean_squared_error(pred_loser_1, y_test1))\n",
        "rmse_loser_1"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 6.12170176  1.69069754 11.          0.92695323  3.          0.36579277]. \t  -0.5875926438135084 \t -0.45245711231879204\n",
            "init   \t [ 1.49162957  0.22478325  6.          0.85613654 15.          0.19363322]. \t  -0.6811472770543097 \t -0.45245711231879204\n",
            "init   \t [ 5.27957805  2.81819356 11.          0.81062146 14.          0.74826845]. \t  -0.46117277972616416 \t -0.45245711231879204\n",
            "init   \t [3.81060005 6.24236569 7.         0.95038709 5.         0.9861745 ]. \t  -0.45245711231879204 \t -0.45245711231879204\n",
            "init   \t [4.77531134 7.87990376 6.         0.78863691 6.         0.47918609]. \t  -0.5612893767574513 \t -0.45245711231879204\n",
            "1      \t [0.  0.  5.  0.5 1.  0.1]. \t  -0.6826322236836013 \t -0.45245711231879204\n",
            "2      \t [ 9.60774341  8.94642175  5.          0.60295867 15.          0.76784922]. \t  -0.5001571385184761 \t -0.45245711231879204\n",
            "3      \t [ 0.4810761   9.31563532  8.          0.92845546 14.          0.70014982]. \t  -0.5056468669267833 \t -0.45245711231879204\n",
            "4      \t [ 9.85849625  0.83439907  7.          0.7705582  19.          0.15487995]. \t  -0.6815452883308839 \t -0.45245711231879204\n",
            "5      \t [ 9.1348132   9.54667443 13.          0.95557113 18.          0.11016711]. \t  -0.6791252728059957 \t -0.45245711231879204\n",
            "6      \t [ 9.97122318  8.75901621 13.          0.9904996   9.          0.60161775]. \t  -0.4895864123043516 \t -0.45245711231879204\n",
            "7      \t [7.19079936 0.87565629 5.         0.54470564 8.         0.4449021 ]. \t  -0.5736147518216788 \t -0.45245711231879204\n",
            "8      \t [ 5.24989998  8.90398408 13.          0.54165135  1.          0.74771929]. \t  -0.46141851807096074 \t -0.45245711231879204\n",
            "9      \t [ 1.7157918   5.89141512 14.          0.5296881  10.          0.1803042 ]. \t  -0.6825568503758982 \t -0.45245711231879204\n",
            "10     \t [ 1.33491429  4.9488224  14.          0.57825982 18.          0.42075517]. \t  -0.5921600742966667 \t -0.45245711231879204\n",
            "11     \t [8.99095022 4.91206514 6.         0.87212346 1.         0.53198502]. \t  -0.5638457176592562 \t -0.45245711231879204\n",
            "12     \t [0.51753753 1.68511981 9.         0.80360039 9.         0.45200858]. \t  -0.5426731188677099 \t -0.45245711231879204\n",
            "13     \t [ 0.47819106  0.69361363 14.          0.7271902   3.          0.48615024]. \t  -0.5388370338312534 \t -0.45245711231879204\n",
            "14     \t [ 3.39786838  7.15897283  5.          0.7138544  19.          0.22047835]. \t  -0.6821252760743777 \t -0.45245711231879204\n",
            "15     \t [0.4939583  9.37390762 8.         0.7641092  1.         0.4917045 ]. \t  -0.5481894484993145 \t -0.45245711231879204\n",
            "16     \t [ 8.70209222  0.21363429 14.          0.98391211 18.          0.50653374]. \t  -0.5341942124085062 \t -0.45245711231879204\n",
            "17     \t [ 5.30693639  9.58195731  9.          0.53796245 11.          0.46099251]. \t  -0.5549909911648074 \t -0.45245711231879204\n",
            "18     \t [ 3.83524595  9.61883757 12.          0.81445794 17.          0.17700713]. \t  -0.6797728963073484 \t -0.45245711231879204\n",
            "19     \t [ 0.25234021  8.87254669 14.          0.7101496   5.          0.12345742]. \t  -0.684133223230854 \t -0.45245711231879204\n",
            "20     \t [9.81917647 9.19103216 5.         0.82619792 9.         0.10869763]. \t  -0.6822789922076578 \t -0.45245711231879204\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.87018129255363"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ClJ9rN2KUJzy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "facd2008-fe09-4945-e71a-ebd55d4c9a38"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 2\n",
        "\n",
        "np.random.seed(run_num_2)\n",
        "surrogate_loser_2 = dGaussianProcess(cov_func)\n",
        "\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X, y, test_size=test_perc, random_state=run_num_2)\n",
        "\n",
        "def f_syn_polarity2(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_2, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train2, y=y_train2).mean())\n",
        "    return operator * score\n",
        "\n",
        "loser_2 = dGPGO(surrogate_loser_2, Acquisition_new(util_loser), f_syn_polarity2, param, n_jobs = -1) # define BayesOpt\n",
        "loser_2.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_loser_2 = loser_2.getResult()[0]\n",
        "params_loser_2['max_depth'] = int(params_loser_2['max_depth'])\n",
        "params_loser_2['min_child_weight'] = int(params_loser_2['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_loser_train2 = xgb.DMatrix(X_train2, y_train2)\n",
        "dX_loser_test2 = xgb.DMatrix(X_test2, y_test2)\n",
        "model_loser_2 = xgb.train(params_loser_2, dX_loser_train2)\n",
        "pred_loser_2 = model_loser_2.predict(dX_loser_test2)\n",
        "\n",
        "rmse_loser_2 = np.sqrt(mean_squared_error(pred_loser_2, y_test2))\n",
        "rmse_loser_2"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 8.51985493  0.73903599  7.          0.71824677 17.          0.61827209]. \t  -0.5233872966482411 \t -0.42848969661986275\n",
            "init   \t [ 8.4047092   4.35120551 12.          0.84231905 16.          0.80172484]. \t  -0.42848969661986275 \t -0.42848969661986275\n",
            "init   \t [ 6.42749368  9.61026168 10.          0.73363448 15.          0.84932207]. \t  -0.43419399571353007 \t -0.42848969661986275\n",
            "init   \t [ 2.66008363  8.36685389  6.          0.92082163 12.          0.69999264]. \t  -0.5353675801921922 \t -0.42848969661986275\n",
            "init   \t [ 1.74779478  2.17691512 12.          0.71794562  2.          0.48990646]. \t  -0.5346620615324694 \t -0.42848969661986275\n",
            "1      \t [0.65899411 0.08911785 5.         0.57513718 8.         0.6649358 ]. \t  -0.5435492302872315 \t -0.42848969661986275\n",
            "2      \t [7.50154303 9.49874736 7.         0.83842364 4.         0.5714185 ]. \t  -0.5422231033003287 \t -0.42848969661986275\n",
            "3      \t [9.88263606 1.71181353 7.         0.97275193 1.         0.74825108]. \t  -0.441747506473179 \t -0.42848969661986275\n",
            "4      \t [ 0.50045611  0.9041258  14.          0.89701685 13.          0.41057236]. \t  -0.5651338432083423 \t -0.42848969661986275\n",
            "5      \t [ 8.92213252  4.56062357 13.          0.93869871  8.          0.53052713]. \t  -0.5289605254310144 \t -0.42848969661986275\n",
            "6      \t [ 3.60181415  8.33097203 14.          0.9882562   5.          0.89449689]. \t  \u001b[92m-0.4137972346428497\u001b[0m \t -0.4137972346428497\n",
            "7      \t [ 0.11546318  7.06183776 14.          0.68323086 19.          0.99255453]. \t  -0.42239160139562504 \t -0.4137972346428497\n",
            "8      \t [0.  0.  5.  0.5 1.  0.1]. \t  -0.6860882831213788 \t -0.4137972346428497\n",
            "9      \t [ 0.10082071  2.20765876  6.          0.79154439 17.          0.83321591]. \t  -0.45448270216379194 \t -0.4137972346428497\n",
            "10     \t [0.32359376 9.19341132 6.         0.74153554 1.         0.25113354]. \t  -0.6853813278526151 \t -0.4137972346428497\n",
            "11     \t [ 9.07851132  6.67468648 14.          0.65555017  1.          0.96610958]. \t  -0.424975755228691 \t -0.4137972346428497\n",
            "12     \t [ 9.80974078  5.67295504  6.          0.91127307 13.          0.44957804]. \t  -0.5591662642332185 \t -0.4137972346428497\n",
            "13     \t [ 2.77198081  0.67245678 14.          0.56883806 19.          0.9553332 ]. \t  -0.4333861030323646 \t -0.4137972346428497\n",
            "14     \t [6.86928577 3.72419603 7.         0.55484489 8.         0.11419186]. \t  -0.6867308079831548 \t -0.4137972346428497\n",
            "15     \t [ 0.83926825  9.31286624 13.          0.83595717 11.          0.92691071]. \t  -0.41668725948491847 \t -0.4137972346428497\n",
            "16     \t [ 2.2599948   8.5777314   7.          0.92101066 18.          0.81278145]. \t  -0.44458847008023056 \t -0.4137972346428497\n",
            "17     \t [ 7.48756016  0.13555104 14.          0.75269236 12.          0.41177876]. \t  -0.5678014017572689 \t -0.4137972346428497\n",
            "18     \t [2.94548912 4.25555055 5.         0.68007425 4.         0.7533029 ]. \t  -0.45906401928062657 \t -0.4137972346428497\n",
            "19     \t [ 8.06744935  9.40296538  6.          0.60147647 19.          0.17678037]. \t  -0.6864884735391246 \t -0.4137972346428497\n",
            "20     \t [ 3.87077387  0.04333662 11.          0.79311966  8.          0.7797114 ]. \t  -0.423652004810958 \t -0.4137972346428497\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.343487868410211"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-45l3NU4UNiI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a5f91f5-870b-470a-fa67-0386e5038028"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 3\n",
        "\n",
        "np.random.seed(run_num_3)\n",
        "surrogate_loser_3 = dGaussianProcess(cov_func)\n",
        "\n",
        "X_train3, X_test3, y_train3, y_test3 = train_test_split(X, y, test_size=test_perc, random_state=run_num_3)\n",
        "\n",
        "def f_syn_polarity3(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_3, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train3, y=y_train3).mean())\n",
        "    return operator * score\n",
        "\n",
        "loser_3 = dGPGO(surrogate_loser_3, Acquisition_new(util_loser), f_syn_polarity3, param, n_jobs = -1) # define BayesOpt\n",
        "loser_3.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_loser_3 = loser_3.getResult()[0]\n",
        "params_loser_3['max_depth'] = int(params_loser_3['max_depth'])\n",
        "params_loser_3['min_child_weight'] = int(params_loser_3['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_loser_train3 = xgb.DMatrix(X_train3, y_train3)\n",
        "dX_loser_test3 = xgb.DMatrix(X_test3, y_test3)\n",
        "model_loser_3 = xgb.train(params_loser_3, dX_loser_train3)\n",
        "pred_loser_3 = model_loser_3.predict(dX_loser_test3)\n",
        "\n",
        "rmse_loser_3 = np.sqrt(mean_squared_error(pred_loser_3, y_test3))\n",
        "rmse_loser_3"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 7.51575607  1.09251275 13.          0.80377147 16.          0.31229624]. \t  -0.6858957949546655 \t -0.5081673732303724\n",
            "init   \t [6.12794891 1.19652403 5.         0.74822174 8.         0.33695838]. \t  -0.6849895205987827 \t -0.5081673732303724\n",
            "init   \t [ 6.21631223  4.4909976  10.          0.66764529  9.          0.77207323]. \t  -0.5081673732303724 \t -0.5081673732303724\n",
            "init   \t [ 0.58127233  4.79918477 11.          0.64526552 15.          0.62431773]. \t  -0.5106501747238601 \t -0.5081673732303724\n",
            "init   \t [ 5.76228258  0.67219724  7.          0.88228691 13.          0.29867219]. \t  -0.6839256266327581 \t -0.5081673732303724\n",
            "1      \t [9.79994489 9.57810833 8.         0.93763368 1.         0.1772549 ]. \t  -0.7316297465421655 \t -0.5081673732303724\n",
            "2      \t [ 3.27075931  5.07207678 14.          0.99587284  2.          0.51564135]. \t  -0.6620507374994331 \t -0.5081673732303724\n",
            "3      \t [0.  0.  5.  0.5 1.  0.1]. \t  -0.7323904210578609 \t -0.5081673732303724\n",
            "4      \t [ 8.88346709  9.03427619  8.          0.525814   19.          0.22866902]. \t  -0.732406939137863 \t -0.5081673732303724\n",
            "5      \t [1.21637479 8.34226514 5.         0.57939054 5.         0.55479603]. \t  -0.6531491980452168 \t -0.5081673732303724\n",
            "6      \t [ 2.93278685  9.15897164 14.          0.67343746 10.          0.85376406]. \t  \u001b[92m-0.504811636744922\u001b[0m \t -0.504811636744922\n",
            "7      \t [ 1.1995358   9.87930749  5.          0.84448568 18.          0.26022822]. \t  -0.7297224033867351 \t -0.504811636744922\n",
            "8      \t [ 1.96525323  0.16048396 14.          0.65133015  9.          0.3003508 ]. \t  -0.6860931128073492 \t -0.504811636744922\n",
            "9      \t [6.19566593 0.54575662 9.         0.53247577 1.         0.58149275]. \t  -0.5171483404397559 \t -0.504811636744922\n",
            "10     \t [ 3.97093318  9.58939295  6.          0.5381492  12.          0.74760437]. \t  -0.5217629748454513 \t -0.504811636744922\n",
            "11     \t [ 9.94719165  9.26136843 13.          0.64663072 11.          0.47018827]. \t  -0.6546346492088932 \t -0.504811636744922\n",
            "12     \t [9.55919323 7.66342182 6.         0.63144136 7.         0.82664124]. \t  -0.5195220027397864 \t -0.504811636744922\n",
            "13     \t [ 2.80079796  0.61761702  5.          0.62606225 19.          0.71334647]. \t  -0.5274647167409906 \t -0.504811636744922\n",
            "14     \t [ 9.59599852  3.11576364  5.          0.59256945 17.          0.56621848]. \t  -0.6538676177721936 \t -0.504811636744922\n",
            "15     \t [ 8.91533927  0.16704192 13.          0.66647549  6.          0.25014   ]. \t  -0.7314177593008601 \t -0.504811636744922\n",
            "16     \t [ 4.33253274  9.14516752 14.          0.94398901 19.          0.74581708]. \t  \u001b[92m-0.5007721901039625\u001b[0m \t -0.5007721901039625\n",
            "17     \t [9.75857066 2.84550968 5.         0.7181157  3.         0.67923616]. \t  -0.5274197984747595 \t -0.5007721901039625\n",
            "18     \t [ 0.          0.          5.          0.5        10.44582355  0.1       ]. \t  -0.7323784626096901 \t -0.5007721901039625\n",
            "19     \t [ 9.94497871  9.34276152 14.          0.66308198  5.          0.66061477]. \t  -0.5102482930096841 \t -0.5007721901039625\n",
            "20     \t [ 0.45956381  8.89338741 11.          0.67604128  4.          0.20043998]. \t  -0.7307540586635348 \t -0.5007721901039625\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.6249009343932785"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voPfk1UDUQU0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ceec47ea-880f-49e4-d822-32b0c6074616"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 4\n",
        "\n",
        "np.random.seed(run_num_4)\n",
        "surrogate_loser_4 = dGaussianProcess(cov_func)\n",
        "\n",
        "X_train4, X_test4, y_train4, y_test4 = train_test_split(X, y, test_size=test_perc, random_state=run_num_4)\n",
        "\n",
        "def f_syn_polarity4(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_4, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train4, y=y_train4).mean())\n",
        "    return operator * score\n",
        "\n",
        "loser_4 = dGPGO(surrogate_loser_4, Acquisition_new(util_loser), f_syn_polarity4, param, n_jobs = -1) # define BayesOpt\n",
        "loser_4.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_loser_4 = loser_4.getResult()[0]\n",
        "params_loser_4['max_depth'] = int(params_loser_4['max_depth'])\n",
        "params_loser_4['min_child_weight'] = int(params_loser_4['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_loser_train4 = xgb.DMatrix(X_train4, y_train4)\n",
        "dX_loser_test4 = xgb.DMatrix(X_test4, y_test4)\n",
        "model_loser_4 = xgb.train(params_loser_4, dX_loser_train4)\n",
        "pred_loser_4 = model_loser_4.predict(dX_loser_test4)\n",
        "\n",
        "rmse_loser_4 = np.sqrt(mean_squared_error(pred_loser_4, y_test4))\n",
        "rmse_loser_4"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 9.42798506  1.23319667 14.          0.81910755  7.          0.7021549 ]. \t  -0.5581090891563811 \t -0.5581090891563811\n",
            "init   \t [ 5.54158317  7.71233962 10.          0.59117644  6.          0.31476458]. \t  -0.6204181120564479 \t -0.5581090891563811\n",
            "init   \t [2.18282634 2.93794127 7.         0.84739327 5.         0.62716229]. \t  -0.5626656925564042 \t -0.5581090891563811\n",
            "init   \t [ 5.66159581  4.68678814 11.          0.54147808  9.          0.58609221]. \t  -0.5595885941852147 \t -0.5581090891563811\n",
            "init   \t [ 3.02160958  6.07432614  6.          0.92038559 14.          0.12778609]. \t  -0.6361772582153176 \t -0.5581090891563811\n",
            "1      \t [ 1.89460697  0.10324856 13.          0.70356349 16.          0.67776197]. \t  \u001b[92m-0.5568243993303088\u001b[0m \t -0.5568243993303088\n",
            "2      \t [ 9.69969     4.32607677 14.          0.87739158 16.          0.66014279]. \t  -0.5573098211239071 \t -0.5568243993303088\n",
            "3      \t [ 2.2210328   8.31054196 13.          0.70317243 19.          0.56953182]. \t  -0.573504425404327 \t -0.5568243993303088\n",
            "4      \t [ 0.14706294  0.75702217 14.          0.56309891  8.          0.29887971]. \t  -0.6254087880261816 \t -0.5568243993303088\n",
            "5      \t [ 6.94218243  0.32144048  6.          0.59286079 19.          0.70636825]. \t  -0.5706574487524134 \t -0.5568243993303088\n",
            "6      \t [ 9.24159749  9.43575685  7.          0.98380459 19.          0.34937346]. \t  -0.6187589419936593 \t -0.5568243993303088\n",
            "7      \t [7.57184449 0.0324327  9.         0.86318723 1.         0.6413689 ]. \t  -0.5611723012436254 \t -0.5568243993303088\n",
            "8      \t [ 1.62755426  6.7921099  14.          0.91903975  1.          0.61791343]. \t  -0.5680865264437001 \t -0.5568243993303088\n",
            "9      \t [ 9.43074486  0.98044049  5.          0.96290856 10.          0.27397556]. \t  -0.6390681992417737 \t -0.5568243993303088\n",
            "10     \t [ 8.73128599  9.22658128 14.          0.98344346  2.          0.13489277]. \t  -0.6368164613179 \t -0.5568243993303088\n",
            "11     \t [ 0.          0.          7.37720743  0.5        12.37720743  0.1       ]. \t  -0.6353530029272869 \t -0.5568243993303088\n",
            "12     \t [8.91398595 7.00476796 6.         0.72156795 1.         0.41673037]. \t  -0.6210807206819127 \t -0.5568243993303088\n",
            "13     \t [ 9.58851274  9.75423983 14.          0.60815214 12.          0.97572229]. \t  \u001b[92m-0.44045254560081915\u001b[0m \t -0.44045254560081915\n",
            "14     \t [ 9.87497198  6.68701692  6.          0.55980787 13.          0.32605223]. \t  -0.6200123103666009 \t -0.44045254560081915\n",
            "15     \t [ 2.16464782  9.35692886 11.          0.97431128 12.          0.43257419]. \t  -0.5747879420555002 \t -0.44045254560081915\n",
            "16     \t [0.  0.  5.  0.5 1.  0.1]. \t  -0.6351353738423114 \t -0.44045254560081915\n",
            "17     \t [0.94208981 7.15934626 8.47738305 0.5        1.47738305 0.1       ]. \t  -0.6346215545948299 \t -0.44045254560081915\n",
            "18     \t [7.36355448 5.3077281  5.         0.5709887  7.         0.87569884]. \t  -0.467633753337906 \t -0.44045254560081915\n",
            "19     \t [ 7.93069008  0.11830338 11.          0.97110397 13.          0.22887984]. \t  -0.6356672520315936 \t -0.44045254560081915\n",
            "20     \t [ 6.39787933  3.84205635 14.          0.72880814  3.          0.21974221]. \t  -0.6364956155454153 \t -0.44045254560081915\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.469425637772456"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kEnTd7MUdlv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a50bdf24-e706-4387-aacb-f663bb0ab4f6"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 5\n",
        "\n",
        "np.random.seed(run_num_5)\n",
        "surrogate_loser_5 = dGaussianProcess(cov_func)\n",
        "\n",
        "X_train5, X_test5, y_train5, y_test5 = train_test_split(X, y, test_size=test_perc, random_state=run_num_5)\n",
        "\n",
        "def f_syn_polarity5(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_5, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train5, y=y_train5).mean())\n",
        "    return operator * score\n",
        "\n",
        "loser_5 = dGPGO(surrogate_loser_5, Acquisition_new(util_loser), f_syn_polarity5, param, n_jobs = -1) # define BayesOpt\n",
        "loser_5.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_loser_5 = loser_5.getResult()[0]\n",
        "params_loser_5['max_depth'] = int(params_loser_5['max_depth'])\n",
        "params_loser_5['min_child_weight'] = int(params_loser_5['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_loser_train5 = xgb.DMatrix(X_train5, y_train5)\n",
        "dX_loser_test5 = xgb.DMatrix(X_test5, y_test5)\n",
        "model_loser_5 = xgb.train(params_loser_5, dX_loser_train5)\n",
        "pred_loser_5 = model_loser_5.predict(dX_loser_test5)\n",
        "\n",
        "rmse_loser_5 = np.sqrt(mean_squared_error(pred_loser_5, y_test5))\n",
        "rmse_loser_5"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 5.57353274  5.41760627  8.          0.53710825 19.          0.26034376]. \t  -0.6523797617249512 \t -0.466901759574539\n",
            "init   \t [5.17826437 9.78817462 5.         0.84048437 6.         0.79712615]. \t  -0.4886603987288415 \t -0.466901759574539\n",
            "init   \t [ 9.84476201  9.65484856  5.          0.59464714 19.          0.34528747]. \t  -0.564220001422124 \t -0.466901759574539\n",
            "init   \t [ 9.17476842  4.53262239 13.          0.64368565 10.          0.515308  ]. \t  -0.5049042831858346 \t -0.466901759574539\n",
            "init   \t [ 8.66719465  7.99685372 12.          0.79833571 17.          0.72443466]. \t  -0.466901759574539 \t -0.466901759574539\n",
            "1      \t [ 3.31480734  6.81602635 12.          0.95330947  1.          0.27334927]. \t  -0.6531167190113543 \t -0.466901759574539\n",
            "2      \t [2.53140694 0.29740057 9.         0.7505917  7.         0.82287404]. \t  -0.47475119282304395 \t -0.466901759574539\n",
            "3      \t [ 9.84728748  0.28222034 13.          0.93969018  3.          0.22339517]. \t  -0.6522247903981228 \t -0.466901759574539\n",
            "4      \t [ 0.68116737  3.94603776 14.          0.80380753 14.          0.77317996]. \t  \u001b[92m-0.4613270217842209\u001b[0m \t -0.4613270217842209\n",
            "5      \t [ 9.6928838   0.38736829  6.          0.56167539 11.          0.4775038 ]. \t  -0.5239028011959734 \t -0.4613270217842209\n",
            "6      \t [0.         2.96817906 5.         0.5        1.         0.1       ]. \t  -0.6542193437616992 \t -0.4613270217842209\n",
            "7      \t [ 0.96088206  9.84240251  9.          0.79592881 15.          0.83460449]. \t  -0.47215351911272185 \t -0.4613270217842209\n",
            "8      \t [ 9.56204849  0.80573312 12.          0.60476955 16.          0.23612747]. \t  -0.6524022297735614 \t -0.4613270217842209\n",
            "9      \t [ 1.05461775  1.17661042  5.          0.5648338  17.          0.90649901]. \t  -0.4702337390488589 \t -0.4613270217842209\n",
            "10     \t [ 1.28998978  8.90442798 10.          0.78625314  8.          0.13448748]. \t  -0.6545570207229267 \t -0.4613270217842209\n",
            "11     \t [9.66604659 1.51730123 5.         0.59597224 4.         0.66643448]. \t  -0.5016316198301449 \t -0.4613270217842209\n",
            "12     \t [ 2.6635752   5.58712964  6.          0.93254798 12.          0.28064449]. \t  -0.6521391482286414 \t -0.4613270217842209\n",
            "13     \t [ 2.76670882  9.36479927 14.          0.77451901 19.          0.10649887]. \t  -0.6539377811460048 \t -0.4613270217842209\n",
            "14     \t [0.02835933 9.74346527 7.         0.54472382 1.         0.83619676]. \t  -0.48645443650554226 \t -0.4613270217842209\n",
            "15     \t [ 9.08851157  7.7443441   6.          0.74314397 12.          0.52508293]. \t  -0.5238407378055661 \t -0.4613270217842209\n",
            "16     \t [ 0.62266475  1.9018111  14.          0.63329124  3.          0.94054341]. \t  \u001b[92m-0.42529491633232686\u001b[0m \t -0.42529491633232686\n",
            "17     \t [ 9.23494676  7.58606552 12.          0.78487568  2.          0.96478853]. \t  -0.42939677763278594 \t -0.42529491633232686\n",
            "18     \t [6.8698644  5.44523052 7.         0.65534714 1.         0.29639138]. \t  -0.5598703662523957 \t -0.42529491633232686\n",
            "19     \t [ 0.          0.          5.          0.5        10.21001817  0.1       ]. \t  -0.6545380212108272 \t -0.42529491633232686\n",
            "20     \t [ 8.32286546  0.75756777  6.          0.69885376 18.          0.98511179]. \t  -0.46068165397024996 \t -0.42529491633232686\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.31722690080873"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjVSH6caUgyy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7095e10d-c05e-45b5-b859-36f1d9a3541b"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 6\n",
        "\n",
        "np.random.seed(run_num_6)\n",
        "surrogate_loser_6 = dGaussianProcess(cov_func)\n",
        "\n",
        "X_train6, X_test6, y_train6, y_test6 = train_test_split(X, y, test_size=test_perc, random_state=run_num_6)\n",
        "\n",
        "def f_syn_polarity6(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_6, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train6, y=y_train6).mean())\n",
        "    return operator * score\n",
        "\n",
        "loser_6 = dGPGO(surrogate_loser_6, Acquisition_new(util_loser), f_syn_polarity6, param, n_jobs = -1) # define BayesOpt\n",
        "loser_6.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_loser_6 = loser_6.getResult()[0]\n",
        "params_loser_6['max_depth'] = int(params_loser_6['max_depth'])\n",
        "params_loser_6['min_child_weight'] = int(params_loser_6['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_loser_train6 = xgb.DMatrix(X_train6, y_train6)\n",
        "dX_loser_test6 = xgb.DMatrix(X_test6, y_test6)\n",
        "model_loser_6 = xgb.train(params_loser_6, dX_loser_train6)\n",
        "pred_loser_6 = model_loser_6.predict(dX_loser_test6)\n",
        "\n",
        "rmse_loser_6 = np.sqrt(mean_squared_error(pred_loser_6, y_test6))\n",
        "rmse_loser_6"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [8.92860151 3.31979805 5.         0.99251441 2.         0.57683563]. \t  -0.5471181110640053 \t -0.5219867521874206\n",
            "init   \t [4.18807429 3.35407849 9.         0.87750649 3.         0.56623277]. \t  -0.629286701398377 \t -0.5219867521874206\n",
            "init   \t [ 5.788586    6.45355096 14.          0.70660047 12.          0.82154882]. \t  -0.5219867521874206 \t -0.5219867521874206\n",
            "init   \t [4.58184578 6.73834679 5.         0.90108528 3.         0.65482895]. \t  -0.5476166257785141 \t -0.5219867521874206\n",
            "init   \t [ 4.42510505  5.75952352 14.          0.97882365 15.          0.29525604]. \t  -0.6655867879161768 \t -0.5219867521874206\n",
            "1      \t [ 7.09628606  0.82458553  6.          0.52978945 17.          0.78827589]. \t  -0.5368551114917748 \t -0.5219867521874206\n",
            "2      \t [ 1.88885215  8.62524025  6.          0.90916563 14.          0.40379086]. \t  -0.6680072912825885 \t -0.5219867521874206\n",
            "3      \t [ 0.5654966   9.52584762 14.          0.82016688  4.          0.10717254]. \t  -0.7389555129670606 \t -0.5219867521874206\n",
            "4      \t [ 9.63033822  0.03793409 13.          0.68076302  7.          0.91391681]. \t  \u001b[92m-0.4301920669254681\u001b[0m \t -0.4301920669254681\n",
            "5      \t [ 9.97469028  9.15579642 12.          0.66745021 19.          0.28959158]. \t  -0.6668248751122167 \t -0.4301920669254681\n",
            "6      \t [ 9.85880187  8.48475629 13.          0.76006171  2.          0.58751173]. \t  -0.5403277210407594 \t -0.4301920669254681\n",
            "7      \t [ 9.51844     9.62786679  5.          0.56989798 15.          0.51068013]. \t  -0.6345288616058514 \t -0.4301920669254681\n",
            "8      \t [ 0.15151883  3.04875397  5.          0.61495161 19.          0.79420245]. \t  -0.5357132607573185 \t -0.4301920669254681\n",
            "9      \t [ 3.08077333  0.89569015  6.          0.84149914 11.          0.94992477]. \t  -0.4472711154368675 \t -0.4301920669254681\n",
            "10     \t [ 0.19816257  1.95434851 14.          0.59292238  7.          0.17971262]. \t  -0.7383261448869979 \t -0.4301920669254681\n",
            "11     \t [ 0.56219975  9.42928268 11.          0.51903033 19.          0.52214278]. \t  -0.6326946461691906 \t -0.4301920669254681\n",
            "12     \t [9.58176304 9.94093363 6.         0.96280567 8.         0.32855082]. \t  -0.6677341620025121 \t -0.4301920669254681\n",
            "13     \t [ 5.43877488  0.17504551 13.          0.58721411 19.          0.60115325]. \t  -0.5401916158723044 \t -0.4301920669254681\n",
            "14     \t [0.  0.  5.  0.5 1.  0.1]. \t  -0.7382709735940614 \t -0.4301920669254681\n",
            "15     \t [9.18163606 3.89469538 8.         0.59772622 8.         0.18901946]. \t  -0.7380580836797348 \t -0.4301920669254681\n",
            "16     \t [0.52725495 9.82259272 6.         0.62080595 7.         0.68770749]. \t  -0.5497290569747493 \t -0.4301920669254681\n",
            "17     \t [ 6.71573436  6.18076517  8.          0.50736725 18.          0.99692636]. \t  -0.4548470613295324 \t -0.4301920669254681\n",
            "18     \t [ 7.23381367  0.21465732 13.          0.75484136 13.          0.44717401]. \t  -0.6306144837329356 \t -0.4301920669254681\n",
            "19     \t [ 0.17525712  4.19183624 13.          0.93093846  1.          0.77496282]. \t  -0.5254340278392874 \t -0.4301920669254681\n",
            "20     \t [ 0.22896089  2.69806987 14.          0.53297649 18.          0.65056758]. \t  -0.5430966302037052 \t -0.4301920669254681\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.1451215046465775"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1WsphKSUj19",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e54a7c73-7c8e-4215-9990-20731367e349"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 7\n",
        "\n",
        "np.random.seed(run_num_7)\n",
        "surrogate_loser_7 = dGaussianProcess(cov_func)\n",
        "\n",
        "X_train7, X_test7, y_train7, y_test7 = train_test_split(X, y, test_size=test_perc, random_state=run_num_7)\n",
        "\n",
        "def f_syn_polarity7(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_7, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train7, y=y_train7).mean())\n",
        "    return operator * score\n",
        "\n",
        "loser_7 = dGPGO(surrogate_loser_7, Acquisition_new(util_loser), f_syn_polarity7, param, n_jobs = -1) # define BayesOpt\n",
        "loser_7.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_loser_7 = loser_7.getResult()[0]\n",
        "params_loser_7['max_depth'] = int(params_loser_7['max_depth'])\n",
        "params_loser_7['min_child_weight'] = int(params_loser_7['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_loser_train7 = xgb.DMatrix(X_train7, y_train7)\n",
        "dX_loser_test7 = xgb.DMatrix(X_test7, y_test7)\n",
        "model_loser_7 = xgb.train(params_loser_7, dX_loser_train7)\n",
        "pred_loser_7 = model_loser_7.predict(dX_loser_test7)\n",
        "\n",
        "rmse_loser_7 = np.sqrt(mean_squared_error(pred_loser_7, y_test7))\n",
        "rmse_loser_7"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 8.11505126  6.92324143 13.          0.94046175 12.          0.3650384 ]. \t  -0.612852517751432 \t -0.4284992540085738\n",
            "init   \t [ 5.15357029  9.0066636   6.          0.74104227 15.          0.8966337 ]. \t  -0.45500673865987984 \t -0.4284992540085738\n",
            "init   \t [ 2.63548913  9.6610934  14.          0.64712408  1.          0.80820948]. \t  -0.43730487111474525 \t -0.4284992540085738\n",
            "init   \t [ 1.40821426  1.72758589 12.          0.70495069  5.          0.86632715]. \t  -0.4284992540085738 \t -0.4284992540085738\n",
            "init   \t [2.85126987 2.73434054 7.         0.88394733 8.         0.65545939]. \t  -0.48253390492139586 \t -0.4284992540085738\n",
            "1      \t [1.03697999e-02 2.51695028e+00 1.40000000e+01 6.19497248e-01\n",
            " 1.90000000e+01 7.01827417e-01]. \t  -0.47758423778274467 \t -0.4284992540085738\n",
            "2      \t [9.9089961  7.50235155 7.         0.67925503 5.         0.2471192 ]. \t  -0.6736712023667494 \t -0.4284992540085738\n",
            "3      \t [ 7.21196079  0.11384945  8.          0.67755731 17.          0.74323626]. \t  -0.4647919397201027 \t -0.4284992540085738\n",
            "4      \t [0.35283087 9.2950614  5.         0.93054416 3.         0.27625531]. \t  -0.670508947897148 \t -0.4284992540085738\n",
            "5      \t [0.  0.  5.  0.5 1.  0.1]. \t  -0.6745888440506937 \t -0.4284992540085738\n",
            "6      \t [9.76759527 0.48761621 7.         0.96362191 1.         0.137157  ]. \t  -0.6715547907624335 \t -0.4284992540085738\n",
            "7      \t [ 8.53387117  0.35946342 13.          0.65332712  6.          0.68302293]. \t  -0.4712802345283902 \t -0.4284992540085738\n",
            "8      \t [ 0.45155781  2.33209808  5.          0.73660734 17.          0.83890491]. \t  -0.4804814871680856 \t -0.4284992540085738\n",
            "9      \t [ 0.30506512  9.51761383 13.          0.5277377  16.          0.92234588]. \t  -0.4390633386106087 \t -0.4284992540085738\n",
            "10     \t [ 9.21287749  3.65759887  5.          0.85687073 12.          0.88459516]. \t  -0.45867733996321497 \t -0.4284992540085738\n",
            "11     \t [ 1.14642054  8.54710172 12.          0.83923566  8.          0.25317439]. \t  -0.6716592254658202 \t -0.4284992540085738\n",
            "12     \t [ 6.43879816  6.53341304 14.          0.70290258 19.          0.43578856]. \t  -0.5879212739277196 \t -0.4284992540085738\n",
            "13     \t [ 0.28968499  1.98768694 14.          0.97996073 12.          0.15615797]. \t  -0.6725851052423111 \t -0.4284992540085738\n",
            "14     \t [5.4721942  4.70341961 9.         0.675658   1.         0.91263074]. \t  -0.44024324260844283 \t -0.4284992540085738\n",
            "15     \t [ 7.56572451  6.67046024 14.          0.88731871  4.          0.29940932]. \t  -0.6117537685654454 \t -0.4284992540085738\n",
            "16     \t [4.54036829 9.23672573 7.         0.78071654 7.         0.925943  ]. \t  -0.44422074369565967 \t -0.4284992540085738\n",
            "17     \t [ 9.52566208  5.8884151   6.          0.78376025 19.          0.40148749]. \t  -0.6258103990077785 \t -0.4284992540085738\n",
            "18     \t [ 7.79919254  0.6285715  14.          0.87477175 15.          0.15632002]. \t  -0.6708453498973665 \t -0.4284992540085738\n",
            "19     \t [ 0.          0.          8.43853294  0.5        11.43853294  0.1       ]. \t  -0.6744471853974358 \t -0.4284992540085738\n",
            "20     \t [ 3.1237334   5.58211355  9.          0.8525378  19.          0.1292964 ]. \t  -0.6703702828267247 \t -0.4284992540085738\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.492050628419268"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hI8sFP4ZUmOs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ccef69b5-9180-4338-a976-39e721cc76e7"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 8\n",
        "\n",
        "np.random.seed(run_num_8)\n",
        "surrogate_loser_8 = dGaussianProcess(cov_func)\n",
        "\n",
        "X_train8, X_test8, y_train8, y_test8 = train_test_split(X, y, test_size=test_perc, random_state=run_num_8)\n",
        "\n",
        "def f_syn_polarity8(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_8, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train8, y=y_train8).mean())\n",
        "    return operator * score\n",
        "\n",
        "loser_8 = dGPGO(surrogate_loser_8, Acquisition_new(util_loser), f_syn_polarity8, param, n_jobs = -1) # define BayesOpt\n",
        "loser_8.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_loser_8 = loser_8.getResult()[0]\n",
        "params_loser_8['max_depth'] = int(params_loser_8['max_depth'])\n",
        "params_loser_8['min_child_weight'] = int(params_loser_8['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_loser_train8 = xgb.DMatrix(X_train8, y_train8)\n",
        "dX_loser_test8 = xgb.DMatrix(X_test8, y_test8)\n",
        "model_loser_8 = xgb.train(params_loser_8, dX_loser_train8)\n",
        "pred_loser_8 = model_loser_8.predict(dX_loser_test8)\n",
        "\n",
        "rmse_loser_8 = np.sqrt(mean_squared_error(pred_loser_8, y_test8))\n",
        "rmse_loser_8"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 1.29221489  0.77396898 11.          0.56062468  3.          0.25451879]. \t  -0.6231794271723858 \t -0.4674926150198253\n",
            "init   \t [ 0.10611429  1.37648061  8.          0.6050375  15.          0.790205  ]. \t  -0.4674926150198253 \t -0.4674926150198253\n",
            "init   \t [ 8.35239571  4.16908043  7.          0.67902746 14.          0.16683749]. \t  -0.6176439249709439 \t -0.4674926150198253\n",
            "init   \t [ 6.06301777  7.27127491 12.          0.87295531 11.          0.37694382]. \t  -0.5951520828762356 \t -0.4674926150198253\n",
            "init   \t [8.85209637 7.89269955 6.         0.72444854 3.         0.47766986]. \t  -0.5688519156580504 \t -0.4674926150198253\n",
            "1      \t [ 1.47825581  9.16010046 11.          0.96606546  1.          0.92358292]. \t  \u001b[92m-0.43518063190798095\u001b[0m \t -0.43518063190798095\n",
            "2      \t [ 7.52273592  2.30141271 14.          0.75505047 19.          0.77804046]. \t  -0.45158957529537797 \t -0.43518063190798095\n",
            "3      \t [1.64420908 9.47353882 5.         0.79993338 7.         0.15673599]. \t  -0.6175838614360663 \t -0.43518063190798095\n",
            "4      \t [ 1.02399475  9.65113789  9.          0.53947972 17.          0.8292164 ]. \t  -0.4719352875928771 \t -0.43518063190798095\n",
            "5      \t [8.14212583 0.61314553 6.         0.86488725 6.         0.28574491]. \t  -0.5954481959734638 \t -0.43518063190798095\n",
            "6      \t [ 8.76387285  7.83054608 13.          0.54159783  4.          0.24710422]. \t  -0.6207082177098945 \t -0.43518063190798095\n",
            "7      \t [ 9.0517509   9.45530828 11.          0.85305105 19.          0.79961763]. \t  -0.4589725747508552 \t -0.43518063190798095\n",
            "8      \t [ 7.77762087  0.97608919 14.          0.56979605  8.          0.73436998]. \t  -0.4483480686509709 \t -0.43518063190798095\n",
            "9      \t [2.67676373 3.95611485 5.         0.79739163 2.         0.89987835]. \t  -0.45674643004535687 \t -0.43518063190798095\n",
            "10     \t [ 0.62854908  4.77207773 14.          0.78182185 18.          0.73284936]. \t  -0.4461225555243119 \t -0.43518063190798095\n",
            "11     \t [3.06290827 1.66002331 6.         0.51411222 9.         0.98529356]. \t  -0.461628022517463 \t -0.43518063190798095\n",
            "12     \t [ 0.55545334  6.5757175   9.          0.88529579 12.          0.24203797]. \t  -0.6187020960077592 \t -0.43518063190798095\n",
            "13     \t [ 9.10053278  1.48753729 11.          0.9741735   1.          0.65977303]. \t  -0.5223704878111852 \t -0.43518063190798095\n",
            "14     \t [ 9.48329103  0.2076913   7.          0.76655383 19.          0.88164364]. \t  -0.4616094756962891 \t -0.43518063190798095\n",
            "15     \t [ 0.81390458  8.42248516 12.          0.78729971  7.          0.71328057]. \t  -0.5176932464043367 \t -0.43518063190798095\n",
            "16     \t [ 5.51535579  8.63063044  6.          0.64760098 18.          0.14185561]. \t  -0.6172074564902792 \t -0.43518063190798095\n",
            "17     \t [ 0.81210087  0.8533868  13.          0.66456412 12.          0.37769434]. \t  -0.5971747957799627 \t -0.43518063190798095\n",
            "18     \t [ 8.74769008  9.46041181  5.          0.76888057 12.          0.28611188]. \t  -0.5996998823178632 \t -0.43518063190798095\n",
            "19     \t [ 6.67918913  1.91467598 12.          0.83062216 13.          0.72788563]. \t  -0.45035357506572193 \t -0.43518063190798095\n",
            "20     \t [ 4.53564763  5.14157265 14.          0.52558575  1.          0.72718907]. \t  -0.45119962298136185 \t -0.43518063190798095\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.711175290813375"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vw5IYus6UpAn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37e960a3-7d76-4ad9-c74a-ea506aa92512"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 9\n",
        "\n",
        "np.random.seed(run_num_9)\n",
        "surrogate_loser_9 = dGaussianProcess(cov_func)\n",
        "\n",
        "X_train9, X_test9, y_train9, y_test9 = train_test_split(X, y, test_size=test_perc, random_state=run_num_9)\n",
        "\n",
        "def f_syn_polarity9(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_9, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train9, y=y_train9).mean())\n",
        "    return operator * score\n",
        "\n",
        "loser_9 = dGPGO(surrogate_loser_9, Acquisition_new(util_loser), f_syn_polarity9, param, n_jobs = -1) # define BayesOpt\n",
        "loser_9.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_loser_9 = loser_9.getResult()[0]\n",
        "params_loser_9['max_depth'] = int(params_loser_9['max_depth'])\n",
        "params_loser_9['min_child_weight'] = int(params_loser_9['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_loser_train9 = xgb.DMatrix(X_train9, y_train9)\n",
        "dX_loser_test9 = xgb.DMatrix(X_test9, y_test9)\n",
        "model_loser_9 = xgb.train(params_loser_9, dX_loser_train9)\n",
        "pred_loser_9 = model_loser_9.predict(dX_loser_test9)\n",
        "\n",
        "rmse_loser_9 = np.sqrt(mean_squared_error(pred_loser_9, y_test9))\n",
        "rmse_loser_9"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 8.0342804   5.27522296 13.          0.60741904 17.          0.15336222]. \t  -0.6890593508433319 \t -0.4886355541929722\n",
            "init   \t [7.56063644 1.79876369 7.         0.84869647 5.         0.55956942]. \t  -0.5532305414240943 \t -0.4886355541929722\n",
            "init   \t [2.43675539 0.20225128 6.         0.51669963 7.         0.4646446 ]. \t  -0.5609155518755553 \t -0.4886355541929722\n",
            "init   \t [ 7.6043547   4.73758376  9.          0.87564624 15.          0.65751286]. \t  -0.4886355541929722 \t -0.4886355541929722\n",
            "init   \t [ 4.70349314  6.71029586  6.          0.97680389 11.          0.41971652]. \t  -0.6548660487332898 \t -0.4886355541929722\n",
            "1      \t [ 0.20910387  7.25545521 14.          0.59407892  3.          0.95410586]. \t  \u001b[92m-0.4312356520914486\u001b[0m \t -0.4312356520914486\n",
            "2      \t [ 2.14892068  9.25577985 14.          0.84828472 11.          0.2793841 ]. \t  -0.6883959263608518 \t -0.4312356520914486\n",
            "3      \t [ 0.37305557  1.02756836 13.          0.57348052 13.          0.68410192]. \t  -0.4836872799099695 \t -0.4312356520914486\n",
            "4      \t [ 7.98252507  8.04957695 12.          0.6367811   3.          0.85775382]. \t  -0.4368210491589304 \t -0.4312356520914486\n",
            "5      \t [ 1.2588195   0.04438454  7.          0.70342864 18.          0.58554745]. \t  -0.4951281025154505 \t -0.4312356520914486\n",
            "6      \t [ 9.17710587  9.71573287  5.          0.9584376  19.          0.78072441]. \t  -0.4682275209845145 \t -0.4312356520914486\n",
            "7      \t [6.11416909 9.54959201 5.         0.91943284 1.         0.10968146]. \t  -0.6901756354808697 \t -0.4312356520914486\n",
            "8      \t [ 0.31270412  9.94818438  8.          0.84397658 19.          0.43974503]. \t  -0.5483333357303245 \t -0.4312356520914486\n",
            "9      \t [ 8.76504058  0.46155195 14.          0.61459938 10.          0.19095511]. \t  -0.6902913672662703 \t -0.4312356520914486\n",
            "10     \t [ 0.77001066  0.48822065 12.          0.89190754  5.          0.37116504]. \t  -0.6599680642282422 \t -0.4312356520914486\n",
            "11     \t [ 0.76412088  5.02527282 14.          0.53794557 19.          0.6779987 ]. \t  -0.4862552398836468 \t -0.4312356520914486\n",
            "12     \t [1.11781249 4.65858905 8.         0.79271199 1.         0.71342306]. \t  -0.4899393997261333 \t -0.4312356520914486\n",
            "13     \t [ 4.60583171  0.06752    13.          0.94036906 18.          0.58293565]. \t  -0.47784115580365294 \t -0.4312356520914486\n",
            "14     \t [6.09359952e-03 8.65908473e+00 6.00000000e+00 6.67169539e-01\n",
            " 6.00000000e+00 4.18236274e-01]. \t  -0.6570878867409317 \t -0.4312356520914486\n",
            "15     \t [ 7.69267031  0.52709121  5.          0.97712342 11.          0.47111142]. \t  -0.5671609865381374 \t -0.4312356520914486\n",
            "16     \t [9.94406911 8.10389604 7.         0.8844164  7.         0.60247845]. \t  -0.4916963401121463 \t -0.4312356520914486\n",
            "17     \t [ 4.9384323   0.37731064 14.          0.9492051   1.          0.8118423 ]. \t  -0.4371623420164621 \t -0.4312356520914486\n",
            "18     \t [0.  0.  5.  0.5 1.  0.1]. \t  -0.6893514282293631 \t -0.4312356520914486\n",
            "19     \t [ 9.68084543  9.58294825 13.          0.63062284  9.          0.21596151]. \t  -0.689113084484204 \t -0.4312356520914486\n",
            "20     \t [ 5.24998729  5.27861916  5.          0.87944065 19.          0.38196712]. \t  -0.656916604582956 \t -0.4312356520914486\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.130636454660636"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YD494io_Ur7V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "edc2ff2c-3f33-45e2-b886-d4bedea8d049"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 10\n",
        "\n",
        "np.random.seed(run_num_10)\n",
        "surrogate_loser_10 = dGaussianProcess(cov_func)\n",
        "\n",
        "X_train10, X_test10, y_train10, y_test10 = train_test_split(X, y, test_size=test_perc, random_state=run_num_10)\n",
        "\n",
        "def f_syn_polarity10(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_10, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train10, y=y_train10).mean())\n",
        "    return operator * score\n",
        "\n",
        "loser_10 = dGPGO(surrogate_loser_10, Acquisition_new(util_loser), f_syn_polarity10, param, n_jobs = -1) # define BayesOpt\n",
        "loser_10.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_loser_10 = loser_10.getResult()[0]\n",
        "params_loser_10['max_depth'] = int(params_loser_10['max_depth'])\n",
        "params_loser_10['min_child_weight'] = int(params_loser_10['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_loser_train10 = xgb.DMatrix(X_train10, y_train10)\n",
        "dX_loser_test10 = xgb.DMatrix(X_test10, y_test10)\n",
        "model_loser_10 = xgb.train(params_loser_10, dX_loser_train10)\n",
        "pred_loser_10 = model_loser_10.predict(dX_loser_test10)\n",
        "\n",
        "rmse_loser_10 = np.sqrt(mean_squared_error(pred_loser_10, y_test10))\n",
        "rmse_loser_10"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 6.53589585  1.15006943  6.          0.93623727 14.          0.13663866]. \t  -0.6929852048256443 \t -0.4483221221388197\n",
            "init   \t [ 3.97194461  2.33132197 14.          0.9533253  19.          0.26403087]. \t  -0.6932702114662359 \t -0.4483221221388197\n",
            "init   \t [ 7.43539415  0.69582081  9.          0.9763222  11.          0.12608349]. \t  -0.6931242497450955 \t -0.4483221221388197\n",
            "init   \t [ 9.82027485  3.39637684  9.          0.78522537 19.          0.87618843]. \t  -0.4483221221388197 \t -0.4483221221388197\n",
            "init   \t [ 0.57576207  5.82646405 12.          0.77704362 18.          0.11865044]. \t  -0.6946096327074635 \t -0.4483221221388197\n",
            "1      \t [ 3.17878299  6.40666671 10.          0.71928645  2.          0.37920433]. \t  -0.6059745513602156 \t -0.4483221221388197\n",
            "2      \t [ 3.37448201  8.10780464 13.          0.50477079 10.          0.914384  ]. \t  \u001b[92m-0.43629727149692393\u001b[0m \t -0.43629727149692393\n",
            "3      \t [0.         0.         5.         0.5        6.20440214 0.1       ]. \t  -0.6921234195705354 \t -0.43629727149692393\n",
            "4      \t [8.35411693 8.91554079 5.         0.73130633 7.         0.67857606]. \t  -0.5771647264709212 \t -0.43629727149692393\n",
            "5      \t [9.65375532 1.28029677 7.         0.79386133 1.         0.88548903]. \t  -0.44750949856169625 \t -0.43629727149692393\n",
            "6      \t [ 5.33344337  9.63007578  7.          0.55704064 18.          0.25018677]. \t  -0.6934805730034364 \t -0.43629727149692393\n",
            "7      \t [ 9.463998    9.23285451 14.          0.59406232  3.          0.52567113]. \t  -0.580720745359468 \t -0.43629727149692393\n",
            "8      \t [ 0.72612997  5.71374346  5.          0.7962252  12.          0.3276676 ]. \t  -0.6111050009653851 \t -0.43629727149692393\n",
            "9      \t [ 0.03680135  0.50694581 14.          0.91124751  8.          0.87026982]. \t  \u001b[92m-0.4215933675272626\u001b[0m \t -0.4215933675272626\n",
            "10     \t [ 9.9457377   8.37767281 13.          0.92820573 13.          0.22203611]. \t  -0.6925765145348073 \t -0.4215933675272626\n",
            "11     \t [ 6.39500048  0.03099442 14.          0.5358351   2.          0.80194525]. \t  -0.5166569861309996 \t -0.4215933675272626\n",
            "12     \t [6.5189698  8.43001454 5.         0.6517671  1.         0.9994397 ]. \t  -0.45630664213640165 \t -0.4215933675272626\n",
            "13     \t [0.50440288 9.75457432 5.         0.8137431  6.         0.15250773]. \t  -0.6941630128319229 \t -0.4215933675272626\n",
            "14     \t [2.58869992 0.21309666 9.         0.79352016 1.         0.74644401]. \t  -0.5123069056709456 \t -0.4215933675272626\n",
            "15     \t [ 0.61215987  1.50253986 10.92723111  0.5        12.92723111  0.1       ]. \t  -0.6934503902545976 \t -0.4215933675272626\n",
            "16     \t [ 6.3510504   9.80953833 14.          0.77195312 18.          0.95584163]. \t  -0.432776560658424 \t -0.4215933675272626\n",
            "17     \t [ 8.82570869  3.65595981 13.          0.7057174   6.          0.34013494]. \t  -0.6037775379099287 \t -0.4215933675272626\n",
            "18     \t [ 5.46843147  5.816847   11.          0.55561922 15.          0.42343459]. \t  -0.6089133208955124 \t -0.4215933675272626\n",
            "19     \t [ 8.52646717  0.12245704 14.          0.80155281 15.          0.7100538 ]. \t  -0.5709119331337676 \t -0.4215933675272626\n",
            "20     \t [5.51569281 2.48923068 5.         0.97468437 6.         0.88188042]. \t  -0.4524796762835521 \t -0.4215933675272626\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.436171669561738"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N03Sq0TvUuhp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c2240f1-0513-4e8e-cba0-08d5f441d62c"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 11\n",
        "\n",
        "np.random.seed(run_num_11)\n",
        "surrogate_loser_11 = dGaussianProcess(cov_func)\n",
        "\n",
        "X_train11, X_test11, y_train11, y_test11 = train_test_split(X, y, test_size=test_perc, random_state=run_num_11)\n",
        "\n",
        "def f_syn_polarity11(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_11, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train11, y=y_train11).mean())\n",
        "    return operator * score\n",
        "\n",
        "loser_11 = dGPGO(surrogate_loser_11, Acquisition_new(util_loser), f_syn_polarity11, param, n_jobs = -1) # define BayesOpt\n",
        "loser_11.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_loser_11 = loser_11.getResult()[0]\n",
        "params_loser_11['max_depth'] = int(params_loser_11['max_depth'])\n",
        "params_loser_11['min_child_weight'] = int(params_loser_11['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_loser_train11 = xgb.DMatrix(X_train11, y_train11)\n",
        "dX_loser_test11 = xgb.DMatrix(X_test11, y_test11)\n",
        "model_loser_11 = xgb.train(params_loser_11, dX_loser_train11)\n",
        "pred_loser_11 = model_loser_11.predict(dX_loser_test11)\n",
        "\n",
        "rmse_loser_11 = np.sqrt(mean_squared_error(pred_loser_11, y_test11))\n",
        "rmse_loser_11"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 9.81035663  8.2124785   9.          0.68487173 15.          0.28207988]. \t  -0.7133787013189323 \t -0.47317045732943896\n",
            "init   \t [ 4.91906771  1.38558332  6.          0.618878   19.          0.10263204]. \t  -0.7112023662943636 \t -0.47317045732943896\n",
            "init   \t [ 5.23426175  1.89971159 13.          0.76313779 17.          0.46597853]. \t  -0.47317045732943896 \t -0.47317045732943896\n",
            "init   \t [ 4.0204056   3.75167577  9.          0.86204818 17.          0.26560056]. \t  -0.7132057194891379 \t -0.47317045732943896\n",
            "init   \t [8.8704459  5.64917815 5.         0.51406171 1.         0.82826731]. \t  -0.48048685248123163 \t -0.47317045732943896\n",
            "1      \t [0.08349406 1.89250335 6.         0.66386636 1.         0.88641723]. \t  \u001b[92m-0.4484239383130805\u001b[0m \t -0.4484239383130805\n",
            "2      \t [ 5.35837408  0.31510164 14.          0.97519815  6.          0.3034681 ]. \t  -0.607905230502132 \t -0.4484239383130805\n",
            "3      \t [ 4.66315557  9.30076713 11.          0.83534598  8.          0.21303816]. \t  -0.7135282416096518 \t -0.4484239383130805\n",
            "4      \t [9.02046973 1.16865322 7.         0.6807515  8.         0.2962635 ]. \t  -0.6068266615066733 \t -0.4484239383130805\n",
            "5      \t [0.79988529 9.85424348 5.         0.5743044  3.         0.67901433]. \t  -0.48209943859810334 \t -0.4484239383130805\n",
            "6      \t [ 0.02057794  7.14089873 13.          0.90855163  1.          0.61823707]. \t  -0.45805892074172583 \t -0.4484239383130805\n",
            "7      \t [ 0.97603689  3.49871064  9.          0.91846837 10.          0.35158251]. \t  -0.6085109103402477 \t -0.4484239383130805\n",
            "8      \t [ 9.27240029  9.57413463 14.          0.58544186  3.          0.73779027]. \t  -0.45267877796490286 \t -0.4484239383130805\n",
            "9      \t [ 0.4027375   9.38447167 12.          0.94595612 15.          0.85310792]. \t  -0.451136347669006 \t -0.4484239383130805\n",
            "10     \t [ 1.3440527   8.97080429  5.          0.80277597 12.          0.63284111]. \t  -0.48349651102568225 \t -0.4484239383130805\n",
            "11     \t [ 7.94225687  3.68352192 13.          0.94902199 11.          0.95471054]. \t  \u001b[92m-0.422771965919715\u001b[0m \t -0.422771965919715\n",
            "12     \t [9.20332934 9.98167579 5.         0.57575366 6.         0.3717594 ]. \t  -0.6046860081027627 \t -0.422771965919715\n",
            "13     \t [5.68758712 2.62033511 9.         0.67614771 3.         0.67632008]. \t  -0.47052528509931824 \t -0.422771965919715\n",
            "14     \t [2.94143112e-03 7.03829535e-01 1.30000000e+01 5.98722168e-01\n",
            " 2.00000000e+00 1.01335355e-01]. \t  -0.7122743790062083 \t -0.422771965919715\n",
            "15     \t [ 0.81561387  9.31451412  5.          0.70594876 19.          0.98284163]. \t  -0.45988817082889427 \t -0.422771965919715\n",
            "16     \t [ 9.8968071   1.89203294 13.          0.7888373   2.          0.82539259]. \t  -0.45294282860990764 \t -0.422771965919715\n",
            "17     \t [ 8.37835592  2.52983026  5.          0.97806207 14.          0.6500905 ]. \t  -0.48098936825483635 \t -0.422771965919715\n",
            "18     \t [1.94243031 0.         5.         0.5        7.86661322 0.1       ]. \t  -0.7118264597599575 \t -0.422771965919715\n",
            "19     \t [ 0.         0.         8.2483413  0.5       14.2483413  0.1      ]. \t  -0.7120633980051373 \t -0.422771965919715\n",
            "20     \t [ 8.98906297  7.01351387 13.          0.77197065 19.          0.47265223]. \t  -0.475345282554151 \t -0.422771965919715\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.29564873555569"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_nP9lQjUztV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec390241-c9ae-468e-fda0-e8ba95ec2ab2"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 12\n",
        "\n",
        "np.random.seed(run_num_12)\n",
        "surrogate_loser_12 = dGaussianProcess(cov_func)\n",
        "\n",
        "X_train12, X_test12, y_train12, y_test12 = train_test_split(X, y, test_size=test_perc, random_state=run_num_12)\n",
        "\n",
        "def f_syn_polarity12(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_12, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train12, y=y_train12).mean())\n",
        "    return operator * score\n",
        "\n",
        "loser_12 = dGPGO(surrogate_loser_12, Acquisition_new(util_loser), f_syn_polarity12, param, n_jobs = -1) # define BayesOpt\n",
        "loser_12.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_loser_12 = loser_12.getResult()[0]\n",
        "params_loser_12['max_depth'] = int(params_loser_12['max_depth'])\n",
        "params_loser_12['min_child_weight'] = int(params_loser_12['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_loser_train12 = xgb.DMatrix(X_train12, y_train12)\n",
        "dX_loser_test12 = xgb.DMatrix(X_test12, y_test12)\n",
        "model_loser_12 = xgb.train(params_loser_12, dX_loser_train12)\n",
        "pred_loser_12 = model_loser_12.predict(dX_loser_test12)\n",
        "\n",
        "rmse_loser_12 = np.sqrt(mean_squared_error(pred_loser_12, y_test12))\n",
        "rmse_loser_12"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 1.9151945   6.22108771  9.          0.89267929 16.          0.87460279]. \t  -0.4420077321695894 \t -0.4420077321695894\n",
            "init   \t [ 1.50636966  1.98518759 11.          0.67890863 17.          0.5381501 ]. \t  -0.5998539491151165 \t -0.4420077321695894\n",
            "init   \t [ 3.31015428  8.02639569  7.          0.75154158 13.          0.11992952]. \t  -0.7324503600335561 \t -0.4420077321695894\n",
            "init   \t [ 2.90728553  2.46394443  5.          0.94461307 17.          0.43194161]. \t  -0.5995790338913064 \t -0.4420077321695894\n",
            "init   \t [ 9.33140102  6.51378143  8.          0.76907392 18.          0.38515251]. \t  -0.5982523495604211 \t -0.4420077321695894\n",
            "1      \t [5.68098653 8.6912739  6.         0.52027807 1.         0.51631894]. \t  -0.5999713561677879 \t -0.4420077321695894\n",
            "2      \t [ 4.61089026  7.78540446 14.          0.61560224  6.          0.60910898]. \t  -0.5310432089168232 \t -0.4420077321695894\n",
            "3      \t [ 7.71510638  1.71782806 11.          0.9748513   2.          0.33442931]. \t  -0.5954035506799399 \t -0.4420077321695894\n",
            "4      \t [ 0.42901773  1.02682351 10.          0.62582208  7.          0.40519792]. \t  -0.600033100066767 \t -0.4420077321695894\n",
            "5      \t [ 9.48743388  2.23858916 14.          0.84020064 13.          0.41361313]. \t  -0.5921503481987865 \t -0.4420077321695894\n",
            "6      \t [ 7.09099339  0.08964775  6.          0.68605854 11.          0.16899055]. \t  -0.7330665763811677 \t -0.4420077321695894\n",
            "7      \t [ 5.1543633   9.69004682 14.          0.74319768 17.          0.13447035]. \t  -0.7322160475319931 \t -0.4420077321695894\n",
            "8      \t [0.99238486 5.31706566 9.61992505 0.5        1.         0.1       ]. \t  -0.7336693179826522 \t -0.4420077321695894\n",
            "9      \t [0.  0.  5.  0.5 1.  0.1]. \t  -0.7334076855722197 \t -0.4420077321695894\n",
            "10     \t [ 9.28455241  7.99305791  9.          0.7151018  11.          0.47357317]. \t  -0.5981360899216244 \t -0.4420077321695894\n",
            "11     \t [0.28778391 6.56278858 6.         0.98668711 8.         0.21832977]. \t  -0.7330268911586586 \t -0.4420077321695894\n",
            "12     \t [8.71347715 2.52417421 5.         0.81450435 4.         0.19261726]. \t  -0.7322521890009479 \t -0.4420077321695894\n",
            "13     \t [ 8.76460845  0.13227845  6.          0.75995694 17.          0.20887306]. \t  -0.7329728586443053 \t -0.4420077321695894\n",
            "14     \t [ 1.31628683  1.0257918   5.          0.7820955  10.          0.83995059]. \t  -0.5114296694528737 \t -0.4420077321695894\n",
            "15     \t [ 1.07765263  6.20138171 14.          0.59898972 11.          0.65973368]. \t  -0.5325207293005967 \t -0.4420077321695894\n",
            "16     \t [6.28736248 5.81565128 5.         0.62918024 8.         0.71228537]. \t  -0.5479129205332877 \t -0.4420077321695894\n",
            "17     \t [ 8.95792842  8.74686238 12.          0.60340754  1.          0.64076061]. \t  -0.5341583767040248 \t -0.4420077321695894\n",
            "18     \t [ 5.72640258  1.49259688 14.          0.92448749  7.          0.73129851]. \t  -0.4929336813081906 \t -0.4420077321695894\n",
            "19     \t [ 0.60473619  9.7310361   5.          0.764657   19.          0.44899982]. \t  -0.602200593591361 \t -0.4420077321695894\n",
            "20     \t [ 7.25143535  1.86402411 14.          0.59310957 19.          0.53879904]. \t  -0.5987977809908415 \t -0.4420077321695894\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.301967512402236"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDI2Bi9vU05U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86107c9c-badb-45a7-cc9c-e6904308a6e3"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 13\n",
        "\n",
        "np.random.seed(run_num_13)\n",
        "surrogate_loser_13 = dGaussianProcess(cov_func)\n",
        "\n",
        "X_train13, X_test13, y_train13, y_test13 = train_test_split(X, y, test_size=test_perc, random_state=run_num_13)\n",
        "\n",
        "def f_syn_polarity13(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_13, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train13, y=y_train13).mean())\n",
        "    return operator * score\n",
        "\n",
        "loser_13 = dGPGO(surrogate_loser_13, Acquisition_new(util_loser), f_syn_polarity13, param, n_jobs = -1) # define BayesOpt\n",
        "loser_13.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_loser_13 = loser_13.getResult()[0]\n",
        "params_loser_13['max_depth'] = int(params_loser_13['max_depth'])\n",
        "params_loser_13['min_child_weight'] = int(params_loser_13['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_loser_train13 = xgb.DMatrix(X_train13, y_train13)\n",
        "dX_loser_test13 = xgb.DMatrix(X_test13, y_test13)\n",
        "model_loser_13 = xgb.train(params_loser_13, dX_loser_train13)\n",
        "pred_loser_13 = model_loser_13.predict(dX_loser_test13)\n",
        "\n",
        "rmse_loser_13 = np.sqrt(mean_squared_error(pred_loser_13, y_test13))\n",
        "rmse_loser_13"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 0.30967343  2.54282073  6.          0.9850871  17.          0.50848087]. \t  -0.5693403578256062 \t -0.4459810811310791\n",
            "init   \t [ 6.95119021  3.25593423 14.          0.8602375   8.          0.82055713]. \t  -0.5112217679187058 \t -0.4459810811310791\n",
            "init   \t [ 5.09252675  8.19921021 10.          0.78792002 16.          0.382158  ]. \t  -0.581693558072188 \t -0.4459810811310791\n",
            "init   \t [ 5.4465191   3.19841462 14.          0.90654581  5.          0.10359415]. \t  -0.6818818523119694 \t -0.4459810811310791\n",
            "init   \t [ 9.8393864   9.89619708  9.          0.63387763 10.          0.88979174]. \t  -0.4459810811310791 \t -0.4459810811310791\n",
            "1      \t [ 9.27118046  9.4344906  10.          0.71578295  1.          0.5463837 ]. \t  -0.5604994598662556 \t -0.4459810811310791\n",
            "2      \t [0.21910824 0.53051294 5.         0.62754372 4.         0.55299122]. \t  -0.578556985871312 \t -0.4459810811310791\n",
            "3      \t [0.32904954 9.80442032 8.         0.7285653  2.         0.94646442]. \t  \u001b[92m-0.43885135786496293\u001b[0m \t -0.43885135786496293\n",
            "4      \t [ 8.54553338  0.49573541 12.          0.81406546 19.          0.20999807]. \t  -0.6795516887212301 \t -0.43885135786496293\n",
            "5      \t [9.40145549 1.43629395 5.         0.63655396 1.         0.63002732]. \t  -0.5662893508870628 \t -0.43885135786496293\n",
            "6      \t [ 8.7238127   0.85665784  6.          0.75815556 12.          0.87180469]. \t  -0.4644522650882914 \t -0.43885135786496293\n",
            "7      \t [ 0.11400844  9.5774521   6.          0.90472821 11.          0.18513309]. \t  -0.6793968950126906 \t -0.43885135786496293\n",
            "8      \t [ 1.95513735  0.12334702 13.          0.54467307 14.          0.86324917]. \t  \u001b[92m-0.43794830902637205\u001b[0m \t -0.43794830902637205\n",
            "9      \t [ 2.89959158  8.10209185 14.          0.9094105   1.          0.86003598]. \t  \u001b[92m-0.42582758559630707\u001b[0m \t -0.42582758559630707\n",
            "10     \t [ 0.63533588  7.88973169 13.          0.71777205  8.          0.22258282]. \t  -0.6847473450330067 \t -0.42582758559630707\n",
            "11     \t [4.59853484 7.19189873 5.         0.92777191 6.         0.5760847 ]. \t  -0.5604008171822471 \t -0.42582758559630707\n",
            "12     \t [ 9.78022669  6.34740254  5.          0.81941991 15.          0.17866883]. \t  -0.6810440437950237 \t -0.42582758559630707\n",
            "13     \t [ 0.44924058  6.01817024 13.          0.62113095 19.          0.22629481]. \t  -0.6838258323776948 \t -0.42582758559630707\n",
            "14     \t [2.08063721 0.20431355 5.85195635 0.5        9.85195635 0.1       ]. \t  -0.6829528263861964 \t -0.42582758559630707\n",
            "15     \t [ 8.93459678  5.75013258 14.          0.86506851 14.          0.11026757]. \t  -0.6798789215383618 \t -0.42582758559630707\n",
            "16     \t [ 1.2199304   9.32862427  5.          0.98694875 18.          0.69275841]. \t  -0.5593626212561581 \t -0.42582758559630707\n",
            "17     \t [7.30733696 1.06961871 9.         0.6583612  6.         0.41488291]. \t  -0.5857440675365031 \t -0.42582758559630707\n",
            "18     \t [ 0.25964514  2.37009305 11.          0.85577938  4.          0.42985865]. \t  -0.556481072440868 \t -0.42582758559630707\n",
            "19     \t [ 5.54919942  8.87232522  5.          0.84416975 12.          0.31830005]. \t  -0.5998464916577737 \t -0.42582758559630707\n",
            "20     \t [ 9.03439965  9.39442441 14.          0.98025678 19.          0.55921955]. \t  -0.550861248380592 \t -0.42582758559630707\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.159065314760995"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z2F_Q194U3uu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3add3e3b-06cb-4ea1-ed1c-fc0ad2a93adc"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 14\n",
        "\n",
        "np.random.seed(run_num_14)\n",
        "surrogate_loser_14 = dGaussianProcess(cov_func)\n",
        "\n",
        "X_train14, X_test14, y_train14, y_test14 = train_test_split(X, y, test_size=test_perc, random_state=run_num_14)\n",
        "\n",
        "def f_syn_polarity14(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_14, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train14, y=y_train14).mean())\n",
        "    return operator * score\n",
        "\n",
        "loser_14 = dGPGO(surrogate_loser_14, Acquisition_new(util_loser), f_syn_polarity14, param, n_jobs = -1) # define BayesOpt\n",
        "loser_14.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_loser_14 = loser_14.getResult()[0]\n",
        "params_loser_14['max_depth'] = int(params_loser_14['max_depth'])\n",
        "params_loser_14['min_child_weight'] = int(params_loser_14['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_loser_train14 = xgb.DMatrix(X_train14, y_train14)\n",
        "dX_loser_test14 = xgb.DMatrix(X_test14, y_test14)\n",
        "model_loser_14 = xgb.train(params_loser_14, dX_loser_train14)\n",
        "pred_loser_14 = model_loser_14.predict(dX_loser_test14)\n",
        "\n",
        "rmse_loser_14 = np.sqrt(mean_squared_error(pred_loser_14, y_test14))\n",
        "rmse_loser_14"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 8.59560606  1.64569495 11.          0.89021195 15.          0.4857008 ]. \t  -0.6102441482204597 \t -0.4517949441804544\n",
            "init   \t [ 0.57460093  9.25007434  5.          0.56647642 17.          0.7533211 ]. \t  -0.5614002279921607 \t -0.4517949441804544\n",
            "init   \t [1.90674396 4.32019547 9.         0.93708225 3.         0.91775905]. \t  -0.4517949441804544 \t -0.4517949441804544\n",
            "init   \t [ 0.36770475  8.43368681 14.          0.6826844  15.          0.6291966 ]. \t  -0.5407924410004543 \t -0.4517949441804544\n",
            "init   \t [ 5.33840324  0.449182   14.          0.52045439  2.          0.48604556]. \t  -0.6242722768101235 \t -0.4517949441804544\n",
            "1      \t [ 9.54492219  9.07631824  8.          0.54579372 12.          0.38295996]. \t  -0.6262431675183743 \t -0.4517949441804544\n",
            "2      \t [9.53270461 0.83608685 6.         0.99976104 3.         0.60747265]. \t  -0.5533922160484371 \t -0.4517949441804544\n",
            "3      \t [ 1.15250646  0.51569303  5.          0.69322415 17.          0.16291682]. \t  -0.6917729046487754 \t -0.4517949441804544\n",
            "4      \t [ 6.37447116  9.98862868 14.          0.72259558  4.          0.61899393]. \t  -0.5434760859463281 \t -0.4517949441804544\n",
            "5      \t [ 8.08070558  7.65123431 14.          0.59104644 19.          0.9302005 ]. \t  -0.45873776977159153 \t -0.4517949441804544\n",
            "6      \t [0.         0.         5.         0.5        8.91797943 0.1       ]. \t  -0.6942441953461963 \t -0.4517949441804544\n",
            "7      \t [1.0907871  9.11150389 5.         0.58744161 9.         0.87439163]. \t  -0.4933476055445382 \t -0.4517949441804544\n",
            "8      \t [ 2.21919917  0.31865167 14.          0.54399684 14.          0.18056849]. \t  -0.6942842535038787 \t -0.4517949441804544\n",
            "9      \t [ 7.34435618  8.39108681  7.          0.81547994 19.          0.62767067]. \t  -0.546522525570801 \t -0.4517949441804544\n",
            "10     \t [ 6.57539862  4.15362663 14.          0.96053826  9.          0.2080482 ]. \t  -0.691194495047777 \t -0.4517949441804544\n",
            "11     \t [6.00529466 8.33461243 6.         0.97581801 4.         0.31582778]. \t  -0.6226835813168086 \t -0.4517949441804544\n",
            "12     \t [ 7.68230622  0.80909901  5.          0.61509794 11.          0.73230761]. \t  -0.5592301279615596 \t -0.4517949441804544\n",
            "13     \t [0.  0.  5.  0.5 1.  0.1]. \t  -0.6943016112587715 \t -0.4517949441804544\n",
            "14     \t [ 0.84209312  7.86749962 14.          0.8648448   1.          0.1570582 ]. \t  -0.6937301125121068 \t -0.4517949441804544\n",
            "15     \t [ 1.22436035  9.67689861 12.          0.7460835   9.          0.69284382]. \t  -0.5399272231860202 \t -0.4517949441804544\n",
            "16     \t [ 4.4386325   0.37972087 10.          0.61118217  8.          0.15198669]. \t  -0.6912722224579347 \t -0.4517949441804544\n",
            "17     \t [ 7.0496565   9.86456792 14.          0.62416475 11.          0.68843039]. \t  -0.5429606831910581 \t -0.4517949441804544\n",
            "18     \t [ 1.55101374  4.15023981  9.          0.60341671 12.          0.53785307]. \t  -0.6138556386215619 \t -0.4517949441804544\n",
            "19     \t [ 9.66203958  6.20819976 11.          0.71875858  5.          0.43480687]. \t  -0.6129974118264951 \t -0.4517949441804544\n",
            "20     \t [ 0.13684155  3.86104617 14.          0.82673561  9.          0.20283653]. \t  -0.6919355943160154 \t -0.4517949441804544\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.334202643456426"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Po5wImJaU6VC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd4c97c8-b6fa-4679-d84d-e54d7a21e31a"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 15\n",
        "\n",
        "np.random.seed(run_num_15)\n",
        "surrogate_loser_15 = dGaussianProcess(cov_func)\n",
        "\n",
        "X_train15, X_test15, y_train15, y_test15 = train_test_split(X, y, test_size=test_perc, random_state=run_num_15)\n",
        "\n",
        "def f_syn_polarity15(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_15, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train15, y=y_train15).mean())\n",
        "    return operator * score\n",
        "\n",
        "loser_15 = dGPGO(surrogate_loser_15, Acquisition_new(util_loser), f_syn_polarity15, param, n_jobs = -1) # define BayesOpt\n",
        "loser_15.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_loser_15 = loser_15.getResult()[0]\n",
        "params_loser_15['max_depth'] = int(params_loser_15['max_depth'])\n",
        "params_loser_15['min_child_weight'] = int(params_loser_15['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_loser_train15 = xgb.DMatrix(X_train15, y_train15)\n",
        "dX_loser_test15 = xgb.DMatrix(X_test15, y_test15)\n",
        "model_loser_15 = xgb.train(params_loser_15, dX_loser_train15)\n",
        "pred_loser_15 = model_loser_15.predict(dX_loser_test15)\n",
        "\n",
        "rmse_loser_15 = np.sqrt(mean_squared_error(pred_loser_15, y_test15))\n",
        "rmse_loser_15"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 5.64315424  8.97297685  6.          0.81493635 11.          0.33008564]. \t  -0.5946480910132583 \t -0.4392277798535851\n",
            "init   \t [9.51455383 9.60803032 5.         0.68485728 1.         0.30145081]. \t  -0.5968379804084254 \t -0.4392277798535851\n",
            "init   \t [ 8.11809889  6.32508536  7.          0.61766329 19.          0.51383159]. \t  -0.5092318398204285 \t -0.4392277798535851\n",
            "init   \t [ 7.28571251  5.36071389 10.          0.76240297 12.          0.12035683]. \t  -0.6346120183758519 \t -0.4392277798535851\n",
            "init   \t [ 6.80847768  3.59767194 14.          0.51365585 16.          0.89730673]. \t  -0.4392277798535851 \t -0.4392277798535851\n",
            "1      \t [ 2.28792065  8.58421878 12.          0.67551126  1.          0.37971264]. \t  -0.596161946051267 \t -0.4392277798535851\n",
            "2      \t [ 9.35473751  1.48895841 12.          0.55362474  4.          0.11106606]. \t  -0.6327865627642436 \t -0.4392277798535851\n",
            "3      \t [3.91572323 2.23174129 6.         0.50937407 1.         0.51336364]. \t  -0.5160141940687206 \t -0.4392277798535851\n",
            "4      \t [ 0.74184801  3.17830864 14.          0.90502664  8.          0.31107615]. \t  -0.5905073342805591 \t -0.4392277798535851\n",
            "5      \t [ 1.33182259  0.8125478   9.          0.94558338 18.          0.71777254]. \t  -0.4467705016632985 \t -0.4392277798535851\n",
            "6      \t [ 0.48538268  1.39730148  6.          0.8238236  11.          0.45741795]. \t  -0.5144696373796631 \t -0.4392277798535851\n",
            "7      \t [ 0.57885492  9.35143843  6.          0.69337529 17.          0.15510814]. \t  -0.6331051904120661 \t -0.4392277798535851\n",
            "8      \t [0.24960251 9.45540541 5.         0.62890215 3.         0.73551484]. \t  -0.47327438015853385 \t -0.4392277798535851\n",
            "9      \t [ 8.23428085  9.77240559 13.          0.85977854  8.          0.93876208]. \t  \u001b[92m-0.4281095031928478\u001b[0m \t -0.4281095031928478\n",
            "10     \t [ 1.35750397  9.83931966 14.          0.92698791 15.          0.2272101 ]. \t  -0.6341880539033447 \t -0.4281095031928478\n",
            "11     \t [ 9.85965179  9.04589828 13.          0.95564786 19.          0.1426314 ]. \t  -0.632071954329094 \t -0.4281095031928478\n",
            "12     \t [9.06175259 4.11518452 5.         0.87621161 6.         0.52300377]. \t  -0.5192107686497183 \t -0.4281095031928478\n",
            "13     \t [ 0.15053668  0.43786928 13.          0.62231098  1.          0.88145445]. \t  -0.43523183945633176 \t -0.4281095031928478\n",
            "14     \t [ 7.18775342  0.2062489   5.          0.98239776 17.          0.28043211]. \t  -0.634545784625454 \t -0.4281095031928478\n",
            "15     \t [0.10178544 7.49949902 9.         0.60008085 8.         0.35520416]. \t  -0.5902884239300654 \t -0.4281095031928478\n",
            "16     \t [0.24880199 0.21304322 8.24146387 0.5        5.24146387 0.1       ]. \t  -0.6326902614705249 \t -0.4281095031928478\n",
            "17     \t [ 8.4324691   8.27251391 11.          0.71354715  2.          0.54685464]. \t  -0.498601956775111 \t -0.4281095031928478\n",
            "18     \t [ 2.12400437  9.06769045 14.          0.70585245  9.          0.90846764]. \t  -0.4286407197787245 \t -0.4281095031928478\n",
            "19     \t [9.58390631 0.99765082 6.         0.72021045 1.         0.39555686]. \t  -0.5960083204162412 \t -0.4281095031928478\n",
            "20     \t [ 9.96011127  1.55235537 14.          0.63004724 11.          0.14483121]. \t  -0.6331081546428745 \t -0.4281095031928478\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.189953800077167"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HrAQN-pU9Qo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ccb0e75c-564b-4768-a564-dbdae620e380"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 16\n",
        "\n",
        "np.random.seed(run_num_16)\n",
        "surrogate_loser_16 = dGaussianProcess(cov_func)\n",
        "\n",
        "X_train16, X_test16, y_train16, y_test16 = train_test_split(X, y, test_size=test_perc, random_state=run_num_16)\n",
        "\n",
        "def f_syn_polarity16(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_16, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train16, y=y_train16).mean())\n",
        "    return operator * score\n",
        "\n",
        "loser_16 = dGPGO(surrogate_loser_16, Acquisition_new(util_loser), f_syn_polarity16, param, n_jobs = -1) # define BayesOpt\n",
        "loser_16.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_loser_16 = loser_16.getResult()[0]\n",
        "params_loser_16['max_depth'] = int(params_loser_16['max_depth'])\n",
        "params_loser_16['min_child_weight'] = int(params_loser_16['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_loser_train16 = xgb.DMatrix(X_train16, y_train16)\n",
        "dX_loser_test16 = xgb.DMatrix(X_test16, y_test16)\n",
        "model_loser_16 = xgb.train(params_loser_16, dX_loser_train16)\n",
        "pred_loser_16 = model_loser_16.predict(dX_loser_test16)\n",
        "\n",
        "rmse_loser_16 = np.sqrt(mean_squared_error(pred_loser_16, y_test16))\n",
        "rmse_loser_16"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [8.63948332 3.16366002 7.         0.75395583 1.         0.56999262]. \t  -0.5713220948397237 \t -0.4331621293825035\n",
            "init   \t [ 3.3426621   9.77029134 14.          0.93751218 17.          0.80549151]. \t  -0.4331621293825035 \t -0.4331621293825035\n",
            "init   \t [ 3.36871073  4.8308288  13.          0.91984982 14.          0.48829226]. \t  -0.5561611494046279 \t -0.4331621293825035\n",
            "init   \t [ 8.84540736  1.59826081  8.          0.75772888 14.          0.29029227]. \t  -0.6835926126526815 \t -0.4331621293825035\n",
            "init   \t [ 7.74981136  8.34377636 13.          0.83182409 15.          0.19378622]. \t  -0.7677243885128009 \t -0.4331621293825035\n",
            "1      \t [ 0.60074712  1.1402995   5.          0.75926551 10.          0.30612858]. \t  -0.6844266759595645 \t -0.4331621293825035\n",
            "2      \t [ 3.6191694   8.20490707  5.          0.81280746 16.          0.99306688]. \t  -0.4581574120389309 \t -0.4331621293825035\n",
            "3      \t [ 0.55111138  3.69634376 11.          0.53864409  3.          0.92267876]. \t  -0.43468988385343615 \t -0.4331621293825035\n",
            "4      \t [9.52987381 8.92087345 5.         0.95441282 7.         0.10412548]. \t  -0.7679863416880384 \t -0.4331621293825035\n",
            "5      \t [ 8.31345174  8.15216774 14.          0.81052242  4.          0.5053732 ]. \t  -0.5586554806770512 \t -0.4331621293825035\n",
            "6      \t [ 7.65476591  1.74203379 12.          0.78837413  7.          0.71460419]. \t  -0.4382687724745528 \t -0.4331621293825035\n",
            "7      \t [1.68021993 9.25252958 9.         0.78643474 9.         0.55639556]. \t  -0.5630452959468858 \t -0.4331621293825035\n",
            "8      \t [0.  0.  5.  0.5 1.  0.1]. \t  -0.7670392049994392 \t -0.4331621293825035\n",
            "9      \t [ 2.6903245   0.3560009   7.          0.98255651 19.          0.9851534 ]. \t  -0.4412750880856537 \t -0.4331621293825035\n",
            "10     \t [ 0.4751313   0.18137204 13.          0.70186285 10.          0.37625565]. \t  -0.68582313055112 \t -0.4331621293825035\n",
            "11     \t [ 8.54895835  0.69961229 14.          0.57654595 19.          0.20454627]. \t  -0.7677953049378372 \t -0.4331621293825035\n",
            "12     \t [ 8.76905299  8.15213676  8.          0.5845007  19.          0.27314045]. \t  -0.7678874315811665 \t -0.4331621293825035\n",
            "13     \t [ 1.48533436  9.57311484 14.          0.8193199   3.          0.99329178]. \t  \u001b[92m-0.4185372839160708\u001b[0m \t -0.4185372839160708\n",
            "14     \t [4.49490325 9.22009337 8.         0.71314799 3.         0.58255672]. \t  -0.48560746523878195 \t -0.4185372839160708\n",
            "15     \t [ 0.76287087  0.49621071 13.          0.81150873 18.          0.76762748]. \t  -0.4363142295126112 \t -0.4185372839160708\n",
            "16     \t [0.56061777 6.48476685 5.         0.81826035 5.         0.59167243]. \t  -0.5183862929687044 \t -0.4185372839160708\n",
            "17     \t [8.8152352  0.5898958  5.         0.831707   9.         0.92137997]. \t  -0.45519808186832184 \t -0.4185372839160708\n",
            "18     \t [ 8.76208148  1.79224635  5.          0.52306305 19.          0.78632749]. \t  -0.4871095639687443 \t -0.4185372839160708\n",
            "19     \t [ 9.63332031  8.534049   10.          0.55556169 10.          0.35894848]. \t  -0.6847217417593005 \t -0.4185372839160708\n",
            "20     \t [ 4.52692506  0.27496659 10.          0.6345842   1.          0.46474283]. \t  -0.5631095705887621 \t -0.4185372839160708\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.361483761377183"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXelbcAVVCqO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65926b0b-5661-4cd3-d775-0800313d29aa"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 17\n",
        "\n",
        "np.random.seed(run_num_17)\n",
        "surrogate_loser_17 = dGaussianProcess(cov_func)\n",
        "\n",
        "X_train17, X_test17, y_train17, y_test17 = train_test_split(X, y, test_size=test_perc, random_state=run_num_17)\n",
        "\n",
        "def f_syn_polarity17(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_17, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train17, y=y_train17).mean())\n",
        "    return operator * score\n",
        "\n",
        "loser_17 = dGPGO(surrogate_loser_17, Acquisition_new(util_loser), f_syn_polarity17, param, n_jobs = -1) # define BayesOpt\n",
        "loser_17.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_loser_17 = loser_17.getResult()[0]\n",
        "params_loser_17['max_depth'] = int(params_loser_17['max_depth'])\n",
        "params_loser_17['min_child_weight'] = int(params_loser_17['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_loser_train17 = xgb.DMatrix(X_train17, y_train17)\n",
        "dX_loser_test17 = xgb.DMatrix(X_test17, y_test17)\n",
        "model_loser_17 = xgb.train(params_loser_17, dX_loser_train17)\n",
        "pred_loser_17 = model_loser_17.predict(dX_loser_test17)\n",
        "\n",
        "rmse_loser_17 = np.sqrt(mean_squared_error(pred_loser_17, y_test17))\n",
        "rmse_loser_17"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 1.85575275  3.86599303 13.          0.85645882  2.          0.78699813]. \t  -0.4597056260580034 \t -0.4597056260580034\n",
            "init   \t [1.97780831 2.92527676 8.         0.92268607 9.         0.39017299]. \t  -0.5687207024308397 \t -0.4597056260580034\n",
            "init   \t [ 7.16656007  1.028324    5.          0.96975483 15.          0.90932849]. \t  -0.4688025715067707 \t -0.4597056260580034\n",
            "init   \t [ 2.46007304  9.53596657  7.          0.71704714 11.          0.17659118]. \t  -0.6983036523024195 \t -0.4597056260580034\n",
            "init   \t [3.44161135 9.92427612 8.         0.96331165 2.         0.29160186]. \t  -0.5675253504862205 \t -0.4597056260580034\n",
            "1      \t [ 9.98828606  3.49693094 13.          0.57156882  8.          0.14604355]. \t  -0.6992595730796743 \t -0.4597056260580034\n",
            "2      \t [ 6.95619362  6.95953721 12.          0.92317057 19.          0.34981799]. \t  -0.5628121282495753 \t -0.4597056260580034\n",
            "3      \t [0.  0.  5.  0.5 1.  0.1]. \t  -0.6990710246949499 \t -0.4597056260580034\n",
            "4      \t [ 1.79054908  0.97767001 14.          0.72051649 17.          0.75055942]. \t  \u001b[92m-0.4586950539387324\u001b[0m \t -0.4586950539387324\n",
            "5      \t [7.85364213 1.7463908  7.         0.80931735 2.         0.57315987]. \t  -0.5442127017186587 \t -0.4586950539387324\n",
            "6      \t [ 1.77308987  6.92082513  5.          0.52059222 19.          0.18788647]. \t  -0.69931956875667 \t -0.4586950539387324\n",
            "7      \t [ 8.86839023  7.16332242  5.          0.6476276  11.          0.20780931]. \t  -0.6987603050812756 \t -0.4586950539387324\n",
            "8      \t [ 9.85681439  0.57206659 14.          0.79465027 19.          0.59553205]. \t  -0.534073864099878 \t -0.4586950539387324\n",
            "9      \t [ 0.90443942  7.22834774 14.          0.92580232 12.          0.13465218]. \t  -0.6965996897404226 \t -0.4586950539387324\n",
            "10     \t [ 9.20219266  7.71975834 14.          0.72531692  2.          0.43032755]. \t  -0.5354718257765292 \t -0.4586950539387324\n",
            "11     \t [ 7.87455923  9.60434668 13.          0.6097695  11.          0.18672661]. \t  -0.6988278637935389 \t -0.4586950539387324\n",
            "12     \t [ 1.83063475  0.7056051   5.          0.62427541 19.          0.60134826]. \t  -0.5515025691718822 \t -0.4586950539387324\n",
            "13     \t [ 1.78559475  9.94726594 14.          0.84124706  5.          0.41412983]. \t  -0.5643771257102934 \t -0.4586950539387324\n",
            "14     \t [ 7.45649622  0.49339183 14.          0.58756242 13.          0.5079427 ]. \t  -0.5393389988159936 \t -0.4586950539387324\n",
            "15     \t [9.52955352 8.15137211 8.         0.76863454 2.         0.57854032]. \t  -0.5395532171313935 \t -0.4586950539387324\n",
            "16     \t [ 1.13635919  9.59512209 12.          0.5730343  19.          0.36892674]. \t  -0.5697966935770927 \t -0.4586950539387324\n",
            "17     \t [6.77242326 0.         5.         0.5        9.09935527 0.1       ]. \t  -0.6997733788661372 \t -0.4586950539387324\n",
            "18     \t [ 5.89593673  0.5459184  14.          0.8691618   4.          0.72423884]. \t  \u001b[92m-0.45644440788193374\u001b[0m \t -0.45644440788193374\n",
            "19     \t [ 7.6390113   9.95449177  6.          0.57337797 19.          0.29431206]. \t  -0.5822507255377387 \t -0.45644440788193374\n",
            "20     \t [ 6.89682696  4.76907936 10.          0.67407985 14.          0.88827919]. \t  \u001b[92m-0.44318669556749074\u001b[0m \t -0.44318669556749074\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.0138942791645995"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJG2fAtAVFDZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1470f94f-a97b-4e6b-ab99-cbd640efe2d3"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 18\n",
        "\n",
        "np.random.seed(run_num_18)\n",
        "surrogate_loser_18 = dGaussianProcess(cov_func)\n",
        "\n",
        "X_train18, X_test18, y_train18, y_test18 = train_test_split(X, y, test_size=test_perc, random_state=run_num_18)\n",
        "\n",
        "def f_syn_polarity18(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_11, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train18, y=y_train18).mean())\n",
        "    return operator * score\n",
        "\n",
        "loser_18 = dGPGO(surrogate_loser_18, Acquisition_new(util_loser), f_syn_polarity18, param, n_jobs = -1) # define BayesOpt\n",
        "loser_18.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_loser_18 = loser_18.getResult()[0]\n",
        "params_loser_18['max_depth'] = int(params_loser_18['max_depth'])\n",
        "params_loser_18['min_child_weight'] = int(params_loser_18['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_loser_train18 = xgb.DMatrix(X_train18, y_train18)\n",
        "dX_loser_test18 = xgb.DMatrix(X_test18, y_test18)\n",
        "model_loser_18 = xgb.train(params_loser_18, dX_loser_train18)\n",
        "pred_loser_18 = model_loser_18.predict(dX_loser_test18)\n",
        "\n",
        "rmse_loser_18 = np.sqrt(mean_squared_error(pred_loser_18, y_test18))\n",
        "rmse_loser_18"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 8.73429403  9.68540663 10.          0.68875849  9.          0.48011572]. \t  -0.48312028405718427 \t -0.4337207096533448\n",
            "init   \t [ 6.12033333  7.66062926  8.          0.76133734 13.          0.93379456]. \t  -0.44701743563076113 \t -0.4337207096533448\n",
            "init   \t [ 1.46524679  7.01527914  7.          0.90913299 10.          0.36016753]. \t  -0.6042122061654378 \t -0.4337207096533448\n",
            "init   \t [ 9.73855241  3.33774046 14.          0.53290419  7.          0.7088681 ]. \t  -0.47205818581725156 \t -0.4337207096533448\n",
            "init   \t [ 3.00618018  1.82702795 11.          0.75681389 14.          0.98627449]. \t  -0.4337207096533448 \t -0.4337207096533448\n",
            "1      \t [ 1.46586883  1.13810325 14.          0.90052794  5.          0.16182112]. \t  -0.7143993241707929 \t -0.4337207096533448\n",
            "2      \t [ 9.17850108  5.22481201 14.          0.60105853 18.          0.63442006]. \t  -0.47652759630987357 \t -0.4337207096533448\n",
            "3      \t [6.20612396 4.37552923 6.         0.71941082 3.         0.90943147]. \t  -0.4522652783180721 \t -0.4337207096533448\n",
            "4      \t [ 0.42107725  9.92938812 12.          0.92462675 18.          0.88352347]. \t  \u001b[92m-0.42696094495506925\u001b[0m \t -0.42696094495506925\n",
            "5      \t [ 1.45418057  8.89399    11.          0.93175096  1.          0.52391385]. \t  -0.4748883821619788 \t -0.42696094495506925\n",
            "6      \t [0.         0.         5.         0.5        5.86208378 0.1       ]. \t  -0.7141834012604253 \t -0.42696094495506925\n",
            "7      \t [ 8.67626106  0.1397848   6.          0.55771681 18.          0.89120888]. \t  -0.4678231542313444 \t -0.42696094495506925\n",
            "8      \t [ 7.20807715  7.41236348  5.          0.52425285 19.          0.24651851]. \t  -0.7151531754828971 \t -0.42696094495506925\n",
            "9      \t [7.94412856 1.24068931 7.         0.96649935 9.         0.47928692]. \t  -0.48764145519377255 \t -0.42696094495506925\n",
            "10     \t [ 2.66410938  9.83743919 13.          0.58899688  8.          0.29675269]. \t  -0.6087724199154481 \t -0.42696094495506925\n",
            "11     \t [ 8.62753755  4.44899956 12.          0.98231134  1.          0.88389967]. \t  \u001b[92m-0.42538744453232347\u001b[0m \t -0.42538744453232347\n",
            "12     \t [ 1.2277143   1.05411485  5.          0.50198686 13.          0.89258454]. \t  -0.4690426518871574 \t -0.42538744453232347\n",
            "13     \t [ 3.86727486  0.04582369 10.          0.54629234  1.          0.59406244]. \t  -0.47507759871416455 \t -0.42538744453232347\n",
            "14     \t [9.89654007 7.91459554 5.         0.64703423 6.         0.95875063]. \t  -0.4610204259667869 \t -0.42538744453232347\n",
            "15     \t [ 9.49381141  1.31696272 11.          0.51283555 13.          0.25364349]. \t  -0.7155727045133423 \t -0.42538744453232347\n",
            "16     \t [0.         3.84904456 5.70916624 0.5        1.         0.1       ]. \t  -0.7130688739294866 \t -0.42538744453232347\n",
            "17     \t [ 1.04974938  7.3117691   5.          0.60120794 17.          0.83587336]. \t  -0.49012043280719225 \t -0.42538744453232347\n",
            "18     \t [5.83203328 9.4812958  5.         0.60479193 1.         0.24995758]. \t  -0.7139401129279975 \t -0.42538744453232347\n",
            "19     \t [ 3.86883296  7.31704712 14.          0.75047101 14.          0.80828494]. \t  -0.4565506921595134 \t -0.42538744453232347\n",
            "20     \t [ 4.0233419   3.91847735 10.          0.87935081  7.          0.57035954]. \t  -0.47552190244029113 \t -0.42538744453232347\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3.937438334950126"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHidSEGcVHvG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cfe84a06-5c1c-442f-b82d-dc6b21b10d10"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 19\n",
        "\n",
        "np.random.seed(run_num_19)\n",
        "surrogate_loser_19 = dGaussianProcess(cov_func)\n",
        "\n",
        "X_train19, X_test19, y_train19, y_test19 = train_test_split(X, y, test_size=test_perc, random_state=run_num_19)\n",
        "\n",
        "def f_syn_polarity19(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_19, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train19, y=y_train19).mean())\n",
        "    return operator * score\n",
        "\n",
        "loser_19 = dGPGO(surrogate_loser_19, Acquisition_new(util_loser), f_syn_polarity19, param, n_jobs = -1) # define BayesOpt\n",
        "loser_19.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_loser_19 = loser_19.getResult()[0]\n",
        "params_loser_19['max_depth'] = int(params_loser_19['max_depth'])\n",
        "params_loser_19['min_child_weight'] = int(params_loser_19['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_loser_train19 = xgb.DMatrix(X_train19, y_train19)\n",
        "dX_loser_test19 = xgb.DMatrix(X_test19, y_test19)\n",
        "model_loser_19 = xgb.train(params_loser_19, dX_loser_train19)\n",
        "pred_loser_19 = model_loser_19.predict(dX_loser_test19)\n",
        "\n",
        "rmse_loser_19 = np.sqrt(mean_squared_error(pred_loser_19, y_test19))\n",
        "rmse_loser_19"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 8.24520174  9.84626759 12.          0.81846905 12.          0.27594309]. \t  -0.6624515538441494 \t -0.4321567975765851\n",
            "init   \t [ 8.6021676   8.64300514 14.          0.95067208 18.          0.13363564]. \t  -0.663790961968593 \t -0.4321567975765851\n",
            "init   \t [ 3.02486657  6.06229509 10.          0.50494482  9.          0.10218675]. \t  -0.668875700942164 \t -0.4321567975765851\n",
            "init   \t [ 6.57586195  8.01231848  7.          0.63945407 18.          0.49206406]. \t  -0.5640092362987262 \t -0.4321567975765851\n",
            "init   \t [ 6.05008856  8.15558548 14.          0.64718943  8.          0.90611106]. \t  -0.4321567975765851 \t -0.4321567975765851\n",
            "1      \t [ 0.85655458  0.69294498  9.          0.90992265 19.          0.25605959]. \t  -0.6632879756016564 \t -0.4321567975765851\n",
            "2      \t [ 1.16239725  9.89254371 11.          0.98469188  1.          0.20271996]. \t  -0.6659068700895453 \t -0.4321567975765851\n",
            "3      \t [9.26425961 9.717621   5.         0.8856331  5.         0.65589573]. \t  -0.5256636565447839 \t -0.4321567975765851\n",
            "4      \t [8.38158931 1.39953008 5.         0.6836482  9.         0.46934447]. \t  -0.5657479189361881 \t -0.4321567975765851\n",
            "5      \t [ 9.89729152  2.88709408 11.          0.55121781  3.          0.58381748]. \t  -0.5217676759230677 \t -0.4321567975765851\n",
            "6      \t [0.  0.  5.  0.5 1.  0.1]. \t  -0.6682393310586452 \t -0.4321567975765851\n",
            "7      \t [ 0.52039992  0.56380532 14.          0.70324781  2.          0.77427978]. \t  -0.44119318049659856 \t -0.4321567975765851\n",
            "8      \t [ 9.45607821  0.96438893 14.          0.56821928 16.          0.48995458]. \t  -0.560546633196372 \t -0.4321567975765851\n",
            "9      \t [ 0.56432836  7.79494932 13.          0.55871992 19.          0.10266045]. \t  -0.6660467600439527 \t -0.4321567975765851\n",
            "10     \t [ 3.16676683  1.78770811 13.          0.5650927  13.          0.7417697 ]. \t  -0.4472490143246649 \t -0.4321567975765851\n",
            "11     \t [5.56939336 3.95670769 5.         0.95243816 3.         0.31115527]. \t  -0.6118696715045784 \t -0.4321567975765851\n",
            "12     \t [0.         0.         6.69414791 0.5        9.69414791 0.1       ]. \t  -0.6680142576361826 \t -0.4321567975765851\n",
            "13     \t [ 7.46605559  8.55652968 13.          0.99452706  1.          0.43764631]. \t  -0.5573240867828692 \t -0.4321567975765851\n",
            "14     \t [ 9.00192632  0.12142981  6.          0.72039779 17.          0.26695117]. \t  -0.6635472276398457 \t -0.4321567975765851\n",
            "15     \t [ 1.20113738  5.88016015  6.          0.57839075 15.          0.60752162]. \t  -0.5267223663659426 \t -0.4321567975765851\n",
            "16     \t [1.7958895  9.19371155 5.         0.54448974 5.         0.93868386]. \t  -0.4669880318809955 \t -0.4321567975765851\n",
            "17     \t [ 9.62487731  6.86253449  5.          0.66465947 12.          0.21107909]. \t  -0.6659019290689656 \t -0.4321567975765851\n",
            "18     \t [ 7.01475491  4.53933393 10.          0.82075044 14.          0.83058729]. \t  -0.4517750017757683 \t -0.4321567975765851\n",
            "19     \t [ 5.75277105  1.25995511 14.          0.97274397  8.          0.39753437]. \t  -0.6135363407765908 \t -0.4321567975765851\n",
            "20     \t [ 2.00664037  5.88682269 14.          0.94696775  4.          0.80981462]. \t  -0.4349577325456077 \t -0.4321567975765851\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.2307134915487135"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWGPYRJhVKsO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f8ee200-26f8-4f7f-f8bf-1ca55f24f9a1"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 20\n",
        "\n",
        "np.random.seed(run_num_20)\n",
        "surrogate_loser_20 = dGaussianProcess(cov_func)\n",
        "\n",
        "X_train20, X_test20, y_train20, y_test20 = train_test_split(X, y, test_size=test_perc, random_state=run_num_20)\n",
        "\n",
        "def f_syn_polarity20(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_20, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train20, y=y_train20).mean())\n",
        "    return operator * score\n",
        "\n",
        "loser_20 = dGPGO(surrogate_loser_20, Acquisition_new(util_loser), f_syn_polarity20, param, n_jobs = -1) # define BayesOpt\n",
        "loser_20.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_loser_20 = loser_20.getResult()[0]\n",
        "params_loser_20['max_depth'] = int(params_loser_20['max_depth'])\n",
        "params_loser_20['min_child_weight'] = int(params_loser_20['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_loser_train20 = xgb.DMatrix(X_train20, y_train20)\n",
        "dX_loser_test20 = xgb.DMatrix(X_test20, y_test20)\n",
        "model_loser_20 = xgb.train(params_loser_20, dX_loser_train20)\n",
        "pred_loser_20 = model_loser_20.predict(dX_loser_test20)\n",
        "\n",
        "rmse_loser_20 = np.sqrt(mean_squared_error(pred_loser_20, y_test20))\n",
        "rmse_loser_20"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 5.70517285  5.64528755 11.          0.51213999 19.          0.57883228]. \t  -0.5510634045248362 \t -0.4667910180825121\n",
            "init   \t [0.68106288 5.8452906  6.         0.58037829 8.         0.87974899]. \t  -0.4667910180825121 \t -0.4667910180825121\n",
            "init   \t [ 4.04121426  7.17971417 11.          0.57603584 10.          0.19954225]. \t  -0.6541702754539586 \t -0.4667910180825121\n",
            "init   \t [ 5.27556761  8.67655329 13.          0.54252273  4.          0.48570796]. \t  -0.5813476654605825 \t -0.4667910180825121\n",
            "init   \t [ 5.30003889  5.73946822 12.          0.52666698 16.          0.77467019]. \t  -0.5078849923055367 \t -0.4667910180825121\n",
            "1      \t [ 9.25190708  0.65043072 10.          0.56676233  5.          0.41289847]. \t  -0.6164858434894066 \t -0.4667910180825121\n",
            "2      \t [8.65689736 8.11684598 5.         0.53907072 1.         0.87574215]. \t  -0.47576906935129115 \t -0.4667910180825121\n",
            "3      \t [6.49728107e-01 7.72862043e-04 7.00000000e+00 5.71905117e-01\n",
            " 2.00000000e+00 1.02014940e-01]. \t  -0.6533990377801203 \t -0.4667910180825121\n",
            "4      \t [ 9.51806562  0.44900923  6.          0.62997227 16.          0.40028974]. \t  -0.6257141325107729 \t -0.4667910180825121\n",
            "5      \t [ 0.48808143  1.97030839  7.          0.79892377 16.          0.98477167]. \t  \u001b[92m-0.4602371335347518\u001b[0m \t -0.4602371335347518\n",
            "6      \t [ 9.98029391  8.23632437  6.          0.63606408 13.          0.22908429]. \t  -0.6532745897130707 \t -0.4602371335347518\n",
            "7      \t [ 3.9245879   0.87058505 14.          0.68269183  1.          0.34048922]. \t  -0.623283664424315 \t -0.4602371335347518\n",
            "8      \t [ 0.45468431  0.13267263 10.          0.667214    9.          0.47071141]. \t  -0.5807933165122651 \t -0.4602371335347518\n",
            "9      \t [ 4.64792903  9.65609988  6.          0.78696753 17.          0.90406502]. \t  -0.47227152594951016 \t -0.4602371335347518\n",
            "10     \t [ 9.73713042  9.77875019 14.          0.8552366  13.          0.6588042 ]. \t  -0.5450528828222597 \t -0.4602371335347518\n",
            "11     \t [ 9.74564452  1.02954501 14.          0.98964301 12.          0.47743205]. \t  -0.5773481317621234 \t -0.4602371335347518\n",
            "12     \t [ 7.76906477  2.6060444   5.          0.53491145 10.          0.41560504]. \t  -0.627993339841767 \t -0.4602371335347518\n",
            "13     \t [6.49958993 1.98622034 6.         0.83635209 1.         0.2931143 ]. \t  -0.6276302072456058 \t -0.4602371335347518\n",
            "14     \t [1.36051555 9.95957797 7.         0.9242116  2.         0.87097462]. \t  \u001b[92m-0.449167771194887\u001b[0m \t -0.449167771194887\n",
            "15     \t [ 0.20978943  8.45447688 14.          0.77351134 17.          0.2656195 ]. \t  -0.6586945947582249 \t -0.449167771194887\n",
            "16     \t [7.55121074 9.86496533 6.         0.88828535 7.         0.3097795 ]. \t  -0.6258969685789838 \t -0.449167771194887\n",
            "17     \t [ 1.88681738  3.46035586 14.          0.68070687  6.          0.9944693 ]. \t  \u001b[92m-0.42905917541157645\u001b[0m \t -0.42905917541157645\n",
            "18     \t [ 1.00455473  4.86452269 10.          0.65242434  1.          0.22603818]. \t  -0.6579989052523156 \t -0.42905917541157645\n",
            "19     \t [ 0.87144588  0.14777143 14.          0.88891044 13.          0.11582186]. \t  -0.6586044677275812 \t -0.42905917541157645\n",
            "20     \t [ 8.17732484  5.22697425 11.          0.61303379  1.          0.32672413]. \t  -0.6191817873624627 \t -0.42905917541157645\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.2900830531756915"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1d_1LyydIfe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d2e9477-7839-4a6d-a261-187ee4c3ab3c"
      },
      "source": [
        "end_lose = time.time()\n",
        "end_lose\n",
        "\n",
        "time_lose = end_lose - start_lose\n",
        "time_lose\n",
        "\n",
        "start_win = time.time()\n",
        "start_win"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1613476577.7161133"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZAyOw7XYVwAf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d59d31a-0c96-4e22-bfe4-bf09318e2c54"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 1 \n",
        "\n",
        "np.random.seed(run_num_1)\n",
        "surrogate_winner_1 = dtStudentProcess(d_cov_func, nu = df)\n",
        "\n",
        "X_train1, X_test1, y_train1, y_test1 = train_test_split(X, y, test_size=test_perc, random_state=run_num_1)\n",
        "\n",
        "def f_syn_polarity1(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_1, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train1, y=y_train1).mean())\n",
        "    return operator * score\n",
        "\n",
        "winner_1 = dGPGO_stp(surrogate_winner_1, Acquisition_new(util_winner), f_syn_polarity1, param, n_jobs = -1) # Define BayesOpt\n",
        "winner_1.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_winner_1 = winner_1.getResult()[0]\n",
        "params_winner_1['max_depth'] = int(params_winner_1['max_depth'])\n",
        "params_winner_1['min_child_weight'] = int(params_winner_1['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_winner_train1 = xgb.DMatrix(X_train1, y_train1)\n",
        "dX_winner_test1 = xgb.DMatrix(X_test1, y_test1)\n",
        "model_winner_1 = xgb.train(params_winner_1, dX_winner_train1)\n",
        "pred_winner_1 = model_winner_1.predict(dX_winner_test1)\n",
        "\n",
        "rmse_winner_1 = np.sqrt(mean_squared_error(pred_winner_1, y_test1))\n",
        "rmse_winner_1"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 6.12170176  1.69069754 11.          0.92695323  3.          0.36579277]. \t  -0.5875926438135084 \t -0.45245711231879204\n",
            "init   \t [ 1.49162957  0.22478325  6.          0.85613654 15.          0.19363322]. \t  -0.6811472770543097 \t -0.45245711231879204\n",
            "init   \t [ 5.27957805  2.81819356 11.          0.81062146 14.          0.74826845]. \t  -0.46117277972616416 \t -0.45245711231879204\n",
            "init   \t [3.81060005 6.24236569 7.         0.95038709 5.         0.9861745 ]. \t  -0.45245711231879204 \t -0.45245711231879204\n",
            "init   \t [4.77531134 7.87990376 6.         0.78863691 6.         0.47918609]. \t  -0.5612893767574513 \t -0.45245711231879204\n",
            "1      \t [ 8.28495996  8.75496601  5.          0.95020974 18.          0.2862647 ]. \t  -0.5992477418063687 \t -0.45245711231879204\n",
            "2      \t [ 1.54653084  8.85277533 10.          0.53898233 18.          0.74883978]. \t  -0.4727718253663954 \t -0.45245711231879204\n",
            "3      \t [ 0.39700633  8.08942284 14.          0.7083877  10.          0.62267441]. \t  -0.493623478965383 \t -0.45245711231879204\n",
            "4      \t [ 9.85849625  0.83439907  7.          0.7705582  19.          0.15487995]. \t  -0.6815452883308839 \t -0.45245711231879204\n",
            "5      \t [ 8.46942679  9.63483083 14.          0.86421899  7.          0.60511455]. \t  -0.4912126856170227 \t -0.45245711231879204\n",
            "6      \t [ 7.02760135  9.27399409 14.          0.61763492 15.          0.97680431]. \t  \u001b[92m-0.4480350915963286\u001b[0m \t -0.4480350915963286\n",
            "7      \t [7.19079936 0.87565629 5.         0.54470564 8.         0.4449021 ]. \t  -0.5736147518216788 \t -0.4480350915963286\n",
            "8      \t [ 0.1853115   8.98526847  5.          0.8540841  12.          0.79920234]. \t  -0.49199328039433976 \t -0.4480350915963286\n",
            "9      \t [ 0.56982715  8.86711094 12.          0.70129136  2.          0.28933245]. \t  -0.5929010136882604 \t -0.4480350915963286\n",
            "10     \t [ 7.35107151  9.52076187  9.01715323  0.9560546  11.01715323  0.97054945]. \t  \u001b[92m-0.4470690655634718\u001b[0m \t -0.4470690655634718\n",
            "11     \t [8.99095022 4.91206514 6.         0.87212346 1.         0.53198502]. \t  -0.5638457176592562 \t -0.4470690655634718\n",
            "12     \t [0.51753753 1.68511981 9.         0.80360039 9.         0.45200858]. \t  -0.5426731188677099 \t -0.4470690655634718\n",
            "13     \t [ 7.24610883  7.56670427 14.          0.55597174  1.          0.37553826]. \t  -0.5950674585896569 \t -0.4470690655634718\n",
            "14     \t [1.07854341 0.8489827  5.         0.60705022 4.         0.31148037]. \t  -0.5991411579830566 \t -0.4470690655634718\n",
            "15     \t [ 2.02410169  1.22367174 12.          0.7653715  19.          0.88484083]. \t  \u001b[92m-0.4468851480454651\u001b[0m \t -0.4468851480454651\n",
            "16     \t [10.          5.72391287 11.46569796  1.         20.          1.        ]. \t  \u001b[92m-0.44181382669037356\u001b[0m \t -0.44181382669037356\n",
            "17     \t [ 4.88315297  6.13465599 15.          1.         20.          1.        ]. \t  \u001b[92m-0.42835370714070786\u001b[0m \t -0.42835370714070786\n",
            "18     \t [ 5.7497153   2.41005924 13.          0.7728762   9.          0.98978178]. \t  -0.44429896132251834 \t -0.42835370714070786\n",
            "19     \t [ 9.16683022  4.45865658  7.79852078  0.96981119 10.79852078  0.86002427]. \t  -0.4539230750165967 \t -0.42835370714070786\n",
            "20     \t [ 8.34906147  0.06267628  6.          0.61163941 14.          0.6519383 ]. \t  -0.5296085687866636 \t -0.42835370714070786\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.3850165384814055"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hrDQbChpZ48F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a7a8011-37c4-40da-bcdf-a390ebed3a32"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 2 \n",
        "\n",
        "np.random.seed(run_num_2)\n",
        "surrogate_winner_2 = dtStudentProcess(d_cov_func, nu = df)\n",
        "\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X, y, test_size=test_perc, random_state=run_num_2)\n",
        "\n",
        "def f_syn_polarity2(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_2, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train2, y=y_train2).mean())\n",
        "    return operator * score\n",
        "\n",
        "winner_2 = dGPGO_stp(surrogate_winner_2, Acquisition_new(util_winner), f_syn_polarity2, param, n_jobs = -1) # Define BayesOpt\n",
        "winner_2.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_winner_2 = winner_2.getResult()[0]\n",
        "params_winner_2['max_depth'] = int(params_winner_2['max_depth'])\n",
        "params_winner_2['min_child_weight'] = int(params_winner_2['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_winner_train2 = xgb.DMatrix(X_train2, y_train2)\n",
        "dX_winner_test2 = xgb.DMatrix(X_test2, y_test2)\n",
        "model_winner_2 = xgb.train(params_winner_2, dX_winner_train2)\n",
        "pred_winner_2 = model_winner_2.predict(dX_winner_test2)\n",
        "\n",
        "rmse_winner_2 = np.sqrt(mean_squared_error(pred_winner_2, y_test2))\n",
        "rmse_winner_2"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 8.51985493  0.73903599  7.          0.71824677 17.          0.61827209]. \t  -0.5233872966482411 \t -0.42848969661986275\n",
            "init   \t [ 8.4047092   4.35120551 12.          0.84231905 16.          0.80172484]. \t  -0.42848969661986275 \t -0.42848969661986275\n",
            "init   \t [ 6.42749368  9.61026168 10.          0.73363448 15.          0.84932207]. \t  -0.43419399571353007 \t -0.42848969661986275\n",
            "init   \t [ 2.66008363  8.36685389  6.          0.92082163 12.          0.69999264]. \t  -0.5353675801921922 \t -0.42848969661986275\n",
            "init   \t [ 1.74779478  2.17691512 12.          0.71794562  2.          0.48990646]. \t  -0.5346620615324694 \t -0.42848969661986275\n",
            "1      \t [0.65899411 0.08911785 5.         0.57513718 8.         0.6649358 ]. \t  -0.5435492302872315 \t -0.42848969661986275\n",
            "2      \t [7.50154303 9.49874736 7.         0.83842364 4.         0.5714185 ]. \t  -0.5422231033003287 \t -0.42848969661986275\n",
            "3      \t [9.88263606 1.71181353 7.         0.97275193 1.         0.74825108]. \t  -0.441747506473179 \t -0.42848969661986275\n",
            "4      \t [ 0.38425009  3.26044865 12.          0.89101602 19.          0.732816  ]. \t  \u001b[92m-0.4225097924337229\u001b[0m \t -0.4225097924337229\n",
            "5      \t [ 0.17098677  4.24340552 14.          0.58045842 10.          0.64319766]. \t  -0.5100599626019375 \t -0.4225097924337229\n",
            "6      \t [ 8.24807017  4.40295761 13.          0.99966982  5.          0.29909156]. \t  -0.5674503952477116 \t -0.4225097924337229\n",
            "7      \t [8.17228996 0.99614058 7.         0.60509314 8.         0.9040458 ]. \t  -0.44411068565064227 \t -0.4225097924337229\n",
            "8      \t [ 2.33543052  9.65234501 14.          0.6896847  19.          0.38329235]. \t  -0.5709606841295877 \t -0.4225097924337229\n",
            "9      \t [ 2.26995028  7.26113474  5.          0.89244153 19.          0.72822627]. \t  -0.46107668663747675 \t -0.4225097924337229\n",
            "10     \t [0.15708054 7.33475875 5.         0.73448573 4.         0.78960732]. \t  -0.45554380802299876 \t -0.4225097924337229\n",
            "11     \t [ 2.40520679  9.030061   11.          0.61309393  6.          0.56126501]. \t  -0.5355680728892867 \t -0.4225097924337229\n",
            "12     \t [ 3.05457734  1.34829356  9.          0.5836481  13.          0.61527323]. \t  -0.5184947344115468 \t -0.4225097924337229\n",
            "13     \t [10. 10. 15.  1. 20.  1.]. \t  \u001b[92m-0.4092462984541993\u001b[0m \t -0.4092462984541993\n",
            "14     \t [ 9.36121058  6.35176728  6.          0.60864885 19.          0.85644281]. \t  -0.46989433163634564 \t -0.4092462984541993\n",
            "15     \t [ 9.59276769  7.37641605  6.          0.89538811 12.          0.94983532]. \t  -0.45071664883790197 \t -0.4092462984541993\n",
            "16     \t [ 1.81325607  0.38056982  5.          0.809705   18.          0.5636356 ]. \t  -0.5668463487185564 \t -0.4092462984541993\n",
            "17     \t [10.          8.48236639 15.          1.          9.17486671  1.        ]. \t  \u001b[92m-0.40738979385352503\u001b[0m \t -0.40738979385352503\n",
            "18     \t [4.3103888  3.16672722 7.         0.53544567 4.         0.24969096]. \t  -0.6863394906633806 \t -0.40738979385352503\n",
            "19     \t [ 8.28617332  0.26605862 13.          0.93560072 10.          0.23137808]. \t  -0.6840898497475586 \t -0.40738979385352503\n",
            "20     \t [ 8.96311905  9.42507441 14.          0.78456647  1.          0.32673768]. \t  -0.5683371749241675 \t -0.40738979385352503\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.270099338431699"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpUPyXRfZ95Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b592f73e-fb97-4858-a3f0-6540cda0f146"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 3 \n",
        "\n",
        "np.random.seed(run_num_3)\n",
        "surrogate_winner_3 = dtStudentProcess(d_cov_func, nu = df)\n",
        "\n",
        "X_train3, X_test3, y_train3, y_test3 = train_test_split(X, y, test_size=test_perc, random_state=run_num_3)\n",
        "\n",
        "def f_syn_polarity3(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_3, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train3, y=y_train3).mean())\n",
        "    return operator * score\n",
        "\n",
        "winner_3 = dGPGO_stp(surrogate_winner_3, Acquisition_new(util_winner), f_syn_polarity3, param, n_jobs = -1) # Define BayesOpt\n",
        "winner_3.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_winner_3 = winner_3.getResult()[0]\n",
        "params_winner_3['max_depth'] = int(params_winner_3['max_depth'])\n",
        "params_winner_3['min_child_weight'] = int(params_winner_3['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_winner_train3 = xgb.DMatrix(X_train3, y_train3)\n",
        "dX_winner_test3 = xgb.DMatrix(X_test3, y_test3)\n",
        "model_winner_3 = xgb.train(params_winner_3, dX_winner_train3)\n",
        "pred_winner_3 = model_winner_3.predict(dX_winner_test3)\n",
        "\n",
        "rmse_winner_3 = np.sqrt(mean_squared_error(pred_winner_3, y_test3))\n",
        "rmse_winner_3"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 7.51575607  1.09251275 13.          0.80377147 16.          0.31229624]. \t  -0.6858957949546655 \t -0.5081673732303724\n",
            "init   \t [6.12794891 1.19652403 5.         0.74822174 8.         0.33695838]. \t  -0.6849895205987827 \t -0.5081673732303724\n",
            "init   \t [ 6.21631223  4.4909976  10.          0.66764529  9.          0.77207323]. \t  -0.5081673732303724 \t -0.5081673732303724\n",
            "init   \t [ 0.58127233  4.79918477 11.          0.64526552 15.          0.62431773]. \t  -0.5106501747238601 \t -0.5081673732303724\n",
            "init   \t [ 5.76228258  0.67219724  7.          0.88228691 13.          0.29867219]. \t  -0.6839256266327581 \t -0.5081673732303724\n",
            "1      \t [ 6.2624148  8.9856789 15.         1.        20.         1.       ]. \t  \u001b[92m-0.40309806524146036\u001b[0m \t -0.40309806524146036\n",
            "2      \t [ 3.27075931  5.07207678 14.          0.99587284  2.          0.51564135]. \t  -0.6620507374994331 \t -0.40309806524146036\n",
            "3      \t [1.12049804 7.19863393 6.         0.79566554 3.         0.23444312]. \t  -0.730351254834763 \t -0.40309806524146036\n",
            "4      \t [ 1.37428023  9.7849173   5.          0.60086897 18.          0.5840189 ]. \t  -0.5267762125052486 \t -0.40309806524146036\n",
            "5      \t [ 3.06688498  9.78761676 14.          0.77000553  8.          0.74528336]. \t  -0.5028132248986308 \t -0.40309806524146036\n",
            "6      \t [ 8.49047509  6.95063855  8.          0.78475048 18.          0.89302182]. \t  -0.43933276511831937 \t -0.40309806524146036\n",
            "7      \t [ 9.9487535   9.16011292 14.6541235   1.         12.6541235   1.        ]. \t  \u001b[92m-0.40168163249930444\u001b[0m \t -0.40168163249930444\n",
            "8      \t [9.23783256 8.06561657 7.         0.71234276 2.         0.66490977]. \t  -0.5196635971869794 \t -0.40168163249930444\n",
            "9      \t [6.19566593 0.54575662 9.         0.53247577 1.         0.58149275]. \t  -0.5171483404397559 \t -0.40168163249930444\n",
            "10     \t [ 2.3087212   0.2642352  14.          0.51839553 10.          0.2734256 ]. \t  -0.7320414151870791 \t -0.40168163249930444\n",
            "11     \t [ 4.50194098  9.75437984  6.          0.65120858 10.          0.70679847]. \t  -0.5243008576265308 \t -0.40168163249930444\n",
            "12     \t [0.68292772 1.29330088 8.         0.55106591 5.         0.72104823]. \t  -0.5120102499343793 \t -0.40168163249930444\n",
            "13     \t [ 2.80079796  0.61761702  5.          0.62606225 19.          0.71334647]. \t  -0.5274647167409906 \t -0.40168163249930444\n",
            "14     \t [10.         10.         12.62644747  1.          5.62644747  1.        ]. \t  -0.40293386097787554 \t -0.40168163249930444\n",
            "15     \t [ 8.91533927  0.16704192 13.          0.66647549  6.          0.25014   ]. \t  -0.7314177593008601 \t -0.40168163249930444\n",
            "16     \t [ 1.84620574  0.32709184 13.          0.76394118 19.          0.90059117]. \t  -0.42118028750387354 \t -0.40168163249930444\n",
            "17     \t [ 9.74335174  1.4461917   6.          0.79897128 17.          0.38412376]. \t  -0.6852329951945343 \t -0.40168163249930444\n",
            "18     \t [7.55452431e-03 9.44297730e+00 1.40000000e+01 9.48110213e-01\n",
            " 1.90000000e+01 5.05826889e-01]. \t  -0.6563162692297837 \t -0.40168163249930444\n",
            "19     \t [ 0.20342348  4.92091808  7.          0.90253054 11.          0.83122375]. \t  -0.5092462198396636 \t -0.40168163249930444\n",
            "20     \t [10.          5.40337348 15.          1.          8.58947633  1.        ]. \t  \u001b[92m-0.3962626321345415\u001b[0m \t -0.3962626321345415\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.504766287214024"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKX_nfEaaAwm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06036bd5-b8b5-4f0d-c1f9-1fc30d4e54fb"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 4 \n",
        "\n",
        "np.random.seed(run_num_4)\n",
        "surrogate_winner_4 = dtStudentProcess(d_cov_func, nu = df)\n",
        "\n",
        "X_train4, X_test4, y_train4, y_test4 = train_test_split(X, y, test_size=test_perc, random_state=run_num_4)\n",
        "\n",
        "def f_syn_polarity4(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_4, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train4, y=y_train4).mean())\n",
        "    return operator * score\n",
        "\n",
        "winner_4 = dGPGO_stp(surrogate_winner_4, Acquisition_new(util_winner), f_syn_polarity4, param, n_jobs = -1) # Define BayesOpt\n",
        "winner_4.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_winner_4 = winner_4.getResult()[0]\n",
        "params_winner_4['max_depth'] = int(params_winner_4['max_depth'])\n",
        "params_winner_4['min_child_weight'] = int(params_winner_4['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_winner_train4 = xgb.DMatrix(X_train4, y_train4)\n",
        "dX_winner_test4 = xgb.DMatrix(X_test4, y_test4)\n",
        "model_winner_4 = xgb.train(params_winner_4, dX_winner_train4)\n",
        "pred_winner_4 = model_winner_4.predict(dX_winner_test4)\n",
        "\n",
        "rmse_winner_4 = np.sqrt(mean_squared_error(pred_winner_4, y_test4))\n",
        "rmse_winner_4"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 9.42798506  1.23319667 14.          0.81910755  7.          0.7021549 ]. \t  -0.5581090891563811 \t -0.5581090891563811\n",
            "init   \t [ 5.54158317  7.71233962 10.          0.59117644  6.          0.31476458]. \t  -0.6204181120564479 \t -0.5581090891563811\n",
            "init   \t [2.18282634 2.93794127 7.         0.84739327 5.         0.62716229]. \t  -0.5626656925564042 \t -0.5581090891563811\n",
            "init   \t [ 5.66159581  4.68678814 11.          0.54147808  9.          0.58609221]. \t  -0.5595885941852147 \t -0.5581090891563811\n",
            "init   \t [ 3.02160958  6.07432614  6.          0.92038559 14.          0.12778609]. \t  -0.6361772582153176 \t -0.5581090891563811\n",
            "1      \t [ 1.89460697  0.10324856 13.          0.70356349 16.          0.67776197]. \t  \u001b[92m-0.5568243993303088\u001b[0m \t -0.5568243993303088\n",
            "2      \t [ 9.69969     4.32607677 14.          0.87739158 16.          0.66014279]. \t  -0.5573098211239071 \t -0.5568243993303088\n",
            "3      \t [ 2.2210328   8.31054196 13.          0.70317243 19.          0.56953182]. \t  -0.573504425404327 \t -0.5568243993303088\n",
            "4      \t [ 0.14706294  0.75702217 14.          0.56309891  8.          0.29887971]. \t  -0.6254087880261816 \t -0.5568243993303088\n",
            "5      \t [ 6.94218243  0.32144048  6.          0.59286079 19.          0.70636825]. \t  -0.5706574487524134 \t -0.5568243993303088\n",
            "6      \t [ 9.24159749  9.43575685  7.          0.98380459 19.          0.34937346]. \t  -0.6187589419936593 \t -0.5568243993303088\n",
            "7      \t [7.57184449 0.0324327  9.         0.86318723 1.         0.6413689 ]. \t  -0.5611723012436254 \t -0.5568243993303088\n",
            "8      \t [ 1.62755426  6.7921099  14.          0.91903975  1.          0.61791343]. \t  -0.5680865264437001 \t -0.5568243993303088\n",
            "9      \t [ 9.43074486  0.98044049  5.          0.96290856 10.          0.27397556]. \t  -0.6390681992417737 \t -0.5568243993303088\n",
            "10     \t [ 8.73128599  9.22658128 14.          0.98344346  2.          0.13489277]. \t  -0.6368164613179 \t -0.5568243993303088\n",
            "11     \t [10.          9.85024967 15.          1.         19.76358417  1.        ]. \t  \u001b[92m-0.42247220611358427\u001b[0m \t -0.42247220611358427\n",
            "12     \t [8.91398595 7.00476796 6.         0.72156795 1.         0.41673037]. \t  -0.6210807206819127 \t -0.42247220611358427\n",
            "13     \t [ 9.58851274  9.75423983 14.          0.60815214 12.          0.97572229]. \t  -0.44045254560081915 \t -0.42247220611358427\n",
            "14     \t [ 9.87497198  6.68701692  6.          0.55980787 13.          0.32605223]. \t  -0.6200123103666009 \t -0.42247220611358427\n",
            "15     \t [ 2.16464782  9.35692886 11.          0.97431128 12.          0.43257419]. \t  -0.5747879420555002 \t -0.42247220611358427\n",
            "16     \t [ 3.24618617  0.72358282  6.          0.95248328 10.          0.4713404 ]. \t  -0.58635886876776 \t -0.42247220611358427\n",
            "17     \t [ 5.57905323  2.63226907 11.          0.80240625 19.          0.689713  ]. \t  -0.5583484597169505 \t -0.42247220611358427\n",
            "18     \t [7.36355448 5.3077281  5.         0.5709887  7.         0.87569884]. \t  -0.467633753337906 \t -0.42247220611358427\n",
            "19     \t [ 7.93069008  0.11830338 11.          0.97110397 13.          0.22887984]. \t  -0.6356672520315936 \t -0.42247220611358427\n",
            "20     \t [ 1.52748847  0.19736792  5.          0.54225494 16.          0.82015445]. \t  -0.5008608278036402 \t -0.42247220611358427\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.323842785931203"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJmI9saAaEG1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc5781de-165f-4547-e51b-893443325834"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 5 \n",
        "\n",
        "np.random.seed(run_num_5)\n",
        "surrogate_winner_5 = dtStudentProcess(d_cov_func, nu = df)\n",
        "\n",
        "X_train5, X_test5, y_train5, y_test5 = train_test_split(X, y, test_size=test_perc, random_state=run_num_5)\n",
        "\n",
        "def f_syn_polarity5(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_5, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train5, y=y_train5).mean())\n",
        "    return operator * score\n",
        "\n",
        "winner_5 = dGPGO_stp(surrogate_winner_5, Acquisition_new(util_winner), f_syn_polarity5, param, n_jobs = -1) # Define BayesOpt\n",
        "winner_5.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_winner_5 = winner_5.getResult()[0]\n",
        "params_winner_5['max_depth'] = int(params_winner_5['max_depth'])\n",
        "params_winner_5['min_child_weight'] = int(params_winner_5['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_winner_train5 = xgb.DMatrix(X_train5, y_train5)\n",
        "dX_winner_test5 = xgb.DMatrix(X_test5, y_test5)\n",
        "model_winner_5 = xgb.train(params_winner_5, dX_winner_train5)\n",
        "pred_winner_5 = model_winner_5.predict(dX_winner_test5)\n",
        "\n",
        "rmse_winner_5 = np.sqrt(mean_squared_error(pred_winner_5, y_test5))\n",
        "rmse_winner_5"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 5.57353274  5.41760627  8.          0.53710825 19.          0.26034376]. \t  -0.6523797617249512 \t -0.466901759574539\n",
            "init   \t [5.17826437 9.78817462 5.         0.84048437 6.         0.79712615]. \t  -0.4886603987288415 \t -0.466901759574539\n",
            "init   \t [ 9.84476201  9.65484856  5.          0.59464714 19.          0.34528747]. \t  -0.564220001422124 \t -0.466901759574539\n",
            "init   \t [ 9.17476842  4.53262239 13.          0.64368565 10.          0.515308  ]. \t  -0.5049042831858346 \t -0.466901759574539\n",
            "init   \t [ 8.66719465  7.99685372 12.          0.79833571 17.          0.72443466]. \t  -0.466901759574539 \t -0.466901759574539\n",
            "1      \t [ 3.31480734  6.81602635 12.          0.95330947  1.          0.27334927]. \t  -0.6531167190113543 \t -0.466901759574539\n",
            "2      \t [2.53140694 0.29740057 9.         0.7505917  7.         0.82287404]. \t  -0.47475119282304395 \t -0.466901759574539\n",
            "3      \t [ 9.84728748  0.28222034 13.          0.93969018  3.          0.22339517]. \t  -0.6522247903981228 \t -0.466901759574539\n",
            "4      \t [ 0.68116737  3.94603776 14.          0.80380753 14.          0.77317996]. \t  \u001b[92m-0.4613270217842209\u001b[0m \t -0.4613270217842209\n",
            "5      \t [ 9.6928838   0.38736829  6.          0.56167539 11.          0.4775038 ]. \t  -0.5239028011959734 \t -0.4613270217842209\n",
            "6      \t [ 1.62699639  9.0444625   7.          0.60549869 13.          0.13190699]. \t  -0.6526723866694732 \t -0.4613270217842209\n",
            "7      \t [ 7.31108123  0.10391064 14.          0.53552151 15.          0.19462546]. \t  -0.6531977730432595 \t -0.4613270217842209\n",
            "8      \t [6.05825818 3.15263959 6.         0.61034148 2.         0.17375174]. \t  -0.6517203832791882 \t -0.4613270217842209\n",
            "9      \t [ 1.05461775  1.17661042  5.          0.5648338  17.          0.90649901]. \t  -0.4702337390488589 \t -0.4613270217842209\n",
            "10     \t [ 0.29281635  9.83144868 11.          0.57517626 19.          0.13846438]. \t  -0.6543975191191339 \t -0.4613270217842209\n",
            "11     \t [10.          8.52701554 10.69499584  1.          4.69499584  1.        ]. \t  \u001b[92m-0.4119757158294016\u001b[0m \t -0.4119757158294016\n",
            "12     \t [0.57140531 6.2823043  5.         0.70475062 2.         0.3328708 ]. \t  -0.5594485185267623 \t -0.4119757158294016\n",
            "13     \t [ 2.5240907   6.84169344 14.          0.61623097  8.          0.50182056]. \t  -0.5037582468108619 \t -0.4119757158294016\n",
            "14     \t [0.0858834  4.38043443 5.         0.57114514 9.         0.11102852]. \t  -0.6531285980012218 \t -0.4119757158294016\n",
            "15     \t [ 9.08851157  7.7443441   6.          0.74314397 12.          0.52508293]. \t  -0.5238407378055661 \t -0.4119757158294016\n",
            "16     \t [ 0.62266475  1.9018111  14.          0.63329124  3.          0.94054341]. \t  -0.42529491633232686 \t -0.4119757158294016\n",
            "17     \t [ 4.30502755  3.91708398  8.          0.6422274  14.          0.74461881]. \t  -0.4811715211316586 \t -0.4119757158294016\n",
            "18     \t [10.          3.53542615 15.          1.         20.          1.        ]. \t  \u001b[92m-0.4006545029709102\u001b[0m \t -0.4006545029709102\n",
            "19     \t [0.62169533 8.085678   9.         0.63966429 7.         0.67068585]. \t  -0.4847242387534262 \t -0.4006545029709102\n",
            "20     \t [ 5.64365991 10.         15.          1.         12.1980017   1.        ]. \t  \u001b[92m-0.39775794942132814\u001b[0m \t -0.39775794942132814\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.192568642647803"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulhEolsxaG4k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b091bbc-0f3f-4555-c607-ce8cc54b7eab"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 6 \n",
        "\n",
        "np.random.seed(run_num_6)\n",
        "surrogate_winner_6 = dtStudentProcess(d_cov_func, nu = df)\n",
        "\n",
        "X_train6, X_test6, y_train6, y_test6 = train_test_split(X, y, test_size=test_perc, random_state=run_num_6)\n",
        "\n",
        "def f_syn_polarity6(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=int(min_child_weight),\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_6, objective = 'reg:squarederror', eval_metric = 'rmse')\n",
        "    score = np.array(cross_val_score(reg, X=X_train6, y=y_train6).mean())\n",
        "    return operator * score\n",
        "\n",
        "winner_6 = dGPGO_stp(surrogate_winner_6, Acquisition_new(util_winner), f_syn_polarity6, param, n_jobs = -1) # Define BayesOpt\n",
        "winner_6.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_winner_6 = winner_6.getResult()[0]\n",
        "params_winner_6['max_depth'] = int(params_winner_6['max_depth'])\n",
        "params_winner_6['min_child_weight'] = int(params_winner_6['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_winner_train6 = xgb.DMatrix(X_train6, y_train6)\n",
        "dX_winner_test6 = xgb.DMatrix(X_test6, y_test6)\n",
        "model_winner_6 = xgb.train(params_winner_6, dX_winner_train6)\n",
        "pred_winner_6 = model_winner_6.predict(dX_winner_test6)\n",
        "\n",
        "rmse_winner_6 = np.sqrt(mean_squared_error(pred_winner_6, y_test6))\n",
        "rmse_winner_6"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [8.92860151 3.31979805 5.         0.99251441 2.         0.57683563]. \t  -0.5471181110640053 \t -0.5219867521874206\n",
            "init   \t [4.18807429 3.35407849 9.         0.87750649 3.         0.56623277]. \t  -0.629286701398377 \t -0.5219867521874206\n",
            "init   \t [ 5.788586    6.45355096 14.          0.70660047 12.          0.82154882]. \t  -0.5219867521874206 \t -0.5219867521874206\n",
            "init   \t [4.58184578 6.73834679 5.         0.90108528 3.         0.65482895]. \t  -0.5476166257785141 \t -0.5219867521874206\n",
            "init   \t [ 4.42510505  5.75952352 14.          0.97882365 15.          0.29525604]. \t  -0.6655867879161768 \t -0.5219867521874206\n",
            "1      \t [ 7.09628606  0.82458553  6.          0.52978945 17.          0.78827589]. \t  -0.5368551114917748 \t -0.5219867521874206\n",
            "2      \t [ 1.88885215  8.62524025  6.          0.90916563 14.          0.40379086]. \t  -0.6680072912825885 \t -0.5219867521874206\n",
            "3      \t [ 0.5654966   9.52584762 14.          0.82016688  4.          0.10717254]. \t  -0.7389555129670606 \t -0.5219867521874206\n",
            "4      \t [ 9.63033822  0.03793409 13.          0.68076302  7.          0.91391681]. \t  \u001b[92m-0.4301920669254681\u001b[0m \t -0.4301920669254681\n",
            "5      \t [ 9.97469028  9.15579642 12.          0.66745021 19.          0.28959158]. \t  -0.6668248751122167 \t -0.4301920669254681\n",
            "6      \t [ 9.85880187  8.48475629 13.          0.76006171  2.          0.58751173]. \t  -0.5403277210407594 \t -0.4301920669254681\n",
            "7      \t [ 9.51844     9.62786679  5.          0.56989798 15.          0.51068013]. \t  -0.6345288616058514 \t -0.4301920669254681\n",
            "8      \t [ 0.15151883  3.04875397  5.          0.61495161 19.          0.79420245]. \t  -0.5357132607573185 \t -0.4301920669254681\n",
            "9      \t [ 3.08077333  0.89569015  6.          0.84149914 11.          0.94992477]. \t  -0.4472711154368675 \t -0.4301920669254681\n",
            "10     \t [ 0.19816257  1.95434851 14.          0.59292238  7.          0.17971262]. \t  -0.7383261448869979 \t -0.4301920669254681\n",
            "11     \t [6.81096959 9.8930679  8.78581227 1.         7.78581227 1.        ]. \t  \u001b[92m-0.42187820855081243\u001b[0m \t -0.42187820855081243\n",
            "12     \t [ 1.90877122  0.88669904 11.          0.72802974 19.          0.27519153]. \t  -0.7388803857120884 \t -0.42187820855081243\n",
            "13     \t [ 8.81165642  4.29421951  7.          0.5916589  11.          0.53517998]. \t  -0.6339625326144551 \t -0.42187820855081243\n",
            "14     \t [ 5.51026131  5.55720773  8.78809245  1.         20.          1.        ]. \t  -0.4275428377294908 \t -0.42187820855081243\n",
            "15     \t [0.02368926 6.73338636 8.         0.75491444 9.         0.43643271]. \t  -0.6313059012485673 \t -0.42187820855081243\n",
            "16     \t [ 0.05509359  8.13873516 11.          0.83199834 17.          0.87257216]. \t  -0.4292721439254903 \t -0.42187820855081243\n",
            "17     \t [ 6.73109999  0.4324142  14.          0.50984814 14.          0.47343031]. \t  -0.6343392694069653 \t -0.42187820855081243\n",
            "18     \t [ 0.1424991   9.83224829 14.          0.82001413 10.          0.36444993]. \t  -0.6674636669202734 \t -0.42187820855081243\n",
            "19     \t [ 0.17525712  4.19183624 13.          0.93093846  1.          0.77496282]. \t  -0.5254340278392874 \t -0.42187820855081243\n",
            "20     \t [ 5.33662834 10.         15.          1.         20.          1.        ]. \t  \u001b[92m-0.4112239513363451\u001b[0m \t -0.4112239513363451\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.2498709602082405"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYebx3RVaJ1w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0df6ebb5-13c9-41d6-98fe-97092924e806"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 7 \n",
        "\n",
        "np.random.seed(run_num_7)\n",
        "surrogate_winner_7 = dtStudentProcess(d_cov_func, nu = df)\n",
        "\n",
        "X_train7, X_test7, y_train7, y_test7 = train_test_split(X, y, test_size=test_perc, random_state=run_num_7)\n",
        "\n",
        "def f_syn_polarity7(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_7, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train7, y=y_train7).mean())\n",
        "    return operator * score\n",
        "\n",
        "winner_7 = dGPGO_stp(surrogate_winner_7, Acquisition_new(util_winner), f_syn_polarity7, param, n_jobs = -1) # Define BayesOpt\n",
        "winner_7.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_winner_7 = winner_7.getResult()[0]\n",
        "params_winner_7['max_depth'] = int(params_winner_7['max_depth'])\n",
        "params_winner_7['min_child_weight'] = int(params_winner_7['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_winner_train7 = xgb.DMatrix(X_train7, y_train7)\n",
        "dX_winner_test7 = xgb.DMatrix(X_test7, y_test7)\n",
        "model_winner_7 = xgb.train(params_winner_7, dX_winner_train7)\n",
        "pred_winner_7 = model_winner_7.predict(dX_winner_test7)\n",
        "\n",
        "rmse_winner_7 = np.sqrt(mean_squared_error(pred_winner_7, y_test7))\n",
        "rmse_winner_7"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 8.11505126  6.92324143 13.          0.94046175 12.          0.3650384 ]. \t  -0.612852517751432 \t -0.4284992540085738\n",
            "init   \t [ 5.15357029  9.0066636   6.          0.74104227 15.          0.8966337 ]. \t  -0.45500673865987984 \t -0.4284992540085738\n",
            "init   \t [ 2.63548913  9.6610934  14.          0.64712408  1.          0.80820948]. \t  -0.43730487111474525 \t -0.4284992540085738\n",
            "init   \t [ 1.40821426  1.72758589 12.          0.70495069  5.          0.86632715]. \t  -0.4284992540085738 \t -0.4284992540085738\n",
            "init   \t [2.85126987 2.73434054 7.         0.88394733 8.         0.65545939]. \t  -0.48253390492139586 \t -0.4284992540085738\n",
            "1      \t [1.03697999e-02 2.51695028e+00 1.40000000e+01 6.19497248e-01\n",
            " 1.90000000e+01 7.01827417e-01]. \t  -0.47758423778274467 \t -0.4284992540085738\n",
            "2      \t [9.9089961  7.50235155 7.         0.67925503 5.         0.2471192 ]. \t  -0.6736712023667494 \t -0.4284992540085738\n",
            "3      \t [ 7.21196079  0.11384945  8.          0.67755731 17.          0.74323626]. \t  -0.4647919397201027 \t -0.4284992540085738\n",
            "4      \t [0.35283087 9.2950614  5.         0.93054416 3.         0.27625531]. \t  -0.670508947897148 \t -0.4284992540085738\n",
            "5      \t [ 0.19352731  8.09930152 12.          0.94917595  9.          0.75994611]. \t  -0.4324409379892192 \t -0.4284992540085738\n",
            "6      \t [9.76759527 0.48761621 7.         0.96362191 1.         0.137157  ]. \t  -0.6715547907624335 \t -0.4284992540085738\n",
            "7      \t [ 8.53387117  0.35946342 13.          0.65332712  6.          0.68302293]. \t  -0.4712802345283902 \t -0.4284992540085738\n",
            "8      \t [ 0.45155781  2.33209808  5.          0.73660734 17.          0.83890491]. \t  -0.4804814871680856 \t -0.4284992540085738\n",
            "9      \t [0.64179114 0.40852847 6.         0.99681473 1.         0.9418993 ]. \t  -0.4466271585670484 \t -0.4284992540085738\n",
            "10     \t [ 9.21287749  3.65759887  5.          0.85687073 12.          0.88459516]. \t  -0.45867733996321497 \t -0.4284992540085738\n",
            "11     \t [ 5.59979238  8.61929001 14.          0.77379667 19.          0.73317435]. \t  -0.4428859637361883 \t -0.4284992540085738\n",
            "12     \t [ 7.8364028   6.92382441 14.          0.64439058  5.          0.16362829]. \t  -0.6733895956434546 \t -0.4284992540085738\n",
            "13     \t [ 0.28968499  1.98768694 14.          0.97996073 12.          0.15615797]. \t  -0.6725851052423111 \t -0.4284992540085738\n",
            "14     \t [ 9.33840851  1.89525337 14.          0.58033244 16.          0.6948334 ]. \t  -0.48503655812010615 \t -0.4284992540085738\n",
            "15     \t [ 6.69461719  2.3571023  14.          0.65877714  1.          0.31333079]. \t  -0.6185519148426568 \t -0.4284992540085738\n",
            "16     \t [4.54036829 9.23672573 7.         0.78071654 7.         0.925943  ]. \t  -0.44422074369565967 \t -0.4284992540085738\n",
            "17     \t [ 9.52566208  5.8884151   6.          0.78376025 19.          0.40148749]. \t  -0.6258103990077785 \t -0.4284992540085738\n",
            "18     \t [ 0.54817901  8.89660506 10.          0.62947663 17.          0.84757194]. \t  -0.4525493854683914 \t -0.4284992540085738\n",
            "19     \t [ 4.03483482  4.30400628 12.          0.75824944 15.          0.33442009]. \t  -0.6164444876695716 \t -0.4284992540085738\n",
            "20     \t [ 2.45273961  5.23216272 10.          0.89291144  1.          0.75188209]. \t  -0.43728627888824734 \t -0.4284992540085738\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.492050628419268"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xk0IPTSTbIl3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46c8912b-f689-4df1-ffff-9f0ba0afc723"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 8 \n",
        "\n",
        "np.random.seed(run_num_8)\n",
        "surrogate_winner_8 = dtStudentProcess(d_cov_func, nu = df)\n",
        "\n",
        "X_train8, X_test8, y_train8, y_test8 = train_test_split(X, y, test_size=test_perc, random_state=run_num_8)\n",
        "\n",
        "def f_syn_polarity8(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_8, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train8, y=y_train8).mean())\n",
        "    return operator * score\n",
        "\n",
        "winner_8 = dGPGO_stp(surrogate_winner_8, Acquisition_new(util_winner), f_syn_polarity8, param, n_jobs = -1) # Define BayesOpt\n",
        "winner_8.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_winner_8 = winner_8.getResult()[0]\n",
        "params_winner_8['max_depth'] = int(params_winner_8['max_depth'])\n",
        "params_winner_8['min_child_weight'] = int(params_winner_8['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_winner_train8 = xgb.DMatrix(X_train8, y_train8)\n",
        "dX_winner_test8 = xgb.DMatrix(X_test8, y_test8)\n",
        "model_winner_8 = xgb.train(params_winner_8, dX_winner_train8)\n",
        "pred_winner_8 = model_winner_8.predict(dX_winner_test8)\n",
        "\n",
        "rmse_winner_8 = np.sqrt(mean_squared_error(pred_winner_8, y_test8))\n",
        "rmse_winner_8"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 1.29221489  0.77396898 11.          0.56062468  3.          0.25451879]. \t  -0.6231794271723858 \t -0.4674926150198253\n",
            "init   \t [ 0.10611429  1.37648061  8.          0.6050375  15.          0.790205  ]. \t  -0.4674926150198253 \t -0.4674926150198253\n",
            "init   \t [ 8.35239571  4.16908043  7.          0.67902746 14.          0.16683749]. \t  -0.6176439249709439 \t -0.4674926150198253\n",
            "init   \t [ 6.06301777  7.27127491 12.          0.87295531 11.          0.37694382]. \t  -0.5951520828762356 \t -0.4674926150198253\n",
            "init   \t [8.85209637 7.89269955 6.         0.72444854 3.         0.47766986]. \t  -0.5688519156580504 \t -0.4674926150198253\n",
            "1      \t [ 1.47825581  9.16010046 11.          0.96606546  1.          0.92358292]. \t  \u001b[92m-0.43518063190798095\u001b[0m \t -0.43518063190798095\n",
            "2      \t [ 7.52273592  2.30141271 14.          0.75505047 19.          0.77804046]. \t  -0.45158957529537797 \t -0.43518063190798095\n",
            "3      \t [1.64420908 9.47353882 5.         0.79993338 7.         0.15673599]. \t  -0.6175838614360663 \t -0.43518063190798095\n",
            "4      \t [ 1.02399475  9.65113789  9.          0.53947972 17.          0.8292164 ]. \t  -0.4719352875928771 \t -0.43518063190798095\n",
            "5      \t [8.14212583 0.61314553 6.         0.86488725 6.         0.28574491]. \t  -0.5954481959734638 \t -0.43518063190798095\n",
            "6      \t [ 8.76387285  7.83054608 13.          0.54159783  4.          0.24710422]. \t  -0.6207082177098945 \t -0.43518063190798095\n",
            "7      \t [ 9.0517509   9.45530828 11.          0.85305105 19.          0.79961763]. \t  -0.4589725747508552 \t -0.43518063190798095\n",
            "8      \t [ 7.77762087  0.97608919 14.          0.56979605  8.          0.73436998]. \t  -0.4483480686509709 \t -0.43518063190798095\n",
            "9      \t [2.67676373 3.95611485 5.         0.79739163 2.         0.89987835]. \t  -0.45674643004535687 \t -0.43518063190798095\n",
            "10     \t [ 0.62854908  4.77207773 14.          0.78182185 18.          0.73284936]. \t  -0.4461225555243119 \t -0.43518063190798095\n",
            "11     \t [10.         10.         15.          1.         13.34986084  1.        ]. \t  \u001b[92m-0.4241615830586962\u001b[0m \t -0.4241615830586962\n",
            "12     \t [2.36382825 1.46562105 7.         0.95741638 8.         0.88392976]. \t  -0.44589385792483893 \t -0.4241615830586962\n",
            "13     \t [ 9.10053278  1.48753729 11.          0.9741735   1.          0.65977303]. \t  -0.5223704878111852 \t -0.4241615830586962\n",
            "14     \t [ 9.48329103  0.2076913   7.          0.76655383 19.          0.88164364]. \t  -0.4616094756962891 \t -0.4241615830586962\n",
            "15     \t [ 0.81390458  8.42248516 12.          0.78729971  7.          0.71328057]. \t  -0.5176932464043367 \t -0.4241615830586962\n",
            "16     \t [9.84944185 9.92792336 8.93867593 0.98042483 7.93867593 0.93918637]. \t  -0.44586868506139526 \t -0.4241615830586962\n",
            "17     \t [ 0.81210087  0.8533868  13.          0.66456412 12.          0.37769434]. \t  -0.5971747957799627 \t -0.4241615830586962\n",
            "18     \t [ 1.65599464  6.3538435   8.          0.53194309 12.          0.76826597]. \t  -0.4697730854599239 \t -0.4241615830586962\n",
            "19     \t [ 4.73314736  4.45326463  8.          0.73319186 19.          0.81497444]. \t  -0.4686423544438131 \t -0.4241615830586962\n",
            "20     \t [ 0.49912916  8.65454214 14.          0.6041034  13.          0.78070099]. \t  -0.4490076307787912 \t -0.4241615830586962\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.350880990767371"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9UroEj_RbLSb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ed217ac-b825-479c-ebb6-e6613017354e"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 9 \n",
        "\n",
        "np.random.seed(run_num_9)\n",
        "surrogate_winner_9 = dtStudentProcess(d_cov_func, nu = df)\n",
        "\n",
        "X_train9, X_test9, y_train9, y_test9 = train_test_split(X, y, test_size=test_perc, random_state=run_num_9)\n",
        "\n",
        "def f_syn_polarity9(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_9, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train9, y=y_train9).mean())\n",
        "    return operator * score\n",
        "\n",
        "winner_9 = dGPGO_stp(surrogate_winner_9, Acquisition_new(util_winner), f_syn_polarity9, param, n_jobs = -1) # Define BayesOpt\n",
        "winner_9.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_winner_9 = winner_9.getResult()[0]\n",
        "params_winner_9['max_depth'] = int(params_winner_9['max_depth'])\n",
        "params_winner_9['min_child_weight'] = int(params_winner_9['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_winner_train9 = xgb.DMatrix(X_train9, y_train9)\n",
        "dX_winner_test9 = xgb.DMatrix(X_test9, y_test9)\n",
        "model_winner_9 = xgb.train(params_winner_9, dX_winner_train9)\n",
        "pred_winner_9 = model_winner_9.predict(dX_winner_test9)\n",
        "\n",
        "rmse_winner_9 = np.sqrt(mean_squared_error(pred_winner_9, y_test9))\n",
        "rmse_winner_9"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 8.0342804   5.27522296 13.          0.60741904 17.          0.15336222]. \t  -0.6890593508433319 \t -0.4886355541929722\n",
            "init   \t [7.56063644 1.79876369 7.         0.84869647 5.         0.55956942]. \t  -0.5532305414240943 \t -0.4886355541929722\n",
            "init   \t [2.43675539 0.20225128 6.         0.51669963 7.         0.4646446 ]. \t  -0.5609155518755553 \t -0.4886355541929722\n",
            "init   \t [ 7.6043547   4.73758376  9.          0.87564624 15.          0.65751286]. \t  -0.4886355541929722 \t -0.4886355541929722\n",
            "init   \t [ 4.70349314  6.71029586  6.          0.97680389 11.          0.41971652]. \t  -0.6548660487332898 \t -0.4886355541929722\n",
            "1      \t [ 0.20910387  7.25545521 14.          0.59407892  3.          0.95410586]. \t  \u001b[92m-0.4312356520914486\u001b[0m \t -0.4312356520914486\n",
            "2      \t [ 2.14892068  9.25577985 14.          0.84828472 11.          0.2793841 ]. \t  -0.6883959263608518 \t -0.4312356520914486\n",
            "3      \t [8.96295733 9.78259943 6.         0.93920561 3.         0.7584395 ]. \t  -0.46031473907857645 \t -0.4312356520914486\n",
            "4      \t [ 0.36183479  2.12051032 11.          0.68315505 14.          0.49641012]. \t  -0.5485412470283623 \t -0.4312356520914486\n",
            "5      \t [ 8.07113618  3.30997738 14.          0.96288466  1.          0.49057087]. \t  -0.5509218104954249 \t -0.4312356520914486\n",
            "6      \t [ 9.17710587  9.71573287  5.          0.9584376  19.          0.78072441]. \t  -0.4682275209845145 \t -0.4312356520914486\n",
            "7      \t [0.25554004 9.32227491 7.         0.5022509  6.         0.29657349]. \t  -0.6584222556106297 \t -0.4312356520914486\n",
            "8      \t [ 0.31270412  9.94818438  8.          0.84397658 19.          0.43974503]. \t  -0.5483333357303245 \t -0.4312356520914486\n",
            "9      \t [ 8.76504058  0.46155195 14.          0.61459938 10.          0.19095511]. \t  -0.6902913672662703 \t -0.4312356520914486\n",
            "10     \t [2.64181806 4.49562913 7.         0.97176789 1.         0.98112344]. \t  -0.4460405981119148 \t -0.4312356520914486\n",
            "11     \t [ 6.27627245  8.67947619 12.65705399  1.          4.65705399  1.        ]. \t  \u001b[92m-0.414830971715495\u001b[0m \t -0.414830971715495\n",
            "12     \t [ 1.24063465  0.49108047  5.          0.7663067  19.          0.77470458]. \t  -0.46859302176079265 \t -0.414830971715495\n",
            "13     \t [ 4.60583171  0.06752    13.          0.94036906 18.          0.58293565]. \t  -0.47784115580365294 \t -0.414830971715495\n",
            "14     \t [ 3.38351629  2.89927867 12.          0.76452212  7.          0.61222347]. \t  -0.4809111856529227 \t -0.414830971715495\n",
            "15     \t [ 7.69267031  0.52709121  5.          0.97712342 11.          0.47111142]. \t  -0.5671609865381374 \t -0.414830971715495\n",
            "16     \t [ 3.03200785  8.68447834 14.          0.9369838  19.          0.61316303]. \t  -0.4768696907965634 \t -0.414830971715495\n",
            "17     \t [ 9.35257803  1.01913578 10.          0.61034129 19.          0.44798121]. \t  -0.5514224578726403 \t -0.414830971715495\n",
            "18     \t [10.          5.31574631 10.02199735  1.          8.02199735  1.        ]. \t  -0.4241302824970875 \t -0.414830971715495\n",
            "19     \t [ 0.48791793  1.09047989 14.          0.80757691  3.          0.37971249]. \t  -0.6640500439913284 \t -0.414830971715495\n",
            "20     \t [10.          8.8308031  13.44132445  1.         12.44132445  1.        ]. \t  -0.41705513938663896 \t -0.414830971715495\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.2082148997229565"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7VgaJOoJbOIE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2a22c10-cd5e-4b71-f64f-9e5117c64230"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 10 \n",
        "\n",
        "np.random.seed(run_num_10)\n",
        "surrogate_winner_10 = dtStudentProcess(d_cov_func, nu = df)\n",
        "\n",
        "X_train10, X_test10, y_train10, y_test10 = train_test_split(X, y, test_size=test_perc, random_state=run_num_10)\n",
        "\n",
        "def f_syn_polarity10(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_10, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train10, y=y_train10).mean())\n",
        "    return operator * score\n",
        "\n",
        "winner_10 = dGPGO_stp(surrogate_winner_10, Acquisition_new(util_winner), f_syn_polarity10, param, n_jobs = -1) # Define BayesOpt\n",
        "winner_10.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_winner_10 = winner_10.getResult()[0]\n",
        "params_winner_10['max_depth'] = int(params_winner_10['max_depth'])\n",
        "params_winner_10['min_child_weight'] = int(params_winner_10['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_winner_train10 = xgb.DMatrix(X_train10, y_train10)\n",
        "dX_winner_test10 = xgb.DMatrix(X_test10, y_test10)\n",
        "model_winner_10 = xgb.train(params_winner_10, dX_winner_train10)\n",
        "pred_winner_10 = model_winner_10.predict(dX_winner_test10)\n",
        "\n",
        "rmse_winner_10 = np.sqrt(mean_squared_error(pred_winner_10, y_test10))\n",
        "rmse_winner_10"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 6.53589585  1.15006943  6.          0.93623727 14.          0.13663866]. \t  -0.6929852048256443 \t -0.4483221221388197\n",
            "init   \t [ 3.97194461  2.33132197 14.          0.9533253  19.          0.26403087]. \t  -0.6932702114662359 \t -0.4483221221388197\n",
            "init   \t [ 7.43539415  0.69582081  9.          0.9763222  11.          0.12608349]. \t  -0.6931242497450955 \t -0.4483221221388197\n",
            "init   \t [ 9.82027485  3.39637684  9.          0.78522537 19.          0.87618843]. \t  -0.4483221221388197 \t -0.4483221221388197\n",
            "init   \t [ 0.57576207  5.82646405 12.          0.77704362 18.          0.11865044]. \t  -0.6946096327074635 \t -0.4483221221388197\n",
            "1      \t [ 6.01651788  9.70919163  7.          0.6223146  15.          0.10507665]. \t  -0.6924325879252461 \t -0.4483221221388197\n",
            "2      \t [1.70339296 0.4436921  5.         0.5844373  2.         0.68058688]. \t  -0.5784474315368291 \t -0.4483221221388197\n",
            "3      \t [ 0.26043995  4.89348176 14.          0.51660447  9.          0.97374739]. \t  \u001b[92m-0.43451777076695297\u001b[0m \t -0.43451777076695297\n",
            "4      \t [ 9.51459367  2.97626838 10.          0.78292126  1.          0.96515375]. \t  -0.43758762459692707 \t -0.43451777076695297\n",
            "5      \t [4.33454338 9.11291731 6.         0.7883404  2.         0.49230158]. \t  -0.5779880165341125 \t -0.43451777076695297\n",
            "6      \t [ 7.90697916  9.14312267 14.          0.75613081  9.          0.87208929]. \t  \u001b[92m-0.4282600941244823\u001b[0m \t -0.4282600941244823\n",
            "7      \t [8.72554417 8.43289928 5.         0.78366846 8.         0.24269244]. \t  -0.6948360255852956 \t -0.4282600941244823\n",
            "8      \t [ 0.72612997  5.71374346  5.          0.7962252  12.          0.3276676 ]. \t  -0.6111050009653851 \t -0.4282600941244823\n",
            "9      \t [ 0.76839003  7.96408221 14.          0.73589275  1.          0.91984177]. \t  \u001b[92m-0.4267572960312064\u001b[0m \t -0.4267572960312064\n",
            "10     \t [ 4.83749944 10.         14.68041846  1.         20.          1.        ]. \t  \u001b[92m-0.4183604439263376\u001b[0m \t -0.4183604439263376\n",
            "11     \t [ 2.37424324  2.33069167 12.          0.80835547  2.          0.90917589]. \t  -0.43025305601156233 \t -0.4183604439263376\n",
            "12     \t [9.43529673 0.18357483 5.         0.85562643 4.         0.49300261]. \t  -0.5821244948315509 \t -0.4183604439263376\n",
            "13     \t [ 7.07869654  9.30947966 13.          0.76949873  1.          0.94513615]. \t  -0.4306670978270799 \t -0.4183604439263376\n",
            "14     \t [0.30340329 0.96498039 8.         0.58209384 7.         0.21352731]. \t  -0.6932909765019089 \t -0.4183604439263376\n",
            "15     \t [ 2.94327639  9.86430569 10.          0.52453661 10.          0.45562946]. \t  -0.581934467726267 \t -0.4183604439263376\n",
            "16     \t [ 0.60051063  0.93658443 10.          0.71787415 13.          0.3650692 ]. \t  -0.6037929489655063 \t -0.4183604439263376\n",
            "17     \t [ 3.18354686  4.79790696  5.          0.81878631 19.          0.2934667 ]. \t  -0.6119259457756776 \t -0.4183604439263376\n",
            "18     \t [ 8.04482724  4.95101092 14.          0.77699007 14.          0.58243015]. \t  -0.5705783073300893 \t -0.4183604439263376\n",
            "19     \t [ 6.64754646  4.1209914  15.          1.          4.63344221  1.        ]. \t  \u001b[92m-0.410939952692691\u001b[0m \t -0.410939952692691\n",
            "20     \t [ 4.60609795  0.33236771 14.          0.97552596  9.          0.78068199]. \t  -0.5028574509009728 \t -0.410939952692691\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.425803308794505"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51z87uHWbRGr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38afe4b9-7176-4f55-f819-b1b28f3b871a"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 11 \n",
        "\n",
        "np.random.seed(run_num_11)\n",
        "surrogate_winner_11 = dtStudentProcess(d_cov_func, nu = df)\n",
        "\n",
        "X_train11, X_test11, y_train11, y_test11 = train_test_split(X, y, test_size=test_perc, random_state=run_num_11)\n",
        "\n",
        "def f_syn_polarity11(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_11, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train11, y=y_train11).mean())\n",
        "    return operator * score\n",
        "\n",
        "winner_11 = dGPGO_stp(surrogate_winner_11, Acquisition_new(util_winner), f_syn_polarity11, param, n_jobs = -1) # Define BayesOpt\n",
        "winner_11.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_winner_11 = winner_11.getResult()[0]\n",
        "params_winner_11['max_depth'] = int(params_winner_11['max_depth'])\n",
        "params_winner_11['min_child_weight'] = int(params_winner_11['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_winner_train11 = xgb.DMatrix(X_train11, y_train11)\n",
        "dX_winner_test11 = xgb.DMatrix(X_test11, y_test11)\n",
        "model_winner_11 = xgb.train(params_winner_11, dX_winner_train11)\n",
        "pred_winner_11 = model_winner_11.predict(dX_winner_test11)\n",
        "\n",
        "rmse_winner_11 = np.sqrt(mean_squared_error(pred_winner_11, y_test11))\n",
        "rmse_winner_11"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 9.81035663  8.2124785   9.          0.68487173 15.          0.28207988]. \t  -0.7133787013189323 \t -0.47317045732943896\n",
            "init   \t [ 4.91906771  1.38558332  6.          0.618878   19.          0.10263204]. \t  -0.7112023662943636 \t -0.47317045732943896\n",
            "init   \t [ 5.23426175  1.89971159 13.          0.76313779 17.          0.46597853]. \t  -0.47317045732943896 \t -0.47317045732943896\n",
            "init   \t [ 4.0204056   3.75167577  9.          0.86204818 17.          0.26560056]. \t  -0.7132057194891379 \t -0.47317045732943896\n",
            "init   \t [8.8704459  5.64917815 5.         0.51406171 1.         0.82826731]. \t  -0.48048685248123163 \t -0.47317045732943896\n",
            "1      \t [0.08349406 1.89250335 6.         0.66386636 1.         0.88641723]. \t  \u001b[92m-0.4484239383130805\u001b[0m \t -0.4484239383130805\n",
            "2      \t [ 5.35837408  0.31510164 14.          0.97519815  6.          0.3034681 ]. \t  -0.607905230502132 \t -0.4484239383130805\n",
            "3      \t [ 4.66315557  9.30076713 11.          0.83534598  8.          0.21303816]. \t  -0.7135282416096518 \t -0.4484239383130805\n",
            "4      \t [9.02046973 1.16865322 7.         0.6807515  8.         0.2962635 ]. \t  -0.6068266615066733 \t -0.4484239383130805\n",
            "5      \t [0.79988529 9.85424348 5.         0.5743044  3.         0.67901433]. \t  -0.48209943859810334 \t -0.4484239383130805\n",
            "6      \t [ 0.02057794  7.14089873 13.          0.90855163  1.          0.61823707]. \t  -0.45805892074172583 \t -0.4484239383130805\n",
            "7      \t [ 0.97603689  3.49871064  9.          0.91846837 10.          0.35158251]. \t  -0.6085109103402477 \t -0.4484239383130805\n",
            "8      \t [ 9.27240029  9.57413463 14.          0.58544186  3.          0.73779027]. \t  -0.45267877796490286 \t -0.4484239383130805\n",
            "9      \t [ 0.4027375   9.38447167 12.          0.94595612 15.          0.85310792]. \t  -0.451136347669006 \t -0.4484239383130805\n",
            "10     \t [ 1.3440527   8.97080429  5.          0.80277597 12.          0.63284111]. \t  -0.48349651102568225 \t -0.4484239383130805\n",
            "11     \t [ 9.73437269  5.73747174 15.          1.          9.64753763  1.        ]. \t  \u001b[92m-0.40781405621978734\u001b[0m \t -0.40781405621978734\n",
            "12     \t [ 7.8737208   8.0742839  13.61843003  1.         20.          1.        ]. \t  -0.41746718663236226 \t -0.40781405621978734\n",
            "13     \t [8.31268495 9.91059357 5.         0.56157362 8.         0.25141967]. \t  -0.7119859457220532 \t -0.40781405621978734\n",
            "14     \t [2.94143112e-03 7.03829535e-01 1.30000000e+01 5.98722168e-01\n",
            " 2.00000000e+00 1.01335355e-01]. \t  -0.7122743790062083 \t -0.40781405621978734\n",
            "15     \t [ 0.81561387  9.31451412  5.          0.70594876 19.          0.98284163]. \t  -0.45988817082889427 \t -0.40781405621978734\n",
            "16     \t [ 9.8968071   1.89203294 13.          0.7888373   2.          0.82539259]. \t  -0.45294282860990764 \t -0.40781405621978734\n",
            "17     \t [ 8.37835592  2.52983026  5.          0.97806207 14.          0.6500905 ]. \t  -0.48098936825483635 \t -0.40781405621978734\n",
            "18     \t [5.29849443 1.26619532 8.         0.57236297 3.         0.56748732]. \t  -0.48144891242793636 \t -0.40781405621978734\n",
            "19     \t [ 8.78470125  0.77923473 11.          0.50537398 13.          0.33338184]. \t  -0.6064335737175908 \t -0.40781405621978734\n",
            "20     \t [5.45914093 8.22813217 9.         0.65793869 3.         0.8752657 ]. \t  -0.4387159838589163 \t -0.40781405621978734\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.175661289540509"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8jZUeoWbTvn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec1bdb45-4423-4981-ed35-619e2ba736f3"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 12\n",
        "\n",
        "np.random.seed(run_num_12)\n",
        "surrogate_winner_12 = dtStudentProcess(d_cov_func, nu = df)\n",
        "\n",
        "X_train12, X_test12, y_train12, y_test12 = train_test_split(X, y, test_size=test_perc, random_state=run_num_12)\n",
        "\n",
        "def f_syn_polarity12(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_12, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train12, y=y_train12).mean())\n",
        "    return operator * score\n",
        "\n",
        "winner_12 = dGPGO_stp(surrogate_winner_12, Acquisition_new(util_winner), f_syn_polarity12, param, n_jobs = -1) # Define BayesOpt\n",
        "winner_12.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_winner_12 = winner_12.getResult()[0]\n",
        "params_winner_12['max_depth'] = int(params_winner_12['max_depth'])\n",
        "params_winner_12['min_child_weight'] = int(params_winner_12['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_winner_train12 = xgb.DMatrix(X_train12, y_train12)\n",
        "dX_winner_test12 = xgb.DMatrix(X_test12, y_test12)\n",
        "model_winner_12 = xgb.train(params_winner_12, dX_winner_train12)\n",
        "pred_winner_12 = model_winner_12.predict(dX_winner_test12)\n",
        "\n",
        "rmse_winner_12 = np.sqrt(mean_squared_error(pred_winner_12, y_test12))\n",
        "rmse_winner_12"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 1.9151945   6.22108771  9.          0.89267929 16.          0.87460279]. \t  -0.4420077321695894 \t -0.4420077321695894\n",
            "init   \t [ 1.50636966  1.98518759 11.          0.67890863 17.          0.5381501 ]. \t  -0.5998539491151165 \t -0.4420077321695894\n",
            "init   \t [ 3.31015428  8.02639569  7.          0.75154158 13.          0.11992952]. \t  -0.7324503600335561 \t -0.4420077321695894\n",
            "init   \t [ 2.90728553  2.46394443  5.          0.94461307 17.          0.43194161]. \t  -0.5995790338913064 \t -0.4420077321695894\n",
            "init   \t [ 9.33140102  6.51378143  8.          0.76907392 18.          0.38515251]. \t  -0.5982523495604211 \t -0.4420077321695894\n",
            "1      \t [5.68098653 8.6912739  6.         0.52027807 1.         0.51631894]. \t  -0.5999713561677879 \t -0.4420077321695894\n",
            "2      \t [ 4.61089026  7.78540446 14.          0.61560224  6.          0.60910898]. \t  -0.5310432089168232 \t -0.4420077321695894\n",
            "3      \t [9.12195032 0.87787126 9.         0.87292886 5.         0.2622847 ]. \t  -0.7326988083055951 \t -0.4420077321695894\n",
            "4      \t [ 0.42901773  1.02682351 10.          0.62582208  7.          0.40519792]. \t  -0.600033100066767 \t -0.4420077321695894\n",
            "5      \t [ 9.48743388  2.23858916 14.          0.84020064 13.          0.41361313]. \t  -0.5921503481987865 \t -0.4420077321695894\n",
            "6      \t [ 6.85696089  9.7830661  14.          0.97897908 13.          0.89524334]. \t  \u001b[92m-0.43162480213383303\u001b[0m \t -0.43162480213383303\n",
            "7      \t [ 7.4487954   0.25501799  6.          0.59353564 11.          0.79320372]. \t  -0.5151049017617342 \t -0.43162480213383303\n",
            "8      \t [ 5.89929767  7.04785293 15.          1.         20.          1.        ]. \t  \u001b[92m-0.4171353794685511\u001b[0m \t -0.4171353794685511\n",
            "9      \t [3.58522218 1.88383687 9.         0.7776694  1.         0.4642546 ]. \t  -0.6003337797165678 \t -0.4171353794685511\n",
            "10     \t [9.69324843 9.42428289 7.         0.6384364  6.         0.83589131]. \t  -0.509004724458299 \t -0.4171353794685511\n",
            "11     \t [ 8.88364352  5.38800449 14.          0.86748322  1.          0.95098406]. \t  -0.43530550198971935 \t -0.4171353794685511\n",
            "12     \t [ 0.97347845  2.70597866 14.          0.59914315  2.          0.4814254 ]. \t  -0.6112493130195443 \t -0.4171353794685511\n",
            "13     \t [ 8.76460845  0.13227845  6.          0.75995694 17.          0.20887306]. \t  -0.7329728586443053 \t -0.4171353794685511\n",
            "14     \t [0.41721862 8.87551388 5.         0.69088851 6.         0.86973069]. \t  -0.468169334397084 \t -0.4171353794685511\n",
            "15     \t [ 1.07765263  6.20138171 14.          0.59898972 11.          0.65973368]. \t  -0.5325207293005967 \t -0.4171353794685511\n",
            "16     \t [ 1.09889115  8.6390199  10.          0.96730813  2.          0.79300245]. \t  -0.5022099845805148 \t -0.4171353794685511\n",
            "17     \t [ 9.26428304  9.74981947  6.          0.90684616 13.          0.82404302]. \t  -0.5075512306015831 \t -0.4171353794685511\n",
            "18     \t [ 5.72640258  1.49259688 14.          0.92448749  7.          0.73129851]. \t  -0.4929336813081906 \t -0.4171353794685511\n",
            "19     \t [ 0.60473619  9.7310361   5.          0.764657   19.          0.44899982]. \t  -0.602200593591361 \t -0.4171353794685511\n",
            "20     \t [10.          5.10843514 11.35521536  1.          8.35521536  1.        ]. \t  -0.4233035708328039 \t -0.4171353794685511\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.013463439067471"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snTrqE2RbWbe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "589c9a5e-b6e7-44d0-88ff-5c1b81a4abfb"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 13 \n",
        "\n",
        "np.random.seed(run_num_13)\n",
        "surrogate_winner_13 = dtStudentProcess(d_cov_func, nu = df)\n",
        "\n",
        "X_train13, X_test13, y_train13, y_test13 = train_test_split(X, y, test_size=test_perc, random_state=run_num_13)\n",
        "\n",
        "def f_syn_polarity13(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_13, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train13, y=y_train13).mean())\n",
        "    return operator * score\n",
        "\n",
        "winner_13 = dGPGO_stp(surrogate_winner_13, Acquisition_new(util_winner), f_syn_polarity13, param, n_jobs = -1) # Define BayesOpt\n",
        "winner_13.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_winner_13 = winner_13.getResult()[0]\n",
        "params_winner_13['max_depth'] = int(params_winner_13['max_depth'])\n",
        "params_winner_13['min_child_weight'] = int(params_winner_13['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_winner_train13 = xgb.DMatrix(X_train13, y_train13)\n",
        "dX_winner_test13 = xgb.DMatrix(X_test13, y_test13)\n",
        "model_winner_13 = xgb.train(params_winner_13, dX_winner_train13)\n",
        "pred_winner_13 = model_winner_13.predict(dX_winner_test13)\n",
        "\n",
        "rmse_winner_13 = np.sqrt(mean_squared_error(pred_winner_13, y_test13))\n",
        "rmse_winner_13"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 0.30967343  2.54282073  6.          0.9850871  17.          0.50848087]. \t  -0.5693403578256062 \t -0.4459810811310791\n",
            "init   \t [ 6.95119021  3.25593423 14.          0.8602375   8.          0.82055713]. \t  -0.5112217679187058 \t -0.4459810811310791\n",
            "init   \t [ 5.09252675  8.19921021 10.          0.78792002 16.          0.382158  ]. \t  -0.581693558072188 \t -0.4459810811310791\n",
            "init   \t [ 5.4465191   3.19841462 14.          0.90654581  5.          0.10359415]. \t  -0.6818818523119694 \t -0.4459810811310791\n",
            "init   \t [ 9.8393864   9.89619708  9.          0.63387763 10.          0.88979174]. \t  -0.4459810811310791 \t -0.4459810811310791\n",
            "1      \t [8.53546208 2.43342626 6.         0.87628893 6.         0.52166423]. \t  -0.5707796370702284 \t -0.4459810811310791\n",
            "2      \t [0.21910824 0.53051294 5.         0.62754372 4.         0.55299122]. \t  -0.578556985871312 \t -0.4459810811310791\n",
            "3      \t [5.19754758 9.80305744 5.         0.76448178 1.         0.12247487]. \t  -0.6809935571585586 \t -0.4459810811310791\n",
            "4      \t [ 8.54553338  0.49573541 12.          0.81406546 19.          0.20999807]. \t  -0.6795516887212301 \t -0.4459810811310791\n",
            "5      \t [ 0.06271075  5.52241404  7.          0.72065611 10.          0.32985926]. \t  -0.5870453453795295 \t -0.4459810811310791\n",
            "6      \t [ 0.24368767  9.44599276 13.          0.54443262  5.          0.20315453]. \t  -0.6891808118620875 \t -0.4459810811310791\n",
            "7      \t [ 9.45160315  2.43691163  6.          0.75490059 14.          0.39257474]. \t  -0.5966501799066168 \t -0.4459810811310791\n",
            "8      \t [ 1.95513735  0.12334702 13.          0.54467307 14.          0.86324917]. \t  \u001b[92m-0.43794830902637205\u001b[0m \t -0.43794830902637205\n",
            "9      \t [10.         10.         14.81308766  1.         18.81308766  1.        ]. \t  \u001b[92m-0.4111601678814976\u001b[0m \t -0.4111601678814976\n",
            "10     \t [ 9.93618483  7.92775724 10.          0.62787028  2.          0.33020774]. \t  -0.5857027943547768 \t -0.4111601678814976\n",
            "11     \t [ 0.47049819  5.74511735 14.          0.50132527 17.          0.92667582]. \t  -0.43851372118819276 \t -0.4111601678814976\n",
            "12     \t [5.4249643  3.78139234 9.         0.79464459 1.         0.59966385]. \t  -0.5428776547212495 \t -0.4111601678814976\n",
            "13     \t [ 4.57815796 10.         15.          1.          9.64871293  1.        ]. \t  \u001b[92m-0.40213734633294607\u001b[0m \t -0.40213734633294607\n",
            "14     \t [ 3.03168537  0.38832061  5.          0.8328381  10.          0.58615566]. \t  -0.5635089044499596 \t -0.40213734633294607\n",
            "15     \t [ 8.93459678  5.75013258 14.          0.86506851 14.          0.11026757]. \t  -0.6798789215383618 \t -0.40213734633294607\n",
            "16     \t [ 1.2199304   9.32862427  5.          0.98694875 18.          0.69275841]. \t  -0.5593626212561581 \t -0.40213734633294607\n",
            "17     \t [4.69840459 9.9377771  9.         0.55456115 7.         0.54186363]. \t  -0.5597171341594869 \t -0.40213734633294607\n",
            "18     \t [ 0.25964514  2.37009305 11.          0.85577938  4.          0.42985865]. \t  -0.556481072440868 \t -0.40213734633294607\n",
            "19     \t [0.03375336 9.27759753 5.         0.74221137 5.         0.67335382]. \t  -0.5606736551844532 \t -0.40213734633294607\n",
            "20     \t [ 5.79298471  3.15568386  7.          0.88801526 19.          0.72270585]. \t  -0.5225608598094562 \t -0.40213734633294607\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.009508878947214"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23OXr6XLbY7_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cfdb37ea-189d-4746-e03a-a0e3315b3f41"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 14 \n",
        "\n",
        "np.random.seed(run_num_14)\n",
        "surrogate_winner_14 = dtStudentProcess(d_cov_func, nu = df)\n",
        "\n",
        "X_train14, X_test14, y_train14, y_test14 = train_test_split(X, y, test_size=test_perc, random_state=run_num_14)\n",
        "\n",
        "def f_syn_polarity14(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_14, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train14, y=y_train14).mean())\n",
        "    return operator * score\n",
        "\n",
        "winner_14 = dGPGO_stp(surrogate_winner_14, Acquisition_new(util_winner), f_syn_polarity14, param, n_jobs = -1) # Define BayesOpt\n",
        "winner_14.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_winner_14 = winner_14.getResult()[0]\n",
        "params_winner_14['max_depth'] = int(params_winner_14['max_depth'])\n",
        "params_winner_14['min_child_weight'] = int(params_winner_14['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_winner_train14 = xgb.DMatrix(X_train14, y_train14)\n",
        "dX_winner_test14 = xgb.DMatrix(X_test14, y_test14)\n",
        "model_winner_14 = xgb.train(params_winner_14, dX_winner_train14)\n",
        "pred_winner_14 = model_winner_14.predict(dX_winner_test14)\n",
        "\n",
        "rmse_winner_14 = np.sqrt(mean_squared_error(pred_winner_14, y_test14))\n",
        "rmse_winner_14"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 8.59560606  1.64569495 11.          0.89021195 15.          0.4857008 ]. \t  -0.6102441482204597 \t -0.4517949441804544\n",
            "init   \t [ 0.57460093  9.25007434  5.          0.56647642 17.          0.7533211 ]. \t  -0.5614002279921607 \t -0.4517949441804544\n",
            "init   \t [1.90674396 4.32019547 9.         0.93708225 3.         0.91775905]. \t  -0.4517949441804544 \t -0.4517949441804544\n",
            "init   \t [ 0.36770475  8.43368681 14.          0.6826844  15.          0.6291966 ]. \t  -0.5407924410004543 \t -0.4517949441804544\n",
            "init   \t [ 5.33840324  0.449182   14.          0.52045439  2.          0.48604556]. \t  -0.6242722768101235 \t -0.4517949441804544\n",
            "1      \t [ 9.54492219  9.07631824  8.          0.54579372 12.          0.38295996]. \t  -0.6262431675183743 \t -0.4517949441804544\n",
            "2      \t [9.53270461 0.83608685 6.         0.99976104 3.         0.60747265]. \t  -0.5533922160484371 \t -0.4517949441804544\n",
            "3      \t [ 1.15250646  0.51569303  5.          0.69322415 17.          0.16291682]. \t  -0.6917729046487754 \t -0.4517949441804544\n",
            "4      \t [ 8.50805687  8.26285095 14.          0.73716155  1.          0.12935726]. \t  -0.69096859473492 \t -0.4517949441804544\n",
            "5      \t [ 8.08070558  7.65123431 14.          0.59104644 19.          0.9302005 ]. \t  -0.45873776977159153 \t -0.4517949441804544\n",
            "6      \t [ 6.26901123  2.37239677  5.          0.95063922 11.          0.52876485]. \t  -0.6158458956422692 \t -0.4517949441804544\n",
            "7      \t [1.0907871  9.11150389 5.         0.58744161 9.         0.87439163]. \t  -0.4933476055445382 \t -0.4517949441804544\n",
            "8      \t [ 2.21919917  0.31865167 14.          0.54399684 14.          0.18056849]. \t  -0.6942842535038787 \t -0.4517949441804544\n",
            "9      \t [ 7.34435618  8.39108681  7.          0.81547994 19.          0.62767067]. \t  -0.546522525570801 \t -0.4517949441804544\n",
            "10     \t [ 4.67438649  9.12283668 14.          0.86393974  9.          0.28445597]. \t  -0.6909478110661416 \t -0.4517949441804544\n",
            "11     \t [ 9.12262801  0.63832928 14.          0.67114219  9.          0.47477925]. \t  -0.6127574020888537 \t -0.4517949441804544\n",
            "12     \t [9.71394099 7.88076583 5.         0.92469304 3.         0.17093777]. \t  -0.6913576852026504 \t -0.4517949441804544\n",
            "13     \t [ 9.97545469 10.         15.          1.         12.81953585  1.        ]. \t  \u001b[92m-0.4176717797943642\u001b[0m \t -0.4176717797943642\n",
            "14     \t [ 0.84209312  7.86749962 14.          0.8648448   1.          0.1570582 ]. \t  -0.6937301125121068 \t -0.4176717797943642\n",
            "15     \t [ 1.4252873   1.40529851 11.          0.87426112  8.          0.55926255]. \t  -0.6154816816568135 \t -0.4176717797943642\n",
            "16     \t [5.14948858 9.9293377  8.         0.76473643 3.         0.51134466]. \t  -0.6119547835147783 \t -0.4176717797943642\n",
            "17     \t [ 2.31178586  4.77539377  9.          0.50906892 18.          0.4515708 ]. \t  -0.6151604317481614 \t -0.4176717797943642\n",
            "18     \t [ 0.19021811  2.03766024  7.          0.57219243 12.          0.84763264]. \t  -0.550986074509402 \t -0.4176717797943642\n",
            "19     \t [ 6.94928146  5.48234431 10.          0.83546414  7.          0.79072037]. \t  -0.533952967310828 \t -0.4176717797943642\n",
            "20     \t [10.         10.         15.          1.          5.72139908  1.        ]. \t  \u001b[92m-0.41572162809010066\u001b[0m \t -0.41572162809010066\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3.9754513513477603"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgxvE7Irbbj_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64616f86-27d2-48d6-c3d2-6f915665f601"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 15 \n",
        "\n",
        "np.random.seed(run_num_15)\n",
        "surrogate_winner_15 = dtStudentProcess(d_cov_func, nu = df)\n",
        "\n",
        "X_train15, X_test15, y_train15, y_test15 = train_test_split(X, y, test_size=test_perc, random_state=run_num_15)\n",
        "\n",
        "def f_syn_polarity15(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_15, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train15, y=y_train15).mean())\n",
        "    return operator * score\n",
        "\n",
        "winner_15 = dGPGO_stp(surrogate_winner_15, Acquisition_new(util_winner), f_syn_polarity15, param, n_jobs = -1) # Define BayesOpt\n",
        "winner_15.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_winner_15 = winner_15.getResult()[0]\n",
        "params_winner_15['max_depth'] = int(params_winner_15['max_depth'])\n",
        "params_winner_15['min_child_weight'] = int(params_winner_15['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_winner_train15 = xgb.DMatrix(X_train15, y_train15)\n",
        "dX_winner_test15 = xgb.DMatrix(X_test15, y_test15)\n",
        "model_winner_15 = xgb.train(params_winner_15, dX_winner_train15)\n",
        "pred_winner_15 = model_winner_15.predict(dX_winner_test15)\n",
        "\n",
        "rmse_winner_15 = np.sqrt(mean_squared_error(pred_winner_15, y_test15))\n",
        "rmse_winner_15"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 5.64315424  8.97297685  6.          0.81493635 11.          0.33008564]. \t  -0.5946480910132583 \t -0.4392277798535851\n",
            "init   \t [9.51455383 9.60803032 5.         0.68485728 1.         0.30145081]. \t  -0.5968379804084254 \t -0.4392277798535851\n",
            "init   \t [ 8.11809889  6.32508536  7.          0.61766329 19.          0.51383159]. \t  -0.5092318398204285 \t -0.4392277798535851\n",
            "init   \t [ 7.28571251  5.36071389 10.          0.76240297 12.          0.12035683]. \t  -0.6346120183758519 \t -0.4392277798535851\n",
            "init   \t [ 6.80847768  3.59767194 14.          0.51365585 16.          0.89730673]. \t  -0.4392277798535851 \t -0.4392277798535851\n",
            "1      \t [ 2.28792065  8.58421878 12.          0.67551126  1.          0.37971264]. \t  -0.596161946051267 \t -0.4392277798535851\n",
            "2      \t [ 9.35473751  1.48895841 12.          0.55362474  4.          0.11106606]. \t  -0.6327865627642436 \t -0.4392277798535851\n",
            "3      \t [3.91572323 2.23174129 6.         0.50937407 1.         0.51336364]. \t  -0.5160141940687206 \t -0.4392277798535851\n",
            "4      \t [ 0.74184801  3.17830864 14.          0.90502664  8.          0.31107615]. \t  -0.5905073342805591 \t -0.4392277798535851\n",
            "5      \t [ 1.33182259  0.8125478   9.          0.94558338 18.          0.71777254]. \t  -0.4467705016632985 \t -0.4392277798535851\n",
            "6      \t [ 0.48538268  1.39730148  6.          0.8238236  11.          0.45741795]. \t  -0.5144696373796631 \t -0.4392277798535851\n",
            "7      \t [ 0.57885492  9.35143843  6.          0.69337529 17.          0.15510814]. \t  -0.6331051904120661 \t -0.4392277798535851\n",
            "8      \t [0.24960251 9.45540541 5.         0.62890215 3.         0.73551484]. \t  -0.47327438015853385 \t -0.4392277798535851\n",
            "9      \t [ 6.19778812 10.         15.          1.          6.61746591  1.        ]. \t  \u001b[92m-0.40153056110096824\u001b[0m \t -0.40153056110096824\n",
            "10     \t [ 1.35750397  9.83931966 14.          0.92698791 15.          0.2272101 ]. \t  -0.6341880539033447 \t -0.40153056110096824\n",
            "11     \t [ 9.85965179  9.04589828 13.          0.95564786 19.          0.1426314 ]. \t  -0.632071954329094 \t -0.40153056110096824\n",
            "12     \t [9.06175259 4.11518452 5.         0.87621161 6.         0.52300377]. \t  -0.5192107686497183 \t -0.40153056110096824\n",
            "13     \t [ 0.15053668  0.43786928 13.          0.62231098  1.          0.88145445]. \t  -0.43523183945633176 \t -0.40153056110096824\n",
            "14     \t [ 7.18775342  0.2062489   5.          0.98239776 17.          0.28043211]. \t  -0.634545784625454 \t -0.40153056110096824\n",
            "15     \t [0.10178544 7.49949902 9.         0.60008085 8.         0.35520416]. \t  -0.5902884239300654 \t -0.40153056110096824\n",
            "16     \t [ 8.32079125  9.9705113  11.          0.62379648  2.          0.23245392]. \t  -0.634001584102645 \t -0.40153056110096824\n",
            "17     \t [ 3.82912379  5.89660711 11.55531556  1.         20.          1.        ]. \t  -0.42127252712969643 \t -0.40153056110096824\n",
            "18     \t [10.          4.52111138 15.          1.          9.52913646  1.        ]. \t  -0.403015166804132 \t -0.40153056110096824\n",
            "19     \t [9.58390631 0.99765082 6.         0.72021045 1.         0.39555686]. \t  -0.5960083204162412 \t -0.40153056110096824\n",
            "20     \t [ 4.88199837  4.70394304 10.          0.92628758  6.          0.92666416]. \t  -0.43330724075449983 \t -0.40153056110096824\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.099622839723655"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ye4UEpNabeCL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02602a38-51a9-45c7-d514-97929ad5ce9c"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 16 \n",
        "\n",
        "np.random.seed(run_num_16)\n",
        "surrogate_winner_16 = dtStudentProcess(d_cov_func, nu = df)\n",
        "\n",
        "X_train16, X_test16, y_train16, y_test16 = train_test_split(X, y, test_size=test_perc, random_state=run_num_16)\n",
        "\n",
        "def f_syn_polarity16(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_16, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train16, y=y_train16).mean())\n",
        "    return operator * score\n",
        "\n",
        "winner_16 = dGPGO_stp(surrogate_winner_16, Acquisition_new(util_winner), f_syn_polarity16, param, n_jobs = -1) # Define BayesOpt\n",
        "winner_16.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_winner_16 = winner_16.getResult()[0]\n",
        "params_winner_16['max_depth'] = int(params_winner_16['max_depth'])\n",
        "params_winner_16['min_child_weight'] = int(params_winner_16['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_winner_train16 = xgb.DMatrix(X_train16, y_train16)\n",
        "dX_winner_test16 = xgb.DMatrix(X_test16, y_test16)\n",
        "model_winner_16 = xgb.train(params_winner_16, dX_winner_train16)\n",
        "pred_winner_16 = model_winner_16.predict(dX_winner_test16)\n",
        "\n",
        "rmse_winner_16 = np.sqrt(mean_squared_error(pred_winner_16, y_test16))\n",
        "rmse_winner_16"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [8.63948332 3.16366002 7.         0.75395583 1.         0.56999262]. \t  -0.5713220948397237 \t -0.4331621293825035\n",
            "init   \t [ 3.3426621   9.77029134 14.          0.93751218 17.          0.80549151]. \t  -0.4331621293825035 \t -0.4331621293825035\n",
            "init   \t [ 3.36871073  4.8308288  13.          0.91984982 14.          0.48829226]. \t  -0.5561611494046279 \t -0.4331621293825035\n",
            "init   \t [ 8.84540736  1.59826081  8.          0.75772888 14.          0.29029227]. \t  -0.6835926126526815 \t -0.4331621293825035\n",
            "init   \t [ 7.74981136  8.34377636 13.          0.83182409 15.          0.19378622]. \t  -0.7677243885128009 \t -0.4331621293825035\n",
            "1      \t [0.90186209 9.9119065  9.         0.96247707 9.         0.82473059]. \t  -0.44000666714577197 \t -0.4331621293825035\n",
            "2      \t [ 4.70398351  0.5091113  14.          0.87365833  5.          0.41660395]. \t  -0.687590458944199 \t -0.4331621293825035\n",
            "3      \t [2.15072525 1.72325427 6.         0.7282133  7.         0.66792915]. \t  -0.510524353390639 \t -0.4331621293825035\n",
            "4      \t [9.52987381 8.92087345 5.         0.95441282 7.         0.10412548]. \t  -0.7679863416880384 \t -0.4331621293825035\n",
            "5      \t [ 8.31345174  8.15216774 14.          0.81052242  4.          0.5053732 ]. \t  -0.5586554806770512 \t -0.4331621293825035\n",
            "6      \t [ 6.04521147  5.48003896  5.          0.75390146 19.          0.34441217]. \t  -0.6853394565576119 \t -0.4331621293825035\n",
            "7      \t [ 0.99986868  7.43906155 10.          0.60070406  1.          0.82153523]. \t  -0.44282879961521837 \t -0.4331621293825035\n",
            "8      \t [ 1.86481894  0.99601669  6.          0.66208635 14.          0.74317901]. \t  -0.4660455507597825 \t -0.4331621293825035\n",
            "9      \t [9.8048903  0.9736784  9.         0.6852998  7.         0.71530424]. \t  -0.4471579166365055 \t -0.4331621293825035\n",
            "10     \t [ 6.92592036  0.8587697  13.          0.97103751 18.          0.97962526]. \t  \u001b[92m-0.4303764405613609\u001b[0m \t -0.4303764405613609\n",
            "11     \t [ 9.67455768  4.0434096  14.          0.73550218  9.          0.70585769]. \t  -0.4727021909313705 \t -0.4303764405613609\n",
            "12     \t [ 6.08409173  7.75770415  6.          0.70965459 12.          0.63532637]. \t  -0.5147980901496266 \t -0.4303764405613609\n",
            "13     \t [1.3221216  1.37348875 6.         0.56714082 1.         0.10339369]. \t  -0.7671246614583467 \t -0.4303764405613609\n",
            "14     \t [ 0.3330471   0.01629553 13.          0.91520431 11.          0.22019979]. \t  -0.7679362913445089 \t -0.4303764405613609\n",
            "15     \t [ 0.65939699  9.22066505  7.          0.55188083 19.          0.57852772]. \t  -0.5026537553241901 \t -0.4303764405613609\n",
            "16     \t [2.66553728 8.77876235 5.         0.50148986 4.         0.70733967]. \t  -0.521502202255312 \t -0.4303764405613609\n",
            "17     \t [ 9.07138669  8.35078331  9.6015499   1.         20.          1.        ]. \t  -0.44031046265205614 \t -0.4303764405613609\n",
            "18     \t [ 9.53093816  5.17524675 15.          1.         20.          1.        ]. \t  \u001b[92m-0.42522730856933305\u001b[0m \t -0.42522730856933305\n",
            "19     \t [7.56513908 9.29495437 8.         0.73448125 2.         0.64501756]. \t  -0.48518176121987455 \t -0.42522730856933305\n",
            "20     \t [ 1.33769147  3.71231435 11.          0.53202809 19.          0.59306876]. \t  -0.4806762042317764 \t -0.42522730856933305\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.457497166809488"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Biq2Uaa5bg3U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd53a059-45c8-4036-b03e-966c8ff4e8eb"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 17 \n",
        "\n",
        "np.random.seed(run_num_17)\n",
        "surrogate_winner_17 = dtStudentProcess(d_cov_func, nu = df)\n",
        "\n",
        "X_train17, X_test17, y_train17, y_test17 = train_test_split(X, y, test_size=test_perc, random_state=run_num_17)\n",
        "\n",
        "def f_syn_polarity17(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_17, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train17, y=y_train17).mean())\n",
        "    return operator * score\n",
        "\n",
        "winner_17 = dGPGO_stp(surrogate_winner_17, Acquisition_new(util_winner), f_syn_polarity17, param, n_jobs = -1) # Define BayesOpt\n",
        "winner_17.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_winner_17 = winner_17.getResult()[0]\n",
        "params_winner_17['max_depth'] = int(params_winner_17['max_depth'])\n",
        "params_winner_17['min_child_weight'] = int(params_winner_17['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_winner_train17 = xgb.DMatrix(X_train17, y_train17)\n",
        "dX_winner_test17 = xgb.DMatrix(X_test17, y_test17)\n",
        "model_winner_17 = xgb.train(params_winner_17, dX_winner_train17)\n",
        "pred_winner_17 = model_winner_17.predict(dX_winner_test17)\n",
        "\n",
        "rmse_winner_17 = np.sqrt(mean_squared_error(pred_winner_17, y_test17))\n",
        "rmse_winner_17"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 1.85575275  3.86599303 13.          0.85645882  2.          0.78699813]. \t  -0.4597056260580034 \t -0.4597056260580034\n",
            "init   \t [1.97780831 2.92527676 8.         0.92268607 9.         0.39017299]. \t  -0.5687207024308397 \t -0.4597056260580034\n",
            "init   \t [ 7.16656007  1.028324    5.          0.96975483 15.          0.90932849]. \t  -0.4688025715067707 \t -0.4597056260580034\n",
            "init   \t [ 2.46007304  9.53596657  7.          0.71704714 11.          0.17659118]. \t  -0.6983036523024195 \t -0.4597056260580034\n",
            "init   \t [3.44161135 9.92427612 8.         0.96331165 2.         0.29160186]. \t  -0.5675253504862205 \t -0.4597056260580034\n",
            "1      \t [9.75596948 2.66622322 5.         0.72612096 7.         0.23269911]. \t  -0.6983835107855046 \t -0.4597056260580034\n",
            "2      \t [ 9.05546364  9.87663276 14.          0.69177779  4.          0.65440109]. \t  -0.5329840951357279 \t -0.4597056260580034\n",
            "3      \t [ 2.91640542  5.22736432 12.          0.92756774 19.          0.99417707]. \t  \u001b[92m-0.43354095768294015\u001b[0m \t -0.43354095768294015\n",
            "4      \t [ 9.44216523  8.74671988 12.          0.51124365 12.          0.91905886]. \t  -0.4465237562223262 \t -0.43354095768294015\n",
            "5      \t [ 8.1891337   8.29486103  6.          0.78123542 19.          0.63887124]. \t  -0.548305094489817 \t -0.43354095768294015\n",
            "6      \t [ 8.55157781  0.59356536 14.          0.69815453  9.          0.39662325]. \t  -0.5664457046969067 \t -0.43354095768294015\n",
            "7      \t [0.84757532 0.06275289 6.         0.97600366 2.         0.14520225]. \t  -0.6945632868954219 \t -0.43354095768294015\n",
            "8      \t [ 9.85681439  0.57206659 14.          0.79465027 19.          0.59553205]. \t  -0.534073864099878 \t -0.43354095768294015\n",
            "9      \t [ 0.58500181  0.46731698 14.          0.75671446 14.          0.26953174]. \t  -0.699608167058346 \t -0.43354095768294015\n",
            "10     \t [6.09046443 5.16422686 5.         0.65602714 1.         0.17084824]. \t  -0.6984131799873137 \t -0.43354095768294015\n",
            "11     \t [ 7.97373067  9.7184741  15.          1.         20.          1.        ]. \t  \u001b[92m-0.41598575219965583\u001b[0m \t -0.41598575219965583\n",
            "12     \t [ 2.72191593  9.0458443  14.          0.69914166 13.          0.99224131]. \t  -0.4317420614539496 \t -0.41598575219965583\n",
            "13     \t [ 0.46952053  9.52164833 13.          0.77701641  6.          0.10604221]. \t  -0.6987462532393639 \t -0.41598575219965583\n",
            "14     \t [ 0.12914328  1.45281346  6.          0.77503847 19.          0.39602864]. \t  -0.5771640468869591 \t -0.41598575219965583\n",
            "15     \t [ 8.16344586  0.63468133 11.          0.72536853  2.          0.51248304]. \t  -0.5375303612256769 \t -0.41598575219965583\n",
            "16     \t [ 5.50051394  3.17563287 11.          0.93191327 13.          0.36476427]. \t  -0.5625389378056868 \t -0.41598575219965583\n",
            "17     \t [ 1.57712162  7.82606789  6.          0.54073047 19.          0.18193559]. \t  -0.6997410785139022 \t -0.41598575219965583\n",
            "18     \t [8.75850674 8.71391374 5.         0.73586193 7.         0.42356074]. \t  -0.5810903153847935 \t -0.41598575219965583\n",
            "19     \t [ 5.02146404  6.02341801 15.          1.          6.63874089  1.        ]. \t  \u001b[92m-0.4088598588083996\u001b[0m \t -0.4088598588083996\n",
            "20     \t [ 8.56102523  6.26223496  6.          0.61165765 13.          0.68975661]. \t  -0.5480420429021533 \t -0.4088598588083996\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3.7829724957594517"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5H4MWSXFcZjO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d53597a0-d2b1-4853-e8ca-5a752726e433"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 18 \n",
        "\n",
        "np.random.seed(run_num_18)\n",
        "surrogate_winner_18 = dtStudentProcess(d_cov_func, nu = df)\n",
        "\n",
        "X_train18, X_test18, y_train18, y_test18 = train_test_split(X, y, test_size=test_perc, random_state=run_num_18)\n",
        "\n",
        "def f_syn_polarity18(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_18, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train18, y=y_train18).mean())\n",
        "    return operator * score\n",
        "\n",
        "winner_18 = dGPGO_stp(surrogate_winner_18, Acquisition_new(util_winner), f_syn_polarity18, param, n_jobs = -1) # Define BayesOpt\n",
        "winner_18.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_winner_18 = winner_18.getResult()[0]\n",
        "params_winner_18['max_depth'] = int(params_winner_18['max_depth'])\n",
        "params_winner_18['min_child_weight'] = int(params_winner_18['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_winner_train18 = xgb.DMatrix(X_train18, y_train18)\n",
        "dX_winner_test18 = xgb.DMatrix(X_test18, y_test18)\n",
        "model_winner_18 = xgb.train(params_winner_18, dX_winner_train18)\n",
        "pred_winner_18 = model_winner_18.predict(dX_winner_test18)\n",
        "\n",
        "rmse_winner_18 = np.sqrt(mean_squared_error(pred_winner_18, y_test18))\n",
        "rmse_winner_18"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 8.73429403  9.68540663 10.          0.68875849  9.          0.48011572]. \t  -0.536396330012702 \t -0.4337279026393176\n",
            "init   \t [ 6.12033333  7.66062926  8.          0.76133734 13.          0.93379456]. \t  -0.4433380408289477 \t -0.4337279026393176\n",
            "init   \t [ 1.46524679  7.01527914  7.          0.90913299 10.          0.36016753]. \t  -0.5541638729833303 \t -0.4337279026393176\n",
            "init   \t [ 9.73855241  3.33774046 14.          0.53290419  7.          0.7088681 ]. \t  -0.5062350384654568 \t -0.4337279026393176\n",
            "init   \t [ 3.00618018  1.82702795 11.          0.75681389 14.          0.98627449]. \t  -0.4337279026393176 \t -0.4337279026393176\n",
            "1      \t [ 1.46586883  1.13810325 14.          0.90052794  5.          0.16182112]. \t  -0.6179729230954069 \t -0.4337279026393176\n",
            "2      \t [ 9.17850108  5.22481201 14.          0.60105853 18.          0.63442006]. \t  -0.5052903785308961 \t -0.4337279026393176\n",
            "3      \t [6.20612396 4.37552923 6.         0.71941082 3.         0.90943147]. \t  -0.45137007433293086 \t -0.4337279026393176\n",
            "4      \t [ 0.42107725  9.92938812 12.          0.92462675 18.          0.88352347]. \t  \u001b[92m-0.4255403331751115\u001b[0m \t -0.4255403331751115\n",
            "5      \t [ 1.45418057  8.89399    11.          0.93175096  1.          0.52391385]. \t  -0.5358666580451181 \t -0.4255403331751115\n",
            "6      \t [ 6.37698126  1.83270742  5.          0.89203056 17.          0.10435121]. \t  -0.6174428103417341 \t -0.4255403331751115\n",
            "7      \t [ 3.76328522  7.15740213 14.          0.83667606 10.          0.79012696]. \t  -0.4458786451533837 \t -0.4255403331751115\n",
            "8      \t [0.18032167 1.73628428 7.         0.68558335 5.         0.33274978]. \t  -0.5593106163222252 \t -0.4255403331751115\n",
            "9      \t [7.94412856 1.24068931 7.         0.96649935 9.         0.47928692]. \t  -0.5420539933608051 \t -0.4255403331751115\n",
            "10     \t [ 6.8932822  10.         14.39561176  1.          4.39561176  1.        ]. \t  \u001b[92m-0.4061636194156867\u001b[0m \t -0.4061636194156867\n",
            "11     \t [ 8.62753755  4.44899956 12.          0.98231134  1.          0.88389967]. \t  -0.4267991891390327 \t -0.4061636194156867\n",
            "12     \t [ 3.63739627  8.35435528  5.          0.58995076 19.          0.32125273]. \t  -0.5635646610193202 \t -0.4061636194156867\n",
            "13     \t [ 3.86727486  0.04582369 10.          0.54629234  1.          0.59406244]. \t  -0.5128905589861811 \t -0.4061636194156867\n",
            "14     \t [ 0.24957112  3.26102415  5.          0.59065773 17.          0.91920206]. \t  -0.4670777155018547 \t -0.4061636194156867\n",
            "15     \t [10.          6.7768771   8.27599668  1.         20.          1.        ]. \t  -0.4297696630159512 \t -0.4061636194156867\n",
            "16     \t [ 8.69377192  0.534461   11.          0.67986061 19.          0.10515379]. \t  -0.6183112018452402 \t -0.4061636194156867\n",
            "17     \t [10.          7.38161028 15.          1.         11.2839727   1.        ]. \t  -0.41092815621257756 \t -0.4061636194156867\n",
            "18     \t [5.80027804 9.4897964  5.         0.50086788 8.         0.40184663]. \t  -0.5692673225904021 \t -0.4061636194156867\n",
            "19     \t [5.81345214 9.69898534 7.         0.79242119 1.         0.72012886]. \t  -0.47859716684644626 \t -0.4061636194156867\n",
            "20     \t [ 8.86269845  1.61788611 11.          0.72867371 13.          0.49923805]. \t  -0.5331316926415038 \t -0.4061636194156867\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3.782019062517165"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-EgxA8k4ccL6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a85636f6-960e-4fa8-8b25-716f51a5d40f"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 19 \n",
        "\n",
        "np.random.seed(run_num_19)\n",
        "surrogate_winner_19 = dtStudentProcess(d_cov_func, nu = df)\n",
        "\n",
        "X_train19, X_test19, y_train19, y_test19 = train_test_split(X, y, test_size=test_perc, random_state=run_num_19)\n",
        "\n",
        "def f_syn_polarity19(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_19, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train19, y=y_train19).mean())\n",
        "    return operator * score\n",
        "\n",
        "winner_19 = dGPGO_stp(surrogate_winner_19, Acquisition_new(util_winner), f_syn_polarity19, param, n_jobs = -1) # Define BayesOpt\n",
        "winner_19.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_winner_19 = winner_19.getResult()[0]\n",
        "params_winner_19['max_depth'] = int(params_winner_19['max_depth'])\n",
        "params_winner_19['min_child_weight'] = int(params_winner_19['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_winner_train19 = xgb.DMatrix(X_train19, y_train19)\n",
        "dX_winner_test19 = xgb.DMatrix(X_test19, y_test19)\n",
        "model_winner_19 = xgb.train(params_winner_19, dX_winner_train19)\n",
        "pred_winner_19 = model_winner_19.predict(dX_winner_test19)\n",
        "\n",
        "rmse_winner_19 = np.sqrt(mean_squared_error(pred_winner_19, y_test19))\n",
        "rmse_winner_19"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 8.24520174  9.84626759 12.          0.81846905 12.          0.27594309]. \t  -0.6624515538441494 \t -0.4321567975765851\n",
            "init   \t [ 8.6021676   8.64300514 14.          0.95067208 18.          0.13363564]. \t  -0.663790961968593 \t -0.4321567975765851\n",
            "init   \t [ 3.02486657  6.06229509 10.          0.50494482  9.          0.10218675]. \t  -0.668875700942164 \t -0.4321567975765851\n",
            "init   \t [ 6.57586195  8.01231848  7.          0.63945407 18.          0.49206406]. \t  -0.5640092362987262 \t -0.4321567975765851\n",
            "init   \t [ 6.05008856  8.15558548 14.          0.64718943  8.          0.90611106]. \t  -0.4321567975765851 \t -0.4321567975765851\n",
            "1      \t [ 0.85655458  0.69294498  9.          0.90992265 19.          0.25605959]. \t  -0.6632879756016564 \t -0.4321567975765851\n",
            "2      \t [ 1.16239725  9.89254371 11.          0.98469188  1.          0.20271996]. \t  -0.6659068700895453 \t -0.4321567975765851\n",
            "3      \t [9.26425961 9.717621   5.         0.8856331  5.         0.65589573]. \t  -0.5256636565447839 \t -0.4321567975765851\n",
            "4      \t [8.38158931 1.39953008 5.         0.6836482  9.         0.46934447]. \t  -0.5657479189361881 \t -0.4321567975765851\n",
            "5      \t [2.87203669 1.73397023 8.         0.5086889  2.         0.62714796]. \t  -0.5302257616505216 \t -0.4321567975765851\n",
            "6      \t [ 8.82143334  0.86468577  8.          0.78593165 17.          0.7837218 ]. \t  -0.46211902797215776 \t -0.4321567975765851\n",
            "7      \t [ 1.85241362  2.79896509  5.          0.93918847 13.          0.45803438]. \t  -0.5641839619741948 \t -0.4321567975765851\n",
            "8      \t [ 8.67180792  0.03760584 13.          0.51007633  3.          0.32299169]. \t  -0.6185370681805598 \t -0.4321567975765851\n",
            "9      \t [ 0.56432836  7.79494932 13.          0.55871992 19.          0.10266045]. \t  -0.6660467600439527 \t -0.4321567975765851\n",
            "10     \t [ 1.23381302  0.08056091 14.          0.74757169  5.          0.12459884]. \t  -0.6658762505283484 \t -0.4321567975765851\n",
            "11     \t [ 9.80450511  5.84849871 10.          0.90258611  2.          0.36085047]. \t  -0.6124103336803165 \t -0.4321567975765851\n",
            "12     \t [ 2.65686167  2.51625847 14.          0.88290639 15.          0.59478451]. \t  -0.5107861739216322 \t -0.4321567975765851\n",
            "13     \t [9.77484633 0.68845774 6.         0.75920375 1.         0.29460664]. \t  -0.6143634084313524 \t -0.4321567975765851\n",
            "14     \t [ 9.53840144  1.46066132 14.          0.95596758 14.          0.90498168]. \t  \u001b[92m-0.42952247663467025\u001b[0m \t -0.42952247663467025\n",
            "15     \t [4.92937392 8.64776166 6.         0.60775534 1.         0.3770526 ]. \t  -0.6130810052126724 \t -0.42952247663467025\n",
            "16     \t [1.17380645 9.71363266 5.         0.89467023 9.         0.26605515]. \t  -0.6648674093386665 \t -0.42952247663467025\n",
            "17     \t [ 9.62487731  6.86253449  5.          0.66465947 12.          0.21107909]. \t  -0.6659019290689656 \t -0.42952247663467025\n",
            "18     \t [ 9.95993424  9.34546245 14.4195503   0.90342313  4.03742759  0.96134541]. \t  -0.4310185842819595 \t -0.42952247663467025\n",
            "19     \t [4.76953178e-03 9.41454834e+00 7.00000000e+00 6.50228385e-01\n",
            " 1.90000000e+01 7.89506044e-01]. \t  -0.46550355426148 \t -0.42952247663467025\n",
            "20     \t [ 2.00664037  5.88682269 14.          0.94696775  4.          0.80981462]. \t  -0.4349577325456077 \t -0.42952247663467025\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.227629657200072"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08y-IGpyceje",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd2ffcde-3621-4b4f-d159-88f0c29d8aea"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 20 \n",
        "\n",
        "np.random.seed(run_num_20)\n",
        "surrogate_winner_20 = dtStudentProcess(d_cov_func, nu = df)\n",
        "\n",
        "X_train20, X_test20, y_train20, y_test20 = train_test_split(X, y, test_size=test_perc, random_state=run_num_20)\n",
        "\n",
        "def f_syn_polarity20(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_20, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train20, y=y_train20).mean())\n",
        "    return operator * score\n",
        "\n",
        "winner_20 = dGPGO_stp(surrogate_winner_20, Acquisition_new(util_winner), f_syn_polarity20, param, n_jobs = -1) # Define BayesOpt\n",
        "winner_20.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_winner_20 = winner_20.getResult()[0]\n",
        "params_winner_20['max_depth'] = int(params_winner_20['max_depth'])\n",
        "params_winner_20['min_child_weight'] = int(params_winner_20['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_winner_train20 = xgb.DMatrix(X_train20, y_train20)\n",
        "dX_winner_test20 = xgb.DMatrix(X_test20, y_test20)\n",
        "model_winner_20 = xgb.train(params_winner_20, dX_winner_train20)\n",
        "pred_winner_20 = model_winner_20.predict(dX_winner_test20)\n",
        "\n",
        "rmse_winner_20 = np.sqrt(mean_squared_error(pred_winner_20, y_test20))\n",
        "rmse_winner_20"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 5.70517285  5.64528755 11.          0.51213999 19.          0.57883228]. \t  -0.5510634045248362 \t -0.4667910180825121\n",
            "init   \t [0.68106288 5.8452906  6.         0.58037829 8.         0.87974899]. \t  -0.4667910180825121 \t -0.4667910180825121\n",
            "init   \t [ 4.04121426  7.17971417 11.          0.57603584 10.          0.19954225]. \t  -0.6541702754539586 \t -0.4667910180825121\n",
            "init   \t [ 5.27556761  8.67655329 13.          0.54252273  4.          0.48570796]. \t  -0.5813476654605825 \t -0.4667910180825121\n",
            "init   \t [ 5.30003889  5.73946822 12.          0.52666698 16.          0.77467019]. \t  -0.5078849923055367 \t -0.4667910180825121\n",
            "1      \t [ 9.25190708  0.65043072 10.          0.56676233  5.          0.41289847]. \t  -0.6164858434894066 \t -0.4667910180825121\n",
            "2      \t [8.65689736 8.11684598 5.         0.53907072 1.         0.87574215]. \t  -0.47576906935129115 \t -0.4667910180825121\n",
            "3      \t [6.49728107e-01 7.72862043e-04 7.00000000e+00 5.71905117e-01\n",
            " 2.00000000e+00 1.02014940e-01]. \t  -0.6533990377801203 \t -0.4667910180825121\n",
            "4      \t [ 9.51806562  0.44900923  6.          0.62997227 16.          0.40028974]. \t  -0.6257141325107729 \t -0.4667910180825121\n",
            "5      \t [ 0.48808143  1.97030839  7.          0.79892377 16.          0.98477167]. \t  \u001b[92m-0.4602371335347518\u001b[0m \t -0.4602371335347518\n",
            "6      \t [ 9.98029391  8.23632437  6.          0.63606408 13.          0.22908429]. \t  -0.6532745897130707 \t -0.4602371335347518\n",
            "7      \t [ 3.9245879   0.87058505 14.          0.68269183  1.          0.34048922]. \t  -0.623283664424315 \t -0.4602371335347518\n",
            "8      \t [ 0.45468431  0.13267263 10.          0.667214    9.          0.47071141]. \t  -0.5807933165122651 \t -0.4602371335347518\n",
            "9      \t [ 9.73917908  4.09196933 14.41953458  0.91590422  9.02066503  0.99279156]. \t  \u001b[92m-0.43010694501136815\u001b[0m \t -0.43010694501136815\n",
            "10     \t [ 0.26650768  8.81821255  6.          0.54118329 18.          0.20228777]. \t  -0.6522760722738631 \t -0.43010694501136815\n",
            "11     \t [ 7.73310578 10.         14.63658132  1.         12.63658132  1.        ]. \t  \u001b[92m-0.4110077876955973\u001b[0m \t -0.4110077876955973\n",
            "12     \t [ 9.96980635  9.3302118  14.50546112  1.         19.50546112  1.        ]. \t  -0.4170181463067479 \t -0.4110077876955973\n",
            "13     \t [6.42736069e+00 1.14311754e-02 1.30000000e+01 8.07947475e-01\n",
            " 1.40000000e+01 4.58287855e-01]. \t  -0.5780838419347841 \t -0.4110077876955973\n",
            "14     \t [1.36051555 9.95957797 7.         0.9242116  2.         0.87097462]. \t  -0.449167771194887 \t -0.4110077876955973\n",
            "15     \t [10.         10.          9.39335757  1.          5.39335757  1.        ]. \t  -0.4226139464028621 \t -0.4110077876955973\n",
            "16     \t [6.91617445 5.5643963  6.         0.86299121 7.         0.84226072]. \t  -0.5155491867186228 \t -0.4110077876955973\n",
            "17     \t [ 4.82227274  0.31446108  6.          0.75575149 12.          0.93023075]. \t  -0.4670203990352901 \t -0.4110077876955973\n",
            "18     \t [ 0.19585457  9.70236501 12.          0.69800118 17.          0.47644801]. \t  -0.5791764001267719 \t -0.4110077876955973\n",
            "19     \t [ 6.53384033  5.82485681  5.          0.72422044 17.          0.61312031]. \t  -0.5669534875260422 \t -0.4110077876955973\n",
            "20     \t [ 8.17732484  5.22697425 11.          0.61303379  1.          0.32672413]. \t  -0.6191817873624627 \t -0.4110077876955973\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.292019604709815"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78QysHAlchIb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "787cd927-92d7-46b7-e39e-cfd6e0a70e46"
      },
      "source": [
        "end_win = time.time()\n",
        "end_win\n",
        "\n",
        "time_win = end_win - start_win\n",
        "time_win"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1145.650500535965"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CU2FlhY4vHUk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3465ecf9-6178-4fe5-e521-88bdd0e04d3e"
      },
      "source": [
        "rmse_loser = [rmse_loser_1,\n",
        "rmse_loser_2,\n",
        "rmse_loser_3,\n",
        "rmse_loser_4,\n",
        "rmse_loser_5,\n",
        "rmse_loser_6,\n",
        "rmse_loser_7,\n",
        "rmse_loser_8,\n",
        "rmse_loser_9,\n",
        "rmse_loser_10,\n",
        "rmse_loser_11,\n",
        "rmse_loser_12,\n",
        "rmse_loser_13,\n",
        "rmse_loser_14,\n",
        "rmse_loser_15,\n",
        "rmse_loser_16,\n",
        "rmse_loser_17,\n",
        "rmse_loser_18,\n",
        "rmse_loser_19,\n",
        "rmse_loser_20]\n",
        "\n",
        "np.mean(rmse_loser)"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.332741455425436"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQ1lotm7vJi9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "743d8262-63fd-4333-c51d-6151e729fa13"
      },
      "source": [
        "rmse_winner = [rmse_winner_1,\n",
        "rmse_winner_2,\n",
        "rmse_winner_3,\n",
        "rmse_winner_4,\n",
        "rmse_winner_5,\n",
        "rmse_winner_6,\n",
        "rmse_winner_7,\n",
        "rmse_winner_8,\n",
        "rmse_winner_9,\n",
        "rmse_winner_10,\n",
        "rmse_winner_11,\n",
        "rmse_winner_12,\n",
        "rmse_winner_13,\n",
        "rmse_winner_14,\n",
        "rmse_winner_15,\n",
        "rmse_winner_16,\n",
        "rmse_winner_17,\n",
        "rmse_winner_18,\n",
        "rmse_winner_19,\n",
        "rmse_winner_20]\n",
        "\n",
        "np.mean(rmse_winner)"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.210948008312053"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1yU4s1GRvMdl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08ee1a94-0825-41ed-85fd-21f940ea5534"
      },
      "source": [
        "min_rmse_loser = min_max_array(rmse_loser)\n",
        "min_rmse_loser, len(min_rmse_loser)"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([4.87018129255363,\n",
              "  4.343487868410211,\n",
              "  4.343487868410211,\n",
              "  4.343487868410211,\n",
              "  4.31722690080873,\n",
              "  4.1451215046465775,\n",
              "  4.1451215046465775,\n",
              "  4.1451215046465775,\n",
              "  4.130636454660636,\n",
              "  4.130636454660636,\n",
              "  4.130636454660636,\n",
              "  4.130636454660636,\n",
              "  4.130636454660636,\n",
              "  4.130636454660636,\n",
              "  4.130636454660636,\n",
              "  4.130636454660636,\n",
              "  4.0138942791645995,\n",
              "  3.937438334950126,\n",
              "  3.937438334950126,\n",
              "  3.937438334950126],\n",
              " 20)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unXOpKHcvO15",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "718e7ad8-d48c-4a9d-ae17-878bd2efb559"
      },
      "source": [
        "min_rmse_winner = min_max_array(rmse_winner)\n",
        "min_rmse_winner, len(min_rmse_winner)"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([4.3850165384814055,\n",
              "  4.270099338431699,\n",
              "  4.270099338431699,\n",
              "  4.270099338431699,\n",
              "  4.192568642647803,\n",
              "  4.192568642647803,\n",
              "  4.192568642647803,\n",
              "  4.192568642647803,\n",
              "  4.192568642647803,\n",
              "  4.192568642647803,\n",
              "  4.175661289540509,\n",
              "  4.013463439067471,\n",
              "  4.009508878947214,\n",
              "  3.9754513513477603,\n",
              "  3.9754513513477603,\n",
              "  3.9754513513477603,\n",
              "  3.7829724957594517,\n",
              "  3.782019062517165,\n",
              "  3.782019062517165,\n",
              "  3.782019062517165],\n",
              " 20)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yxo85-HEvRPi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "outputId": "cc4a4cf6-8175-4c5e-dbb9-0a500224e69a"
      },
      "source": [
        "### Visualise!\n",
        "\n",
        "title = obj_func\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(min_rmse_loser, color = 'Green', label='RMSE: GP dEI ')\n",
        "plt.plot(min_rmse_winner, color = 'Purple', label='RMSE: STP dCBM ')\n",
        "\n",
        "plt.title(title, weight = 'bold', family = 'Arial')\n",
        "plt.xlabel('Experiment(s)', weight = 'bold', family = 'Arial') # x-axis label\n",
        "plt.ylabel('RMSE', weight = 'bold', family = 'Arial') # y-axis label\n",
        "plt.legend(loc=0) # add plot legend\n",
        "\n",
        "### Make the x-ticks integers, not floats:\n",
        "count = len(min_rmse_loser)\n",
        "plt.xticks(np.arange(count), np.arange(1, count + 1))\n",
        "plt.grid(b=None)\n",
        "plt.show() #visualize!\n"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwyO7_iZvT7a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ab16fb4-8142-4bc8-89d2-dc1cf416e521"
      },
      "source": [
        "time_lose, time_win"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1529.2834980487823, 1145.650500535965)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    }
  ]
}