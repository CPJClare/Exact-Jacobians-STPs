{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Hartmann3__STP__dCBM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RaQo_XsusPbm"
      },
      "source": [
        "Hartmann3 synthetic function:\r\n",
        "\r\n",
        "GP EI: (exact GP EI gradients) vs. STP CBM: (exact STP CBM gradients)\r\n",
        "\r\n",
        "https://www.sfu.ca/~ssurjano/hart3.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bhgucjJgsYfm",
        "outputId": "ba3f7444-af76-4c0a-ce87-70885fd1132d"
      },
      "source": [
        "!pip install pyGPGO"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyGPGO\n",
            "  Downloading https://files.pythonhosted.org/packages/8d/87/a113c91ba014708114f7635d5c0f6a5e5c773480c5f0a537b257a02d180d/pyGPGO-0.4.0.dev1.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pyGPGO) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from pyGPGO) (1.4.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from pyGPGO) (1.0.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from pyGPGO) (0.22.2.post1)\n",
            "Requirement already satisfied: theano in /usr/local/lib/python3.7/dist-packages (from pyGPGO) (1.0.5)\n",
            "Requirement already satisfied: pyMC3 in /usr/local/lib/python3.7/dist-packages (from pyGPGO) (3.7)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from theano->pyGPGO) (1.15.0)\n",
            "Requirement already satisfied: h5py>=2.7.0 in /usr/local/lib/python3.7/dist-packages (from pyMC3->pyGPGO) (2.10.0)\n",
            "Requirement already satisfied: tqdm>=4.8.4 in /usr/local/lib/python3.7/dist-packages (from pyMC3->pyGPGO) (4.41.1)\n",
            "Requirement already satisfied: pandas>=0.18.0 in /usr/local/lib/python3.7/dist-packages (from pyMC3->pyGPGO) (1.1.5)\n",
            "Requirement already satisfied: patsy>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from pyMC3->pyGPGO) (0.5.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.18.0->pyMC3->pyGPGO) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.18.0->pyMC3->pyGPGO) (2.8.1)\n",
            "Building wheels for collected packages: pyGPGO\n",
            "  Building wheel for pyGPGO (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyGPGO: filename=pyGPGO-0.4.0.dev1-cp37-none-any.whl size=19866 sha256=678beaefe2d3950c60c8fee24d052bc18be23fc169befeb2d4ad5342bdb96fe2\n",
            "  Stored in directory: /root/.cache/pip/wheels/d1/27/04/c4fa3bfe194d36e3cd51542132f43415a6813114a5e8301acb\n",
            "Successfully built pyGPGO\n",
            "Installing collected packages: pyGPGO\n",
            "Successfully installed pyGPGO-0.4.0.dev1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E99qMZverfrP"
      },
      "source": [
        "### Import modules:\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "import scipy as sp\r\n",
        "import pandas as pd\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import warnings\r\n",
        "import time\r\n",
        "\r\n",
        "from pyGPGO.logger import EventLogger\r\n",
        "from pyGPGO.GPGO import GPGO\r\n",
        "from pyGPGO.surrogates.GaussianProcess import GaussianProcess\r\n",
        "from pyGPGO.surrogates.tStudentProcess import tStudentProcess\r\n",
        "from pyGPGO.acquisition import Acquisition\r\n",
        "from pyGPGO.covfunc import squaredExponential\r\n",
        "\r\n",
        "from collections import OrderedDict\r\n",
        "from joblib import Parallel, delayed\r\n",
        "from numpy.linalg import slogdet, inv, cholesky, solve\r\n",
        "from scipy.optimize import minimize\r\n",
        "from scipy.spatial.distance import cdist\r\n",
        "from scipy.special import gamma\r\n",
        "from scipy.stats import norm, t\r\n",
        "from matplotlib.pyplot import rc\r\n",
        "from mpl_toolkits.axes_grid1.inset_locator import inset_axes, mark_inset\r\n",
        "\r\n",
        "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xiJSpz2P9qXK"
      },
      "source": [
        "n_start_AcqFunc = 250 #multi-start iterations to avoid local optima in AcqFunc optimization"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzPUbCs3s9nu"
      },
      "source": [
        "### Inputs:\r\n",
        "\r\n",
        "obj_func = 'Hartmann3'\r\n",
        "n_test = n_start_AcqFunc # test points\r\n",
        "df = 3 # nu\r\n",
        "\r\n",
        "util_loser = 'dEI_GP'\r\n",
        "util_winner = 'dCBM_STP'\r\n",
        "\r\n",
        "n_init = 5 # random initialisations"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-AMqjmpbtAb1"
      },
      "source": [
        "### Objective function:\r\n",
        "\r\n",
        "if obj_func == 'Hartmann3': # 3-D\r\n",
        "            \r\n",
        "    # True y bounds:\r\n",
        "    y_lb = -3.86278\r\n",
        "    operator = -1 # targets global minimum \r\n",
        "    y_global_orig = y_lb * operator # targets global minimum\r\n",
        "            \r\n",
        "    # Constraints:\r\n",
        "    lb = 0\r\n",
        "    ub = 1\r\n",
        "    \r\n",
        "    # Input array dimension(s):\r\n",
        "    dim = 3\r\n",
        "\r\n",
        "    # 3-D inputs' parameter bounds:\r\n",
        "    param = {'x1_training': ('cont', [lb, ub]),\r\n",
        "             'x2_training': ('cont', [lb, ub]),\r\n",
        "             'x3_training': ('cont', [lb, ub])}\r\n",
        "    \r\n",
        "    max_iter = 100  # iterations of Bayesian optimisation\r\n",
        "    \r\n",
        "    # Test data:\r\n",
        "    x1_test = np.linspace(lb, ub, n_test) \r\n",
        "    x2_test = np.linspace(lb, ub, n_test)\r\n",
        "    x3_test = np.linspace(lb, ub, n_test)\r\n",
        "    Xstar_d = np.column_stack((x1_test, x2_test, x3_test))\r\n",
        "    \r\n",
        "    def f_syn_polarity(x1_training, x2_training, x3_training):\r\n",
        "       \r\n",
        "        value = np.array([x1_training, x2_training, x3_training])\r\n",
        "      \r\n",
        "        a = np.array([[3.0, 10, 30],\r\n",
        "                      [0.1, 10, 35],\r\n",
        "                      [3.0, 10, 30],\r\n",
        "                      [0.1, 10, 35]])\r\n",
        "        \r\n",
        "        alpha = np.array([1.0, 1.2, 3.0, 3.2])\r\n",
        "      \r\n",
        "        p = np.array([[.3689, .1170, .2673],\r\n",
        "                      [.4699, .4387, .7470],\r\n",
        "                      [.1091, .8732, .5547],\r\n",
        "                      [.3810, .5743, .8828]])\r\n",
        "  \r\n",
        "        s = 0\r\n",
        "        for i in [0,1,2,3]:\r\n",
        "            sm = a[i,0]*(value[0]-p[i,0])**2\r\n",
        "            sm += a[i,1]*(value[1]-p[i,1])**2\r\n",
        "            sm += a[i,2]*(value[2]-p[i,2])**2\r\n",
        "            s += alpha[i]*np.exp(-sm)\r\n",
        "        result = -s\r\n",
        "        \r\n",
        "        return operator * result"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5GlOrB5CyJkY"
      },
      "source": [
        "Beta_CBM = 1.5 # Default UCB Acquisition function parameter in pyGPGO https://github.com/josejimenezluna/pyGPGO/blob/master/pyGPGO/acquisition.py#L83"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyQX-59DtC7-"
      },
      "source": [
        "### Cumulative Regret Calculator:\r\n",
        "\r\n",
        "def min_max_array(x):\r\n",
        "    new_list = []\r\n",
        "    for i, num in enumerate(x):\r\n",
        "            new_list.append(np.min(x[0:i+1]))\r\n",
        "    return new_list"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dE_I_YSntHxl"
      },
      "source": [
        "### Set-seeds:\r\n",
        "\r\n",
        "run_num_1 = 111\r\n",
        "run_num_2 = 113\r\n",
        "run_num_3 = 3333\r\n",
        "run_num_4 = 444\r\n",
        "run_num_5 = 5555\r\n",
        "run_num_6 = 6\r\n",
        "run_num_7 = 777\r\n",
        "run_num_8 = 887\r\n",
        "run_num_9 = 99\r\n",
        "run_num_10 = 1000\r\n",
        "run_num_11 = 1113\r\n",
        "run_num_12 = 1234\r\n",
        "run_num_13 = 2345\r\n",
        "run_num_14 = 88\r\n",
        "run_num_15 = 1556\r\n",
        "run_num_16 = 1666\r\n",
        "run_num_17 = 717\r\n",
        "run_num_18 = 8\r\n",
        "run_num_19 = 1998\r\n",
        "run_num_20 = 2000"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oe29v5N5tKjF"
      },
      "source": [
        "### Derivatives - Squared-exponential covariance function:\r\n",
        "\r\n",
        "def l2norm_(X, Xstar):\r\n",
        "    \r\n",
        "    return cdist(X, Xstar)\r\n",
        "\r\n",
        "def kronDelta(X, Xstar):\r\n",
        "\r\n",
        "    return cdist(X, Xstar) < np.finfo(np.float32).eps\r\n",
        "\r\n",
        "class squaredExponentialDeriv(squaredExponential):\r\n",
        "    \r\n",
        "    def K(self, X, Xstar):\r\n",
        "        \r\n",
        "        r = (l2norm_(X, Xstar)/self.l)\r\n",
        "        K = self.sigmaf * np.exp(-1/2*r **2) + self.sigman * kronDelta(X, Xstar)\r\n",
        "        return K\r\n",
        "    \r\n",
        "    def dK(self, X, Xstar):\r\n",
        "        \r\n",
        "        r = (l2norm_(X, Xstar)/self.l)\r\n",
        "        dK = self.sigmaf/self.l**2 * np.exp(-1/2 * r **2) * l2norm_(X, Xstar)\r\n",
        "        return dK\r\n",
        "    \r\n",
        "        \r\n",
        "    def d2K(self, X, Xstar):\r\n",
        "        \r\n",
        "        r = (l2norm_(X, Xstar)/self.l)\r\n",
        "        d2K = self.sigmaf/self.l**2 * np.exp(-1/2 * r **2) * (r**2-1)\r\n",
        "        return d2K\r\n",
        "    \r\n",
        "cov_func = squaredExponential()\r\n",
        "d_cov_func = squaredExponentialDeriv()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEBX5l7TtNSN"
      },
      "source": [
        "class Acquisition_new(Acquisition):    \r\n",
        "    def __init__(self, mode, eps=1e-08, **params):\r\n",
        "        \r\n",
        "        self.params = params\r\n",
        "        self.eps = eps\r\n",
        "\r\n",
        "        mode_dict = {\r\n",
        "            'dEI_GP': self.dEI_GP,\r\n",
        "            'dCBM_STP': self.dCBM_STP\r\n",
        "        }\r\n",
        "\r\n",
        "        self.f = mode_dict[mode]\r\n",
        "    \r\n",
        "    def dEI_GP(self, tau, mean, std, ds, dm, dvdv, d2v, d2m):\r\n",
        "        z = -1 * (tau - mean - self.eps) / (std + self.eps)\r\n",
        "        \r\n",
        "        dsdx = ds / 2 * (std + self.eps)\r\n",
        "        d2sdx = -dsdx**2 / ((std + self.eps)) - dvdv / (std + self.eps) - d2v / (std + self.eps)\r\n",
        "        dmdx = (dm - z * dsdx) / (std + self.eps)\r\n",
        "        d2mdx = (d2m - (z * d2sdx + 2 * dmdx * dsdx)) / (std + self.eps)\r\n",
        "        \r\n",
        "        f = (std + self.eps) * (z * norm.cdf(z) + norm.pdf(z)[0])\r\n",
        "        df = (f / (std + self.eps) * dsdx + (std + self.eps) * norm.cdf(z) * dmdx)\r\n",
        "        d2f = (f / (std + self.eps) * d2sdx + dsdx * dmdx * norm.cdf(z) \\\r\n",
        "            + d2mdx * (std + self.eps) * norm.cdf(z) + dsdx * norm.cdf(z) * dmdx \\\r\n",
        "            + norm.pdf(z)[0] * (std + self.eps) * dmdx)\r\n",
        "            \r\n",
        "        return f, df, d2f\r\n",
        "\r\n",
        "    def dCBM_STP(self, tau, mean, std, ds, dm, dvdv, d2v, d2m, nu=3.0):\r\n",
        "    \r\n",
        "        gamma = -1 * (y_global_orig - mean - self.eps) / (std + self.eps)\r\n",
        "        dsdx = ds / 2 * (std + self.eps)\r\n",
        "        dmdx = (dm - gamma * dsdx) / (std + self.eps)\r\n",
        "\r\n",
        "        f = (std + self.eps) * (gamma + np.sqrt(Beta_CBM))\r\n",
        "        df = dsdx * (gamma + np.sqrt(Beta_CBM)) + (std + self.eps) * (dmdx + np.sqrt(Beta_CBM))\r\n",
        "        return f, df\r\n",
        "    \r\n",
        "    def _eval(self, tau, mean, std):\r\n",
        "    \r\n",
        "        return self.f(tau, mean, std, **self.params)\r\n",
        "    \r\n",
        "    def d_eval(self, tau, mean, std, ds, dm, dvdv, d2v, d2m):\r\n",
        "    \r\n",
        "        return self.f(tau, mean, std, ds, dm, dvdv, d2v, d2m, **self.params)\r\n",
        "\r\n",
        "    def d_eval_stp(self, tau, mean, std, ds, dm, dvdv, d2v, d2m, nu=3.0):\r\n",
        "    \r\n",
        "        return self.f(tau, mean, std, ds, dm, dvdv, d2v, d2m, nu=3.0, **self.params)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SdY8rHwtRJ_"
      },
      "source": [
        "### Surrogate derivatives: \r\n",
        "\r\n",
        "from scipy.linalg import cholesky, solve\r\n",
        "\r\n",
        "class dGaussianProcess(GaussianProcess):\r\n",
        "    l = 1\r\n",
        "    sigmaf = 1\r\n",
        "    sigman = 1e-6\r\n",
        "\r\n",
        "    def AcqGrad(self, Xstar, return_std=False):\r\n",
        "        r_X = l2norm_(self.X, self.X)/self.l\r\n",
        "        K = self.sigmaf * np.exp(-1/2*r_X **2) + self.sigman * kronDelta(self.X, self.X)\r\n",
        "        L = cholesky(K).T\r\n",
        "        alpha = solve(L.T, solve(L, self.y))\r\n",
        "        Xstar = np.atleast_2d(Xstar)\r\n",
        "        Kstar = squaredExponentialDeriv.K(self, self.X, Xstar).T\r\n",
        "        dKstar = squaredExponentialDeriv.dK(self, self.X, Xstar).T\r\n",
        "        d2Kstar = squaredExponentialDeriv.d2K(self, self.X, Xstar).T\r\n",
        "        v = solve(self.L, Kstar.T)\r\n",
        "        dv = solve(self.L, dKstar.T)\r\n",
        "        d2v = solve(self.L, d2Kstar.T)\r\n",
        "        \r\n",
        "        ds = -2 * np.dot(dv.T, v)\r\n",
        "        dvdv = np.dot(dv.T, dv)\r\n",
        "        d2s = -2 * (dvdv + d2v)\r\n",
        "        \r\n",
        "        dm = np.dot(dKstar, alpha)\r\n",
        "        d2m = np.dot(d2Kstar, alpha)\r\n",
        "        return ds, dm, dvdv, d2v, d2m\r\n",
        "\r\n",
        "class dtStudentProcess(tStudentProcess):\r\n",
        "    l = 1\r\n",
        "    sigmaf = 1\r\n",
        "    sigman = 1e-6\r\n",
        "    \r\n",
        "    def AcqGrad(self, Xstar, return_std=False):\r\n",
        "        r_X = l2norm_(self.X, self.X)/self.l\r\n",
        "        K = self.sigmaf * np.exp(-1/2*r_X **2) + self.sigman * kronDelta(self.X, self.X)\r\n",
        "        L = cholesky(self.K11).T\r\n",
        "        alpha = solve(L.T, solve(L, self.y))\r\n",
        "        Xstar = np.atleast_2d(Xstar)\r\n",
        "        Kstar = squaredExponentialDeriv.K(self, self.X, Xstar).T\r\n",
        "        dKstar = squaredExponentialDeriv.dK(self, self.X, Xstar).T\r\n",
        "        d2Kstar = squaredExponentialDeriv.d2K(self, self.X, Xstar).T\r\n",
        "        v = solve(L, Kstar.T)\r\n",
        "        dv = solve(L, dKstar.T)\r\n",
        "        d2v = solve(L, d2Kstar.T)\r\n",
        "        \r\n",
        "        ds = -2 * np.dot(dv.T, v)\r\n",
        "        dvdv = np.dot(dv.T, dv)\r\n",
        "        d2s = -2 * (dvdv + d2v)\r\n",
        "        \r\n",
        "        dm = np.dot(dKstar, alpha)\r\n",
        "        d2m = np.dot(d2Kstar, alpha)\r\n",
        "        return ds, dm, dvdv, d2v, d2m"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNv4Z0EqtUi2"
      },
      "source": [
        "class dGPGO(GPGO):  \r\n",
        "    n_start = n_start_AcqFunc\r\n",
        "    eps = 1e-08\r\n",
        "        \r\n",
        "    def func(self, xnew):\r\n",
        "        new_mean, new_var = self.GP.predict(xnew, return_std=True)\r\n",
        "        new_std = np.sqrt(new_var + 1e-6)\r\n",
        "        ds, dm, dvdv, d2v, d2m = self.GP.AcqGrad(xnew, return_std=True)\r\n",
        "        f  = np.empty((self.n_start,))\r\n",
        "        df = np.empty((self.n_start,))\r\n",
        "        f  = -self.A.d_eval(self.tau, new_mean, new_std, ds=ds, dm=dm, dvdv=dvdv, d2v=d2v, d2m=d2m)[0]\r\n",
        "        df = -self.A.d_eval(self.tau, new_mean, new_std, ds=ds, dm=dm, dvdv=dvdv, d2v=d2v, d2m=d2m)[1]\r\n",
        "        df_array = np.full((len(xnew),),df)\r\n",
        "        return f, df_array\r\n",
        "        \r\n",
        "    def d_optimizeAcq(self, method='L-BFGS-B', n_start=n_start):\r\n",
        "        start_points_dict = [self._sampleParam() for i in range(n_start)]\r\n",
        "        start_points_arr = np.array([list(s.values())\r\n",
        "                                     for s in start_points_dict])\r\n",
        "        x_best = np.empty((n_start, len(self.parameter_key)))\r\n",
        "        f_best = np.empty((n_start,))\r\n",
        "        opt = Parallel(n_jobs=self.n_jobs)(delayed(minimize)(self.func,\r\n",
        "                                                                 x0=start_point,\r\n",
        "                                                                 method=method,\r\n",
        "                                                                 jac = True,\r\n",
        "                                                                 bounds=self.parameter_range) for start_point in\r\n",
        "                                               start_points_arr)\r\n",
        "        x_best = np.array([res.x for res in opt])\r\n",
        "        f_best = np.array([np.atleast_1d(res.fun)[0] for res in opt])\r\n",
        "\r\n",
        "        self.x_best = x_best\r\n",
        "        self.f_best = f_best\r\n",
        "        self.best = x_best[np.argmin(f_best)]\r\n",
        "    \r\n",
        "    def run(self, max_iter=10, init_evals=3, resume=False):\r\n",
        "        \r\n",
        "        if not resume:\r\n",
        "            self.init_evals = init_evals\r\n",
        "            self._firstRun(self.init_evals)\r\n",
        "            self.logger._printInit(self)\r\n",
        "        for iteration in range(max_iter):\r\n",
        "            self.d_optimizeAcq()\r\n",
        "            self.updateGP()\r\n",
        "            self.logger._printCurrent(self)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7vea4uj-GOi"
      },
      "source": [
        "## dGPGO_stp - BayesOpt derivatives' class: Student's-t\r\n",
        "\r\n",
        "class dGPGO_stp(GPGO):  \r\n",
        "    n_start = 100\r\n",
        "        \r\n",
        "    def func_stp(self, xnew):\r\n",
        "        new_mean, new_var = self.GP.predict(xnew, return_std=True)\r\n",
        "        new_std = np.sqrt(new_var + 1e-6)\r\n",
        "        ds, dm, dvdv, d2v, d2m = self.GP.AcqGrad(xnew, return_std=True)\r\n",
        "        f  = np.empty((self.n_start,))\r\n",
        "        df = np.empty((self.n_start,))\r\n",
        "        f  = -self.A.d_eval_stp(self.tau, new_mean, new_std, ds=ds, dm=dm, dvdv=dvdv, d2v=d2v, d2m=d2m, nu=df)[0]\r\n",
        "        df = -self.A.d_eval_stp(self.tau, new_mean, new_std, ds=ds, dm=dm, dvdv=dvdv, d2v=d2v, d2m=d2m, nu=df)[1]\r\n",
        "        df_array = np.full((len(xnew),),df)\r\n",
        "        return f, df_array\r\n",
        "        \r\n",
        "    def d_optimizeAcq_stp(self, method='L-BFGS-B', n_start=n_start):\r\n",
        "        start_points_dict = [self._sampleParam() for i in range(n_start)]\r\n",
        "        start_points_arr = np.array([list(s.values())\r\n",
        "                                     for s in start_points_dict])\r\n",
        "        x_best = np.empty((n_start, len(self.parameter_key)))\r\n",
        "        f_best = np.empty((n_start,))\r\n",
        "        opt = Parallel(n_jobs=self.n_jobs)(delayed(minimize)(self.func_stp,\r\n",
        "                                                                 x0=start_point,\r\n",
        "                                                                 method=method,\r\n",
        "                                                                 jac = True,\r\n",
        "                                                                 bounds=self.parameter_range) for start_point in\r\n",
        "                                               start_points_arr)\r\n",
        "        x_best = np.array([res.x for res in opt])\r\n",
        "        f_best = np.array([np.atleast_1d(res.fun)[0] for res in opt])\r\n",
        "\r\n",
        "        self.x_best = x_best\r\n",
        "        self.f_best = f_best\r\n",
        "        self.best = x_best[np.argmin(f_best)]\r\n",
        "    \r\n",
        "    def run(self, max_iter=10, init_evals=3, resume=False):\r\n",
        "        \r\n",
        "        if not resume:\r\n",
        "            self.init_evals = init_evals\r\n",
        "            self._firstRun(self.init_evals)\r\n",
        "            self.logger._printInit(self)\r\n",
        "        for iteration in range(max_iter):\r\n",
        "            self.d_optimizeAcq_stp()\r\n",
        "            self.updateGP()\r\n",
        "            self.logger._printCurrent(self)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "expMSaSUtcB-"
      },
      "source": [
        "### d2GPGO - BayesOpt class: \r\n",
        "\r\n",
        "class d2GPGO(GPGO):  \r\n",
        "    n_start = 100\r\n",
        "    p = np.full((n_start,1),1)\r\n",
        "    \r\n",
        "    def func(self, xnew):\r\n",
        "        new_mean, new_var = self.GP.predict(xnew, return_std=True)\r\n",
        "        new_std = np.sqrt(new_var + 1e-6)\r\n",
        "        ds, dm, dvdv, d2s, d2m = self.GP.AcqGrad(xnew, return_std=True)\r\n",
        "        f  = np.empty((self.n_start,))\r\n",
        "        df = np.empty((self.n_start,))\r\n",
        "        f  = -self.A.d_eval(self.tau, new_mean, new_std, ds=ds, dm=dm, dvdv=dvdv, d2s=d2s, d2m=d2m)[0]\r\n",
        "        df = -self.A.d_eval(self.tau, new_mean, new_std, ds=ds, dm=dm, dvdv=dvdv, d2s=d2s, d2m=d2m)[1]\r\n",
        "        df_array = np.full((len(xnew),),df)\r\n",
        "        return f, df_array\r\n",
        "    \r\n",
        "    def hessp_nonzero(self, xnew, p):\r\n",
        "        new_mean, new_var = self.GP.predict(xnew, return_std=True)\r\n",
        "        new_std = np.sqrt(new_var + 1e-6)\r\n",
        "        ds, dm, dvdv, d2s, d2m = self.GP.AcqGrad(xnew, return_std=True)\r\n",
        "        df2 = np.empty((self.n_start,))\r\n",
        "        df2 = -self.A.d_eval(self.tau, new_mean, new_std, ds=ds, dm=dm, dvdv=dvdv, d2s=d2s, d2m=d2m)[2]\r\n",
        "        H2 = np.empty((self.n_start,))\r\n",
        "        df2 = np.asarray(df2)\r\n",
        "        p = np.asarray(p)\r\n",
        "        H2 = np.multiply(df2,p)\r\n",
        "        return H2\r\n",
        "    \r\n",
        "    def d_optimizeAcq(self, method='Newton-CG', n_start=n_start):\r\n",
        "        start_points_dict = [self._sampleParam() for i in range(n_start)]\r\n",
        "        start_points_arr = np.array([list(s.values())\r\n",
        "                                     for s in start_points_dict])\r\n",
        "        x_best = np.empty((n_start, len(self.parameter_key)))\r\n",
        "        f_best = np.empty((n_start,))\r\n",
        "        opt = Parallel(n_jobs=self.n_jobs)(delayed(minimize)(self.func,\r\n",
        "                                                                 x0=start_point,\r\n",
        "                                                                 method=method,\r\n",
        "                                                                 jac = True,\r\n",
        "                                                                 hessp = self.hessp_nonzero,\r\n",
        "                                                                 bounds=self.parameter_range) for start_point in\r\n",
        "                                               start_points_arr)\r\n",
        "        \r\n",
        "        x_best = np.array([res.x for res in opt])\r\n",
        "        f_best = np.array([np.atleast_1d(res.fun)[0] for res in opt])\r\n",
        "\r\n",
        "        self.best = x_best[np.argmin(f_best)]     \r\n",
        "    \r\n",
        "    def run(self, max_iter=10, init_evals=3, resume=False):\r\n",
        "        \r\n",
        "        if not resume:\r\n",
        "            self.init_evals = init_evals\r\n",
        "            self._firstRun(self.init_evals)\r\n",
        "            self.logger._printInit(self)\r\n",
        "        for iteration in range(max_iter):\r\n",
        "            self.d_optimizeAcq()\r\n",
        "            self.updateGP()\r\n",
        "            self.logger._printCurrent(self)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "umKPgDuEtcxH",
        "outputId": "9e7c353d-d606-42f4-969d-8a7f5415fff7"
      },
      "source": [
        "start_lose = time.time()\r\n",
        "start_lose"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1614278044.0048807"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mfEuNIqvtdSv",
        "outputId": "dca8e24d-fb95-4ad5-b25c-1921c8f84d48"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 1\r\n",
        "\r\n",
        "np.random.seed(run_num_1)\r\n",
        "surrogate_loser_1 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "loser_1 = dGPGO(surrogate_loser_1, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_1.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.61217018 0.16906975 0.43605902]. \t  0.37345684462559386 \t 2.3951473341797507\n",
            "init   \t [0.76926247 0.2953253  0.14916296]. \t  0.296162062221701 \t 2.3951473341797507\n",
            "init   \t [0.02247832 0.42022449 0.23868214]. \t  0.2904113287153621 \t 2.3951473341797507\n",
            "init   \t [0.33765619 0.99071246 0.23772645]. \t  0.11013785080555143 \t 2.3951473341797507\n",
            "init   \t [0.08119266 0.66960024 0.62124292]. \t  2.3951473341797507 \t 2.3951473341797507\n",
            "1      \t [0.00485804 0.68286261 0.96018946]. \t  \u001b[92m2.420367869177603\u001b[0m \t 2.420367869177603\n",
            "2      \t [0.02880528 0.86879906 0.77145199]. \t  1.7601887902025246 \t 2.420367869177603\n",
            "3      \t [0.07492704 0.49363721 0.85736657]. \t  \u001b[92m3.6977257680054527\u001b[0m \t 3.6977257680054527\n",
            "4      \t [0.01690823 0.20303658 0.75373379]. \t  1.1282788852081818 \t 3.6977257680054527\n",
            "5      \t [0.8642259  0.77437694 0.99435991]. \t  1.4015971324220011 \t 3.6977257680054527\n",
            "6      \t [0.45635757 0.34106775 0.99893775]. \t  1.2764378468913773 \t 3.6977257680054527\n",
            "7      \t [0.51811283 0.98946856 0.87150225]. \t  0.67902146488023 \t 3.6977257680054527\n",
            "8      \t [0.99661639 0.78557754 0.7341533 ]. \t  1.357762610045871 \t 3.6977257680054527\n",
            "9      \t [0.23891206 0.52279384 0.744595  ]. \t  2.9890323710727635 \t 3.6977257680054527\n",
            "10     \t [0.81264104 0.02508176 0.05331337]. \t  0.1288747745884618 \t 3.6977257680054527\n",
            "11     \t [0.04939583 0.93739076 0.03475733]. \t  0.0010293495994891455 \t 3.6977257680054527\n",
            "12     \t [0.98025994 0.94493063 0.93523017]. \t  0.7397702819810867 \t 3.6977257680054527\n",
            "13     \t [0.98287372 0.41064655 0.75548703]. \t  2.5063594077603826 \t 3.6977257680054527\n",
            "14     \t [0.00356336 0.52795408 0.85605193]. \t  \u001b[92m3.784229583259046\u001b[0m \t 3.784229583259046\n",
            "15     \t [0.0224553  0.43458959 0.97377221]. \t  2.141981244272377 \t 3.784229583259046\n",
            "16     \t [0.89463363 0.84820422 0.04541421]. \t  0.0006702216295822134 \t 3.784229583259046\n",
            "17     \t [0.44206035 0.23963528 0.53817297]. \t  0.32404118535996623 \t 3.784229583259046\n",
            "18     \t [0.95149581 0.20190542 0.27573327]. \t  0.3360499969635434 \t 3.784229583259046\n",
            "19     \t [0.0256771  0.06257825 0.03723693]. \t  0.13933363855005157 \t 3.784229583259046\n",
            "20     \t [9.74869324e-01 5.78320164e-04 8.56021504e-01]. \t  0.2252628604478815 \t 3.784229583259046\n",
            "21     \t [0.0315056  0.47574841 0.74104847]. \t  2.7939271436222963 \t 3.784229583259046\n",
            "22     \t [0.28843827 0.47548806 0.0361873 ]. \t  0.054818027981571635 \t 3.784229583259046\n",
            "23     \t [0.96154876 0.6577064  0.49507711]. \t  0.28915685733921626 \t 3.784229583259046\n",
            "24     \t [0.07414375 0.54304308 0.82810303]. \t  3.77544594612987 \t 3.784229583259046\n",
            "25     \t [0.02938756 0.45830168 0.01424345]. \t  0.03241094855247772 \t 3.784229583259046\n",
            "26     \t [0.15112422 0.57169865 0.8695877 ]. \t  \u001b[92m3.8131284252695528\u001b[0m \t 3.8131284252695528\n",
            "27     \t [0.15043162 0.60970521 0.85780357]. \t  3.7470332461824105 \t 3.8131284252695528\n",
            "28     \t [0.74361257 0.16587207 0.9127195 ]. \t  0.7937697608354773 \t 3.8131284252695528\n",
            "29     \t [0.33554799 0.1960815  0.29285402]. \t  0.9219313064285296 \t 3.8131284252695528\n",
            "30     \t [0.11181592 0.64508821 0.86932481]. \t  3.552307619911147 \t 3.8131284252695528\n",
            "31     \t [0.34104117 0.1003848  0.35929984]. \t  0.7758886749514444 \t 3.8131284252695528\n",
            "32     \t [0.74081242 0.2689585  0.55926302]. \t  0.3561168332330422 \t 3.8131284252695528\n",
            "33     \t [0.78657803 0.00727757 0.24876982]. \t  0.5199892114983524 \t 3.8131284252695528\n",
            "34     \t [0.71467268 0.1858588  0.43815269]. \t  0.30645954895088945 \t 3.8131284252695528\n",
            "35     \t [0.35345103 0.44407979 0.5372887 ]. \t  0.7310458138475027 \t 3.8131284252695528\n",
            "36     \t [0.99817061 0.28845339 0.96459069]. \t  1.2542554717761243 \t 3.8131284252695528\n",
            "37     \t [0.13551095 0.6147752  0.86099234]. \t  3.7217456557246953 \t 3.8131284252695528\n",
            "38     \t [0.10902695 0.23131956 0.48553758]. \t  0.2882119729872925 \t 3.8131284252695528\n",
            "39     \t [0.02291614 0.84016591 0.0677154 ]. \t  0.0034917025704461944 \t 3.8131284252695528\n",
            "40     \t [0.73759773 0.4973644  0.27133928]. \t  0.17695216392903418 \t 3.8131284252695528\n",
            "41     \t [0.09340409 0.63120493 0.81880677]. \t  3.5500371462617144 \t 3.8131284252695528\n",
            "42     \t [0.15975379 0.0859061  0.84242869]. \t  0.5263639474321362 \t 3.8131284252695528\n",
            "43     \t [0.67406653 0.50991186 0.8188701 ]. \t  3.624086501118737 \t 3.8131284252695528\n",
            "44     \t [0.56610359 0.55232818 0.85012893]. \t  \u001b[92m3.825256006651254\u001b[0m \t 3.825256006651254\n",
            "45     \t [0.58436425 0.14790159 0.82402201]. \t  0.8773608434753184 \t 3.825256006651254\n",
            "46     \t [0.63902865 0.60026654 0.85043859]. \t  3.722030318046956 \t 3.825256006651254\n",
            "47     \t [0.17251606 0.20286351 0.62992262]. \t  0.5516254039236159 \t 3.825256006651254\n",
            "48     \t [0.17397627 0.42220867 0.13521323]. \t  0.2102484680473599 \t 3.825256006651254\n",
            "49     \t [0.01465029 0.57513231 0.85630011]. \t  3.8014188742411545 \t 3.825256006651254\n",
            "50     \t [0.09463249 0.74337459 0.42976448]. \t  1.6086984706306757 \t 3.825256006651254\n",
            "51     \t [0.55394923 0.60701978 0.86924581]. \t  3.7135686672921917 \t 3.825256006651254\n",
            "52     \t [0.15384586 0.58434252 0.88508595]. \t  3.7217618326684545 \t 3.825256006651254\n",
            "53     \t [0.97344354 0.58114841 0.77316942]. \t  2.992698046283317 \t 3.825256006651254\n",
            "54     \t [0.5634305  0.72109247 0.84012619]. \t  2.9223035948746707 \t 3.825256006651254\n",
            "55     \t [0.94325517 0.68353852 0.09056374]. \t  0.006284268308091021 \t 3.825256006651254\n",
            "56     \t [0.52111688 0.40014383 0.78101946]. \t  2.817593833921735 \t 3.825256006651254\n",
            "57     \t [0.55152553 0.57153675 0.89655024]. \t  3.6490884963887 \t 3.825256006651254\n",
            "58     \t [0.50107907 0.74984541 0.40285632]. \t  0.8316034757573986 \t 3.825256006651254\n",
            "59     \t [0.6360143  0.17643848 0.6802233 ]. \t  0.6809071664843318 \t 3.825256006651254\n",
            "60     \t [0.80083048 0.87131534 0.70079956]. \t  0.954051341659558 \t 3.825256006651254\n",
            "61     \t [0.82945898 0.45413776 0.05193529]. \t  0.04229423308324152 \t 3.825256006651254\n",
            "62     \t [0.00645321 0.41674021 0.89383033]. \t  3.012380711862429 \t 3.825256006651254\n",
            "63     \t [0.03893406 0.41065296 0.22052005]. \t  0.29746538350005025 \t 3.825256006651254\n",
            "64     \t [0.41043802 0.7724817  0.31190719]. \t  0.36545761370116336 \t 3.825256006651254\n",
            "65     \t [0.10812468 0.72529452 0.25465347]. \t  0.18203581392260854 \t 3.825256006651254\n",
            "66     \t [0.02337902 0.57170695 0.84412219]. \t  3.802239441730874 \t 3.825256006651254\n",
            "67     \t [0.36281072 0.18520399 0.67829577]. \t  0.7170562744784025 \t 3.825256006651254\n",
            "68     \t [0.15911236 0.11059501 0.65089183]. \t  0.3669042997391726 \t 3.825256006651254\n",
            "69     \t [0.86350529 0.8473428  0.26259772]. \t  0.04416400359341738 \t 3.825256006651254\n",
            "70     \t [0.91551351 0.96186868 0.09233347]. \t  0.0007755993199785646 \t 3.825256006651254\n",
            "71     \t [0.54915703 0.56985442 0.26726008]. \t  0.173083197347087 \t 3.825256006651254\n",
            "72     \t [0.25627324 0.96587806 0.97601747]. \t  0.5330995829102597 \t 3.825256006651254\n",
            "73     \t [0.94115139 0.34761178 0.07356107]. \t  0.07136458178756505 \t 3.825256006651254\n",
            "74     \t [0.48760414 0.88948427 0.93965469]. \t  1.1227699240858733 \t 3.825256006651254\n",
            "75     \t [0.11304749 0.9094962  0.33213995]. \t  0.671613628713174 \t 3.825256006651254\n",
            "76     \t [0.27651602 0.0953211  0.81854326]. \t  0.5872636327758214 \t 3.825256006651254\n",
            "77     \t [0.85890336 0.09590625 0.90600383]. \t  0.4619459676508832 \t 3.825256006651254\n",
            "78     \t [0.23733868 0.82048291 0.73915572]. \t  2.124101861946425 \t 3.825256006651254\n",
            "79     \t [0.14510227 0.95514932 0.67105549]. \t  2.084284345766824 \t 3.825256006651254\n",
            "80     \t [0.62299085 0.06122236 0.99608914]. \t  0.17879623427674915 \t 3.825256006651254\n",
            "81     \t [0.29088308 0.7915848  0.41403377]. \t  1.4173589192745526 \t 3.825256006651254\n",
            "82     \t [0.04076394 0.57291442 0.54014621]. \t  1.474778439507063 \t 3.825256006651254\n",
            "83     \t [0.75370983 0.59648656 0.59667461]. \t  0.9825631878460681 \t 3.825256006651254\n",
            "84     \t [0.59983715 0.47183885 0.69463084]. \t  2.06949302073836 \t 3.825256006651254\n",
            "85     \t [0.33242802 0.83723309 0.23185022]. \t  0.11719684568063422 \t 3.825256006651254\n",
            "86     \t [0.07444371 0.91348219 0.27435046]. \t  0.27969467234132755 \t 3.825256006651254\n",
            "87     \t [0.12414229 0.18733278 0.12349369]. \t  0.4276920407976786 \t 3.825256006651254\n",
            "88     \t [0.04087888 0.24199679 0.91266509]. \t  1.323603308572735 \t 3.825256006651254\n",
            "89     \t [0.82437346 0.62993189 0.86780455]. \t  3.530389788348192 \t 3.825256006651254\n",
            "90     \t [0.51211929 0.25967277 0.9643693 ]. \t  1.1073875208142783 \t 3.825256006651254\n",
            "91     \t [0.37851489 0.68753796 0.86748997]. \t  3.2710525685540577 \t 3.825256006651254\n",
            "92     \t [0.91092111 0.42303455 0.93094604]. \t  2.642199576880523 \t 3.825256006651254\n",
            "93     \t [0.84348377 0.94747191 0.42779077]. \t  0.35056261602863853 \t 3.825256006651254\n",
            "94     \t [0.91441912 0.00287177 0.42191247]. \t  0.18004712031761186 \t 3.825256006651254\n",
            "95     \t [0.73026186 0.35943098 0.6904105 ]. \t  1.5865167617832956 \t 3.825256006651254\n",
            "96     \t [0.49324108 0.4044849  0.28058902]. \t  0.43862141669775573 \t 3.825256006651254\n",
            "97     \t [0.7886927  0.92922132 0.01214836]. \t  0.00022036803397238445 \t 3.825256006651254\n",
            "98     \t [0.80472456 0.53074051 0.03159763]. \t  0.01934647763876607 \t 3.825256006651254\n",
            "99     \t [0.39965895 0.49225993 0.45506877]. \t  0.5537815478508364 \t 3.825256006651254\n",
            "100    \t [0.32115229 0.55739422 0.943681  ]. \t  3.080532019803937 \t 3.825256006651254\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1j6UBhkdtdVe",
        "outputId": "07752dc6-0e7e-410b-8df1-1798cf86e87b"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 2\r\n",
        "\r\n",
        "np.random.seed(run_num_2)\r\n",
        "surrogate_loser_2 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "loser_2 = dGPGO(surrogate_loser_2, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_2.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.85198549 0.0739036  0.89493176]. \t  0.3999083566189884 \t 2.6229838112516717\n",
            "init   \t [0.43649355 0.12767773 0.57585787]. \t  0.24461577848211966 \t 2.6229838112516717\n",
            "init   \t [0.84047092 0.43512055 0.69591056]. \t  1.8897715258798413 \t 2.6229838112516717\n",
            "init   \t [0.6846381  0.70064837 0.77969426]. \t  2.6229838112516717 \t 2.6229838112516717\n",
            "init   \t [0.64274937 0.96102617 0.10846489]. \t  0.003309399496320042 \t 2.6229838112516717\n",
            "1      \t [0.84869923 0.93241717 0.93297728]. \t  0.833544515077231 \t 2.6229838112516717\n",
            "2      \t [0.02123335 0.81643687 0.92868758]. \t  1.7643065017177344 \t 2.6229838112516717\n",
            "3      \t [0.53772176 0.60822836 0.78939081]. \t  \u001b[92m3.334335833419579\u001b[0m \t 3.334335833419579\n",
            "4      \t [0.41955576 0.63153118 0.95115261]. \t  2.832879013202632 \t 3.334335833419579\n",
            "5      \t [0.04108525 0.77996289 0.25475702]. \t  0.19144274065115907 \t 3.334335833419579\n",
            "6      \t [0.33921093 0.6247264  0.68984899]. \t  2.4018311508594015 \t 3.334335833419579\n",
            "7      \t [0.4698131 1.        1.       ]. \t  0.3328206922837246 \t 3.334335833419579\n",
            "8      \t [0.5556955  0.49822112 0.91801822]. \t  3.306460837857002 \t 3.334335833419579\n",
            "9      \t [0.55486262 0.48000381 0.80443181]. \t  \u001b[92m3.45892051020631\u001b[0m \t 3.45892051020631\n",
            "10     \t [0.49254267 0.51666065 0.09211929]. \t  0.07789428322196056 \t 3.45892051020631\n",
            "11     \t [0.56427197 0.47315337 0.80769217]. \t  3.4524476308700676 \t 3.45892051020631\n",
            "12     \t [0.20076159 0.94802777 0.08215082]. \t  0.0037369434913272913 \t 3.45892051020631\n",
            "13     \t [0.59511958 0.52829156 0.7703814 ]. \t  3.2000191832757725 \t 3.45892051020631\n",
            "14     \t [0.88475754 0.03414516 0.00250664]. \t  0.05128069141159111 \t 3.45892051020631\n",
            "15     \t [0.93786595 0.53122158 0.05330172]. \t  0.017297868965155307 \t 3.45892051020631\n",
            "16     \t [0.55954194 0.47340423 0.8693643 ]. \t  \u001b[92m3.5813279149003128\u001b[0m \t 3.5813279149003128\n",
            "17     \t [0.04768557 0.32850307 0.06774003]. \t  0.14217133957014558 \t 3.5813279149003128\n",
            "18     \t [0.526396   0.39045094 0.86418002]. \t  2.984508448221251 \t 3.5813279149003128\n",
            "19     \t [0.43142122 0.02467197 0.00728288]. \t  0.11940469697941827 \t 3.5813279149003128\n",
            "20     \t [0.74314632 0.5090107  0.94480178]. \t  2.9362200818931545 \t 3.5813279149003128\n",
            "21     \t [0.22895586 0.32638851 0.44386193]. \t  0.38289739220068636 \t 3.5813279149003128\n",
            "22     \t [0.46913283 0.82262434 0.20066583]. \t  0.05199322975955362 \t 3.5813279149003128\n",
            "23     \t [0.09778912 0.09784369 0.99987747]. \t  0.24248164089538843 \t 3.5813279149003128\n",
            "24     \t [0.03443249 0.4132959  0.79333907]. \t  2.9934704340345526 \t 3.5813279149003128\n",
            "25     \t [0.38506754 0.63776631 0.47773801]. \t  1.2393662793255629 \t 3.5813279149003128\n",
            "26     \t [0.31206494 0.14585568 0.16799338]. \t  0.7307676260265302 \t 3.5813279149003128\n",
            "27     \t [0.01981209 0.39317072 0.99526576]. \t  1.5953649003670003 \t 3.5813279149003128\n",
            "28     \t [0. 0. 0.]. \t  0.06797411659013229 \t 3.5813279149003128\n",
            "29     \t [0.27452343 0.49067769 0.19135809]. \t  0.21490083226807472 \t 3.5813279149003128\n",
            "30     \t [0.41783706 0.46348636 0.08625525]. \t  0.11238386633298683 \t 3.5813279149003128\n",
            "31     \t [0.00644994 0.58259736 0.77864353]. \t  3.3564867701344787 \t 3.5813279149003128\n",
            "32     \t [0.44338236 0.20986102 0.51086629]. \t  0.28470762022492946 \t 3.5813279149003128\n",
            "33     \t [0.85132587 0.65806184 0.68313386]. \t  1.5772789254037718 \t 3.5813279149003128\n",
            "34     \t [0.72357086 0.17284508 0.5742109 ]. \t  0.2758162501943998 \t 3.5813279149003128\n",
            "35     \t [0.38871972 0.08593287 0.46946098]. \t  0.31822054482285544 \t 3.5813279149003128\n",
            "36     \t [2.55343109e-01 6.77527185e-01 3.57837085e-04]. \t  0.005091131363578613 \t 3.5813279149003128\n",
            "37     \t [0.03071868 0.06496602 0.57653961]. \t  0.15761028036633287 \t 3.5813279149003128\n",
            "38     \t [0.66009546 0.39983393 0.51405981]. \t  0.37461569752794827 \t 3.5813279149003128\n",
            "39     \t [0.59449511 0.41620195 0.91113527]. \t  2.8805701809722963 \t 3.5813279149003128\n",
            "40     \t [0.2256699  0.07992419 0.41748843]. \t  0.4819630201952553 \t 3.5813279149003128\n",
            "41     \t [0.97615885 0.05411726 0.32503146]. \t  0.28831141983778286 \t 3.5813279149003128\n",
            "42     \t [0.53328464 0.57774717 0.88782334]. \t  \u001b[92m3.7094133224093757\u001b[0m \t 3.7094133224093757\n",
            "43     \t [0.52246356 0.82266054 0.13911684]. \t  0.013759219160063875 \t 3.7094133224093757\n",
            "44     \t [0.72982604 0.28184031 0.86459949]. \t  1.9044776108865855 \t 3.7094133224093757\n",
            "45     \t [0.63818509 0.39198514 0.45536791]. \t  0.2892748188116343 \t 3.7094133224093757\n",
            "46     \t [0.73136775 0.30934836 0.92232944]. \t  1.8276447749873623 \t 3.7094133224093757\n",
            "47     \t [0.77047443 0.64214483 0.12903927]. \t  0.024102114086029674 \t 3.7094133224093757\n",
            "48     \t [0.4634485  0.65534339 0.23827586]. \t  0.11593556414242427 \t 3.7094133224093757\n",
            "49     \t [0.26527368 0.43158941 0.30440152]. \t  0.4071890908589547 \t 3.7094133224093757\n",
            "50     \t [0.98012153 0.29487901 0.54063701]. \t  0.273707971859134 \t 3.7094133224093757\n",
            "51     \t [0.20844235 0.33272232 0.11427776]. \t  0.2883861055346616 \t 3.7094133224093757\n",
            "52     \t [0.83243348 0.27176422 0.0498824 ]. \t  0.10004439130921053 \t 3.7094133224093757\n",
            "53     \t [0.31228377 0.9497917  0.78925978]. \t  1.1372129515685026 \t 3.7094133224093757\n",
            "54     \t [0.81204389 0.10745564 0.99319514]. \t  0.2793664207698889 \t 3.7094133224093757\n",
            "55     \t [0.30559737 0.74140756 0.20001864]. \t  0.0690608605636907 \t 3.7094133224093757\n",
            "56     \t [0.30355771 0.60105121 0.66701023]. \t  2.232680275582264 \t 3.7094133224093757\n",
            "57     \t [0.32655384 0.45283861 0.70914601]. \t  2.3157027494336946 \t 3.7094133224093757\n",
            "58     \t [0.28164058 0.91846989 0.13751527]. \t  0.015473546478944136 \t 3.7094133224093757\n",
            "59     \t [0.04941912 0.54241983 0.36607595]. \t  0.4385741473197778 \t 3.7094133224093757\n",
            "60     \t [0.17888481 0.27261652 0.57983518]. \t  0.5075266432155109 \t 3.7094133224093757\n",
            "61     \t [0.60451084 0.31746966 0.12296527]. \t  0.30343594735798185 \t 3.7094133224093757\n",
            "62     \t [0.69919355 0.08866689 0.64966157]. \t  0.30694751763473066 \t 3.7094133224093757\n",
            "63     \t [0.89315899 0.15618225 0.85935459]. \t  0.8735852961978772 \t 3.7094133224093757\n",
            "64     \t [0.1521253  0.35901651 0.22965336]. \t  0.4723852558929425 \t 3.7094133224093757\n",
            "65     \t [0.23231678 0.10972766 0.93641936]. \t  0.44882914638217647 \t 3.7094133224093757\n",
            "66     \t [0.84188226 0.70723005 0.54045426]. \t  0.6264810020583895 \t 3.7094133224093757\n",
            "67     \t [0.01124263 0.9495282  0.89764946]. \t  0.8860680765708808 \t 3.7094133224093757\n",
            "68     \t [0.64184498 0.26595085 0.80861442]. \t  1.7950464127622898 \t 3.7094133224093757\n",
            "69     \t [0.0229726  0.00499224 0.67044876]. \t  0.17729951913373557 \t 3.7094133224093757\n",
            "70     \t [0.51570772 0.65260972 0.10114952]. \t  0.025592110981376887 \t 3.7094133224093757\n",
            "71     \t [0.60347441 0.897619   0.45446129]. \t  1.06949486043582 \t 3.7094133224093757\n",
            "72     \t [0.95758577 0.64651273 0.9322479 ]. \t  2.9288989872413875 \t 3.7094133224093757\n",
            "73     \t [0.78857677 0.49823768 0.51203495]. \t  0.38734467751295526 \t 3.7094133224093757\n",
            "74     \t [0.43165387 0.02353938 0.45456709]. \t  0.3284207744430256 \t 3.7094133224093757\n",
            "75     \t [0.93712698 0.40753674 0.39799078]. \t  0.1357588290930862 \t 3.7094133224093757\n",
            "76     \t [0.75852101 0.56650411 0.29588659]. \t  0.12717945507032985 \t 3.7094133224093757\n",
            "77     \t [0.65699665 0.67754593 0.95225407]. \t  2.573110674137345 \t 3.7094133224093757\n",
            "78     \t [0.21411235 0.92073944 0.18720244]. \t  0.050558991438194395 \t 3.7094133224093757\n",
            "79     \t [0.81186963 0.95466875 0.19513996]. \t  0.013622693124886443 \t 3.7094133224093757\n",
            "80     \t [0.13321787 0.16889317 0.15110243]. \t  0.549728293542864 \t 3.7094133224093757\n",
            "81     \t [0.50055678 0.40552636 0.35889602]. \t  0.39460953216206407 \t 3.7094133224093757\n",
            "82     \t [0.68937348 0.74734445 0.93696005]. \t  2.2621603595408324 \t 3.7094133224093757\n",
            "83     \t [0.24422687 0.25258948 0.42558462]. \t  0.43458421843040185 \t 3.7094133224093757\n",
            "84     \t [0.33290521 0.92785349 0.70086949]. \t  1.7092276465727636 \t 3.7094133224093757\n",
            "85     \t [0.23066553 0.98634866 0.51216754]. \t  2.405127214497184 \t 3.7094133224093757\n",
            "86     \t [0.77062047 0.44235163 0.41154312]. \t  0.2070073279439252 \t 3.7094133224093757\n",
            "87     \t [0.89972384 0.5416144  0.91950053]. \t  3.316834149142124 \t 3.7094133224093757\n",
            "88     \t [0.35886225 0.27348523 0.98584931]. \t  1.0168568786796437 \t 3.7094133224093757\n",
            "89     \t [0.24008624 0.71862237 0.60323517]. \t  2.5243228209177246 \t 3.7094133224093757\n",
            "90     \t [0.70227469 0.71073892 0.38639049]. \t  0.3630229515433469 \t 3.7094133224093757\n",
            "91     \t [0.04630934 0.53729892 0.00134421]. \t  0.015083670432843367 \t 3.7094133224093757\n",
            "92     \t [0.68416238 0.83798606 0.92513195]. \t  1.5835707271787056 \t 3.7094133224093757\n",
            "93     \t [0.80857863 0.30322871 0.42341   ]. \t  0.23276557630011466 \t 3.7094133224093757\n",
            "94     \t [0.87240962 0.53505616 0.5612704 ]. \t  0.5766218070407124 \t 3.7094133224093757\n",
            "95     \t [0.37075559 0.35467445 0.40266831]. \t  0.4292705700245347 \t 3.7094133224093757\n",
            "96     \t [0.07623101 0.7603222  0.69765839]. \t  2.4874462002268505 \t 3.7094133224093757\n",
            "97     \t [0.52298903 0.07651244 0.54207474]. \t  0.17715826053064726 \t 3.7094133224093757\n",
            "98     \t [0.18967587 0.49028662 0.51106987]. \t  0.8680665472248712 \t 3.7094133224093757\n",
            "99     \t [0.77481162 0.28016538 0.68979909]. \t  1.2006144140525907 \t 3.7094133224093757\n",
            "100    \t [0.34253365 0.56155319 0.19047772]. \t  0.13390384213700912 \t 3.7094133224093757\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eWnL1rwztdYG",
        "outputId": "969a8d30-19a5-47a4-daad-b3271f3b783e"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 3\r\n",
        "\r\n",
        "np.random.seed(run_num_3)\r\n",
        "surrogate_loser_3 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "loser_3 = dGPGO(surrogate_loser_3, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_3.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.75157561 0.10925128 0.48612128]. \t  0.19395982560567754 \t 0.5647137279144399\n",
            "init   \t [0.49983118 0.65711    0.23588471]. \t  0.10633623603471726 \t 0.5647137279144399\n",
            "init   \t [0.61279489 0.1196524  0.7122023 ]. \t  0.5647137279144399 \t 0.5647137279144399\n",
            "init   \t [0.37256054 0.52476827 0.26328708]. \t  0.24645583811739444 \t 0.5647137279144399\n",
            "init   \t [0.62163122 0.44909976 0.21906361]. \t  0.2633149120115605 \t 0.5647137279144399\n",
            "1      \t [0.03146046 0.20692188 0.93064958]. \t  \u001b[92m0.9684227062893344\u001b[0m \t 0.9684227062893344\n",
            "2      \t [0.22074887 0.05835947 0.90763313]. \t  0.3319318657983171 \t 0.9684227062893344\n",
            "3      \t [0.80398301 0.59732101 0.97755957]. \t  \u001b[92m2.4285425171285553\u001b[0m \t 2.4285425171285553\n",
            "4      \t [0.5136305  0.6806365  0.98913253]. \t  2.0107525433877536 \t 2.4285425171285553\n",
            "5      \t [0.96047188 0.84635755 0.98454941]. \t  1.059610295269561 \t 2.4285425171285553\n",
            "6      \t [0.73881103 0.2852016  0.92866174]. \t  1.569439758670843 \t 2.4285425171285553\n",
            "7      \t [0.71874866 0.52230578 0.95817827]. \t  \u001b[92m2.7596541737687836\u001b[0m \t 2.7596541737687836\n",
            "8      \t [0.58383333 0.51359399 0.72217957]. \t  2.534851203896329 \t 2.7596541737687836\n",
            "9      \t [0.69948316 0.50135082 0.96939443]. \t  2.5149949368150564 \t 2.7596541737687836\n",
            "10     \t [1.62021478e-14 1.11382987e-14 1.16864725e-14]. \t  0.06797411659014926 \t 2.7596541737687836\n",
            "11     \t [0.08297588 0.58449891 0.0494654 ]. \t  0.02180149043689334 \t 2.7596541737687836\n",
            "12     \t [0.72187606 0.64625932 0.88228903]. \t  \u001b[92m3.435042406543167\u001b[0m \t 3.435042406543167\n",
            "13     \t [0.69808759 0.73788415 0.86826237]. \t  2.744041156678493 \t 3.435042406543167\n",
            "14     \t [0.93887588 0.6613653  0.69546595]. \t  1.6275056193081119 \t 3.435042406543167\n",
            "15     \t [0.31568132 0.43634948 0.00142586]. \t  0.04293482559335524 \t 3.435042406543167\n",
            "16     \t [0.16842711 0.275181   0.2593233 ]. \t  0.6951641784284915 \t 3.435042406543167\n",
            "17     \t [0.         0.06579622 0.28490418]. \t  0.6422561348126641 \t 3.435042406543167\n",
            "18     \t [0.71655162 0.49206118 0.79554346]. \t  3.3736815291998044 \t 3.435042406543167\n",
            "19     \t [0.75518888 0.55697264 0.75159371]. \t  2.855037242391764 \t 3.435042406543167\n",
            "20     \t [0.70228575 0.97471212 0.02582005]. \t  0.00029310748513941917 \t 3.435042406543167\n",
            "21     \t [0.99752384 0.064151   0.1703046 ]. \t  0.22410412246044556 \t 3.435042406543167\n",
            "22     \t [0.0228455  0.90537222 0.38683236]. \t  1.2492930756187308 \t 3.435042406543167\n",
            "23     \t [0.13378523 0.95916986 0.14208018]. \t  0.017267101982617633 \t 3.435042406543167\n",
            "24     \t [0.15772068 0.87127358 0.74372625]. \t  1.8724552511399 \t 3.435042406543167\n",
            "25     \t [0.14574635 0.38726702 0.24731617]. \t  0.42664930173697285 \t 3.435042406543167\n",
            "26     \t [0.04777267 0.86009005 0.91542791]. \t  1.4808874257581612 \t 3.435042406543167\n",
            "27     \t [0.63444916 0.36426605 0.85539499]. \t  2.7493518034550775 \t 3.435042406543167\n",
            "28     \t [0.73479632 0.49575709 0.51112471]. \t  0.4256175209454758 \t 3.435042406543167\n",
            "29     \t [0.30819472 0.01158196 0.12503412]. \t  0.48221799946723587 \t 3.435042406543167\n",
            "30     \t [0.74662962 0.58954966 0.84542697]. \t  \u001b[92m3.7070039032667954\u001b[0m \t 3.7070039032667954\n",
            "31     \t [0.42596168 0.35197751 0.56469788]. \t  0.5906940593598009 \t 3.7070039032667954\n",
            "32     \t [0.80411671 0.9277025  0.12495525]. \t  0.0031149831092267023 \t 3.7070039032667954\n",
            "33     \t [0.66688846 0.22554472 0.58509531]. \t  0.3956915884135331 \t 3.7070039032667954\n",
            "34     \t [0.96402316 0.80627495 0.02410483]. \t  0.0005753484237622942 \t 3.7070039032667954\n",
            "35     \t [0.46624611 0.63255562 0.52234503]. \t  1.2945035042624384 \t 3.7070039032667954\n",
            "36     \t [0.75738113 0.92448556 0.82374733]. \t  1.0051096970615077 \t 3.7070039032667954\n",
            "37     \t [0.13401842 0.59394694 0.07117342]. \t  0.02871568488814516 \t 3.7070039032667954\n",
            "38     \t [0.5921114  0.60036898 0.98206343]. \t  2.377598838026082 \t 3.7070039032667954\n",
            "39     \t [0.79172764 0.90784428 0.70914857]. \t  0.8431319054674943 \t 3.7070039032667954\n",
            "40     \t [0.1529027  0.26265257 0.99129722]. \t  0.9064177756037938 \t 3.7070039032667954\n",
            "41     \t [0.62518867 0.7121498  0.30259927]. \t  0.17816033134214168 \t 3.7070039032667954\n",
            "42     \t [0.24439409 0.39110835 0.57603432]. \t  0.8043123257896625 \t 3.7070039032667954\n",
            "43     \t [0.23812336 0.07427496 0.89625034]. \t  0.40560748042148276 \t 3.7070039032667954\n",
            "44     \t [0.04418732 0.53134902 0.61276949]. \t  1.6534094783377127 \t 3.7070039032667954\n",
            "45     \t [0.89190225 0.97916204 0.0016274 ]. \t  7.543539199836043e-05 \t 3.7070039032667954\n",
            "46     \t [0.8607458  0.29914426 0.76705565]. \t  1.8822648550157148 \t 3.7070039032667954\n",
            "47     \t [0.48787013 0.40950719 0.58412121]. \t  0.8188835841989224 \t 3.7070039032667954\n",
            "48     \t [0.6043061  0.76298401 0.62408145]. \t  1.5630631055365873 \t 3.7070039032667954\n",
            "49     \t [0.40684603 0.86167236 0.56044735]. \t  2.3905743122833742 \t 3.7070039032667954\n",
            "50     \t [0.62111368 0.52704732 0.84922528]. \t  \u001b[92m3.7896923749467715\u001b[0m \t 3.7896923749467715\n",
            "51     \t [0.08214015 0.55596698 0.52951788]. \t  1.3249204381796214 \t 3.7896923749467715\n",
            "52     \t [0.49073662 0.44830916 0.62947485]. \t  1.3035237788368443 \t 3.7896923749467715\n",
            "53     \t [0.61633848 0.39414783 0.60448986]. \t  0.8720175831755752 \t 3.7896923749467715\n",
            "54     \t [0.09632054 0.64536185 0.89614376]. \t  3.407552915077267 \t 3.7896923749467715\n",
            "55     \t [0.92134716 0.78477873 0.66050099]. \t  0.9011550630172612 \t 3.7896923749467715\n",
            "56     \t [0.57293523 0.53361841 0.30639247]. \t  0.22796920303046153 \t 3.7896923749467715\n",
            "57     \t [0.21923538 0.73686176 0.76555192]. \t  2.631803742382601 \t 3.7896923749467715\n",
            "58     \t [0.28290393 0.40273006 0.21517779]. \t  0.40796869278351705 \t 3.7896923749467715\n",
            "59     \t [0.7297733  0.40900862 0.17685063]. \t  0.2271653108899221 \t 3.7896923749467715\n",
            "60     \t [0.2230724  0.51649058 0.0899105 ]. \t  0.07523571607849378 \t 3.7896923749467715\n",
            "61     \t [0.47269005 0.57372622 0.30365754]. \t  0.24086025892892132 \t 3.7896923749467715\n",
            "62     \t [0.19179956 0.04726392 0.22522958]. \t  0.8223003040779117 \t 3.7896923749467715\n",
            "63     \t [0.00454622 0.61282583 0.98623381]. \t  2.2601194402741687 \t 3.7896923749467715\n",
            "64     \t [0.21059232 0.42055126 0.12793231]. \t  0.20770560044238096 \t 3.7896923749467715\n",
            "65     \t [0.34021681 0.82098355 0.4395948 ]. \t  1.6861927233521885 \t 3.7896923749467715\n",
            "66     \t [0.39788785 0.59683922 0.81430545]. \t  3.6427668029762037 \t 3.7896923749467715\n",
            "67     \t [0.46904421 0.31390676 0.81104597]. \t  2.257544468526626 \t 3.7896923749467715\n",
            "68     \t [0.84478817 0.17962491 0.48977645]. \t  0.17730670216691743 \t 3.7896923749467715\n",
            "69     \t [0.65164826 0.42992198 0.58646353]. \t  0.786818072813842 \t 3.7896923749467715\n",
            "70     \t [0.61884566 0.31746878 0.49688504]. \t  0.29554206320383436 \t 3.7896923749467715\n",
            "71     \t [0.61376377 0.70204201 0.35125962]. \t  0.3258708768234984 \t 3.7896923749467715\n",
            "72     \t [0.70209444 0.16663276 0.15167109]. \t  0.46828875679206056 \t 3.7896923749467715\n",
            "73     \t [0.26262437 0.44250019 0.03060716]. \t  0.06252148954452384 \t 3.7896923749467715\n",
            "74     \t [0.16253149 0.66009353 0.2770664 ]. \t  0.2333070716240491 \t 3.7896923749467715\n",
            "75     \t [0.54743938 0.95930507 0.37960054]. \t  0.6252381735775987 \t 3.7896923749467715\n",
            "76     \t [0.16019912 0.90675208 0.41598391]. \t  1.6567154722530448 \t 3.7896923749467715\n",
            "77     \t [0.14125313 0.56127449 0.28467157]. \t  0.24526769934086157 \t 3.7896923749467715\n",
            "78     \t [0.68087076 0.47864738 0.02789553]. \t  0.03623559685115433 \t 3.7896923749467715\n",
            "79     \t [0.84363628 0.39010202 0.36788822]. \t  0.20606981964993962 \t 3.7896923749467715\n",
            "80     \t [0.88060534 0.81553029 0.87952935]. \t  1.9182660692088596 \t 3.7896923749467715\n",
            "81     \t [0.89877928 0.3459699  0.20278669]. \t  0.22577900317369806 \t 3.7896923749467715\n",
            "82     \t [0.45857582 0.75096334 0.18121827]. \t  0.04132361494283127 \t 3.7896923749467715\n",
            "83     \t [0.63085465 0.39247245 0.10795824]. \t  0.1782535802299659 \t 3.7896923749467715\n",
            "84     \t [0.95316593 0.55188064 0.16964544]. \t  0.04218875364955372 \t 3.7896923749467715\n",
            "85     \t [0.85684935 0.75766339 0.033176  ]. \t  0.0017000811369795696 \t 3.7896923749467715\n",
            "86     \t [0.78716689 0.04587408 0.94080521]. \t  0.239651019612562 \t 3.7896923749467715\n",
            "87     \t [0.43911044 0.6981408  0.64567541]. \t  2.053757934020994 \t 3.7896923749467715\n",
            "88     \t [0.8694776  0.36424848 0.71022565]. \t  1.7943091250771404 \t 3.7896923749467715\n",
            "89     \t [0.13016603 0.16838239 0.61643476]. \t  0.4054373001335664 \t 3.7896923749467715\n",
            "90     \t [0.02729288 0.32701445 0.32086059]. \t  0.4466395844396876 \t 3.7896923749467715\n",
            "91     \t [0.37397447 0.81674824 0.92999278]. \t  1.7677722791097792 \t 3.7896923749467715\n",
            "92     \t [0.18208795 0.60697419 0.59659828]. \t  1.9675248424084217 \t 3.7896923749467715\n",
            "93     \t [0.23646816 0.25281473 0.09853109]. \t  0.3358113649828574 \t 3.7896923749467715\n",
            "94     \t [0.33836395 0.50746848 0.85995754]. \t  3.776428935520155 \t 3.7896923749467715\n",
            "95     \t [0.01149402 0.12150612 0.0622546 ]. \t  0.19307208285276578 \t 3.7896923749467715\n",
            "96     \t [0.54711447 0.76849605 0.56712612]. \t  1.70301075667317 \t 3.7896923749467715\n",
            "97     \t [0.89431243 0.73140871 0.6123873 ]. \t  0.803263834874783 \t 3.7896923749467715\n",
            "98     \t [0.18324495 0.1323387  0.43675612]. \t  0.404632292432565 \t 3.7896923749467715\n",
            "99     \t [0.38974735 0.09538962 0.96336092]. \t  0.3290183762762645 \t 3.7896923749467715\n",
            "100    \t [0.19820904 0.06434005 0.77592979]. \t  0.4449318315806874 \t 3.7896923749467715\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zciLjgNltda2",
        "outputId": "5a32f6f5-a621-46b2-ec00-5300c85ffb56"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 4\r\n",
        "\r\n",
        "np.random.seed(run_num_4)\r\n",
        "surrogate_loser_4 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "loser_4 = dGPGO(surrogate_loser_4, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_4.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.83936105 0.82129497 0.64220716]. \t  0.8796727111154451 \t 1.9592421489197056\n",
            "init   \t [0.66722262 0.03913991 0.06567239]. \t  0.21283931850025994 \t 1.9592421489197056\n",
            "init   \t [0.27650064 0.3163605  0.19359241]. \t  0.5590312289203825 \t 1.9592421489197056\n",
            "init   \t [0.19709288 0.84920429 0.84235809]. \t  1.818649590001362 \t 1.9592421489197056\n",
            "init   \t [0.34736473 0.7397423  0.49340914]. \t  1.9592421489197056 \t 1.9592421489197056\n",
            "1      \t [0.22412884 0.99744654 0.22062981]. \t  0.08723427566372202 \t 1.9592421489197056\n",
            "2      \t [0.4057286  0.31242791 0.80779792]. \t  \u001b[92m2.2369433002019576\u001b[0m \t 2.2369433002019576\n",
            "3      \t [0.32214328 0.501755   0.69916371]. \t  \u001b[92m2.347550186167049\u001b[0m \t 2.347550186167049\n",
            "4      \t [0.39005329 0.54644944 0.69817317]. \t  \u001b[92m2.3848842680130833\u001b[0m \t 2.3848842680130833\n",
            "5      \t [0.34533898 0.52489446 0.79943823]. \t  \u001b[92m3.5836920400373287\u001b[0m \t 3.5836920400373287\n",
            "6      \t [0.63960296 0.91555258 0.0555591 ]. \t  0.0010744359811289052 \t 3.5836920400373287\n",
            "7      \t [0.22351983 0.28690065 0.99397278]. \t  1.0190572439287129 \t 3.5836920400373287\n",
            "8      \t [1. 1. 1.]. \t  0.31688362070415665 \t 3.5836920400373287\n",
            "9      \t [0.77141999 0.13456388 0.98918372]. \t  0.3672430639616408 \t 3.5836920400373287\n",
            "10     \t [1.27534868e-15 1.77850620e-15 2.70523161e-15]. \t  0.06797411659013573 \t 3.5836920400373287\n",
            "11     \t [0.51437176 0.48035947 0.99743479]. \t  1.9786849166252316 \t 3.5836920400373287\n",
            "12     \t [0.84655016 0.99679085 0.6174574 ]. \t  0.5215294903522869 \t 3.5836920400373287\n",
            "13     \t [0.17365811 0.53915787 0.02836154]. \t  0.02730999347437992 \t 3.5836920400373287\n",
            "14     \t [0.29207719 0.024576   0.05725103]. \t  0.24009054578274172 \t 3.5836920400373287\n",
            "15     \t [0.98578944 0.13581165 0.27179492]. \t  0.3182599136266915 \t 3.5836920400373287\n",
            "16     \t [0.34593306 0.65138162 0.95430717]. \t  2.7032184610475998 \t 3.5836920400373287\n",
            "17     \t [0.99991374 0.24441493 0.03350525]. \t  0.0499541826127307 \t 3.5836920400373287\n",
            "18     \t [0.99429147 0.7531398  0.66991977]. \t  0.9772652638068244 \t 3.5836920400373287\n",
            "19     \t [0.27805725 0.59470274 0.85747296]. \t  \u001b[92m3.8051821097914003\u001b[0m \t 3.8051821097914003\n",
            "20     \t [0.94383316 0.48004807 0.41985768]. \t  0.12403116135075266 \t 3.8051821097914003\n",
            "21     \t [0.27516191 0.5050454  0.91115678]. \t  3.4233821782694847 \t 3.8051821097914003\n",
            "22     \t [0.88210647 0.22206517 0.85819382]. \t  1.3626645528941668 \t 3.8051821097914003\n",
            "23     \t [0.465022   0.28549004 0.02474828]. \t  0.12537651819404955 \t 3.8051821097914003\n",
            "24     \t [0.77736605 0.42924543 0.32339966]. \t  0.23232259737210692 \t 3.8051821097914003\n",
            "25     \t [0.48323297 0.97953335 0.58107143]. \t  1.7743492083378223 \t 3.8051821097914003\n",
            "26     \t [0.07390852 0.47026935 0.4418145 ]. \t  0.5388868547402469 \t 3.8051821097914003\n",
            "27     \t [0.43391942 0.96545355 0.6232254 ]. \t  1.8531866038798177 \t 3.8051821097914003\n",
            "28     \t [0.72373636 0.89800689 0.19750102]. \t  0.022219851170923014 \t 3.8051821097914003\n",
            "29     \t [0.23399885 0.62804062 0.83057931]. \t  3.6327766749305987 \t 3.8051821097914003\n",
            "30     \t [0.47646467 0.31690246 0.92015299]. \t  1.9335877861944526 \t 3.8051821097914003\n",
            "31     \t [0.26793312 0.20835569 0.39616386]. \t  0.5675650435515753 \t 3.8051821097914003\n",
            "32     \t [0.15449961 0.0739904  0.02383857]. \t  0.1444824248167775 \t 3.8051821097914003\n",
            "33     \t [0.22224128 0.58835294 0.54227105]. \t  1.5620019707976203 \t 3.8051821097914003\n",
            "34     \t [0.7624105  0.83740933 0.4174219 ]. \t  0.475673205419742 \t 3.8051821097914003\n",
            "35     \t [0.38226498 0.56576795 0.31580928]. \t  0.2940891935717929 \t 3.8051821097914003\n",
            "36     \t [0.42122065 0.03334281 0.87861693]. \t  0.2979774737400446 \t 3.8051821097914003\n",
            "37     \t [0.95577186 0.2331745  0.18906347]. \t  0.25888078497782724 \t 3.8051821097914003\n",
            "38     \t [0.77686775 0.19817622 0.50516741]. \t  0.20303404687426588 \t 3.8051821097914003\n",
            "39     \t [0.02572599 0.03736815 0.77135148]. \t  0.34568347538666316 \t 3.8051821097914003\n",
            "40     \t [0.53961985 0.04018105 0.36712348]. \t  0.6427412747817036 \t 3.8051821097914003\n",
            "41     \t [0.09117621 0.44826011 0.64659412]. \t  1.6004301056836554 \t 3.8051821097914003\n",
            "42     \t [0.69797215 0.92699324 0.60669458]. \t  1.068196523028918 \t 3.8051821097914003\n",
            "43     \t [0.14716524 0.6846356  0.55273114]. \t  2.331277322632874 \t 3.8051821097914003\n",
            "44     \t [0.94460183 0.39841865 0.20035792]. \t  0.14743471184946214 \t 3.8051821097914003\n",
            "45     \t [0.02849758 0.52533294 0.99914079]. \t  2.0416369587013725 \t 3.8051821097914003\n",
            "46     \t [0.59525019 0.69610244 0.04470115]. \t  0.0072205823831998734 \t 3.8051821097914003\n",
            "47     \t [0.18193296 0.12830195 0.57593137]. \t  0.24216357204494532 \t 3.8051821097914003\n",
            "48     \t [0.20414017 0.28332865 0.57594635]. \t  0.5157278007739652 \t 3.8051821097914003\n",
            "49     \t [0.51788579 0.81395536 0.34779211]. \t  0.4928912781654505 \t 3.8051821097914003\n",
            "50     \t [0.19029052 0.2022316  0.06057696]. \t  0.23450252136128893 \t 3.8051821097914003\n",
            "51     \t [0.31404645 0.34552945 0.79524926]. \t  2.489876466905594 \t 3.8051821097914003\n",
            "52     \t [0.38784961 0.87753762 0.74012079]. \t  1.6471620810320635 \t 3.8051821097914003\n",
            "53     \t [0.86772633 0.88541181 0.69753257]. \t  0.7934431092025374 \t 3.8051821097914003\n",
            "54     \t [0.46112737 0.30964653 0.05452543]. \t  0.17298332743033695 \t 3.8051821097914003\n",
            "55     \t [0.26064166 0.75821426 0.03807247]. \t  0.0040868577728126795 \t 3.8051821097914003\n",
            "56     \t [0.54614286 0.98388172 0.59399202]. \t  1.4879629764781568 \t 3.8051821097914003\n",
            "57     \t [0.45408303 0.80825061 0.6365093 ]. \t  2.0677163745827047 \t 3.8051821097914003\n",
            "58     \t [0.70741816 0.07928674 0.66920884]. \t  0.32737925886271296 \t 3.8051821097914003\n",
            "59     \t [0.06057931 0.81118412 0.58648205]. \t  2.9844350007817253 \t 3.8051821097914003\n",
            "60     \t [0.18077291 0.11516388 0.30186734]. \t  0.8693688111325402 \t 3.8051821097914003\n",
            "61     \t [0.78261206 0.08739428 0.30479699]. \t  0.569307619631024 \t 3.8051821097914003\n",
            "62     \t [0.20286778 0.48979907 0.2163436 ]. \t  0.2338871807716341 \t 3.8051821097914003\n",
            "63     \t [0.5386693  0.27612135 0.13522479]. \t  0.4221550183648063 \t 3.8051821097914003\n",
            "64     \t [0.22997948 0.52205122 0.23892099]. \t  0.220726732621446 \t 3.8051821097914003\n",
            "65     \t [0.85129755 0.30898539 0.42632346]. \t  0.20407290980443352 \t 3.8051821097914003\n",
            "66     \t [0.5290956  0.06631674 0.6350847 ]. \t  0.23937763515635238 \t 3.8051821097914003\n",
            "67     \t [0.32225612 0.58867405 0.66683327]. \t  2.187367589850706 \t 3.8051821097914003\n",
            "68     \t [0.70417805 0.87920468 0.75425424]. \t  1.185947839625982 \t 3.8051821097914003\n",
            "69     \t [0.68707938 0.97773471 0.3911807 ]. \t  0.44383116056510996 \t 3.8051821097914003\n",
            "70     \t [0.36894736 0.56217032 0.37926925]. \t  0.4739893548207636 \t 3.8051821097914003\n",
            "71     \t [0.91906465 0.12271966 0.32488807]. \t  0.3661618263517013 \t 3.8051821097914003\n",
            "72     \t [0.79562402 0.78140355 0.99146553]. \t  1.4024275536421427 \t 3.8051821097914003\n",
            "73     \t [0.19952009 0.46582031 0.97637385]. \t  2.2772004000300323 \t 3.8051821097914003\n",
            "74     \t [0.42921714 0.21734282 0.37283993]. \t  0.6569917965717333 \t 3.8051821097914003\n",
            "75     \t [0.58785692 0.23925236 0.34858759]. \t  0.6224387919467267 \t 3.8051821097914003\n",
            "76     \t [0.83910257 0.94330969 0.78787869]. \t  0.7862412827987283 \t 3.8051821097914003\n",
            "77     \t [0.56118624 0.59986808 0.49463569]. \t  0.8250381800002857 \t 3.8051821097914003\n",
            "78     \t [0.7662562  0.14790208 0.78952916]. \t  0.8577485172452937 \t 3.8051821097914003\n",
            "79     \t [0.21712483 0.17339281 0.04403016]. \t  0.20263511578268303 \t 3.8051821097914003\n",
            "80     \t [0.34812389 0.0410566  0.25745782]. \t  0.9402321835937894 \t 3.8051821097914003\n",
            "81     \t [0.80787489 0.49984609 0.19371546]. \t  0.11358820295056764 \t 3.8051821097914003\n",
            "82     \t [0.77992988 0.67256643 0.99579517]. \t  1.9093788624256178 \t 3.8051821097914003\n",
            "83     \t [0.74233078 0.51867866 0.07880477]. \t  0.04543925368190092 \t 3.8051821097914003\n",
            "84     \t [0.42375063 0.18059118 0.13005468]. \t  0.5409662204364257 \t 3.8051821097914003\n",
            "85     \t [0.82159906 0.2629594  0.99301144]. \t  0.8829369483160052 \t 3.8051821097914003\n",
            "86     \t [0.87004636 0.48103157 0.64604538]. \t  1.304640200884708 \t 3.8051821097914003\n",
            "87     \t [0.00611828 0.39700184 0.56633479]. \t  0.7582299138150631 \t 3.8051821097914003\n",
            "88     \t [0.66081675 0.24908963 0.36671335]. \t  0.49741112338686794 \t 3.8051821097914003\n",
            "89     \t [0.37145448 0.12239177 0.85745563]. \t  0.6941282348959765 \t 3.8051821097914003\n",
            "90     \t [0.50921909 0.11168804 0.67675224]. \t  0.44129452824360094 \t 3.8051821097914003\n",
            "91     \t [0.12022965 0.95893321 0.59043519]. \t  2.751577852315629 \t 3.8051821097914003\n",
            "92     \t [0.61692084 0.26266657 0.40584368]. \t  0.41060339066604384 \t 3.8051821097914003\n",
            "93     \t [0.38132105 0.65477671 0.33897365]. \t  0.41884361706401735 \t 3.8051821097914003\n",
            "94     \t [0.90274566 0.39309968 0.42848717]. \t  0.1538058248859996 \t 3.8051821097914003\n",
            "95     \t [0.3874207  0.91625586 0.45246377]. \t  1.7139535209488614 \t 3.8051821097914003\n",
            "96     \t [0.9037247  0.67953797 0.64761112]. \t  1.108383505018141 \t 3.8051821097914003\n",
            "97     \t [0.2129587  0.5704972  0.34719778]. \t  0.42129348276748696 \t 3.8051821097914003\n",
            "98     \t [0.5736689  0.22695589 0.48933553]. \t  0.27844968171053286 \t 3.8051821097914003\n",
            "99     \t [0.27718184 0.48726663 0.59624794]. \t  1.2942319378416134 \t 3.8051821097914003\n",
            "100    \t [0.77226186 0.20972114 0.79326813]. \t  1.2850883206415245 \t 3.8051821097914003\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GH76hlE2tddm",
        "outputId": "19a79db1-0ee6-4e34-e711-981137f62d47"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 5\r\n",
        "\r\n",
        "np.random.seed(run_num_5)\r\n",
        "surrogate_loser_5 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "loser_5 = dGPGO(surrogate_loser_5, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_5.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.55735327 0.54176063 0.56042378]. \t  0.9599610140567721 \t 0.9810564697651996\n",
            "init   \t [0.38204809 0.11909418 0.84599912]. \t  0.6913364116577386 \t 0.9810564697651996\n",
            "init   \t [0.73974161 0.93235756 0.85932801]. \t  0.9810564697651996 \t 0.9810564697651996\n",
            "init   \t [0.70810463 0.68096875 0.4086023 ]. \t  0.4017443293787304 \t 0.9810564697651996\n",
            "init   \t [0.77458462 0.9844762  0.96548486]. \t  0.4769494155906823 \t 0.9810564697651996\n",
            "1      \t [0.0655006  0.90661895 0.81035589]. \t  \u001b[92m1.403888408720423\u001b[0m \t 1.403888408720423\n",
            "2      \t [0.41215072 0.7779011  0.74763494]. \t  \u001b[92m2.1757126346478852\u001b[0m \t 2.1757126346478852\n",
            "3      \t [0.34717117 0.77095612 0.78484957]. \t  \u001b[92m2.3966577054831983\u001b[0m \t 2.3966577054831983\n",
            "4      \t [0.30370554 0.5420168  0.98229967]. \t  \u001b[92m2.396700843210034\u001b[0m \t 2.396700843210034\n",
            "5      \t [0.03830972 0.41319886 0.87948681]. \t  \u001b[92m3.0868801903159286\u001b[0m \t 3.0868801903159286\n",
            "6      \t [0.0550351  0.47493583 0.91186515]. \t  \u001b[92m3.247821453024615\u001b[0m \t 3.247821453024615\n",
            "7      \t [0.13007588 0.76889517 0.96291702]. \t  1.8354815792825336 \t 3.247821453024615\n",
            "8      \t [0.06020638 0.52263597 0.83656287]. \t  \u001b[92m3.772434542680999\u001b[0m \t 3.772434542680999\n",
            "9      \t [0.05709412 0.58527165 0.69772483]. \t  2.531288763638517 \t 3.772434542680999\n",
            "10     \t [0.91025839 0.97028742 0.12792309]. \t  0.0018457322577184834 \t 3.772434542680999\n",
            "11     \t [0.99573914 0.49153797 0.75591514]. \t  2.7894953486682232 \t 3.772434542680999\n",
            "12     \t [0.10059082 0.55459117 0.835178  ]. \t  \u001b[92m3.8123132323574875\u001b[0m \t 3.8123132323574875\n",
            "13     \t [0.96087615 0.67807239 0.2789962 ]. \t  0.03903754399449888 \t 3.8123132323574875\n",
            "14     \t [0.85683623 0.07037409 0.99016484]. \t  0.20334698494229897 \t 3.8123132323574875\n",
            "15     \t [0.97447876 0.84968796 0.99975702]. \t  0.9204545784886076 \t 3.8123132323574875\n",
            "16     \t [0.61918291 0.63706631 0.36531518]. \t  0.31507380690837034 \t 3.8123132323574875\n",
            "17     \t [0.60892365 0.82768428 0.00560017]. \t  0.0008542513403068642 \t 3.8123132323574875\n",
            "18     \t [0.88794259 0.11985922 0.05976312]. \t  0.12240316278901528 \t 3.8123132323574875\n",
            "19     \t [0.18565789 0.41975985 0.79881607]. \t  3.104257557926961 \t 3.8123132323574875\n",
            "20     \t [0.54787183 0.252763   0.54415079]. \t  0.33307288848698713 \t 3.8123132323574875\n",
            "21     \t [0.20787217 0.61004851 0.74845814]. \t  3.0360388299352037 \t 3.8123132323574875\n",
            "22     \t [0.98303924 0.28190876 0.89179177]. \t  1.7480026944637124 \t 3.8123132323574875\n",
            "23     \t [0.61780318 0.02581508 0.05799669]. \t  0.20530916988886677 \t 3.8123132323574875\n",
            "24     \t [0.65433974 0.02262776 0.44688952]. \t  0.28211827697204606 \t 3.8123132323574875\n",
            "25     \t [0.02006712 0.54361265 0.85674407]. \t  3.8106114172417715 \t 3.8123132323574875\n",
            "26     \t [0.04408369 0.57798277 0.84008442]. \t  3.792111144405652 \t 3.8123132323574875\n",
            "27     \t [0.14501681 0.47075112 0.91461001]. \t  3.2110042352024024 \t 3.8123132323574875\n",
            "28     \t [0.03558969 0.66343753 0.93451965]. \t  2.8921565718998465 \t 3.8123132323574875\n",
            "29     \t [0.01404798 0.87419782 0.65725577]. \t  2.4792575908001058 \t 3.8123132323574875\n",
            "30     \t [0.18633038 0.99262283 0.31785779]. \t  0.4753520265355485 \t 3.8123132323574875\n",
            "31     \t [0.89852877 0.01203974 0.63453993]. \t  0.14481656909288962 \t 3.8123132323574875\n",
            "32     \t [0.97150247 0.69294377 0.22199652]. \t  0.019920135017441068 \t 3.8123132323574875\n",
            "33     \t [0.46085152 0.56645311 0.57553687]. \t  1.2863944466686195 \t 3.8123132323574875\n",
            "34     \t [0.82964158 0.57980175 0.14424181]. \t  0.041147105173632396 \t 3.8123132323574875\n",
            "35     \t [0.27448468 0.17200083 0.38396597]. \t  0.6423027740941983 \t 3.8123132323574875\n",
            "36     \t [0.26566642 0.00681334 0.42585266]. \t  0.40953511308472856 \t 3.8123132323574875\n",
            "37     \t [0.06013269 0.35026807 0.49586356]. \t  0.3954287956437817 \t 3.8123132323574875\n",
            "38     \t [0.65855735 0.7339546  0.17842301]. \t  0.027927969983747716 \t 3.8123132323574875\n",
            "39     \t [0.75833171 0.253738   0.0292391 ]. \t  0.09612694912065668 \t 3.8123132323574875\n",
            "40     \t [0.56494099 0.01288046 0.30065057]. \t  0.7736377060560666 \t 3.8123132323574875\n",
            "41     \t [0.44126691 0.35921091 0.76431648]. \t  2.3882885446221644 \t 3.8123132323574875\n",
            "42     \t [0.09772268 0.39730314 0.0571514 ]. \t  0.09736614631874568 \t 3.8123132323574875\n",
            "43     \t [0.04390447 0.99697316 0.73397787]. \t  1.2650016562313509 \t 3.8123132323574875\n",
            "44     \t [0.99299479 0.54594283 0.06288426]. \t  0.014164106367212341 \t 3.8123132323574875\n",
            "45     \t [0.83645947 0.11483328 0.79232853]. \t  0.671516469465977 \t 3.8123132323574875\n",
            "46     \t [0.25559256 0.4677096  0.20778315]. \t  0.26763763133381235 \t 3.8123132323574875\n",
            "47     \t [0.52134778 0.64412547 0.66476584]. \t  1.9387526401965522 \t 3.8123132323574875\n",
            "48     \t [0.97960237 0.58509312 0.39486473]. \t  0.0980438150330066 \t 3.8123132323574875\n",
            "49     \t [0.93425858 0.61751435 0.93636713]. \t  3.0007595366600857 \t 3.8123132323574875\n",
            "50     \t [0.93665032 0.83303117 0.3717566 ]. \t  0.14220193723144822 \t 3.8123132323574875\n",
            "51     \t [0.01043746 0.06645344 0.0582733 ]. \t  0.17874671309400159 \t 3.8123132323574875\n",
            "52     \t [0.42146344 0.43093162 0.21313343]. \t  0.34858766310202843 \t 3.8123132323574875\n",
            "53     \t [0.85740045 0.72358599 0.10595874]. \t  0.0067127424362161785 \t 3.8123132323574875\n",
            "54     \t [0.74536496 0.74030997 0.81142443]. \t  2.5236996121014146 \t 3.8123132323574875\n",
            "55     \t [0.48596649 0.05583247 0.71267398]. \t  0.34836328475177203 \t 3.8123132323574875\n",
            "56     \t [0.27943898 0.74737658 0.31328012]. \t  0.42639692017161174 \t 3.8123132323574875\n",
            "57     \t [0.7121061  0.70213367 0.23237757]. \t  0.05544100754439492 \t 3.8123132323574875\n",
            "58     \t [0.89629735 0.50531801 0.27715699]. \t  0.10830442188564005 \t 3.8123132323574875\n",
            "59     \t [0.81710177 0.67511305 0.7989285 ]. \t  2.909475046834837 \t 3.8123132323574875\n",
            "60     \t [0.27827065 0.70878759 0.22002966]. \t  0.10050040713970884 \t 3.8123132323574875\n",
            "61     \t [0.12217544 0.78496978 0.79026058]. \t  2.37095832378724 \t 3.8123132323574875\n",
            "62     \t [0.65576377 0.4536732  0.46632376]. \t  0.32528800321453955 \t 3.8123132323574875\n",
            "63     \t [0.78302288 0.13617894 0.70367363]. \t  0.5995147367720821 \t 3.8123132323574875\n",
            "64     \t [0.27565291 0.35679019 0.68211396]. \t  1.5721358806728192 \t 3.8123132323574875\n",
            "65     \t [0.50685866 0.05015652 0.57968208]. \t  0.1581913686140206 \t 3.8123132323574875\n",
            "66     \t [0.21613274 0.78748947 0.61979404]. \t  2.752520587780271 \t 3.8123132323574875\n",
            "67     \t [0.97683169 0.98537602 0.47034241]. \t  0.22879582575819743 \t 3.8123132323574875\n",
            "68     \t [0.8247913  0.00865746 0.77607339]. \t  0.2671791607087722 \t 3.8123132323574875\n",
            "69     \t [0.6154278  0.16102298 0.72380069]. \t  0.7868765266505973 \t 3.8123132323574875\n",
            "70     \t [0.44639332 0.52624246 0.94752527]. \t  2.9776268007255116 \t 3.8123132323574875\n",
            "71     \t [0.35265186 0.07195568 0.39129923]. \t  0.622943735926077 \t 3.8123132323574875\n",
            "72     \t [0.01915566 0.7514324  0.80350619]. \t  2.641516303394991 \t 3.8123132323574875\n",
            "73     \t [0.31166647 0.75193678 0.43966625]. \t  1.5656143444144268 \t 3.8123132323574875\n",
            "74     \t [0.057032   0.85634764 0.70514785]. \t  2.17220634986867 \t 3.8123132323574875\n",
            "75     \t [0.17449207 0.38170487 0.29543257]. \t  0.4687919094460895 \t 3.8123132323574875\n",
            "76     \t [0.18262366 0.40234101 0.78174162]. \t  2.853310454318601 \t 3.8123132323574875\n",
            "77     \t [0.23326724 0.12923415 0.21844802]. \t  0.8800126024126476 \t 3.8123132323574875\n",
            "78     \t [0.76291281 0.1939433  0.15790554]. \t  0.4132222812659086 \t 3.8123132323574875\n",
            "79     \t [0.09675512 0.75527026 0.79408307]. \t  2.6065132144451044 \t 3.8123132323574875\n",
            "80     \t [0.32685191 0.0862388  0.628197  ]. \t  0.2658594374175428 \t 3.8123132323574875\n",
            "81     \t [0.26988867 0.88735757 0.70824977]. \t  1.9302015782523108 \t 3.8123132323574875\n",
            "82     \t [0.84299487 0.1543878  0.18070156]. \t  0.4012743622864063 \t 3.8123132323574875\n",
            "83     \t [0.41130472 0.07171761 0.33969365]. \t  0.8345360731124587 \t 3.8123132323574875\n",
            "84     \t [7.35487395e-04 7.71143033e-01 7.78163031e-01]. \t  2.4185376828203515 \t 3.8123132323574875\n",
            "85     \t [0.27421342 0.3677449  0.62104297]. \t  1.042391069581968 \t 3.8123132323574875\n",
            "86     \t [0.02541217 0.25237601 0.45267658]. \t  0.29580436684080935 \t 3.8123132323574875\n",
            "87     \t [0.44317316 0.12260709 0.52398733]. \t  0.22575510537096177 \t 3.8123132323574875\n",
            "88     \t [0.57293859 0.99880182 0.4018766 ]. \t  0.6680010837087829 \t 3.8123132323574875\n",
            "89     \t [0.8022128  0.10624647 0.516911  ]. \t  0.15447209300219936 \t 3.8123132323574875\n",
            "90     \t [0.29358989 0.02299719 0.73290056]. \t  0.2828304241174419 \t 3.8123132323574875\n",
            "91     \t [0.54505805 0.52661682 0.47490715]. \t  0.5606793615622767 \t 3.8123132323574875\n",
            "92     \t [0.77323979 0.16021315 0.81059028]. \t  0.9486529552671926 \t 3.8123132323574875\n",
            "93     \t [0.45302664 0.21908675 0.15855595]. \t  0.618922530291222 \t 3.8123132323574875\n",
            "94     \t [0.82084058 0.40124898 0.96684758]. \t  2.0325833187159366 \t 3.8123132323574875\n",
            "95     \t [0.09455294 0.52870413 0.13348204]. \t  0.0900712093873404 \t 3.8123132323574875\n",
            "96     \t [0.44929156 0.89821783 0.27772   ]. \t  0.21315354397159983 \t 3.8123132323574875\n",
            "97     \t [0.28590341 0.55800049 0.51488452]. \t  1.1719963153531516 \t 3.8123132323574875\n",
            "98     \t [0.11525554 0.82021185 0.11016824]. \t  0.010565660616481393 \t 3.8123132323574875\n",
            "99     \t [0.13037669 0.19765145 0.80752148]. \t  1.2195795915290009 \t 3.8123132323574875\n",
            "100    \t [0.35073261 0.92595667 0.04860803]. \t  0.0014692202765782152 \t 3.8123132323574875\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fh_Amb8TtdgO",
        "outputId": "4c754e92-3853-41ab-93c3-4bc9aace2cf6"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 6\r\n",
        "\r\n",
        "np.random.seed(run_num_6)\r\n",
        "surrogate_loser_6 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "loser_6 = dGPGO(surrogate_loser_6, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_6.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.89286015 0.33197981 0.82122912]. \t  2.3879127825536575 \t 2.5106636917702634\n",
            "init   \t [0.04169663 0.10765668 0.59505206]. \t  0.2321934630074273 \t 2.5106636917702634\n",
            "init   \t [0.52981736 0.41880743 0.33540785]. \t  0.37989557867087576 \t 2.5106636917702634\n",
            "init   \t [0.62251943 0.43814143 0.73588211]. \t  2.5106636917702634 \t 2.5106636917702634\n",
            "init   \t [0.51803641 0.5788586  0.6453551 ]. \t  1.7289203948034264 \t 2.5106636917702634\n",
            "1      \t [0.62339102 0.39276721 0.97743955]. \t  1.8557550530508597 \t 2.5106636917702634\n",
            "2      \t [0.81081535 0.27720993 0.64760969]. \t  0.8549986775341755 \t 2.5106636917702634\n",
            "3      \t [0.97311854 0.60304551 0.92068359]. \t  \u001b[92m3.2276797271364055\u001b[0m \t 3.2276797271364055\n",
            "4      \t [0.89264006 0.58864963 0.95652948]. \t  2.776092518207796 \t 3.2276797271364055\n",
            "5      \t [0.96507948 0.60085005 0.82529255]. \t  \u001b[92m3.479322232477214\u001b[0m \t 3.479322232477214\n",
            "6      \t [0.95842992 0.48714277 0.81520669]. \t  3.4274222523101296 \t 3.479322232477214\n",
            "7      \t [0.95032843 0.60164973 0.75457865]. \t  2.678691870274754 \t 3.479322232477214\n",
            "8      \t [0. 0. 0.]. \t  0.06797411659013229 \t 3.479322232477214\n",
            "9      \t [0.01270824 0.5371693  0.06605772]. \t  0.0354315091022392 \t 3.479322232477214\n",
            "10     \t [0.20140086 0.25986668 0.00643016]. \t  0.09731674626066539 \t 3.479322232477214\n",
            "11     \t [0.9954704  0.56038921 0.8728146 ]. \t  \u001b[92m3.6483342701470103\u001b[0m \t 3.6483342701470103\n",
            "12     \t [0.08776956 0.94212277 0.01891421]. \t  0.0006566083725651407 \t 3.6483342701470103\n",
            "13     \t [0.4346958  0.60889655 0.8201429 ]. \t  3.631042399892129 \t 3.6483342701470103\n",
            "14     \t [0.01161386 0.66515041 0.99551219]. \t  1.9498416196162505 \t 3.6483342701470103\n",
            "15     \t [0.33254032 0.89414231 0.98713051]. \t  0.8152211979376042 \t 3.6483342701470103\n",
            "16     \t [0.22420658 0.65264203 0.9182928 ]. \t  3.1764750208328505 \t 3.6483342701470103\n",
            "17     \t [0.38995695 0.50459032 0.97495208]. \t  2.4537818811544887 \t 3.6483342701470103\n",
            "18     \t [0.89731428 0.0644819  0.04485822]. \t  0.09540218126323545 \t 3.6483342701470103\n",
            "19     \t [0.5430287  0.62248852 0.87089541]. \t  3.6481824403125525 \t 3.6483342701470103\n",
            "20     \t [0.02245446 0.71257964 0.70832644]. \t  2.542735102304769 \t 3.6483342701470103\n",
            "21     \t [0.84916036 0.43499921 0.05964426]. \t  0.050000621163526424 \t 3.6483342701470103\n",
            "22     \t [0.72328122 0.78476196 0.91791194]. \t  2.0917540572107587 \t 3.6483342701470103\n",
            "23     \t [0.50671568 0.59326383 0.87247359]. \t  \u001b[92m3.7574239741369033\u001b[0m \t 3.7574239741369033\n",
            "24     \t [0.80771617 0.77253684 0.56420031]. \t  0.8074089229060448 \t 3.7574239741369033\n",
            "25     \t [0.82280098 0.98264258 0.03503644]. \t  0.00023447364217244306 \t 3.7574239741369033\n",
            "26     \t [0.36120819 0.01883246 0.12314037]. \t  0.48675579172091765 \t 3.7574239741369033\n",
            "27     \t [0.61004954 0.97795743 0.5380338 ]. \t  1.2796657235618687 \t 3.7574239741369033\n",
            "28     \t [0.50645476 0.26659896 0.96461242]. \t  1.1510964734131832 \t 3.7574239741369033\n",
            "29     \t [0.00523641 0.84890647 0.76701844]. \t  1.8904386191124445 \t 3.7574239741369033\n",
            "30     \t [0.07455069 0.71285656 0.57962799]. \t  2.5839320858154777 \t 3.7574239741369033\n",
            "31     \t [0.83401196 0.42203083 0.26259397]. \t  0.2125427448756169 \t 3.7574239741369033\n",
            "32     \t [0.00550112 0.79531694 0.3026345 ]. \t  0.4132763468583321 \t 3.7574239741369033\n",
            "33     \t [0.33510451 0.26649245 0.92074688]. \t  1.4902139984299103 \t 3.7574239741369033\n",
            "34     \t [0.69362147 0.34044285 0.45542876]. \t  0.2582487569586175 \t 3.7574239741369033\n",
            "35     \t [0.27408507 0.37107772 0.1495041 ]. \t  0.33823696322386365 \t 3.7574239741369033\n",
            "36     \t [0.14167772 0.6583676  0.23549375]. \t  0.13307732649588877 \t 3.7574239741369033\n",
            "37     \t [0.36784136 0.65945815 0.86312928]. \t  3.4849121667963368 \t 3.7574239741369033\n",
            "38     \t [0.50130668 0.46810664 0.92565854]. \t  3.071990928694588 \t 3.7574239741369033\n",
            "39     \t [0.11506585 0.08818824 0.01786167]. \t  0.1264166391740304 \t 3.7574239741369033\n",
            "40     \t [0.61125555 0.20673819 0.86929837]. \t  1.2342396193127003 \t 3.7574239741369033\n",
            "41     \t [0.11552913 0.937178   0.91319381]. \t  0.9229498004302739 \t 3.7574239741369033\n",
            "42     \t [0.73894287 0.33413814 0.6103939 ]. \t  0.7455446737025775 \t 3.7574239741369033\n",
            "43     \t [0.58721719 0.38510632 0.8374942 ]. \t  2.96017619594543 \t 3.7574239741369033\n",
            "44     \t [0.2924251  0.24847362 0.29458142]. \t  0.8162192034311126 \t 3.7574239741369033\n",
            "45     \t [0.9399308  0.69448975 0.89984114]. \t  2.934326827015603 \t 3.7574239741369033\n",
            "46     \t [0.49109484 0.33504597 0.0306225 ]. \t  0.11075140000922437 \t 3.7574239741369033\n",
            "47     \t [0.95946823 0.76273733 0.74217599]. \t  1.6013158015827202 \t 3.7574239741369033\n",
            "48     \t [0.00768901 0.57624884 0.45984971]. \t  1.006739146423282 \t 3.7574239741369033\n",
            "49     \t [0.7588967  0.6338851  0.46007792]. \t  0.4302286953977035 \t 3.7574239741369033\n",
            "50     \t [0.43681923 0.26771217 0.525546  ]. \t  0.3357713031640264 \t 3.7574239741369033\n",
            "51     \t [0.84650032 0.32968555 0.67756352]. \t  1.3030194973795366 \t 3.7574239741369033\n",
            "52     \t [0.29116669 0.1959375  0.57236862]. \t  0.33835299363392896 \t 3.7574239741369033\n",
            "53     \t [0.49086338 0.2402136  0.66310662]. \t  0.8580888362708491 \t 3.7574239741369033\n",
            "54     \t [0.67174643 0.13687066 0.29850672]. \t  0.7358259286026887 \t 3.7574239741369033\n",
            "55     \t [0.90824711 0.51686926 0.94068471]. \t  2.9773967891772 \t 3.7574239741369033\n",
            "56     \t [0.96313273 0.38871968 0.58126843]. \t  0.56783893931115 \t 3.7574239741369033\n",
            "57     \t [0.47678022 0.58904368 0.60675061]. \t  1.527985140761224 \t 3.7574239741369033\n",
            "58     \t [0.61939634 0.49537322 0.21904615]. \t  0.19585169709126193 \t 3.7574239741369033\n",
            "59     \t [0.89125536 0.52848028 0.37504456]. \t  0.12161102080979641 \t 3.7574239741369033\n",
            "60     \t [0.2492909  0.99861842 0.46834362]. \t  1.9369879716347638 \t 3.7574239741369033\n",
            "61     \t [0.89311458 0.0260461  0.41271687]. \t  0.21862146592906784 \t 3.7574239741369033\n",
            "62     \t [0.75365534 0.64361142 0.56627093]. \t  0.8495271886852758 \t 3.7574239741369033\n",
            "63     \t [0.10353124 0.02964342 0.15018766]. \t  0.4970848309262672 \t 3.7574239741369033\n",
            "64     \t [0.38652703 0.09628612 0.01688402]. \t  0.15160735034832432 \t 3.7574239741369033\n",
            "65     \t [0.90753459 0.45836172 0.5257814 ]. \t  0.3378386372294143 \t 3.7574239741369033\n",
            "66     \t [0.97786691 0.23969996 0.30313812]. \t  0.2737430610896056 \t 3.7574239741369033\n",
            "67     \t [0.2985376  0.45515157 0.72666153]. \t  2.5519283059711437 \t 3.7574239741369033\n",
            "68     \t [0.71962825 0.37640665 0.92594521]. \t  2.3788782644785234 \t 3.7574239741369033\n",
            "69     \t [0.91095124 0.26026786 0.08100662]. \t  0.11910272542970807 \t 3.7574239741369033\n",
            "70     \t [0.2007796  0.40410126 0.04414278]. \t  0.09057280874521183 \t 3.7574239741369033\n",
            "71     \t [0.98066823 0.17618395 0.30887276]. \t  0.29940407941322894 \t 3.7574239741369033\n",
            "72     \t [0.26486167 0.03322341 0.3011884 ]. \t  0.8724455649378886 \t 3.7574239741369033\n",
            "73     \t [0.19341254 0.9115757  0.8193853 ]. \t  1.3477910671526903 \t 3.7574239741369033\n",
            "74     \t [0.85383639 0.303693   0.58484458]. \t  0.4983868149187136 \t 3.7574239741369033\n",
            "75     \t [0.11261631 0.64295778 0.35291107]. \t  0.5654453165628954 \t 3.7574239741369033\n",
            "76     \t [0.25255531 0.58296163 0.5355288 ]. \t  1.4636644975046686 \t 3.7574239741369033\n",
            "77     \t [0.69044902 0.1928956  0.86895197]. \t  1.1234012728940739 \t 3.7574239741369033\n",
            "78     \t [0.06808589 0.44438348 0.31510407]. \t  0.33025696989363273 \t 3.7574239741369033\n",
            "79     \t [0.1535016  0.47575467 0.38377442]. \t  0.4277339308242742 \t 3.7574239741369033\n",
            "80     \t [0.88950651 0.15575891 0.5753877 ]. \t  0.23678865995367532 \t 3.7574239741369033\n",
            "81     \t [0.60563334 0.94996264 0.53201211]. \t  1.357208075848746 \t 3.7574239741369033\n",
            "82     \t [0.03640684 0.90685607 0.13933935]. \t  0.017359262999842685 \t 3.7574239741369033\n",
            "83     \t [0.83622549 0.00479024 0.15238126]. \t  0.30811922728605484 \t 3.7574239741369033\n",
            "84     \t [0.03047936 0.45130896 0.09831493]. \t  0.0994418319705768 \t 3.7574239741369033\n",
            "85     \t [0.56096791 0.42054039 0.53609475]. \t  0.5375442750723753 \t 3.7574239741369033\n",
            "86     \t [0.69089327 0.41434753 0.52576339]. \t  0.41222705943601856 \t 3.7574239741369033\n",
            "87     \t [0.33303682 0.85963986 0.88246611]. \t  1.6270249322213164 \t 3.7574239741369033\n",
            "88     \t [0.76694449 0.44906852 0.62817908]. \t  1.1229951985849844 \t 3.7574239741369033\n",
            "89     \t [0.38506973 0.2254537  0.89158306]. \t  1.3124017684339946 \t 3.7574239741369033\n",
            "90     \t [0.94760925 0.81277586 0.24814905]. \t  0.02384324621445827 \t 3.7574239741369033\n",
            "91     \t [0.22321952 0.85183336 0.71042249]. \t  2.116278041457136 \t 3.7574239741369033\n",
            "92     \t [0.63232264 0.79212972 0.83959709]. \t  2.2158432287888115 \t 3.7574239741369033\n",
            "93     \t [0.89311012 0.27852264 0.10452753]. \t  0.15260228430094222 \t 3.7574239741369033\n",
            "94     \t [0.65225194 0.56959118 0.85237454]. \t  \u001b[92m3.7923072368609834\u001b[0m \t 3.7923072368609834\n",
            "95     \t [0.45988746 0.15648535 0.31912992]. \t  0.889225446032996 \t 3.7923072368609834\n",
            "96     \t [0.91897039 0.23558463 0.23921164]. \t  0.34277234372304527 \t 3.7923072368609834\n",
            "97     \t [0.84404778 0.5475815  0.87225978]. \t  3.714655484967224 \t 3.7923072368609834\n",
            "98     \t [0.22581407 0.36439774 0.63953126]. \t  1.194698309876963 \t 3.7923072368609834\n",
            "99     \t [0.36129501 0.29655172 0.46130896]. \t  0.3620060373190372 \t 3.7923072368609834\n",
            "100    \t [0.21910047 0.55932934 0.67552251]. \t  2.2680994506603955 \t 3.7923072368609834\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XNmoDTuUtdi3",
        "outputId": "4a078da0-9605-489c-cd0f-8ce6c5efcbb4"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 7\r\n",
        "\r\n",
        "np.random.seed(run_num_7)\r\n",
        "surrogate_loser_7 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "loser_7 = dGPGO(surrogate_loser_7, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_7.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.15266373 0.30235661 0.06203641]. \t  0.1742288077468587 \t 1.6237282255098657\n",
            "init   \t [0.45986034 0.83525338 0.92699705]. \t  1.6237282255098657 \t 1.6237282255098657\n",
            "init   \t [0.72698898 0.76849622 0.26920507]. \t  0.08405715787489784 \t 1.6237282255098657\n",
            "init   \t [0.64402929 0.09337326 0.07968589]. \t  0.2756481743251193 \t 1.6237282255098657\n",
            "init   \t [0.58961375 0.34334054 0.98887615]. \t  1.4020548914621052 \t 1.6237282255098657\n",
            "1      \t [0.32658881 0.90655956 0.99955954]. \t  0.6793796294845356 \t 1.6237282255098657\n",
            "2      \t [0.97122775 0.93199907 0.87171651]. \t  0.9308657054768225 \t 1.6237282255098657\n",
            "3      \t [0.57909004 0.58214012 0.79783263]. \t  \u001b[92m3.478089696581643\u001b[0m \t 3.478089696581643\n",
            "4      \t [0.49507021 0.39113672 0.52467305]. \t  0.478096743295257 \t 3.478089696581643\n",
            "5      \t [0.96188519 0.49079304 0.94964007]. \t  2.739322132683216 \t 3.478089696581643\n",
            "6      \t [0.90946938 0.60810834 0.78435192]. \t  3.0774531590511494 \t 3.478089696581643\n",
            "7      \t [0.06261914 0.66505939 0.93875608]. \t  2.8328149285258104 \t 3.478089696581643\n",
            "8      \t [0.63975981 0.37291359 0.00880596]. \t  0.05617173979970511 \t 3.478089696581643\n",
            "9      \t [0.76047284 0.60730235 0.87877328]. \t  \u001b[92m3.623712367146717\u001b[0m \t 3.623712367146717\n",
            "10     \t [0.6876877  0.55744025 0.93478593]. \t  3.1829751880829784 \t 3.623712367146717\n",
            "11     \t [0.08676052 0.79807152 0.87850186]. \t  2.220829089713525 \t 3.623712367146717\n",
            "12     \t [0.28916277 0.04583535 0.00261902]. \t  0.11401832354434507 \t 3.623712367146717\n",
            "13     \t [0.71667679 0.57664501 0.81395165]. \t  3.5779368756411625 \t 3.623712367146717\n",
            "14     \t [0.11508561 0.71452061 0.02534668]. \t  0.004527603857513539 \t 3.623712367146717\n",
            "15     \t [0.7138761  0.68377075 0.77379694]. \t  2.655945564466352 \t 3.623712367146717\n",
            "16     \t [0.62595126 0.94264969 0.00840659]. \t  0.00028614683822522104 \t 3.623712367146717\n",
            "17     \t [0.99588291 0.81961183 0.15742965]. \t  0.003958213795357282 \t 3.623712367146717\n",
            "18     \t [2.82391757e-15 2.82391757e-15 2.82391757e-15]. \t  0.06797411659013629 \t 3.623712367146717\n",
            "19     \t [0.87761517 0.03556698 0.76435623]. \t  0.3351948214917791 \t 3.623712367146717\n",
            "20     \t [0.97857549 0.00446178 0.16841397]. \t  0.2154343354374641 \t 3.623712367146717\n",
            "21     \t [0.81221252 0.525542   0.85463545]. \t  \u001b[92m3.7301762932112674\u001b[0m \t 3.7301762932112674\n",
            "22     \t [0.900185   0.75296151 0.1168608 ]. \t  0.005072749512409333 \t 3.7301762932112674\n",
            "23     \t [0.81353052 0.51280105 0.82744971]. \t  3.6314788999948577 \t 3.7301762932112674\n",
            "24     \t [0.96913426 0.69361638 0.55129864]. \t  0.45446883683488787 \t 3.7301762932112674\n",
            "25     \t [0.45611335 0.03406024 0.61887374]. \t  0.1704663937816772 \t 3.7301762932112674\n",
            "26     \t [0.59223331 0.04267828 0.1338271 ]. \t  0.47744590290110217 \t 3.7301762932112674\n",
            "27     \t [0.83943545 0.77914392 0.08044774]. \t  0.0029026804888624066 \t 3.7301762932112674\n",
            "28     \t [0.20846012 0.53348377 0.51992205]. \t  1.1202841961222842 \t 3.7301762932112674\n",
            "29     \t [0.67681719 0.52661603 0.85870288]. \t  \u001b[92m3.7745252416454917\u001b[0m \t 3.7745252416454917\n",
            "30     \t [0.12509163 0.87757482 0.5510732 ]. \t  3.0681829219005508 \t 3.7745252416454917\n",
            "31     \t [0.14369607 0.95138149 0.31002503]. \t  0.4675814613706676 \t 3.7745252416454917\n",
            "32     \t [0.03528407 0.40941311 0.95717746]. \t  2.236590588082838 \t 3.7745252416454917\n",
            "33     \t [0.44280206 0.63711349 0.86091196]. \t  3.611710810406915 \t 3.7745252416454917\n",
            "34     \t [0.26180585 0.73568205 0.73537597]. \t  2.5128286579035257 \t 3.7745252416454917\n",
            "35     \t [0.00487776 0.79900419 0.58464229]. \t  2.8879111571102887 \t 3.7745252416454917\n",
            "36     \t [0.62391776 0.3903563  0.68428808]. \t  1.6717627713433323 \t 3.7745252416454917\n",
            "37     \t [0.22203301 0.58076122 0.99938986]. \t  2.090782773049574 \t 3.7745252416454917\n",
            "38     \t [0.0497901  0.70348162 0.40504456]. \t  1.1607301643572914 \t 3.7745252416454917\n",
            "39     \t [0.08902865 0.97575397 0.57315788]. \t  2.714889081910986 \t 3.7745252416454917\n",
            "40     \t [0.71455558 0.69017883 0.69063379]. \t  1.73744604695138 \t 3.7745252416454917\n",
            "41     \t [0.0498949  0.78122685 0.55605748]. \t  2.879384773660573 \t 3.7745252416454917\n",
            "42     \t [0.45781108 0.26174588 0.94199186]. \t  1.2974802200354303 \t 3.7745252416454917\n",
            "43     \t [0.80913553 0.35598418 0.04574654]. \t  0.07245009830167098 \t 3.7745252416454917\n",
            "44     \t [0.39885319 0.30427766 0.15303229]. \t  0.4753994581941886 \t 3.7745252416454917\n",
            "45     \t [0.83497665 0.25408047 0.74194063]. \t  1.4079264679802885 \t 3.7745252416454917\n",
            "46     \t [0.05275221 0.73612313 0.71259125]. \t  2.516696422081942 \t 3.7745252416454917\n",
            "47     \t [0.03962876 0.04670778 0.97027546]. \t  0.19387205368595167 \t 3.7745252416454917\n",
            "48     \t [0.86384956 0.12288378 0.74114512]. \t  0.6383704123542232 \t 3.7745252416454917\n",
            "49     \t [0.79372708 0.35345592 0.53009712]. \t  0.32798013380319835 \t 3.7745252416454917\n",
            "50     \t [0.97791284 0.38396962 0.41962348]. \t  0.1246402598256812 \t 3.7745252416454917\n",
            "51     \t [0.64270236 0.75777217 0.68904066]. \t  1.6447147713498858 \t 3.7745252416454917\n",
            "52     \t [0.59139013 0.74227825 0.5157312 ]. \t  1.2993767437115655 \t 3.7745252416454917\n",
            "53     \t [0.0323653 0.3292039 0.1426402]. \t  0.2856535148959515 \t 3.7745252416454917\n",
            "54     \t [0.14383193 0.24141099 0.57065719]. \t  0.40695734141747664 \t 3.7745252416454917\n",
            "55     \t [0.89946978 0.08606762 0.85056182]. \t  0.5104618044370799 \t 3.7745252416454917\n",
            "56     \t [0.51976017 0.22889251 0.35232833]. \t  0.6751039301685013 \t 3.7745252416454917\n",
            "57     \t [0.34184268 0.52468794 0.68221237]. \t  2.1899374017129496 \t 3.7745252416454917\n",
            "58     \t [0.8893635  0.6882076  0.46426269]. \t  0.3181762541715076 \t 3.7745252416454917\n",
            "59     \t [0.24686786 0.27282795 0.58364829]. \t  0.5249811572852917 \t 3.7745252416454917\n",
            "60     \t [0.70830618 0.04660946 0.32106337]. \t  0.6183033089960998 \t 3.7745252416454917\n",
            "61     \t [0.65447025 0.27748361 0.42481929]. \t  0.33403303258545897 \t 3.7745252416454917\n",
            "62     \t [0.37683677 0.11525415 0.83630861]. \t  0.6800501825362579 \t 3.7745252416454917\n",
            "63     \t [0.6491765  0.27384865 0.4162992 ]. \t  0.3572171488569962 \t 3.7745252416454917\n",
            "64     \t [0.56966596 0.46622878 0.01058221]. \t  0.036280012296470564 \t 3.7745252416454917\n",
            "65     \t [0.45698753 0.27685337 0.04188856]. \t  0.16480651689762704 \t 3.7745252416454917\n",
            "66     \t [0.78088958 0.25464311 0.24869187]. \t  0.49327180578225444 \t 3.7745252416454917\n",
            "67     \t [0.61253894 0.36137669 0.63838783]. \t  1.0866471780065128 \t 3.7745252416454917\n",
            "68     \t [0.73972948 0.36316492 0.22201953]. \t  0.3420994600975986 \t 3.7745252416454917\n",
            "69     \t [0.598202   0.82681672 0.4747132 ]. \t  1.208649785030718 \t 3.7745252416454917\n",
            "70     \t [0.46091194 0.43140288 0.24909247]. \t  0.377258888183895 \t 3.7745252416454917\n",
            "71     \t [0.30980585 0.49272807 0.36154795]. \t  0.3955171481806316 \t 3.7745252416454917\n",
            "72     \t [0.96726563 0.89399728 0.96826019]. \t  0.8901370600589371 \t 3.7745252416454917\n",
            "73     \t [0.94643135 0.6245116  0.67643341]. \t  1.505106316787355 \t 3.7745252416454917\n",
            "74     \t [0.60589302 0.39114068 0.79202229]. \t  2.8228352062288518 \t 3.7745252416454917\n",
            "75     \t [0.82920723 0.53231563 0.33423074]. \t  0.13146137802733474 \t 3.7745252416454917\n",
            "76     \t [0.40343961 0.54806134 0.28058575]. \t  0.23948447099976833 \t 3.7745252416454917\n",
            "77     \t [0.69097311 0.11912294 0.61050187]. \t  0.27857093327605525 \t 3.7745252416454917\n",
            "78     \t [0.25755065 0.94546291 0.61372001]. \t  2.5138857218938946 \t 3.7745252416454917\n",
            "79     \t [0.09052689 0.51850689 0.63509928]. \t  1.7795558465731527 \t 3.7745252416454917\n",
            "80     \t [0.08479463 0.43788564 0.85789907]. \t  3.3742250385770323 \t 3.7745252416454917\n",
            "81     \t [0.70973134 0.52561147 0.93043662]. \t  3.200528830327633 \t 3.7745252416454917\n",
            "82     \t [0.07887098 0.62444953 0.75322688]. \t  3.047981909596093 \t 3.7745252416454917\n",
            "83     \t [0.43462338 0.85841275 0.40296966]. \t  1.097896948788411 \t 3.7745252416454917\n",
            "84     \t [0.07294121 0.01268299 0.75749395]. \t  0.27074762988187523 \t 3.7745252416454917\n",
            "85     \t [0.44020526 0.37645287 0.00968987]. \t  0.06863582851943462 \t 3.7745252416454917\n",
            "86     \t [0.96569043 0.0641876  0.43360957]. \t  0.15547161313291824 \t 3.7745252416454917\n",
            "87     \t [0.9198246  0.75940701 0.3705377 ]. \t  0.14052092081423212 \t 3.7745252416454917\n",
            "88     \t [0.22721022 0.26302799 0.52202655]. \t  0.3376790780837995 \t 3.7745252416454917\n",
            "89     \t [0.49843932 0.33475206 0.39853827]. \t  0.4193223537377658 \t 3.7745252416454917\n",
            "90     \t [0.88988416 0.29059567 0.72660669]. \t  1.5339975868716995 \t 3.7745252416454917\n",
            "91     \t [0.72225574 0.33315413 0.03442952]. \t  0.08471335964138589 \t 3.7745252416454917\n",
            "92     \t [0.43095451 0.12771229 0.9532665 ]. \t  0.46890119473138564 \t 3.7745252416454917\n",
            "93     \t [0.89322154 0.53536312 0.32746133]. \t  0.10282724257581978 \t 3.7745252416454917\n",
            "94     \t [0.77737971 0.27780371 0.64434659]. \t  0.8377082090623511 \t 3.7745252416454917\n",
            "95     \t [0.35604936 0.29752458 0.55250137]. \t  0.44763709345367153 \t 3.7745252416454917\n",
            "96     \t [0.30570847 0.99814714 0.78796821]. \t  0.8833022204035772 \t 3.7745252416454917\n",
            "97     \t [0.00304443 0.23869801 0.80155107]. \t  1.5294429905123614 \t 3.7745252416454917\n",
            "98     \t [0.23894336 0.95513749 0.52767956]. \t  2.6336435430210194 \t 3.7745252416454917\n",
            "99     \t [0.96334931 0.53329215 0.96711868]. \t  2.5687415955445196 \t 3.7745252416454917\n",
            "100    \t [0.07733048 0.16211484 0.24032112]. \t  0.7439261134149052 \t 3.7745252416454917\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_21yVprtdlu",
        "outputId": "ac1cc379-84a7-440d-b995-db1bb80b5c4f"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 8\r\n",
        "\r\n",
        "np.random.seed(run_num_8)\r\n",
        "surrogate_loser_8 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "loser_8 = dGPGO(surrogate_loser_8, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_8.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.10949987 0.04221501 0.59969258]. \t  0.15732379460832624 \t 0.8830091449513892\n",
            "init   \t [0.29987071 0.79661178 0.36619613]. \t  0.8830091449513892 \t 0.8830091449513892\n",
            "init   \t [0.88060834 0.29784163 0.32910326]. \t  0.2992873424905509 \t 0.8830091449513892\n",
            "init   \t [0.56871692 0.74392742 0.05385289]. \t  0.0051668955191694075 \t 0.8830091449513892\n",
            "init   \t [0.60207437 0.42931858 0.20547361]. \t  0.2908249205255991 \t 0.8830091449513892\n",
            "1      \t [0.19816249 0.92572195 0.98825083]. \t  0.653164991364588 \t 0.8830091449513892\n",
            "2      \t [0.03762084 0.88738442 0.32955494]. \t  0.6464733859302134 \t 0.8830091449513892\n",
            "3      \t [0.59298739 0.68240117 0.69116296]. \t  \u001b[92m1.9678379589841422\u001b[0m \t 1.9678379589841422\n",
            "4      \t [0.99676486 0.73834616 0.85474135]. \t  \u001b[92m2.622597831543382\u001b[0m \t 2.622597831543382\n",
            "5      \t [0.93909608 0.58044829 0.98333517]. \t  2.313423464876202 \t 2.622597831543382\n",
            "6      \t [0.99228465 0.88181182 0.94936817]. \t  1.067160942216205 \t 2.622597831543382\n",
            "7      \t [0.99710209 0.50230383 0.74747112]. \t  \u001b[92m2.685143360380625\u001b[0m \t 2.685143360380625\n",
            "8      \t [0.9896991  0.65501731 0.64538838]. \t  1.0539030471951414 \t 2.685143360380625\n",
            "9      \t [0.7541667  0.03842059 0.03250692]. \t  0.11522480748951529 \t 2.685143360380625\n",
            "10     \t [0.07190656 0.53540888 0.79364133]. \t  \u001b[92m3.532755150026225\u001b[0m \t 3.532755150026225\n",
            "11     \t [0.05636162 0.68631151 0.69982546]. \t  2.571862442260311 \t 3.532755150026225\n",
            "12     \t [0.01897474 0.65584092 0.8751561 ]. \t  3.445941905700823 \t 3.532755150026225\n",
            "13     \t [0.2026131  0.90973368 0.02599246]. \t  0.0009568874318973229 \t 3.532755150026225\n",
            "14     \t [0.02015943 0.17546544 0.86007435]. \t  1.0094731274153572 \t 3.532755150026225\n",
            "15     \t [0.19305313 0.4204086  0.91237006]. \t  2.90434651604119 \t 3.532755150026225\n",
            "16     \t [0.01195417 0.52698999 0.94314404]. \t  3.0097456026265337 \t 3.532755150026225\n",
            "17     \t [0.25596225 0.39703781 0.71712214]. \t  2.163721458320449 \t 3.532755150026225\n",
            "18     \t [0.25710802 0.65710627 0.8981127 ]. \t  3.343555192423963 \t 3.532755150026225\n",
            "19     \t [0.16048547 0.50023439 0.88621337]. \t  \u001b[92m3.6214125818552776\u001b[0m \t 3.6214125818552776\n",
            "20     \t [0.1447954  0.99892044 0.43056541]. \t  1.609250144807879 \t 3.6214125818552776\n",
            "21     \t [0.11841363 0.58660831 0.83267808]. \t  \u001b[92m3.772577454290076\u001b[0m \t 3.772577454290076\n",
            "22     \t [0.97987785 0.13714279 0.00248463]. \t  0.039646571425264975 \t 3.772577454290076\n",
            "23     \t [0.92705413 0.12267879 0.87792272]. \t  0.6413473884501208 \t 3.772577454290076\n",
            "24     \t [0.54587708 0.31433642 0.57322102]. \t  0.5247106459907515 \t 3.772577454290076\n",
            "25     \t [0.65244562 0.21314933 0.18035824]. \t  0.5712228373905991 \t 3.772577454290076\n",
            "26     \t [0.04984912 0.5167762  0.42371492]. \t  0.6001261842172103 \t 3.772577454290076\n",
            "27     \t [0.49843812 0.60544155 0.73993733]. \t  2.788419670954484 \t 3.772577454290076\n",
            "28     \t [0.00250769 0.18540452 0.04799546]. \t  0.15072785648040404 \t 3.772577454290076\n",
            "29     \t [0.14683982 0.04030608 0.72601804]. \t  0.3195768296436863 \t 3.772577454290076\n",
            "30     \t [0.6154916  0.22834525 0.33458037]. \t  0.649724837300166 \t 3.772577454290076\n",
            "31     \t [0.94447738 0.03541163 0.59211034]. \t  0.12339061425489205 \t 3.772577454290076\n",
            "32     \t [0.80518828 0.21820939 0.23047591]. \t  0.4900798351740443 \t 3.772577454290076\n",
            "33     \t [0.49180021 0.67289655 0.09640609]. \t  0.020477956936715228 \t 3.772577454290076\n",
            "34     \t [0.18869189 0.24753615 0.03695648]. \t  0.15575914337109642 \t 3.772577454290076\n",
            "35     \t [0.2574242  0.34725895 0.79140007]. \t  2.4829744875783115 \t 3.772577454290076\n",
            "36     \t [0.56853046 0.35320536 0.40936881]. \t  0.3551657529796507 \t 3.772577454290076\n",
            "37     \t [0.45222453 0.34502697 0.26254166]. \t  0.5921939442113907 \t 3.772577454290076\n",
            "38     \t [0.37352425 0.49742226 0.6706283 ]. \t  1.9661671443461928 \t 3.772577454290076\n",
            "39     \t [0.33441288 0.55681958 0.67261602]. \t  2.1628781305061278 \t 3.772577454290076\n",
            "40     \t [0.8265361  0.4549186  0.12056968]. \t  0.08966259473256484 \t 3.772577454290076\n",
            "41     \t [0.01969536 0.35498542 0.88037469]. \t  2.548617770891581 \t 3.772577454290076\n",
            "42     \t [0.37671893 0.37852214 0.4858566 ]. \t  0.41712957272792933 \t 3.772577454290076\n",
            "43     \t [0.94598866 0.17874027 0.78746887]. \t  1.0359074807983142 \t 3.772577454290076\n",
            "44     \t [0.49561461 0.44173046 0.18714477]. \t  0.27897904173413063 \t 3.772577454290076\n",
            "45     \t [0.65245155 0.41173725 0.90895695]. \t  2.858627770566837 \t 3.772577454290076\n",
            "46     \t [0.32817392 0.43930842 0.76576021]. \t  2.93773136755562 \t 3.772577454290076\n",
            "47     \t [0.47310704 0.73225946 0.77967682]. \t  2.567374504345819 \t 3.772577454290076\n",
            "48     \t [0.9492419  0.6437365  0.83270298]. \t  3.3208203778497323 \t 3.772577454290076\n",
            "49     \t [0.37866648 0.48483831 0.11681013]. \t  0.13267064162900638 \t 3.772577454290076\n",
            "50     \t [0.63776864 0.13811317 0.10232408]. \t  0.3542316780422566 \t 3.772577454290076\n",
            "51     \t [0.0226272  0.46039584 0.39306004]. \t  0.39248366552278097 \t 3.772577454290076\n",
            "52     \t [0.74226553 0.68447382 0.74109946]. \t  2.258201061656978 \t 3.772577454290076\n",
            "53     \t [0.73671863 0.51163238 0.43451977]. \t  0.2618360570450024 \t 3.772577454290076\n",
            "54     \t [0.28537774 0.79787034 0.66787496]. \t  2.4079245833904652 \t 3.772577454290076\n",
            "55     \t [0.52755091 0.87057573 0.11727033]. \t  0.007314436154976636 \t 3.772577454290076\n",
            "56     \t [0.29346589 0.65910737 0.11163608]. \t  0.029896622802024427 \t 3.772577454290076\n",
            "57     \t [0.2895177  0.92186079 0.05601142]. \t  0.0019237374875095135 \t 3.772577454290076\n",
            "58     \t [0.88082954 0.71100467 0.07799923]. \t  0.004986407593590679 \t 3.772577454290076\n",
            "59     \t [0.73226542 0.65192332 0.47391306]. \t  0.5463663067102064 \t 3.772577454290076\n",
            "60     \t [0.02333221 0.35878397 0.34093755]. \t  0.38738388979188215 \t 3.772577454290076\n",
            "61     \t [0.98356167 0.53763874 0.63483676]. \t  1.1181476260591983 \t 3.772577454290076\n",
            "62     \t [0.156046   0.59655081 0.86958024]. \t  3.7669941228717163 \t 3.772577454290076\n",
            "63     \t [0.88684696 0.12634136 0.08895455]. \t  0.17206619872067863 \t 3.772577454290076\n",
            "64     \t [0.18218602 0.45036847 0.09175881]. \t  0.1184082809363772 \t 3.772577454290076\n",
            "65     \t [0.34292031 0.68519737 0.09262511]. \t  0.018785249890164495 \t 3.772577454290076\n",
            "66     \t [0.7317639  0.74224622 0.95671144]. \t  2.077065612238471 \t 3.772577454290076\n",
            "67     \t [0.87137829 0.5695713  0.62329453]. \t  1.0607784657356756 \t 3.772577454290076\n",
            "68     \t [0.40862621 0.14201035 0.85863338]. \t  0.8060903991006787 \t 3.772577454290076\n",
            "69     \t [0.16367498 0.61225246 0.02678576]. \t  0.013724982809582762 \t 3.772577454290076\n",
            "70     \t [0.22148134 0.37110448 0.85048554]. \t  2.8364247037422916 \t 3.772577454290076\n",
            "71     \t [0.13958531 0.2856406  0.7196961 ]. \t  1.5031167436047488 \t 3.772577454290076\n",
            "72     \t [0.07848445 0.86996977 0.15270764]. \t  0.02527200462831359 \t 3.772577454290076\n",
            "73     \t [0.58561207 0.76780412 0.07594845]. \t  0.0055932062345625125 \t 3.772577454290076\n",
            "74     \t [0.43456286 0.62971159 0.41523235]. \t  0.7294054153517803 \t 3.772577454290076\n",
            "75     \t [0.70086648 0.12372512 0.64641762]. \t  0.38204950898303597 \t 3.772577454290076\n",
            "76     \t [0.91643672 0.5876769  0.04810105]. \t  0.010587036778547476 \t 3.772577454290076\n",
            "77     \t [0.51723626 0.13900936 0.15596207]. \t  0.6423475702077569 \t 3.772577454290076\n",
            "78     \t [0.08522595 0.01635146 0.36754278]. \t  0.5270746580713294 \t 3.772577454290076\n",
            "79     \t [0.78306988 0.08787704 0.37528958]. \t  0.4211244425314013 \t 3.772577454290076\n",
            "80     \t [0.19974885 0.85121287 0.97113527]. \t  1.1809105868252145 \t 3.772577454290076\n",
            "81     \t [0.90777519 0.74850197 0.25926154]. \t  0.03547903149967979 \t 3.772577454290076\n",
            "82     \t [0.1885716  0.12356481 0.45988309]. \t  0.3315845871223555 \t 3.772577454290076\n",
            "83     \t [0.63074269 0.83889432 0.31506937]. \t  0.23857930741350988 \t 3.772577454290076\n",
            "84     \t [0.7426569  0.81448563 0.09728448]. \t  0.003764932984621751 \t 3.772577454290076\n",
            "85     \t [0.15288894 0.84947162 0.48361687]. \t  2.5748938282286176 \t 3.772577454290076\n",
            "86     \t [0.14127014 0.02953937 0.32421009]. \t  0.7204917512255963 \t 3.772577454290076\n",
            "87     \t [0.89069861 0.32297623 0.05610986]. \t  0.07585405621148908 \t 3.772577454290076\n",
            "88     \t [0.89113566 0.81003517 0.79241111]. \t  1.7044650571138988 \t 3.772577454290076\n",
            "89     \t [0.29976282 0.72376121 0.38408178]. \t  0.9206494684935415 \t 3.772577454290076\n",
            "90     \t [0.21801703 0.69174281 0.33881151]. \t  0.5459252904266572 \t 3.772577454290076\n",
            "91     \t [0.32550266 0.96841791 0.94725396]. \t  0.6263481766596568 \t 3.772577454290076\n",
            "92     \t [0.67032488 0.96302862 0.87442259]. \t  0.7919522796466973 \t 3.772577454290076\n",
            "93     \t [0.16572149 0.0161577  0.12188675]. \t  0.4232172729959182 \t 3.772577454290076\n",
            "94     \t [0.31948346 0.05319824 0.01070769]. \t  0.13222931057489118 \t 3.772577454290076\n",
            "95     \t [0.8957484  0.2366864  0.66129404]. \t  0.7942865476787239 \t 3.772577454290076\n",
            "96     \t [0.6343747  0.42482422 0.42699233]. \t  0.28859725211452913 \t 3.772577454290076\n",
            "97     \t [0.09414835 0.47320232 0.48502237]. \t  0.6944438440132831 \t 3.772577454290076\n",
            "98     \t [0.35682058 0.62212976 0.54162852]. \t  1.5788029312392569 \t 3.772577454290076\n",
            "99     \t [0.48811289 0.59955038 0.18383557]. \t  0.09066010487259671 \t 3.772577454290076\n",
            "100    \t [0.52697289 0.4467923  0.3546839 ]. \t  0.34114377715747707 \t 3.772577454290076\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vkq32q1utdoO",
        "outputId": "f3b08131-d91b-4558-8105-cc2d6ebe27c0"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 9\r\n",
        "\r\n",
        "np.random.seed(run_num_9)\r\n",
        "surrogate_loser_9 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "loser_9 = dGPGO(surrogate_loser_9, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_9.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.67227856 0.4880784  0.82549517]. \t  3.595021899183128 \t 3.595021899183128\n",
            "init   \t [0.03144639 0.80804996 0.56561742]. \t  2.9633561694281085 \t 3.595021899183128\n",
            "init   \t [0.2976225  0.04669572 0.9906274 ]. \t  0.16382388103073592 \t 3.595021899183128\n",
            "init   \t [0.00682573 0.76979303 0.7467671 ]. \t  2.382987807172393 \t 3.595021899183128\n",
            "init   \t [0.37743894 0.49414745 0.92894839]. \t  3.1588932929069533 \t 3.595021899183128\n",
            "1      \t [0.52764856 0.9939604  0.48209367]. \t  1.3155723134440893 \t 3.595021899183128\n",
            "2      \t [0.00227164 0.46291529 0.00323185]. \t  0.024986269174128935 \t 3.595021899183128\n",
            "3      \t [0.74154712 0.47672217 0.94815008]. \t  2.7597734226906323 \t 3.595021899183128\n",
            "4      \t [0.48243272 0.44732888 0.61651095]. \t  1.1836247573325926 \t 3.595021899183128\n",
            "5      \t [0.70693919 0.69691564 0.77816739]. \t  2.617415638076941 \t 3.595021899183128\n",
            "6      \t [0.07934713 0.95674979 0.04699611]. \t  0.0013796279485808415 \t 3.595021899183128\n",
            "7      \t [0.65517338 0.28012834 0.91985488]. \t  1.6015545545828125 \t 3.595021899183128\n",
            "8      \t [0.01853941 0.15725345 0.94284286]. \t  0.6282623839658739 \t 3.595021899183128\n",
            "9      \t [0.02067804 0.91099811 0.59912263]. \t  2.8424606783464554 \t 3.595021899183128\n",
            "10     \t [0.95658795 0.69880071 0.42710878]. \t  0.1814711238186602 \t 3.595021899183128\n",
            "11     \t [0.46522942 0.57369835 0.81781897]. \t  \u001b[92m3.702263783113012\u001b[0m \t 3.702263783113012\n",
            "12     \t [0.52555208 0.63524234 0.70123616]. \t  2.2598822232696696 \t 3.702263783113012\n",
            "13     \t [2.30960091e-02 1.23461596e-05 4.87108367e-01]. \t  0.16085778753369612 \t 3.702263783113012\n",
            "14     \t [0.88687089 0.98168636 0.08500084]. \t  0.0006734646054055867 \t 3.702263783113012\n",
            "15     \t [0.1857406  0.4133887  0.72298641]. \t  2.318528253278158 \t 3.702263783113012\n",
            "16     \t [0.97830435 0.12137651 0.02410498]. \t  0.055652804826250325 \t 3.702263783113012\n",
            "17     \t [0.99032894 0.22521379 0.4339514 ]. \t  0.14898371036442423 \t 3.702263783113012\n",
            "18     \t [0.54520167 0.52135039 0.90221158]. \t  3.5576649552395327 \t 3.702263783113012\n",
            "19     \t [0.96126208 0.55993652 0.82732719]. \t  3.5929279326682915 \t 3.702263783113012\n",
            "20     \t [0.94533867 0.98291765 0.51459507]. \t  0.32528953460659615 \t 3.702263783113012\n",
            "21     \t [0.15819272 0.99949992 0.58768626]. \t  2.5035349811071463 \t 3.702263783113012\n",
            "22     \t [0.95987731 0.49283666 0.8029144 ]. \t  3.3486451827858508 \t 3.702263783113012\n",
            "23     \t [0.25528111 0.48415676 0.36037925]. \t  0.39866790747783953 \t 3.702263783113012\n",
            "24     \t [0.20306951 0.99222542 0.24800054]. \t  0.1512861506621417 \t 3.702263783113012\n",
            "25     \t [0.27036096 0.91077795 0.38093927]. \t  1.1084812363376046 \t 3.702263783113012\n",
            "26     \t [0.79705018 0.02438926 0.25197161]. \t  0.525917058461396 \t 3.702263783113012\n",
            "27     \t [0.49265462 0.54289676 0.85190865]. \t  \u001b[92m3.838836456729617\u001b[0m \t 3.838836456729617\n",
            "28     \t [0.39445906 0.51019958 0.43097223]. \t  0.5296688656203746 \t 3.838836456729617\n",
            "29     \t [0.45832238 0.61082287 0.02385105]. \t  0.014621353799380968 \t 3.838836456729617\n",
            "30     \t [0.97455044 0.4977706  0.74525868]. \t  2.658394790619422 \t 3.838836456729617\n",
            "31     \t [0.13780277 0.00119429 0.86335846]. \t  0.2266684606696992 \t 3.838836456729617\n",
            "32     \t [0.04934746 0.9126289  0.30728679]. \t  0.46718190652705865 \t 3.838836456729617\n",
            "33     \t [0.63083083 0.80771709 0.94964549]. \t  1.66189665408206 \t 3.838836456729617\n",
            "34     \t [0.47935411 0.11732356 0.64248618]. \t  0.363283541668328 \t 3.838836456729617\n",
            "35     \t [0.61687711 0.09510488 0.91547047]. \t  0.44474067889320423 \t 3.838836456729617\n",
            "36     \t [0.78219097 0.51341686 0.56040375]. \t  0.632511600681982 \t 3.838836456729617\n",
            "37     \t [0.45342712 0.29246917 0.08607081]. \t  0.26866794578786274 \t 3.838836456729617\n",
            "38     \t [0.79231077 0.41076059 0.45925204]. \t  0.21737004359724799 \t 3.838836456729617\n",
            "39     \t [0.54742893 0.33386352 0.6356025 ]. \t  0.9920591266767318 \t 3.838836456729617\n",
            "40     \t [0.30756416 0.97485586 0.24073759]. \t  0.12554771190166797 \t 3.838836456729617\n",
            "41     \t [0.60765123 0.16909337 0.90400786]. \t  0.8513283924330438 \t 3.838836456729617\n",
            "42     \t [0.02609647 0.25664567 0.42214936]. \t  0.34216163993403137 \t 3.838836456729617\n",
            "43     \t [0.53773788 0.36460807 0.23858312]. \t  0.49174845219631486 \t 3.838836456729617\n",
            "44     \t [0.96152213 0.82641581 0.10945664]. \t  0.0019440563357673208 \t 3.838836456729617\n",
            "45     \t [0.79290613 0.85126246 0.36330886]. \t  0.2480413986069324 \t 3.838836456729617\n",
            "46     \t [0.36693992 0.5510474  0.54730084]. \t  1.207137595299256 \t 3.838836456729617\n",
            "47     \t [0.96470959 0.60942616 0.90145165]. \t  3.4020498560367365 \t 3.838836456729617\n",
            "48     \t [0.83564559 0.59730719 0.3737175 ]. \t  0.15190635573334832 \t 3.838836456729617\n",
            "49     \t [0.84113746 0.21218613 0.48897705]. \t  0.18643348600582219 \t 3.838836456729617\n",
            "50     \t [0.87319286 0.9637986  0.04682065]. \t  0.0002923853238965718 \t 3.838836456729617\n",
            "51     \t [0.15868669 0.58519333 0.11021103]. \t  0.05012181383179967 \t 3.838836456729617\n",
            "52     \t [0.52305601 0.70289761 0.29157513]. \t  0.19819230143825314 \t 3.838836456729617\n",
            "53     \t [0.6400917  0.45691768 0.67169099]. \t  1.712734043008807 \t 3.838836456729617\n",
            "54     \t [0.28146427 0.33584932 0.58268669]. \t  0.6754966180919455 \t 3.838836456729617\n",
            "55     \t [0.51813264 0.72474541 0.2269723 ]. \t  0.08029306698449311 \t 3.838836456729617\n",
            "56     \t [0.19934762 0.49715661 0.04974183]. \t  0.052606553273321296 \t 3.838836456729617\n",
            "57     \t [0.91096375 0.47376646 0.8982599 ]. \t  3.3133478775835847 \t 3.838836456729617\n",
            "58     \t [0.96119224 0.1761734  0.50729654]. \t  0.1456110852282695 \t 3.838836456729617\n",
            "59     \t [0.28176789 0.67036869 0.25019929]. \t  0.15806582287163762 \t 3.838836456729617\n",
            "60     \t [0.85359838 0.82557268 0.75249523]. \t  1.355038799317163 \t 3.838836456729617\n",
            "61     \t [0.35716294 0.50607502 0.95971011]. \t  2.722902045977438 \t 3.838836456729617\n",
            "62     \t [0.42153072 0.83281484 0.35735336]. \t  0.6905915555116233 \t 3.838836456729617\n",
            "63     \t [0.14581925 0.96649475 0.98547217]. \t  0.4930788184241869 \t 3.838836456729617\n",
            "64     \t [0.37640393 0.23769001 0.39894108]. \t  0.5463276150929359 \t 3.838836456729617\n",
            "65     \t [0.9781367  0.84839682 0.41277241]. \t  0.17489809045160942 \t 3.838836456729617\n",
            "66     \t [0.3251493  0.73720585 0.12627524]. \t  0.02049352192856097 \t 3.838836456729617\n",
            "67     \t [0.7100949  0.0961176  0.64271501]. \t  0.30709097445666406 \t 3.838836456729617\n",
            "68     \t [0.65824962 0.69439832 0.72715427]. \t  2.152139382115979 \t 3.838836456729617\n",
            "69     \t [0.87557455 0.74930866 0.33004303]. \t  0.1057796659536137 \t 3.838836456729617\n",
            "70     \t [0.15115266 0.3255929  0.11656677]. \t  0.28441747665591366 \t 3.838836456729617\n",
            "71     \t [0.40161095 0.14330153 0.07912775]. \t  0.34220197048906614 \t 3.838836456729617\n",
            "72     \t [0.42432498 0.83660477 0.62693789]. \t  2.190133108141262 \t 3.838836456729617\n",
            "73     \t [0.30306727 0.23131915 0.0366332 ]. \t  0.17555082997327207 \t 3.838836456729617\n",
            "74     \t [0.07995938 0.42066868 0.51111481]. \t  0.6046239929769441 \t 3.838836456729617\n",
            "75     \t [0.59839891 0.84095705 0.65613497]. \t  1.5002171944513882 \t 3.838836456729617\n",
            "76     \t [0.89083205 0.30444691 0.91605057]. \t  1.810568198950642 \t 3.838836456729617\n",
            "77     \t [0.62443328 0.85865884 0.42726402]. \t  0.8373908278862687 \t 3.838836456729617\n",
            "78     \t [0.50969006 0.90259795 0.4377578 ]. \t  1.2261056077922827 \t 3.838836456729617\n",
            "79     \t [0.77621412 0.58111152 0.05490291]. \t  0.018409940709691444 \t 3.838836456729617\n",
            "80     \t [0.4826354  0.22150444 0.13440055]. \t  0.5078311586541444 \t 3.838836456729617\n",
            "81     \t [0.28505127 0.33246699 0.83330575]. \t  2.4724974913509064 \t 3.838836456729617\n",
            "82     \t [0.99137479 0.72353283 0.21374937]. \t  0.01436863814474256 \t 3.838836456729617\n",
            "83     \t [0.25536089 0.2923841  0.95535917]. \t  1.4120529804189925 \t 3.838836456729617\n",
            "84     \t [0.68554222 0.94377583 0.3566769 ]. \t  0.32595809242863 \t 3.838836456729617\n",
            "85     \t [0.00257283 0.98064215 0.46917557]. \t  2.0802613281659084 \t 3.838836456729617\n",
            "86     \t [0.76426008 0.22618128 0.17754331]. \t  0.4363121439109615 \t 3.838836456729617\n",
            "87     \t [0.1904641  0.81242336 0.43071404]. \t  1.80071597008606 \t 3.838836456729617\n",
            "88     \t [0.0906485  0.02573558 0.0191743 ]. \t  0.11503131228834358 \t 3.838836456729617\n",
            "89     \t [0.39433212 0.39069809 0.85675531]. \t  3.014316632906568 \t 3.838836456729617\n",
            "90     \t [0.67749328 0.72883553 0.7487094 ]. \t  2.1450116026714365 \t 3.838836456729617\n",
            "91     \t [0.86893183 0.83153693 0.7423304 ]. \t  1.2417906801469307 \t 3.838836456729617\n",
            "92     \t [0.22134224 0.60622318 0.00599918]. \t  0.011200136663916813 \t 3.838836456729617\n",
            "93     \t [0.52759496 0.24928289 0.83633028]. \t  1.6666359908101493 \t 3.838836456729617\n",
            "94     \t [0.48191241 0.90687268 0.65217406]. \t  1.732280853468756 \t 3.838836456729617\n",
            "95     \t [0.22739074 0.69964776 0.51761201]. \t  2.1686681936323704 \t 3.838836456729617\n",
            "96     \t [0.85732851 0.88204729 0.4189794 ]. \t  0.326832007774965 \t 3.838836456729617\n",
            "97     \t [0.27436566 0.47429614 0.33407217]. \t  0.3713947852116168 \t 3.838836456729617\n",
            "98     \t [0.80144005 0.62222005 0.48333435]. \t  0.42246043421277657 \t 3.838836456729617\n",
            "99     \t [0.11137178 0.24205983 0.83749476]. \t  1.5897244175653458 \t 3.838836456729617\n",
            "100    \t [0.76642793 0.13416178 0.66281868]. \t  0.45882963914336505 \t 3.838836456729617\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5Aur-aRtdq-",
        "outputId": "54c66e9d-9535-4310-b7c2-b4f3d4ed75d4"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 10\r\n",
        "\r\n",
        "np.random.seed(run_num_10)\r\n",
        "surrogate_loser_10 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "loser_10 = dGPGO(surrogate_loser_10, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_10.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.65358959 0.11500694 0.95028286]. \t  0.42730315147591735 \t 1.1029187088185965\n",
            "init   \t [0.4821914  0.87247454 0.21233268]. \t  0.06161964400032635 \t 1.1029187088185965\n",
            "init   \t [0.04070962 0.39719446 0.2331322 ]. \t  0.33269334660262956 \t 1.1029187088185965\n",
            "init   \t [0.84174072 0.20708234 0.74246953]. \t  1.1029187088185965 \t 1.1029187088185965\n",
            "init   \t [0.39215413 0.18225652 0.74353941]. \t  0.9779763535009853 \t 1.1029187088185965\n",
            "1      \t [0.86210958 0.05779215 0.34314786]. \t  0.3927532126432764 \t 1.1029187088185965\n",
            "2      \t [0.92518579 0.78050514 0.87881645]. \t  \u001b[92m2.2443644989048672\u001b[0m \t 2.2443644989048672\n",
            "3      \t [0.98338289 0.93392737 0.81033881]. \t  0.8330483590093913 \t 2.2443644989048672\n",
            "4      \t [0.24984974 0.81883701 0.99874938]. \t  1.135383544316759 \t 2.2443644989048672\n",
            "5      \t [0.84241011 0.46743734 0.97175067]. \t  \u001b[92m2.3195199773796715\u001b[0m \t 2.3195199773796715\n",
            "6      \t [0.77577588 0.62900507 0.95536967]. \t  \u001b[92m2.727536856652607\u001b[0m \t 2.727536856652607\n",
            "7      \t [0.65288565 0.62205817 0.86074034]. \t  \u001b[92m3.6353071006528688\u001b[0m \t 3.6353071006528688\n",
            "8      \t [0.66640478 0.61877439 0.58484337]. \t  1.0883761860789831 \t 3.6353071006528688\n",
            "9      \t [0.11836853 0.92421572 0.40077358]. \t  1.4382047023912417 \t 3.6353071006528688\n",
            "10     \t [1.01422253e-14 2.13687495e-14 1.95448945e-14]. \t  0.06797411659015849 \t 3.6353071006528688\n",
            "11     \t [0.91498214 0.04875353 0.00402212]. \t  0.048769558323538735 \t 3.6353071006528688\n",
            "12     \t [0.28501363 0.56069794 0.81105969]. \t  \u001b[92m3.701139938792839\u001b[0m \t 3.701139938792839\n",
            "13     \t [0.06191779 0.75578134 0.02041316]. \t  0.0025418667181896612 \t 3.701139938792839\n",
            "14     \t [0.44724157 0.86154826 0.74155042]. \t  1.6436280199400373 \t 3.701139938792839\n",
            "15     \t [0.32900204 0.5222541  0.95157111]. \t  2.9035028561512806 \t 3.701139938792839\n",
            "16     \t [0.20016925 0.69643574 0.79177916]. \t  3.024073779204188 \t 3.701139938792839\n",
            "17     \t [0.42243564 0.66380211 0.71533146]. \t  2.4692385729881137 \t 3.701139938792839\n",
            "18     \t [0.09177594 0.9678026  0.06127449]. \t  0.0020036168886189963 \t 3.701139938792839\n",
            "19     \t [0.68362641 0.98533124 0.05418554]. \t  0.0006363090741918352 \t 3.701139938792839\n",
            "20     \t [0.1121814  0.57163451 0.79125054]. \t  3.5216749058979495 \t 3.701139938792839\n",
            "21     \t [0.5783509  0.49008829 0.81913902]. \t  3.5934784878492065 \t 3.701139938792839\n",
            "22     \t [0.51884018 0.57083199 0.85833946]. \t  \u001b[92m3.825950075211796\u001b[0m \t 3.825950075211796\n",
            "23     \t [0.0093238  0.61751695 0.70112407]. \t  2.564612796557479 \t 3.825950075211796\n",
            "24     \t [0.0306889  0.97870848 0.73586423]. \t  1.337128891077002 \t 3.825950075211796\n",
            "25     \t [0.27980633 0.46148506 0.1752227 ]. \t  0.2378344066527602 \t 3.825950075211796\n",
            "26     \t [0.32699639 0.02315428 0.06128359]. \t  0.2549683775569132 \t 3.825950075211796\n",
            "27     \t [0.73893811 0.68093479 0.04192289]. \t  0.006243422255177505 \t 3.825950075211796\n",
            "28     \t [0.20139786 0.88767482 0.66395525]. \t  2.388005815326538 \t 3.825950075211796\n",
            "29     \t [0.02551235 0.3423593  0.75184502]. \t  2.1388202357199315 \t 3.825950075211796\n",
            "30     \t [0.6657422  0.53602642 0.87915674]. \t  3.7327606277135947 \t 3.825950075211796\n",
            "31     \t [0.53783974 0.37586609 0.11422795]. \t  0.23297846283288623 \t 3.825950075211796\n",
            "32     \t [0.17733175 0.19713417 0.53632996]. \t  0.27802306505192764 \t 3.825950075211796\n",
            "33     \t [0.03047332 0.0352073  0.14699465]. \t  0.42970513491749057 \t 3.825950075211796\n",
            "34     \t [0.92196866 0.20235519 0.22032218]. \t  0.34779631994302274 \t 3.825950075211796\n",
            "35     \t [0.41268804 0.21677931 0.07668479]. \t  0.3026346220661753 \t 3.825950075211796\n",
            "36     \t [0.28033917 0.9940199  0.44363132]. \t  1.6428151604376122 \t 3.825950075211796\n",
            "37     \t [0.91502671 0.69756974 0.91725515]. \t  2.787194121186799 \t 3.825950075211796\n",
            "38     \t [0.04794957 0.42614372 0.76621089]. \t  2.846087590501406 \t 3.825950075211796\n",
            "39     \t [0.84554329 0.19749171 0.53590731]. \t  0.21072762937007228 \t 3.825950075211796\n",
            "40     \t [0.10248424 0.96645507 0.94996451]. \t  0.6251604128490882 \t 3.825950075211796\n",
            "41     \t [0.54897872 0.42031727 0.66842942]. \t  1.6163134760994067 \t 3.825950075211796\n",
            "42     \t [0.94325126 0.54732528 0.85763938]. \t  3.69811452912248 \t 3.825950075211796\n",
            "43     \t [0.90677211 0.67028093 0.75452163]. \t  2.372125722503001 \t 3.825950075211796\n",
            "44     \t [0.25070052 0.63827166 0.40552541]. \t  0.884706677204184 \t 3.825950075211796\n",
            "45     \t [0.32103194 0.78721674 0.28324319]. \t  0.2781771042452093 \t 3.825950075211796\n",
            "46     \t [0.73990251 0.38721673 0.73946956]. \t  2.273626035083554 \t 3.825950075211796\n",
            "47     \t [0.61276184 0.23705712 0.22676888]. \t  0.6905085975860578 \t 3.825950075211796\n",
            "48     \t [0.79204942 0.18643486 0.93466089]. \t  0.8194979038041548 \t 3.825950075211796\n",
            "49     \t [0.2182822  0.82045885 0.34387207]. \t  0.7485559328161449 \t 3.825950075211796\n",
            "50     \t [0.79434093 0.75456143 0.80972959]. \t  2.3576544924574 \t 3.825950075211796\n",
            "51     \t [0.99685024 0.77141333 0.03923149]. \t  0.0009762900711944165 \t 3.825950075211796\n",
            "52     \t [0.33678804 0.22014992 0.00684788]. \t  0.11712495717363505 \t 3.825950075211796\n",
            "53     \t [0.41028139 0.5719381  0.26311632]. \t  0.19774787863788867 \t 3.825950075211796\n",
            "54     \t [0.60077419 0.77319619 0.97632717]. \t  1.6470965209265724 \t 3.825950075211796\n",
            "55     \t [0.95857662 0.47617844 0.80837472]. \t  3.338517307679115 \t 3.825950075211796\n",
            "56     \t [0.16839819 0.09440242 0.80593045]. \t  0.5819460844192055 \t 3.825950075211796\n",
            "57     \t [0.77126344 0.27787038 0.57095094]. \t  0.40678651464373805 \t 3.825950075211796\n",
            "58     \t [0.23622091 0.68363914 0.77638665]. \t  2.9986286541218723 \t 3.825950075211796\n",
            "59     \t [0.98691583 0.16799015 0.96810489]. \t  0.5602547142275782 \t 3.825950075211796\n",
            "60     \t [0.09323872 0.98788564 0.33246815]. \t  0.5978453646759858 \t 3.825950075211796\n",
            "61     \t [0.20585617 0.57446848 0.03650038]. \t  0.02341783038978431 \t 3.825950075211796\n",
            "62     \t [0.91977899 0.02827927 0.32915799]. \t  0.33213211617093846 \t 3.825950075211796\n",
            "63     \t [0.1744179  0.49781195 0.54077387]. \t  1.051091442156995 \t 3.825950075211796\n",
            "64     \t [0.99869586 0.95597948 0.00719737]. \t  6.747924827195337e-05 \t 3.825950075211796\n",
            "65     \t [0.05923735 0.65355236 0.73897894]. \t  2.847661076986637 \t 3.825950075211796\n",
            "66     \t [0.01269454 0.01430481 0.53274047]. \t  0.11687636895554003 \t 3.825950075211796\n",
            "67     \t [0.83162222 0.16189688 0.6821508 ]. \t  0.6204044967331243 \t 3.825950075211796\n",
            "68     \t [0.10307635 0.98704216 0.62230977]. \t  2.3851373682146537 \t 3.825950075211796\n",
            "69     \t [0.81310371 0.96461204 0.89116056]. \t  0.7398602162679827 \t 3.825950075211796\n",
            "70     \t [0.96392299 0.0732633  0.82991453]. \t  0.4700895298354437 \t 3.825950075211796\n",
            "71     \t [0.72813753 0.72226023 0.02144692]. \t  0.0029895386586784 \t 3.825950075211796\n",
            "72     \t [0.20147301 0.83427847 0.02792526]. \t  0.0016588547472213234 \t 3.825950075211796\n",
            "73     \t [0.96753452 0.40978965 0.46139521]. \t  0.14787896680659463 \t 3.825950075211796\n",
            "74     \t [0.24613548 0.25790695 0.36722679]. \t  0.6088691497095003 \t 3.825950075211796\n",
            "75     \t [0.61869238 0.30960007 0.75805921]. \t  1.9422037888060717 \t 3.825950075211796\n",
            "76     \t [0.06499414 0.21232159 0.37895357]. \t  0.497457472021624 \t 3.825950075211796\n",
            "77     \t [0.03885248 0.33913528 0.45254913]. \t  0.33612032404781683 \t 3.825950075211796\n",
            "78     \t [0.55379685 0.80032251 0.19912658]. \t  0.04278284053876489 \t 3.825950075211796\n",
            "79     \t [0.02021057 0.15528238 0.76939561]. \t  0.8700524306842632 \t 3.825950075211796\n",
            "80     \t [0.322192   0.09348805 0.37668228]. \t  0.6953889615688063 \t 3.825950075211796\n",
            "81     \t [0.09095127 0.93642432 0.61762491]. \t  2.685359651785647 \t 3.825950075211796\n",
            "82     \t [0.10159781 0.36047065 0.96154357]. \t  1.8417153323280768 \t 3.825950075211796\n",
            "83     \t [0.57007758 0.64436134 0.09782204]. \t  0.02497798409309806 \t 3.825950075211796\n",
            "84     \t [0.07585703 0.84169618 0.31840343]. \t  0.5586293380163749 \t 3.825950075211796\n",
            "85     \t [0.26852638 0.32867212 0.25219684]. \t  0.625031988431732 \t 3.825950075211796\n",
            "86     \t [0.03915346 0.47901975 0.39966145]. \t  0.4367926667230772 \t 3.825950075211796\n",
            "87     \t [0.79613901 0.22285573 0.8464027 ]. \t  1.401327231034421 \t 3.825950075211796\n",
            "88     \t [0.78634327 0.08272775 0.40411308]. \t  0.34048301845184237 \t 3.825950075211796\n",
            "89     \t [0.25075283 0.42774475 0.05874369]. \t  0.09926859078580283 \t 3.825950075211796\n",
            "90     \t [0.52020758 0.06642892 0.61019363]. \t  0.20301745333843152 \t 3.825950075211796\n",
            "91     \t [0.04145178 0.9982077  0.04650104]. \t  0.0011636459714390384 \t 3.825950075211796\n",
            "92     \t [0.6463775  0.3034504  0.95923746]. \t  1.4498903641763436 \t 3.825950075211796\n",
            "93     \t [0.21348632 0.41152009 0.08247436]. \t  0.14062703500889415 \t 3.825950075211796\n",
            "94     \t [0.52393133 0.71775999 0.69267821]. \t  2.0244943228113934 \t 3.825950075211796\n",
            "95     \t [0.92048307 0.02856064 0.60107968]. \t  0.12706036817059735 \t 3.825950075211796\n",
            "96     \t [0.55966008 0.00459803 0.75177789]. \t  0.2510841678353941 \t 3.825950075211796\n",
            "97     \t [0.06118166 0.51334582 0.17268385]. \t  0.12985995150060514 \t 3.825950075211796\n",
            "98     \t [0.67207969 0.29327723 0.95465068]. \t  1.416516516636988 \t 3.825950075211796\n",
            "99     \t [0.21093958 0.37233381 0.00276439]. \t  0.05926409704295324 \t 3.825950075211796\n",
            "100    \t [0.06026911 0.76609599 0.98141921]. \t  1.630173690362098 \t 3.825950075211796\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MMu7U9NNtdtn",
        "outputId": "0fddca8c-01ca-4fe9-e31d-30e71832e54d"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 11\r\n",
        "\r\n",
        "np.random.seed(run_num_11)\r\n",
        "surrogate_loser_11 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "loser_11 = dGPGO(surrogate_loser_11, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_11.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.98103566 0.82124785 0.64589361]. \t  0.6570240879762975 \t 0.687459437576373\n",
            "init   \t [0.42368801 0.20231098 0.49190677]. \t  0.29901296656621285 \t 0.687459437576373\n",
            "init   \t [0.13855833 0.45252104 0.11373463]. \t  0.1378633538075966 \t 0.687459437576373\n",
            "init   \t [0.00292449 0.52342617 0.18997116]. \t  0.12299763462541902 \t 0.687459437576373\n",
            "init   \t [0.14171979 0.33586637 0.58369219]. \t  0.687459437576373 \t 0.687459437576373\n",
            "1      \t [0.23267662 0.55632866 0.98247629]. \t  \u001b[92m2.401417505244251\u001b[0m \t 2.401417505244251\n",
            "2      \t [0.1018956  0.95528749 0.95450529]. \t  0.6625688909370097 \t 2.401417505244251\n",
            "3      \t [0.49069589 0.32394686 0.99166679]. \t  1.2576491073928158 \t 2.401417505244251\n",
            "4      \t [0.99622896 0.50984357 0.00987988]. \t  0.008998496185923171 \t 2.401417505244251\n",
            "5      \t [0.17147829 0.44852386 0.99855404]. \t  1.8327027793895603 \t 2.401417505244251\n",
            "6      \t [0.52755702 0.61099275 0.96085651]. \t  \u001b[92m2.731594261003499\u001b[0m \t 2.731594261003499\n",
            "7      \t [0.50454698 0.68368804 0.91827122]. \t  \u001b[92m2.973325813164906\u001b[0m \t 2.973325813164906\n",
            "8      \t [0.45418709 0.73924568 0.73077065]. \t  2.2589019118704425 \t 2.973325813164906\n",
            "9      \t [0.53897689 0.95646638 0.88685629]. \t  0.8406578977033827 \t 2.973325813164906\n",
            "10     \t [0.60547932 0.56968479 0.81474854]. \t  \u001b[92m3.6411377654076196\u001b[0m \t 3.6411377654076196\n",
            "11     \t [0.9286639  0.28303617 0.64074614]. \t  0.8065492930206374 \t 3.6411377654076196\n",
            "12     \t [0.6863489  0.52266533 0.67959972]. \t  1.8802784961872008 \t 3.6411377654076196\n",
            "13     \t [0.81516659 0.04230252 0.0144339 ]. \t  0.07642119456660208 \t 3.6411377654076196\n",
            "14     \t [3.56002803e-02 9.17723598e-01 3.28278754e-04]. \t  0.0004253659194999738 \t 3.6411377654076196\n",
            "15     \t [0.93259193 0.59135126 0.8998208 ]. \t  3.4797007688347623 \t 3.6411377654076196\n",
            "16     \t [0.03180589 0.80324037 0.77495056]. \t  2.2037135700505095 \t 3.6411377654076196\n",
            "17     \t [0.11708033 0.14671029 0.01423623]. \t  0.11999746361879633 \t 3.6411377654076196\n",
            "18     \t [0.50657018 0.49089223 0.85138264]. \t  \u001b[92m3.707332576217637\u001b[0m \t 3.707332576217637\n",
            "19     \t [0.80737161 0.0155614  0.66176037]. \t  0.18365620244113123 \t 3.707332576217637\n",
            "20     \t [0.37567124 0.48150598 0.85948258]. \t  3.6688440344570115 \t 3.707332576217637\n",
            "21     \t [0.48075247 0.4868558  0.84329412]. \t  3.6882293140094626 \t 3.707332576217637\n",
            "22     \t [0.41981534 0.53971767 0.86767719]. \t  \u001b[92m3.82609236093791\u001b[0m \t 3.82609236093791\n",
            "23     \t [0.02516017 0.58371562 0.30762436]. \t  0.28033234832751935 \t 3.82609236093791\n",
            "24     \t [0.48944405 0.6559092  0.83877525]. \t  3.459362722987617 \t 3.82609236093791\n",
            "25     \t [0.42029516 0.21429986 0.30920755]. \t  0.8618548387131872 \t 3.82609236093791\n",
            "26     \t [0.96711039 0.14576491 0.91452994]. \t  0.6616327147356602 \t 3.82609236093791\n",
            "27     \t [0.38311766 0.57716223 0.84585103]. \t  \u001b[92m3.832003952745011\u001b[0m \t 3.832003952745011\n",
            "28     \t [0.77891632 0.65166351 0.98580516]. \t  2.150894348089524 \t 3.832003952745011\n",
            "29     \t [0.9881304  0.83507513 0.9180588 ]. \t  1.588643416372116 \t 3.832003952745011\n",
            "30     \t [0.7384121  0.63332243 0.15380586]. \t  0.03551552148442237 \t 3.832003952745011\n",
            "31     \t [0.89618434 0.95859692 0.01322118]. \t  0.00011836310433250633 \t 3.832003952745011\n",
            "32     \t [0.3222753  0.04010627 0.03332996]. \t  0.18124540626821045 \t 3.832003952745011\n",
            "33     \t [0.88887881 0.39281112 0.92796296]. \t  2.4565050300574933 \t 3.832003952745011\n",
            "34     \t [0.31470288 0.14953304 0.93760765]. \t  0.6195643860286129 \t 3.832003952745011\n",
            "35     \t [0.75331242 0.27518024 0.74191406]. \t  1.5632474996030026 \t 3.832003952745011\n",
            "36     \t [0.01390206 0.79841187 0.82989979]. \t  2.270327964643763 \t 3.832003952745011\n",
            "37     \t [0.41503592 0.54752483 0.84909805]. \t  \u001b[92m3.8511165079641128\u001b[0m \t 3.8511165079641128\n",
            "38     \t [0.93771621 0.40085258 0.90840629]. \t  2.710212038381022 \t 3.8511165079641128\n",
            "39     \t [0.14965119 0.83004101 0.03678273]. \t  0.0020265420497056937 \t 3.8511165079641128\n",
            "40     \t [0.65825822 0.03070688 0.99081436]. \t  0.13820090658858034 \t 3.8511165079641128\n",
            "41     \t [0.81284622 0.54746547 0.93318464]. \t  3.1695915074968855 \t 3.8511165079641128\n",
            "42     \t [0.06732819 0.25070027 0.34887756]. \t  0.5420588033558534 \t 3.8511165079641128\n",
            "43     \t [0.53629411 0.64714885 0.64894173]. \t  1.7993921357365534 \t 3.8511165079641128\n",
            "44     \t [0.76632994 0.38500206 0.54507039]. \t  0.42354185795531757 \t 3.8511165079641128\n",
            "45     \t [0.86768148 0.71598166 0.27425688]. \t  0.05270532364446077 \t 3.8511165079641128\n",
            "46     \t [0.23880265 0.48264459 0.03340302]. \t  0.04854418552914468 \t 3.8511165079641128\n",
            "47     \t [0.57775579 0.25342925 0.84516579]. \t  1.6930206158719372 \t 3.8511165079641128\n",
            "48     \t [0.64910131 0.22567323 0.61880267]. \t  0.5435876480199389 \t 3.8511165079641128\n",
            "49     \t [0.01604554 0.33670665 0.18461822]. \t  0.3487165227529128 \t 3.8511165079641128\n",
            "50     \t [0.79408129 0.56766075 0.71696715]. \t  2.3064305635288314 \t 3.8511165079641128\n",
            "51     \t [0.7402549  0.3655645  0.54355901]. \t  0.406519646040674 \t 3.8511165079641128\n",
            "52     \t [0.3419478  0.37953654 0.70966463]. \t  1.978436778247392 \t 3.8511165079641128\n",
            "53     \t [0.88811151 0.16272661 0.61333689]. \t  0.3545873365654224 \t 3.8511165079641128\n",
            "54     \t [0.12063825 0.28654585 0.99348593]. \t  1.0168883573826288 \t 3.8511165079641128\n",
            "55     \t [0.60034281 0.33793455 0.11347122]. \t  0.2572321192658406 \t 3.8511165079641128\n",
            "56     \t [0.39737068 0.82353818 0.94749653]. \t  1.5740956107911792 \t 3.8511165079641128\n",
            "57     \t [0.41435179 0.284656   0.94117003]. \t  1.4810304390715325 \t 3.8511165079641128\n",
            "58     \t [0.28734283 0.70631795 0.44272901]. \t  1.4552116958940275 \t 3.8511165079641128\n",
            "59     \t [0.64582064 0.27048736 0.60267387]. \t  0.5685328343977809 \t 3.8511165079641128\n",
            "60     \t [0.00961796 0.99646683 0.43966302]. \t  1.6845678787749219 \t 3.8511165079641128\n",
            "61     \t [0.79067284 0.4889186  0.34277043]. \t  0.1720656681141432 \t 3.8511165079641128\n",
            "62     \t [0.73614964 0.08873081 0.08123764]. \t  0.23429503064823837 \t 3.8511165079641128\n",
            "63     \t [0.4074256  0.8972777  0.15489097]. \t  0.020428079250579424 \t 3.8511165079641128\n",
            "64     \t [0.37505358 0.98290616 0.57097691]. \t  2.1753048118193075 \t 3.8511165079641128\n",
            "65     \t [0.95340121 0.51222226 0.23034747]. \t  0.07641901044959246 \t 3.8511165079641128\n",
            "66     \t [0.66780369 0.70433721 0.20511346]. \t  0.04426141336992313 \t 3.8511165079641128\n",
            "67     \t [0.04098637 0.00125381 0.04984295]. \t  0.1533279300561005 \t 3.8511165079641128\n",
            "68     \t [0.00668649 0.08940948 0.4725365 ]. \t  0.21997486144380612 \t 3.8511165079641128\n",
            "69     \t [0.50161296 0.95814489 0.13143229]. \t  0.008605766789104212 \t 3.8511165079641128\n",
            "70     \t [0.89116531 0.99014832 0.88225675]. \t  0.5995239394899143 \t 3.8511165079641128\n",
            "71     \t [0.52987778 0.82918509 0.07807094]. \t  0.003878824742720693 \t 3.8511165079641128\n",
            "72     \t [0.46976662 0.07514661 0.19917063]. \t  0.8292924932649325 \t 3.8511165079641128\n",
            "73     \t [0.69407614 0.03869317 0.24545797]. \t  0.67522371915888 \t 3.8511165079641128\n",
            "74     \t [0.8785674  0.28201439 0.88658166]. \t  1.7952803792715935 \t 3.8511165079641128\n",
            "75     \t [0.11661158 0.57658081 0.89141671]. \t  3.6830251509630694 \t 3.8511165079641128\n",
            "76     \t [0.59244815 0.46942989 0.82981494]. \t  3.5506139509376005 \t 3.8511165079641128\n",
            "77     \t [0.15000929 0.83777228 0.81145404]. \t  1.947239797927471 \t 3.8511165079641128\n",
            "78     \t [0.03822197 0.16401539 0.05399344]. \t  0.17994921078638834 \t 3.8511165079641128\n",
            "79     \t [0.47399276 0.3863694  0.51186274]. \t  0.4426504618250182 \t 3.8511165079641128\n",
            "80     \t [0.07164216 0.1899446  0.51708782]. \t  0.2455051318790618 \t 3.8511165079641128\n",
            "81     \t [0.38550927 0.95537209 0.6460422 ]. \t  1.8994677417985926 \t 3.8511165079641128\n",
            "82     \t [0.62526033 0.68415627 0.34354801]. \t  0.2776421583690714 \t 3.8511165079641128\n",
            "83     \t [0.26733556 0.17048883 0.76671184]. \t  0.9703752437022648 \t 3.8511165079641128\n",
            "84     \t [0.0319908  0.90880231 0.79468209]. \t  1.423155602228559 \t 3.8511165079641128\n",
            "85     \t [0.89718587 0.84522385 0.93800549]. \t  1.412830789733427 \t 3.8511165079641128\n",
            "86     \t [0.45140418 0.1109365  0.8919736 ]. \t  0.5691786665236267 \t 3.8511165079641128\n",
            "87     \t [0.88238899 0.29590736 0.9282799 ]. \t  1.6420849187800834 \t 3.8511165079641128\n",
            "88     \t [0.51218717 0.00332253 0.90375347]. \t  0.1970479087957312 \t 3.8511165079641128\n",
            "89     \t [0.93373041 0.30008258 0.2868365 ]. \t  0.2738027942787708 \t 3.8511165079641128\n",
            "90     \t [0.82243595 0.81083886 0.19101491]. \t  0.015539666017811682 \t 3.8511165079641128\n",
            "91     \t [0.41487269 0.86355672 0.87969103]. \t  1.5871950857407389 \t 3.8511165079641128\n",
            "92     \t [0.85695397 0.31416628 0.88882694]. \t  2.089645940612437 \t 3.8511165079641128\n",
            "93     \t [0.30654317 0.68735427 0.25532736]. \t  0.1666041551928666 \t 3.8511165079641128\n",
            "94     \t [0.67643792 0.00669018 0.92556499]. \t  0.17923468057010708 \t 3.8511165079641128\n",
            "95     \t [0.41761799 0.16804838 0.19966897]. \t  0.8437008962970893 \t 3.8511165079641128\n",
            "96     \t [0.79041122 0.13903144 0.95899516]. \t  0.4865923987032969 \t 3.8511165079641128\n",
            "97     \t [0.27664727 0.08700359 0.81163344]. \t  0.5498838079004507 \t 3.8511165079641128\n",
            "98     \t [0.29130459 0.70467093 0.66824017]. \t  2.401895065242285 \t 3.8511165079641128\n",
            "99     \t [0.60433714 0.04489796 0.1987617 ]. \t  0.6982724795764844 \t 3.8511165079641128\n",
            "100    \t [0.92270353 0.34908719 0.75873467]. \t  2.179299016401862 \t 3.8511165079641128\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UEjKllHqtdwW",
        "outputId": "8c04bfdb-b875-4800-d1f4-cce56b73e3b7"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 12\r\n",
        "\r\n",
        "np.random.seed(run_num_12)\r\n",
        "surrogate_loser_12 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "loser_12 = dGPGO(surrogate_loser_12, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_12.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.19151945 0.62210877 0.43772774]. \t  1.1006281843679786 \t 1.6482992955272024\n",
            "init   \t [0.78535858 0.77997581 0.27259261]. \t  0.07153771680480671 \t 1.6482992955272024\n",
            "init   \t [0.27646426 0.80187218 0.95813935]. \t  1.6482992955272024 \t 1.6482992955272024\n",
            "init   \t [0.87593263 0.35781727 0.50099513]. \t  0.2282556248207173 \t 1.6482992955272024\n",
            "init   \t [0.68346294 0.71270203 0.37025075]. \t  0.33032494760932407 \t 1.6482992955272024\n",
            "1      \t [0.01398415 0.60897714 0.98856638]. \t  \u001b[92m2.228026380269567\u001b[0m \t 2.228026380269567\n",
            "2      \t [0.03494356 0.11089546 0.89536938]. \t  0.5535687532914756 \t 2.228026380269567\n",
            "3      \t [0.00761147 0.89633798 0.99755295]. \t  0.7296905183544732 \t 2.228026380269567\n",
            "4      \t [0.42621529 0.52201143 0.94674436]. \t  \u001b[92m2.981393016278439\u001b[0m \t 2.981393016278439\n",
            "5      \t [0.62997572 0.44202894 0.99618939]. \t  1.8391155649095643 \t 2.981393016278439\n",
            "6      \t [0.32429872 0.48995801 0.97777927]. \t  2.3563074830909687 \t 2.981393016278439\n",
            "7      \t [0.99206005 0.87479422 0.59276177]. \t  0.4186473279210114 \t 2.981393016278439\n",
            "8      \t [0.05476847 0.53285389 0.64325851]. \t  1.8981988573620965 \t 2.981393016278439\n",
            "9      \t [1.57405837e-14 1.15917238e-14 1.86440247e-14]. \t  0.06797411659015684 \t 2.981393016278439\n",
            "10     \t [0.0322103  0.83671651 0.03102825]. \t  0.0015280586580840726 \t 2.981393016278439\n",
            "11     \t [0.7698327  0.69612096 0.89058708]. \t  \u001b[92m3.029485017097235\u001b[0m \t 3.029485017097235\n",
            "12     \t [0.43926768 0.5817004  0.86293835]. \t  \u001b[92m3.8177647120619196\u001b[0m \t 3.8177647120619196\n",
            "13     \t [0.94895416 0.92227183 0.04577195]. \t  0.00027661898739457985 \t 3.8177647120619196\n",
            "14     \t [0.96716075 0.55712453 0.89266817]. \t  3.5604352487382096 \t 3.8177647120619196\n",
            "15     \t [0.9867346  0.37982781 0.09809661]. \t  0.06760397508437056 \t 3.8177647120619196\n",
            "16     \t [0.90017763 0.51696429 0.78507178]. \t  3.237173868723097 \t 3.8177647120619196\n",
            "17     \t [0.27326543 0.62896984 0.83586619]. \t  3.6455175745982222 \t 3.8177647120619196\n",
            "18     \t [0.41927477 0.56180083 0.80926929]. \t  3.6659304271249677 \t 3.8177647120619196\n",
            "19     \t [0.35657319 0.69697249 0.11065344]. \t  0.021505444175504602 \t 3.8177647120619196\n",
            "20     \t [0.19309422 0.2903732  0.41339479]. \t  0.4298102396014697 \t 3.8177647120619196\n",
            "21     \t [0.20463233 0.04324562 0.65697062]. \t  0.23108242657891664 \t 3.8177647120619196\n",
            "22     \t [1.57842901e-02 8.20918142e-01 7.03733502e-04]. \t  0.0008601507897846089 \t 3.8177647120619196\n",
            "23     \t [0.60248017 0.3231838  0.54524734]. \t  0.40827501689705414 \t 3.8177647120619196\n",
            "24     \t [0.95934255 0.13483385 0.58978918]. \t  0.23503660187818645 \t 3.8177647120619196\n",
            "25     \t [0.944549   0.01940069 0.0303243 ]. \t  0.062405931694092066 \t 3.8177647120619196\n",
            "26     \t [0.55017482 0.2753227  0.00227453]. \t  0.08574825192552826 \t 3.8177647120619196\n",
            "27     \t [0.6303606  0.91787344 0.54542479]. \t  1.3455171557446375 \t 3.8177647120619196\n",
            "28     \t [0.41436159 0.5956259  0.79046636]. \t  3.439291049398898 \t 3.8177647120619196\n",
            "29     \t [0.43817054 0.11696402 0.3778372 ]. \t  0.6896644481940144 \t 3.8177647120619196\n",
            "30     \t [0.06346045 0.63738689 0.33134359]. \t  0.4292681917320401 \t 3.8177647120619196\n",
            "31     \t [0.02637106 0.98958525 0.7432182 ]. \t  1.225135757871817 \t 3.8177647120619196\n",
            "32     \t [0.82033973 0.79716428 0.08591558]. \t  0.0028307305418085256 \t 3.8177647120619196\n",
            "33     \t [0.45938261 0.75250997 0.30932276]. \t  0.3116825248599445 \t 3.8177647120619196\n",
            "34     \t [0.14857774 0.26493928 0.00777946]. \t  0.09209710617458892 \t 3.8177647120619196\n",
            "35     \t [0.60832238 0.55700038 0.81874419]. \t  3.683568939368378 \t 3.8177647120619196\n",
            "36     \t [0.17079159 0.75188383 0.20204625]. \t  0.07527308098946896 \t 3.8177647120619196\n",
            "37     \t [0.88857253 0.13831501 0.99785855]. \t  0.3461011837569411 \t 3.8177647120619196\n",
            "38     \t [0.43495971 0.82343517 0.56848465]. \t  2.260262249034812 \t 3.8177647120619196\n",
            "39     \t [0.05922251 0.19985131 0.18276069]. \t  0.5656147149471785 \t 3.8177647120619196\n",
            "40     \t [0.17134421 0.86725833 0.08159063]. \t  0.004730173871140647 \t 3.8177647120619196\n",
            "41     \t [0.42662425 0.69689899 0.79793902]. \t  2.9774239054093257 \t 3.8177647120619196\n",
            "42     \t [0.18872382 0.51514048 0.22488052]. \t  0.20745664175205186 \t 3.8177647120619196\n",
            "43     \t [0.06999489 0.27249309 0.77138987]. \t  1.7229732668908813 \t 3.8177647120619196\n",
            "44     \t [0.42434339 0.83062154 0.07494507]. \t  0.004198432986741109 \t 3.8177647120619196\n",
            "45     \t [0.52873134 0.48130385 0.74748806]. \t  2.8460017521193435 \t 3.8177647120619196\n",
            "46     \t [0.85544522 0.01913392 0.08899941]. \t  0.17209892135155425 \t 3.8177647120619196\n",
            "47     \t [0.94509336 0.52652727 0.52765881]. \t  0.355756562588529 \t 3.8177647120619196\n",
            "48     \t [0.21204997 0.52165853 0.10561627]. \t  0.08444321506372225 \t 3.8177647120619196\n",
            "49     \t [0.49512097 0.96958735 0.07743071]. \t  0.0021082109387776248 \t 3.8177647120619196\n",
            "50     \t [0.86690057 0.3390747  0.56690559]. \t  0.4488104603551687 \t 3.8177647120619196\n",
            "51     \t [0.43918792 0.76714066 0.46667066]. \t  1.5678915085471918 \t 3.8177647120619196\n",
            "52     \t [0.98846208 0.47663677 0.92545313]. \t  3.0092802227686923 \t 3.8177647120619196\n",
            "53     \t [0.47038939 0.8521396  0.9974098 ]. \t  0.9628504485489497 \t 3.8177647120619196\n",
            "54     \t [0.77998072 0.34352677 0.73868039]. \t  1.9942376878878827 \t 3.8177647120619196\n",
            "55     \t [0.68813783 0.09659201 0.92520935]. \t  0.4257988157189452 \t 3.8177647120619196\n",
            "56     \t [0.50350165 0.82297981 0.52319174]. \t  1.8476111133622661 \t 3.8177647120619196\n",
            "57     \t [0.70155505 0.48281139 0.71915125]. \t  2.381860570908207 \t 3.8177647120619196\n",
            "58     \t [0.55925288 0.44245054 0.32789276]. \t  0.3357905619753193 \t 3.8177647120619196\n",
            "59     \t [0.49978886 0.66873457 0.25703783]. \t  0.13282998981277314 \t 3.8177647120619196\n",
            "60     \t [0.84105879 0.5629506  0.34627985]. \t  0.1243490147263995 \t 3.8177647120619196\n",
            "61     \t [0.12603402 0.63989199 0.71126356]. \t  2.6777207524288986 \t 3.8177647120619196\n",
            "62     \t [0.39964433 0.99658814 0.45459576]. \t  1.484373314649264 \t 3.8177647120619196\n",
            "63     \t [0.44241317 0.17151992 0.84918487]. \t  1.0160410107130888 \t 3.8177647120619196\n",
            "64     \t [0.18133544 0.67887524 0.60018155]. \t  2.3929382970522894 \t 3.8177647120619196\n",
            "65     \t [0.81359004 0.76129887 0.49975164]. \t  0.6095207961373732 \t 3.8177647120619196\n",
            "66     \t [0.06363343 0.5422766  0.31167274]. \t  0.2877952793680176 \t 3.8177647120619196\n",
            "67     \t [0.63145594 0.62557868 0.9352136 ]. \t  3.0671100960742277 \t 3.8177647120619196\n",
            "68     \t [0.50259193 0.833834   0.27032893]. \t  0.16973072351208715 \t 3.8177647120619196\n",
            "69     \t [0.86423622 0.32921701 0.78258116]. \t  2.2149879950678066 \t 3.8177647120619196\n",
            "70     \t [0.09070466 0.39075461 0.30718995]. \t  0.40511813609791275 \t 3.8177647120619196\n",
            "71     \t [0.92027701 0.90035824 0.55900314]. \t  0.4813358852570495 \t 3.8177647120619196\n",
            "72     \t [0.07396428 0.72569041 0.44195972]. \t  1.6725069055792048 \t 3.8177647120619196\n",
            "73     \t [0.38433519 0.8620133  0.47753883]. \t  2.017900529756957 \t 3.8177647120619196\n",
            "74     \t [0.70212302 0.08661498 0.22402478]. \t  0.671412664775161 \t 3.8177647120619196\n",
            "75     \t [0.43888827 0.73929972 0.25561194]. \t  0.14414037023096218 \t 3.8177647120619196\n",
            "76     \t [0.75182032 0.06531014 0.26174813]. \t  0.626724796742964 \t 3.8177647120619196\n",
            "77     \t [0.09764391 0.93899129 0.53823406]. \t  2.882858654763731 \t 3.8177647120619196\n",
            "78     \t [0.77977615 0.7150441  0.1566065 ]. \t  0.01689399623485657 \t 3.8177647120619196\n",
            "79     \t [0.1475432  0.68460258 0.02741935]. \t  0.006626606260917234 \t 3.8177647120619196\n",
            "80     \t [0.10978135 0.16938244 0.0893402 ]. \t  0.30763139312569293 \t 3.8177647120619196\n",
            "81     \t [0.91764548 0.85015027 0.7222717 ]. \t  0.9821056508060743 \t 3.8177647120619196\n",
            "82     \t [0.36101895 0.51007173 0.34447252]. \t  0.35858889577856073 \t 3.8177647120619196\n",
            "83     \t [0.55837414 0.61348561 0.73980716]. \t  2.7160211996294548 \t 3.8177647120619196\n",
            "84     \t [0.00460694 0.52167147 0.3961229 ]. \t  0.49170667493883247 \t 3.8177647120619196\n",
            "85     \t [0.31677985 0.2602402  0.8693305 ]. \t  1.7042213605703371 \t 3.8177647120619196\n",
            "86     \t [0.18255073 0.2406186  0.76028492]. \t  1.433703322006356 \t 3.8177647120619196\n",
            "87     \t [0.41735198 0.83179659 0.40232147]. \t  1.1129711832789138 \t 3.8177647120619196\n",
            "88     \t [0.79543712 0.53168433 0.72799723]. \t  2.503079766226982 \t 3.8177647120619196\n",
            "89     \t [0.53838599 0.95932856 0.43034474]. \t  1.0109630170128627 \t 3.8177647120619196\n",
            "90     \t [0.25203354 0.22257782 0.88271513]. \t  1.3217363185741255 \t 3.8177647120619196\n",
            "91     \t [0.60582574 0.32749757 0.65001159]. \t  1.0835379561877838 \t 3.8177647120619196\n",
            "92     \t [0.90664635 0.39572821 0.15957417]. \t  0.13677434322670798 \t 3.8177647120619196\n",
            "93     \t [0.0285213  0.76993271 0.50713399]. \t  2.540627034382265 \t 3.8177647120619196\n",
            "94     \t [0.55089586 0.71819762 0.38418863]. \t  0.5712272229635553 \t 3.8177647120619196\n",
            "95     \t [0.90917994 0.50707355 0.56146547]. \t  0.5385477977482964 \t 3.8177647120619196\n",
            "96     \t [0.41922349 0.57565327 0.70638927]. \t  2.4807373000665214 \t 3.8177647120619196\n",
            "97     \t [0.92676762 0.83831109 0.71541354]. \t  0.9938525433078214 \t 3.8177647120619196\n",
            "98     \t [0.67512551 0.55953668 0.47623847]. \t  0.47444785276726686 \t 3.8177647120619196\n",
            "99     \t [0.14963312 0.34455413 0.24765072]. \t  0.5208219028417973 \t 3.8177647120619196\n",
            "100    \t [0.72125536 0.92491891 0.11602707]. \t  0.0034592068050073 \t 3.8177647120619196\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QoONg4VEtdy-",
        "outputId": "4555d565-23b0-4bc6-f42b-56fb6f9c6fe4"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 13\r\n",
        "\r\n",
        "np.random.seed(run_num_13)\r\n",
        "surrogate_loser_13 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "loser_13 = dGPGO(surrogate_loser_13, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_13.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.66064431 0.41360065 0.66810256]. \t  1.5457680899590245 \t 2.6697919207500047\n",
            "init   \t [0.22994342 0.80767834 0.63681846]. \t  2.6697919207500047 \t 2.6697919207500047\n",
            "init   \t [0.17219385 0.26038587 0.91531999]. \t  1.4685599870430508 \t 2.6697919207500047\n",
            "init   \t [0.46281551 0.12019095 0.88088551]. \t  0.639297028201682 \t 2.6697919207500047\n",
            "init   \t [0.22621895 0.81144033 0.44587892]. \t  1.960293029067079 \t 2.6697919207500047\n",
            "1      \t [0.54200533 0.91610447 0.94072107]. \t  0.9345749070922738 \t 2.6697919207500047\n",
            "2      \t [0.02210984 0.92408402 0.84182838]. \t  1.1986876793966825 \t 2.6697919207500047\n",
            "3      \t [0.2551603  0.64940006 0.63764251]. \t  2.2608836438455646 \t 2.6697919207500047\n",
            "4      \t [0.66842278 0.8625381  0.47685558]. \t  0.9979464612823808 \t 2.6697919207500047\n",
            "5      \t [0.28295395 0.99213626 0.55559161]. \t  2.407164166226089 \t 2.6697919207500047\n",
            "6      \t [0.88103078 0.40276708 0.88651999]. \t  \u001b[92m2.915655348161583\u001b[0m \t 2.915655348161583\n",
            "7      \t [0.85186124 0.38309496 0.96481362]. \t  1.9341935747459023 \t 2.915655348161583\n",
            "8      \t [0.99269926 0.32021245 0.76400007]. \t  1.9945111951258503 \t 2.915655348161583\n",
            "9      \t [0.9725296  0.74703252 0.78270881]. \t  2.1046312358842396 \t 2.915655348161583\n",
            "10     \t [0.64921845 0.58807206 0.82377664]. \t  \u001b[92m3.6487778595796403\u001b[0m \t 3.6487778595796403\n",
            "11     \t [0.67136868 0.65587793 0.78364563]. \t  2.96634855335022 \t 3.6487778595796403\n",
            "12     \t [0.93628178 0.98787552 0.00269434]. \t  5.9873549531252935e-05 \t 3.6487778595796403\n",
            "13     \t [0.20452643 0.95991774 0.00283118]. \t  0.0003842936989728932 \t 3.6487778595796403\n",
            "14     \t [0.28742108 0.58414624 0.97025443]. \t  2.6197401819259363 \t 3.6487778595796403\n",
            "15     \t [0.00918113 0.97131325 0.42350288]. \t  1.5801725863686558 \t 3.6487778595796403\n",
            "16     \t [0.48726769 0.41055569 0.89191979]. \t  3.0159354059023435 \t 3.6487778595796403\n",
            "17     \t [0.96241204 0.4977143  0.74078178]. \t  2.599080513436684 \t 3.6487778595796403\n",
            "18     \t [0.74037132 0.56122025 0.89563392]. \t  3.6190597808260043 \t 3.6487778595796403\n",
            "19     \t [0.43409753 0.96598791 0.28398903]. \t  0.22325574543686724 \t 3.6487778595796403\n",
            "20     \t [2.61236103e-14 2.05086078e-13 2.40083696e-14]. \t  0.06797411659019496 \t 3.6487778595796403\n",
            "21     \t [0.07733708 0.51291034 0.03301615]. \t  0.03137606726735763 \t 3.6487778595796403\n",
            "22     \t [0.00463129 0.16486818 0.46556882]. \t  0.2530080813283786 \t 3.6487778595796403\n",
            "23     \t [0.00320505 0.76662497 0.0974604 ]. \t  0.009031250496607012 \t 3.6487778595796403\n",
            "24     \t [0.94297442 0.99829129 0.44240974]. \t  0.22086974728088565 \t 3.6487778595796403\n",
            "25     \t [0.21806645 0.88497201 0.6999076 ]. \t  2.063426247291155 \t 3.6487778595796403\n",
            "26     \t [0.20999599 0.09189476 0.71383494]. \t  0.4644809892599904 \t 3.6487778595796403\n",
            "27     \t [0.74413859 0.50011464 0.86989513]. \t  \u001b[92m3.6589177774387114\u001b[0m \t 3.6589177774387114\n",
            "28     \t [0.73999113 0.51658583 0.89231812]. \t  3.5895994430253797 \t 3.6589177774387114\n",
            "29     \t [2.42898347e-04 6.38186582e-01 9.27581282e-01]. \t  3.1001233077485315 \t 3.6589177774387114\n",
            "30     \t [0.59203947 0.80806188 0.30643635]. \t  0.23209380349430725 \t 3.6589177774387114\n",
            "31     \t [0.99851265 0.8170673  0.06665683]. \t  0.000890527374873991 \t 3.6589177774387114\n",
            "32     \t [0.68939258 0.52375207 0.88165166]. \t  \u001b[92m3.691592425297216\u001b[0m \t 3.691592425297216\n",
            "33     \t [0.71862106 0.57869395 0.48791097]. \t  0.4877635725523503 \t 3.691592425297216\n",
            "34     \t [0.71015982 0.06240761 0.63233308]. \t  0.22214275190933627 \t 3.691592425297216\n",
            "35     \t [0.29537222 0.35230617 0.91184422]. \t  2.3294800034017893 \t 3.691592425297216\n",
            "36     \t [0.12149322 0.05671003 0.94459882]. \t  0.26115399667729156 \t 3.691592425297216\n",
            "37     \t [0.99370289 0.94780616 0.6772205 ]. \t  0.4206480841228158 \t 3.691592425297216\n",
            "38     \t [0.79155526 0.75895341 0.35415099]. \t  0.20442672556046407 \t 3.691592425297216\n",
            "39     \t [0.41819026 0.01110224 0.42672782]. \t  0.42020265486539904 \t 3.691592425297216\n",
            "40     \t [0.78397085 0.65300818 0.7031766 ]. \t  1.901811970012256 \t 3.691592425297216\n",
            "41     \t [0.77590574 0.03251059 0.70689557]. \t  0.27454686586139604 \t 3.691592425297216\n",
            "42     \t [0.60493648 0.51070951 0.91896704]. \t  3.3321496123694563 \t 3.691592425297216\n",
            "43     \t [0.04359117 0.24712361 0.53820461]. \t  0.3207940100321773 \t 3.691592425297216\n",
            "44     \t [0.92602388 0.994073   0.32371181]. \t  0.07090802659903218 \t 3.691592425297216\n",
            "45     \t [0.57344005 0.61197114 0.18922437]. \t  0.07785242306504275 \t 3.691592425297216\n",
            "46     \t [0.12012265 0.14761798 0.26838165]. \t  0.8242627741310308 \t 3.691592425297216\n",
            "47     \t [0.27362221 0.36267462 0.50258601]. \t  0.44167122163239614 \t 3.691592425297216\n",
            "48     \t [0.46186667 0.12699026 0.51178451]. \t  0.23846173888788497 \t 3.691592425297216\n",
            "49     \t [0.76667116 0.6392036  0.80705852]. \t  3.2443713047496106 \t 3.691592425297216\n",
            "50     \t [0.13767661 0.66241837 0.82792657]. \t  3.425531063299217 \t 3.691592425297216\n",
            "51     \t [0.95793867 0.66615194 0.60097035]. \t  0.7192507904766757 \t 3.691592425297216\n",
            "52     \t [0.05145473 0.64851336 0.21348459]. \t  0.09474949345676345 \t 3.691592425297216\n",
            "53     \t [0.48623021 0.77377589 0.411413  ]. \t  0.9734191014544429 \t 3.691592425297216\n",
            "54     \t [0.78882542 0.13270961 0.90543571]. \t  0.6333344930577843 \t 3.691592425297216\n",
            "55     \t [0.24220402 0.25654683 0.99013943]. \t  0.8858897160755633 \t 3.691592425297216\n",
            "56     \t [0.22829701 0.05339116 0.54151275]. \t  0.16354623570708623 \t 3.691592425297216\n",
            "57     \t [0.76016192 0.74881222 0.56770784]. \t  0.9373503703113361 \t 3.691592425297216\n",
            "58     \t [0.2795376  0.58991046 0.2037006 ]. \t  0.12301042643614361 \t 3.691592425297216\n",
            "59     \t [0.31980344 0.82750827 0.11581836]. \t  0.011157555616638023 \t 3.691592425297216\n",
            "60     \t [0.20118285 0.96464589 0.24263923]. \t  0.14556622456810533 \t 3.691592425297216\n",
            "61     \t [0.51970053 0.84549219 0.4922885 ]. \t  1.6293240662063382 \t 3.691592425297216\n",
            "62     \t [0.82379254 0.81675594 0.85550943]. \t  1.9278312884785196 \t 3.691592425297216\n",
            "63     \t [0.40139966 0.0505134  0.44444517]. \t  0.38492138667391107 \t 3.691592425297216\n",
            "64     \t [0.41306454 0.59975919 0.95354358]. \t  2.885245533239425 \t 3.691592425297216\n",
            "65     \t [0.90762895 0.09043539 0.35045986]. \t  0.33954753185910946 \t 3.691592425297216\n",
            "66     \t [0.56437488 0.00299382 0.07478562]. \t  0.2575720757158796 \t 3.691592425297216\n",
            "67     \t [0.04035773 0.82036326 0.80315336]. \t  2.0799583721374217 \t 3.691592425297216\n",
            "68     \t [0.17023444 0.85258575 0.91493466]. \t  1.5565545405274104 \t 3.691592425297216\n",
            "69     \t [0.87734285 0.27734693 0.62186276]. \t  0.6661400065856827 \t 3.691592425297216\n",
            "70     \t [0.85442665 0.36765057 0.29538047]. \t  0.2636416785367353 \t 3.691592425297216\n",
            "71     \t [0.65398105 0.18685371 0.2556733 ]. \t  0.7441875730102886 \t 3.691592425297216\n",
            "72     \t [0.16483906 0.02722539 0.07838903]. \t  0.27911575215119816 \t 3.691592425297216\n",
            "73     \t [0.88555469 0.94382983 0.28432556]. \t  0.05271556107938427 \t 3.691592425297216\n",
            "74     \t [0.68988585 0.09996215 0.53871476]. \t  0.1713866911101003 \t 3.691592425297216\n",
            "75     \t [0.0180464  0.85478503 0.24884179]. \t  0.17919690815782152 \t 3.691592425297216\n",
            "76     \t [0.78321123 0.04993389 0.03396936]. \t  0.11155547807075329 \t 3.691592425297216\n",
            "77     \t [0.83488428 0.17038079 0.32578126]. \t  0.4593399119267525 \t 3.691592425297216\n",
            "78     \t [0.00337352 0.81377532 0.47141789]. \t  2.3007426133611486 \t 3.691592425297216\n",
            "79     \t [0.44659059 0.34840865 0.05190856]. \t  0.14299750465571026 \t 3.691592425297216\n",
            "80     \t [0.82874866 0.46665737 0.88077643]. \t  3.426233378521397 \t 3.691592425297216\n",
            "81     \t [0.44960303 0.01595051 0.18324486]. \t  0.716359984072404 \t 3.691592425297216\n",
            "82     \t [0.24540929 0.96089703 0.58222289]. \t  2.6288618583053487 \t 3.691592425297216\n",
            "83     \t [0.60404809 0.72598803 0.35607217]. \t  0.37369385522183546 \t 3.691592425297216\n",
            "84     \t [0.48995829 0.47442484 0.90045138]. \t  3.391988206652351 \t 3.691592425297216\n",
            "85     \t [0.86483266 0.77760985 0.20264433]. \t  0.017360610657446037 \t 3.691592425297216\n",
            "86     \t [0.96077586 0.70534906 0.83842714]. \t  2.8847770418582708 \t 3.691592425297216\n",
            "87     \t [0.46111202 0.66460214 0.78780807]. \t  3.0911449545854603 \t 3.691592425297216\n",
            "88     \t [0.17321434 0.20357407 0.55873031]. \t  0.3164822550587725 \t 3.691592425297216\n",
            "89     \t [0.03865646 0.34254306 0.56508583]. \t  0.5978670329887741 \t 3.691592425297216\n",
            "90     \t [0.79060194 0.25759698 0.43351173]. \t  0.24940990598258378 \t 3.691592425297216\n",
            "91     \t [0.32236939 0.42199787 0.28337111]. \t  0.42707895490724906 \t 3.691592425297216\n",
            "92     \t [0.67223202 0.36264268 0.17064459]. \t  0.31461328905573827 \t 3.691592425297216\n",
            "93     \t [0.62734209 0.15873131 0.39466884]. \t  0.505366693689942 \t 3.691592425297216\n",
            "94     \t [0.86554748 0.19661497 0.24919996]. \t  0.4438912830916337 \t 3.691592425297216\n",
            "95     \t [0.98606367 0.93178489 0.99407399]. \t  0.5702372182084068 \t 3.691592425297216\n",
            "96     \t [0.13527758 0.02125589 0.12677246]. \t  0.42834497445761877 \t 3.691592425297216\n",
            "97     \t [0.6710944  0.88514049 0.09061597]. \t  0.0026314471908069825 \t 3.691592425297216\n",
            "98     \t [0.13400302 0.73596897 0.96113073]. \t  2.091712838707545 \t 3.691592425297216\n",
            "99     \t [0.8586582  0.53204264 0.85304921]. \t  \u001b[92m3.721611532889103\u001b[0m \t 3.721611532889103\n",
            "100    \t [0.86204299 0.60518289 0.98541934]. \t  2.265846033509685 \t 3.721611532889103\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CoQHSe2Dtd1m",
        "outputId": "75f2e1f1-1d75-421e-d398-4f7b5a516af8"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 14\r\n",
        "\r\n",
        "np.random.seed(run_num_14)\r\n",
        "surrogate_loser_14 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "loser_14 = dGPGO(surrogate_loser_14, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_14.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.64755105 0.50714969 0.52834138]. \t  0.5963212059988954 \t 2.610000357863649\n",
            "init   \t [0.8962852  0.69999119 0.7142971 ]. \t  1.7197848290620104 \t 2.610000357863649\n",
            "init   \t [0.71733838 0.22281946 0.17515452]. \t  0.48166052848103497 \t 2.610000357863649\n",
            "init   \t [0.45684149 0.92873843 0.00988589]. \t  0.0004588015757462679 \t 2.610000357863649\n",
            "init   \t [0.08992219 0.85020027 0.48562106]. \t  2.610000357863649 \t 2.610000357863649\n",
            "1      \t [0.07426471 0.99752623 0.88021278]. \t  0.6630933245090485 \t 2.610000357863649\n",
            "2      \t [0.00180211 0.46271645 0.11240184]. \t  0.09986378420438345 \t 2.610000357863649\n",
            "3      \t [0.04945322 0.96746298 0.34336512]. \t  0.71192198606886 \t 2.610000357863649\n",
            "4      \t [0.95164465 0.9953009  0.00385032]. \t  5.429819046242939e-05 \t 2.610000357863649\n",
            "5      \t [0.9979422  0.99655151 0.98186954]. \t  0.37591226277898976 \t 2.610000357863649\n",
            "6      \t [0.95837713 0.62910703 0.12515201]. \t  0.014715862521621976 \t 2.610000357863649\n",
            "7      \t [0.05380284 0.69099212 0.61559705]. \t  2.477025978003247 \t 2.610000357863649\n",
            "8      \t [0.4099993  0.92486099 0.52611019]. \t  2.2038453653786894 \t 2.610000357863649\n",
            "9      \t [0.12502002 0.82889781 0.49304721]. \t  \u001b[92m2.6590433374920455\u001b[0m \t 2.6590433374920455\n",
            "10     \t [0.0432031  0.81394199 0.52604994]. \t  \u001b[92m2.863126326791752\u001b[0m \t 2.863126326791752\n",
            "11     \t [0.93728378 0.01172871 0.05842301]. \t  0.0917317165535674 \t 2.863126326791752\n",
            "12     \t [0.04830445 0.79599636 0.56926789]. \t  \u001b[92m2.94888161804253\u001b[0m \t 2.94888161804253\n",
            "13     \t [0.96219208 0.92550704 0.48515165]. \t  0.2980789003451671 \t 2.94888161804253\n",
            "14     \t [0.49305204 0.0469595  0.        ]. \t  0.10658903029135189 \t 2.94888161804253\n",
            "15     \t [0.9725237  0.48844653 0.87837032]. \t  \u001b[92m3.4955070864224314\u001b[0m \t 3.4955070864224314\n",
            "16     \t [0.97765512 0.28908983 0.8882185 ]. \t  1.8331935020921892 \t 3.4955070864224314\n",
            "17     \t [0.82336931 0.69094476 0.99535318]. \t  1.8317234802335283 \t 3.4955070864224314\n",
            "18     \t [0.96873907 0.56322882 0.9383951 ]. \t  3.050598972616365 \t 3.4955070864224314\n",
            "19     \t [0.98414681 0.41566923 0.49281313]. \t  0.19462008725546937 \t 3.4955070864224314\n",
            "20     \t [0.55403896 0.41378808 0.03051479]. \t  0.0696049977228835 \t 3.4955070864224314\n",
            "21     \t [0.82438027 0.50574454 0.9092123 ]. \t  3.376368871912553 \t 3.4955070864224314\n",
            "22     \t [0.50527634 0.4021167  0.905708  ]. \t  2.827435466911807 \t 3.4955070864224314\n",
            "23     \t [0.33973592 0.83048357 0.79975323]. \t  1.9524561078382523 \t 3.4955070864224314\n",
            "24     \t [0.65056615 0.4367664  0.9827399 ]. \t  2.025405392267641 \t 3.4955070864224314\n",
            "25     \t [0.15264283 0.02107091 0.5549017 ]. \t  0.12891552497089895 \t 3.4955070864224314\n",
            "26     \t [0.5351411  0.09445077 0.68338355]. \t  0.4051948265659164 \t 3.4955070864224314\n",
            "27     \t [0.28528739 0.46336557 0.26731631]. \t  0.33819052205087036 \t 3.4955070864224314\n",
            "28     \t [0.14266977 0.41799958 0.07239393]. \t  0.11124283833397586 \t 3.4955070864224314\n",
            "29     \t [0.55257834 0.99265052 0.26703074]. \t  0.1208697033520126 \t 3.4955070864224314\n",
            "30     \t [0.01806789 0.96107533 0.70194425]. \t  1.71026065747257 \t 3.4955070864224314\n",
            "31     \t [0.6096993  0.99899235 0.85987141]. \t  0.6218218845016854 \t 3.4955070864224314\n",
            "32     \t [0.47334064 0.29370186 0.6061317 ]. \t  0.6727834909940495 \t 3.4955070864224314\n",
            "33     \t [0.07255508 0.10238236 0.99487251]. \t  0.2646432290137243 \t 3.4955070864224314\n",
            "34     \t [0.07668192 0.4967238  0.99851593]. \t  1.9951281424451577 \t 3.4955070864224314\n",
            "35     \t [0.16316254 0.77607392 0.10225318]. \t  0.01087632488374639 \t 3.4955070864224314\n",
            "36     \t [0.17653826 0.21738289 0.96215749]. \t  0.8595982659733136 \t 3.4955070864224314\n",
            "37     \t [0. 0. 0.]. \t  0.06797411659013229 \t 3.4955070864224314\n",
            "38     \t [0.91865577 0.36275365 0.1912791 ]. \t  0.1862391981836753 \t 3.4955070864224314\n",
            "39     \t [0.41527166 0.64813462 0.16672562]. \t  0.058604953951955356 \t 3.4955070864224314\n",
            "40     \t [0.79917452 0.98775728 0.7286888 ]. \t  0.5596406347526663 \t 3.4955070864224314\n",
            "41     \t [0.39789933 0.57798059 0.91184543]. \t  \u001b[92m3.509310013992248\u001b[0m \t 3.509310013992248\n",
            "42     \t [0.07907219 0.0441213  0.49501061]. \t  0.18632550644606571 \t 3.509310013992248\n",
            "43     \t [0.43329081 0.40589903 0.72027082]. \t  2.2226473633261703 \t 3.509310013992248\n",
            "44     \t [0.12907142 0.51616038 0.51373335]. \t  1.0164915943293173 \t 3.509310013992248\n",
            "45     \t [0.4413456  0.07346512 0.83536804]. \t  0.48157224856520825 \t 3.509310013992248\n",
            "46     \t [0.42542469 0.33852438 0.46777118]. \t  0.35843440517809333 \t 3.509310013992248\n",
            "47     \t [0.45205372 0.86171738 0.8772896 ]. \t  1.6020784611943335 \t 3.509310013992248\n",
            "48     \t [0.06500231 0.74732896 0.95425446]. \t  2.0866199126866483 \t 3.509310013992248\n",
            "49     \t [0.40675235 0.20604412 0.32647895]. \t  0.835147485697324 \t 3.509310013992248\n",
            "50     \t [0.51629496 0.44238532 0.11428872]. \t  0.1618554098435086 \t 3.509310013992248\n",
            "51     \t [0.01841433 0.66213843 0.44030181]. \t  1.3100501508645412 \t 3.509310013992248\n",
            "52     \t [0.42750177 0.08095008 0.22881845]. \t  0.934718699821992 \t 3.509310013992248\n",
            "53     \t [0.99132776 0.56247312 0.87898254]. \t  \u001b[92m3.6264090638744766\u001b[0m \t 3.6264090638744766\n",
            "54     \t [0.19229006 0.27583495 0.78979266]. \t  1.8392142767687603 \t 3.6264090638744766\n",
            "55     \t [0.99607341 0.95060923 0.07884714]. \t  0.00040076247048857424 \t 3.6264090638744766\n",
            "56     \t [0.72236324 0.20397123 0.68392677]. \t  0.809154011084921 \t 3.6264090638744766\n",
            "57     \t [0.82726544 0.56984414 0.91488448]. \t  3.402460992571767 \t 3.6264090638744766\n",
            "58     \t [0.4496368  0.09232231 0.65260425]. \t  0.3286036761854578 \t 3.6264090638744766\n",
            "59     \t [0.66686898 0.36122784 0.22139213]. \t  0.3992469319683874 \t 3.6264090638744766\n",
            "60     \t [0.84343985 0.47811337 0.0260408 ]. \t  0.024123015022092978 \t 3.6264090638744766\n",
            "61     \t [0.50786571 0.30890895 0.09449962]. \t  0.26672971047047095 \t 3.6264090638744766\n",
            "62     \t [0.15524704 0.78149988 0.03254962]. \t  0.002786228877873154 \t 3.6264090638744766\n",
            "63     \t [0.29530089 0.8267771  0.50007408]. \t  2.4621184302121764 \t 3.6264090638744766\n",
            "64     \t [0.73916819 0.92720124 0.92738278]. \t  0.8970108912251867 \t 3.6264090638744766\n",
            "65     \t [0.67444173 0.44494752 0.87215447]. \t  3.372482011479299 \t 3.6264090638744766\n",
            "66     \t [0.56649389 0.50233033 0.51945521]. \t  0.6376902576687437 \t 3.6264090638744766\n",
            "67     \t [0.28076717 0.11678024 0.47315713]. \t  0.3132157828560709 \t 3.6264090638744766\n",
            "68     \t [0.34975716 0.55735162 0.44800429]. \t  0.7645979471726574 \t 3.6264090638744766\n",
            "69     \t [0.79661882 0.26738063 0.53155921]. \t  0.2656598661584797 \t 3.6264090638744766\n",
            "70     \t [0.14360166 0.87285564 0.22607183]. \t  0.11979374408608724 \t 3.6264090638744766\n",
            "71     \t [0.20273741 0.68270988 0.68133879]. \t  2.5067715263777925 \t 3.6264090638744766\n",
            "72     \t [0.10236774 0.30777171 0.08361548]. \t  0.2042348819686146 \t 3.6264090638744766\n",
            "73     \t [0.67858085 0.6219549  0.55325466]. \t  0.9070086533155038 \t 3.6264090638744766\n",
            "74     \t [0.14445184 0.18590658 0.93168239]. \t  0.8378508615508395 \t 3.6264090638744766\n",
            "75     \t [0.33188654 0.9244444  0.23513361]. \t  0.11906325846609561 \t 3.6264090638744766\n",
            "76     \t [0.48097666 0.13009226 0.29282477]. \t  0.9441138008571816 \t 3.6264090638744766\n",
            "77     \t [0.32990587 0.58124315 0.16500403]. \t  0.09588791985837758 \t 3.6264090638744766\n",
            "78     \t [0.55142752 0.00357512 0.70672671]. \t  0.21495598343963285 \t 3.6264090638744766\n",
            "79     \t [0.49708493 0.55071354 0.15142348]. \t  0.10212397980838571 \t 3.6264090638744766\n",
            "80     \t [0.83839049 0.61356518 0.90770868]. \t  3.37999894933908 \t 3.6264090638744766\n",
            "81     \t [0.10514481 0.86268084 0.29104502]. \t  0.3755481141119861 \t 3.6264090638744766\n",
            "82     \t [0.77926204 0.03190372 0.28453967]. \t  0.5564609157951522 \t 3.6264090638744766\n",
            "83     \t [0.0844148  0.96013624 0.30520214]. \t  0.42971674692981515 \t 3.6264090638744766\n",
            "84     \t [0.91312378 0.47041951 0.20526919]. \t  0.1073063230476932 \t 3.6264090638744766\n",
            "85     \t [0.90271173 0.30248265 0.60457918]. \t  0.6061828342528154 \t 3.6264090638744766\n",
            "86     \t [0.56818296 0.13607622 0.98090366]. \t  0.40439730435804294 \t 3.6264090638744766\n",
            "87     \t [0.02036094 0.91618381 0.91600987]. \t  1.0458528763005754 \t 3.6264090638744766\n",
            "88     \t [0.58408506 0.22139976 0.15178313]. \t  0.5231401031426556 \t 3.6264090638744766\n",
            "89     \t [0.7460948 0.9881278 0.4693557]. \t  0.6309118965643825 \t 3.6264090638744766\n",
            "90     \t [0.64502969 0.7036153  0.33497261]. \t  0.24718341667757182 \t 3.6264090638744766\n",
            "91     \t [0.81336943 0.7955296  0.2408704 ]. \t  0.038691247046843534 \t 3.6264090638744766\n",
            "92     \t [0.22908182 0.34958393 0.70349495]. \t  1.7538685921576378 \t 3.6264090638744766\n",
            "93     \t [0.02556752 0.98745888 0.72362373]. \t  1.3883167758583452 \t 3.6264090638744766\n",
            "94     \t [0.59237836 0.93434985 0.51741072]. \t  1.4000596558201313 \t 3.6264090638744766\n",
            "95     \t [0.07210507 0.7999767  0.69767257]. \t  2.4017222558308857 \t 3.6264090638744766\n",
            "96     \t [0.01540501 0.18169681 0.53582139]. \t  0.23750339174042914 \t 3.6264090638744766\n",
            "97     \t [0.38957153 0.32283297 0.30482957]. \t  0.6454892445601828 \t 3.6264090638744766\n",
            "98     \t [0.38533378 0.41887816 0.54780256]. \t  0.6877116008288234 \t 3.6264090638744766\n",
            "99     \t [0.14901495 0.8771433  0.04026063]. \t  0.001634100766366568 \t 3.6264090638744766\n",
            "100    \t [0.62120192 0.76496167 0.20277625]. \t  0.04053907768932854 \t 3.6264090638744766\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aq4J_FTytd4O",
        "outputId": "ac18c423-003a-468a-f749-1777163815cd"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 15\r\n",
        "\r\n",
        "np.random.seed(run_num_15)\r\n",
        "surrogate_loser_15 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "loser_15 = dGPGO(surrogate_loser_15, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_15.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.57788186 0.25550133 0.25252687]. \t  0.7217338409961529 \t 1.540625560354162\n",
            "init   \t [0.70990435 0.44755236 0.22694296]. \t  0.23199690080875204 \t 1.540625560354162\n",
            "init   \t [0.40170957 0.88225774 0.43721347]. \t  1.540625560354162 \t 1.540625560354162\n",
            "init   \t [0.87842518 0.78052775 0.53421429]. \t  0.5653855252190516 \t 1.540625560354162\n",
            "init   \t [0.01173301 0.26575648 0.3311941 ]. \t  0.5019682819961726 \t 1.540625560354162\n",
            "1      \t [0.45870705 0.97826428 0.97993811]. \t  0.46749494069699443 \t 1.540625560354162\n",
            "2      \t [0.66075471 0.         0.61312367]. \t  0.12159642826997123 \t 1.540625560354162\n",
            "3      \t [0.46645428 0.99752683 0.41295545]. \t  0.9605705723922775 \t 1.540625560354162\n",
            "4      \t [0.34390964 0.64896828 0.49739212]. \t  1.5093166326574845 \t 1.540625560354162\n",
            "5      \t [0.05653148 0.81602006 0.23121467]. \t  0.13016751165670606 \t 1.540625560354162\n",
            "6      \t [0.29839982 0.94779509 0.61815286]. \t  \u001b[92m2.376883919188496\u001b[0m \t 2.376883919188496\n",
            "7      \t [0.20225734 0.91762416 0.5993738 ]. \t  \u001b[92m2.8143502335642547\u001b[0m \t 2.8143502335642547\n",
            "8      \t [0.05726953 0.95949547 0.73838485]. \t  1.4281001613955855 \t 2.8143502335642547\n",
            "9      \t [0.96497544 0.07697573 0.01139424]. \t  0.04752173213497603 \t 2.8143502335642547\n",
            "10     \t [1. 1. 1.]. \t  0.31688362070415665 \t 2.8143502335642547\n",
            "11     \t [0.06083111 0.95090286 0.46023173]. \t  2.1523413244692358 \t 2.8143502335642547\n",
            "12     \t [0.29544804 0.00749755 0.1138019 ]. \t  0.4304462537200079 \t 2.8143502335642547\n",
            "13     \t [0.9788367  0.13663849 0.46305662]. \t  0.1333115859356235 \t 2.8143502335642547\n",
            "14     \t [0.99248475 0.89393169 0.25379208]. \t  0.019773995845805473 \t 2.8143502335642547\n",
            "15     \t [0.12141814 0.77615604 0.59627157]. \t  \u001b[92m2.88255169201502\u001b[0m \t 2.88255169201502\n",
            "16     \t [0.09484896 0.05628241 0.76753651]. \t  0.40772636461818845 \t 2.88255169201502\n",
            "17     \t [0.98855647 0.64396695 0.96994006]. \t  2.387841970988428 \t 2.88255169201502\n",
            "18     \t [0.10907278 0.82392297 0.59561025]. \t  \u001b[92m3.000256288747202\u001b[0m \t 3.000256288747202\n",
            "19     \t [0.96921191 0.20641809 0.99642329]. \t  0.5856509656909898 \t 3.000256288747202\n",
            "20     \t [0.99071045 0.11175798 0.0756385 ]. \t  0.10411761062601754 \t 3.000256288747202\n",
            "21     \t [0.17049127 0.30955908 0.5032451 ]. \t  0.36574896563945947 \t 3.000256288747202\n",
            "22     \t [0.35626726 0.30263896 0.91762246]. \t  1.8275297317309531 \t 3.000256288747202\n",
            "23     \t [0.26082213 0.52658403 0.99305453]. \t  2.176598638221769 \t 3.000256288747202\n",
            "24     \t [0.75812964 0.4744     0.99163431]. \t  2.031412034788625 \t 3.000256288747202\n",
            "25     \t [0.30805774 0.03183635 0.97626546]. \t  0.1605391479077385 \t 3.000256288747202\n",
            "26     \t [0.67703224 0.08606007 0.20809089]. \t  0.6706776152497954 \t 3.000256288747202\n",
            "27     \t [0.22027058 0.82222382 0.71245758]. \t  2.222960151813486 \t 3.000256288747202\n",
            "28     \t [0.04551279 0.41571432 0.91993079]. \t  2.7629813981502034 \t 3.000256288747202\n",
            "29     \t [0.02701892 0.73247455 0.54921471]. \t  2.5873896378178856 \t 3.000256288747202\n",
            "30     \t [0.16202041 0.86821179 0.56604579]. \t  \u001b[92m3.062704393865933\u001b[0m \t 3.062704393865933\n",
            "31     \t [0.37857704 0.4008916  0.57095718]. \t  0.7635747625375403 \t 3.062704393865933\n",
            "32     \t [0.09185293 0.74550382 0.61485494]. \t  2.7270945552334847 \t 3.062704393865933\n",
            "33     \t [0.81464089 0.20410306 0.19092096]. \t  0.42888437233317794 \t 3.062704393865933\n",
            "34     \t [0.85842146 0.29924661 0.15859815]. \t  0.24543263868828105 \t 3.062704393865933\n",
            "35     \t [0.03667396 0.16455783 0.93639795]. \t  0.6920956476994375 \t 3.062704393865933\n",
            "36     \t [0.01042273 0.54901765 0.83205998]. \t  \u001b[92m3.77490607109067\u001b[0m \t 3.77490607109067\n",
            "37     \t [0.09235068 0.59453282 0.5299634 ]. \t  1.5829305238168287 \t 3.77490607109067\n",
            "38     \t [0.13771917 0.43744959 0.88943158]. \t  3.232777206779901 \t 3.77490607109067\n",
            "39     \t [0.05362766 0.30928959 0.06249417]. \t  0.14577172423795093 \t 3.77490607109067\n",
            "40     \t [0.03454882 0.5530963  0.43818732]. \t  0.7889596293821415 \t 3.77490607109067\n",
            "41     \t [0.06175869 0.84298002 0.17074262]. \t  0.03836987173902284 \t 3.77490607109067\n",
            "42     \t [0.17364572 0.64082997 0.5845864 ]. \t  2.1333882315447505 \t 3.77490607109067\n",
            "43     \t [0.19228284 0.34767431 0.35689909]. \t  0.48325991939194524 \t 3.77490607109067\n",
            "44     \t [0.23354636 0.46147258 0.26207807]. \t  0.32929384743697176 \t 3.77490607109067\n",
            "45     \t [0.55513085 0.78585576 0.17222904]. \t  0.026846057895913045 \t 3.77490607109067\n",
            "46     \t [0.32640468 0.99936566 0.57634998]. \t  2.227868485959561 \t 3.77490607109067\n",
            "47     \t [0.10589905 0.5469575  0.0158781 ]. \t  0.019376708817783993 \t 3.77490607109067\n",
            "48     \t [0.72025457 0.42821853 0.89108759]. \t  3.1298633504098006 \t 3.77490607109067\n",
            "49     \t [0.95041933 0.47369464 0.77930951]. \t  3.0573049834269135 \t 3.77490607109067\n",
            "50     \t [0.37463577 0.52589471 0.87634371]. \t  3.7726339942689924 \t 3.77490607109067\n",
            "51     \t [0.80954807 0.70749514 0.87223495]. \t  2.9788339110325452 \t 3.77490607109067\n",
            "52     \t [0.04936964 0.76643056 0.27365458]. \t  0.25866039619980574 \t 3.77490607109067\n",
            "53     \t [0.20524069 0.76502703 0.87035564]. \t  2.5771584611667055 \t 3.77490607109067\n",
            "54     \t [0.44579854 0.10044851 0.46713745]. \t  0.32543707520716264 \t 3.77490607109067\n",
            "55     \t [0.76353136 0.02023589 0.33067034]. \t  0.5065642586272455 \t 3.77490607109067\n",
            "56     \t [0.05689745 0.55302702 0.21793852]. \t  0.139296626823076 \t 3.77490607109067\n",
            "57     \t [0.93704072 0.31603255 0.75654058]. \t  1.9235605761300654 \t 3.77490607109067\n",
            "58     \t [0.92634453 0.55250125 0.37701181]. \t  0.10624015191215301 \t 3.77490607109067\n",
            "59     \t [0.98566888 0.93332484 0.38748483]. \t  0.12626546681121031 \t 3.77490607109067\n",
            "60     \t [0.52099922 0.16674173 0.54409066]. \t  0.2500879587637723 \t 3.77490607109067\n",
            "61     \t [0.45686738 0.7185135  0.97018309]. \t  2.0936312622869466 \t 3.77490607109067\n",
            "62     \t [0.38554434 0.52484819 0.3410793 ]. \t  0.3446974468763136 \t 3.77490607109067\n",
            "63     \t [0.48744492 0.75878045 0.40262775]. \t  0.8724685464007343 \t 3.77490607109067\n",
            "64     \t [0.32494146 0.55551894 0.11106917]. \t  0.07247095796871711 \t 3.77490607109067\n",
            "65     \t [0.22437916 0.18511023 0.39884955]. \t  0.554974513879724 \t 3.77490607109067\n",
            "66     \t [0.66195118 0.83031165 0.1263408 ]. \t  0.007416368719395564 \t 3.77490607109067\n",
            "67     \t [0.45277534 0.90837962 0.37948517]. \t  0.8302928560503484 \t 3.77490607109067\n",
            "68     \t [0.11030779 0.93358032 0.79663281]. \t  1.2673113215586038 \t 3.77490607109067\n",
            "69     \t [0.75689381 0.26626376 0.40769996]. \t  0.3093906641862753 \t 3.77490607109067\n",
            "70     \t [0.25249093 0.86169084 0.93775836]. \t  1.3487665227408552 \t 3.77490607109067\n",
            "71     \t [0.07417741 0.78342391 0.77384638]. \t  2.3549929045130416 \t 3.77490607109067\n",
            "72     \t [0.47843585 0.0947401  0.45417235]. \t  0.35892328654689665 \t 3.77490607109067\n",
            "73     \t [0.0028475  0.80972295 0.83880453]. \t  2.1616271984147324 \t 3.77490607109067\n",
            "74     \t [0.18412308 0.40611329 0.4560012 ]. \t  0.4478611342438572 \t 3.77490607109067\n",
            "75     \t [0.72015424 0.33382483 0.25526604]. \t  0.43357330396654686 \t 3.77490607109067\n",
            "76     \t [0.63260961 0.21704913 0.29129282]. \t  0.7245423555057698 \t 3.77490607109067\n",
            "77     \t [0.55578338 0.43008045 0.87967857]. \t  3.2470912244888375 \t 3.77490607109067\n",
            "78     \t [0.12989691 0.37828551 0.63240513]. \t  1.1871928238269298 \t 3.77490607109067\n",
            "79     \t [0.53736537 0.60544502 0.00838678]. \t  0.01141987847330488 \t 3.77490607109067\n",
            "80     \t [0.53955518 0.92113759 0.71411087]. \t  1.251299842896169 \t 3.77490607109067\n",
            "81     \t [0.78266961 0.80364241 0.95875221]. \t  1.5914169852142543 \t 3.77490607109067\n",
            "82     \t [0.98124241 0.27066251 0.49880464]. \t  0.16785373269959117 \t 3.77490607109067\n",
            "83     \t [0.86908715 0.7723944  0.27931136]. \t  0.05584375404926373 \t 3.77490607109067\n",
            "84     \t [0.18191085 0.87279776 0.06389839]. \t  0.0030067773074108337 \t 3.77490607109067\n",
            "85     \t [0.68267667 0.23806959 0.53615028]. \t  0.27687692743916364 \t 3.77490607109067\n",
            "86     \t [0.51580088 0.11030196 0.4589715 ]. \t  0.33837634771568686 \t 3.77490607109067\n",
            "87     \t [0.96852447 0.15481291 0.01517424]. \t  0.049788937359754426 \t 3.77490607109067\n",
            "88     \t [0.57673346 0.28305391 0.46648613]. \t  0.3037232717970031 \t 3.77490607109067\n",
            "89     \t [0.74398022 0.26145588 0.77234634]. \t  1.630304906430959 \t 3.77490607109067\n",
            "90     \t [0.9634963  0.27587429 0.56715608]. \t  0.35600452235267915 \t 3.77490607109067\n",
            "91     \t [0.73619882 0.71342445 0.18271428]. \t  0.026606237937590572 \t 3.77490607109067\n",
            "92     \t [0.57409829 0.41833827 0.3448306 ]. \t  0.3538976036382143 \t 3.77490607109067\n",
            "93     \t [0.56102639 0.06679605 0.64037298]. \t  0.2483887387972291 \t 3.77490607109067\n",
            "94     \t [0.93882985 0.30550525 0.23690212]. \t  0.25813917911444156 \t 3.77490607109067\n",
            "95     \t [0.05289148 0.90095206 0.18316854]. \t  0.04819088234293035 \t 3.77490607109067\n",
            "96     \t [0.47581993 0.98761237 0.71369248]. \t  1.0932033178536977 \t 3.77490607109067\n",
            "97     \t [0.79326914 0.88886517 0.95242994]. \t  1.0291534985796806 \t 3.77490607109067\n",
            "98     \t [0.90504706 0.25651419 0.73046914]. \t  1.3447868969921708 \t 3.77490607109067\n",
            "99     \t [0.35541618 0.87374982 0.17466265]. \t  0.03535399187079531 \t 3.77490607109067\n",
            "100    \t [0.61162121 0.13982352 0.67154308]. \t  0.513200668672658 \t 3.77490607109067\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4tJ-ta9Gtd7H",
        "outputId": "f3a97799-5ba2-4ed7-d644-f6fd1a129c5e"
      },
      "source": [
        "\r\n",
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 16\r\n",
        "\r\n",
        "np.random.seed(run_num_16)\r\n",
        "surrogate_loser_16 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "loser_16 = dGPGO(surrogate_loser_16, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_16.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.86394833 0.316366   0.67215078]. \t  1.1947633822631252 \t 3.8084053754826726\n",
            "init   \t [0.50791167 0.78166043 0.28368168]. \t  0.20052765717741963 \t 3.8084053754826726\n",
            "init   \t [0.23370878 0.56379969 0.87502436]. \t  3.8084053754826726 \t 3.8084053754826726\n",
            "init   \t [0.71894292 0.18213174 0.24380041]. \t  0.6532826728302027 \t 3.8084053754826726\n",
            "init   \t [0.08605673 0.53424539 0.83969965]. \t  3.8072565344663847 \t 3.8084053754826726\n",
            "1      \t [0.12378065 0.3168353  0.9881258 ]. \t  1.2451880833929392 \t 3.8084053754826726\n",
            "2      \t [0.27777137 0.94321124 0.96390235]. \t  0.6864412627036871 \t 3.8084053754826726\n",
            "3      \t [0.04467565 0.50685234 0.49507307]. \t  0.8672168540674181 \t 3.8084053754826726\n",
            "4      \t [0.00628562 0.62733695 0.98683809]. \t  2.216178855902818 \t 3.8084053754826726\n",
            "5      \t [0.24302952 0.56104258 0.71810923]. \t  2.714059998275844 \t 3.8084053754826726\n",
            "6      \t [0.96778771 0.7812586  0.04735995]. \t  0.0011023859383055771 \t 3.8084053754826726\n",
            "7      \t [0.94597768 0.01177013 0.21221742]. \t  0.30096480916717455 \t 3.8084053754826726\n",
            "8      \t [1. 1. 1.]. \t  0.31688362070422427 \t 3.8084053754826726\n",
            "9      \t [0.86553754 0.59748584 0.96361124]. \t  2.6531081511639076 \t 3.8084053754826726\n",
            "10     \t [0.64363474 0.74123114 0.01847141]. \t  0.002719251460440959 \t 3.8084053754826726\n",
            "11     \t [0.50952739 0.70630666 0.95920627]. \t  2.319429064390622 \t 3.8084053754826726\n",
            "12     \t [0.85522449 0.97974245 0.20836416]. \t  0.014056808087911507 \t 3.8084053754826726\n",
            "13     \t [0.90774286 0.35181844 0.01944449]. \t  0.03818621659189494 \t 3.8084053754826726\n",
            "14     \t [0.10442144 0.99757424 0.15075461]. \t  0.019461290813885892 \t 3.8084053754826726\n",
            "15     \t [0.14617975 0.59194485 0.82341198]. \t  3.724750123953375 \t 3.8084053754826726\n",
            "16     \t [1.         0.50084317 0.99314177]. \t  2.040382274736559 \t 3.8084053754826726\n",
            "17     \t [0.98906708 0.49449814 0.55793286]. \t  0.4720653973501239 \t 3.8084053754826726\n",
            "18     \t [0.62536537 0.37354973 0.93829796]. \t  2.2286937559849958 \t 3.8084053754826726\n",
            "19     \t [0.18821051 0.55334529 0.97120305]. \t  2.600081052146005 \t 3.8084053754826726\n",
            "20     \t [2.77424263e-13 8.12241987e-15 8.50081015e-15]. \t  0.06797411659018458 \t 3.8084053754826726\n",
            "21     \t [0.04213704 0.63940159 0.82388213]. \t  3.5206016329691163 \t 3.8084053754826726\n",
            "22     \t [0.05848135 0.47135776 0.81994148]. \t  3.5213504013127426 \t 3.8084053754826726\n",
            "23     \t [0.02438986 0.40362393 0.08475438]. \t  0.11377877239053279 \t 3.8084053754826726\n",
            "24     \t [0.9949863  0.74522997 0.86488097]. \t  2.5691256948847254 \t 3.8084053754826726\n",
            "25     \t [0.0918845  0.35222564 0.4215209 ]. \t  0.3685063985974734 \t 3.8084053754826726\n",
            "26     \t [0.52488244 0.425859   0.11355576]. \t  0.17691971367879086 \t 3.8084053754826726\n",
            "27     \t [0.6074091  0.5651644  0.76113511]. \t  3.062024712880875 \t 3.8084053754826726\n",
            "28     \t [0.22418673 0.0710134  0.17935842]. \t  0.7291417604546632 \t 3.8084053754826726\n",
            "29     \t [0.23621156 0.77273158 0.72314536]. \t  2.3689595603050706 \t 3.8084053754826726\n",
            "30     \t [0.77679747 0.65878278 0.78907801]. \t  2.9409868761613063 \t 3.8084053754826726\n",
            "31     \t [0.45034395 0.31184179 0.49118223]. \t  0.34019795084950083 \t 3.8084053754826726\n",
            "32     \t [0.46134655 0.08631294 0.92137812]. \t  0.4002426920815167 \t 3.8084053754826726\n",
            "33     \t [0.19064436 0.41679247 0.29594489]. \t  0.4111794760569056 \t 3.8084053754826726\n",
            "34     \t [0.75430526 0.46631548 0.45988832]. \t  0.2589634023281027 \t 3.8084053754826726\n",
            "35     \t [0.13569583 0.80181312 0.26061919]. \t  0.2203440587860694 \t 3.8084053754826726\n",
            "36     \t [0.0210929  0.27206192 0.20612286]. \t  0.490978954096385 \t 3.8084053754826726\n",
            "37     \t [0.02629512 0.25758702 0.77117475]. \t  1.5962871548899078 \t 3.8084053754826726\n",
            "38     \t [0.0730912  0.3120373  0.68606544]. \t  1.373801062907681 \t 3.8084053754826726\n",
            "39     \t [0.60526256 0.23723161 0.17647999]. \t  0.5717865799947605 \t 3.8084053754826726\n",
            "40     \t [0.72925291 0.17985652 0.1038405 ]. \t  0.292122585001201 \t 3.8084053754826726\n",
            "41     \t [0.79270642 0.632309   0.21490531]. \t  0.050736446991021644 \t 3.8084053754826726\n",
            "42     \t [0.74023708 0.53345148 0.69811026]. \t  2.097988154667948 \t 3.8084053754826726\n",
            "43     \t [0.93793031 0.3497155  0.18397546]. \t  0.1792599539864782 \t 3.8084053754826726\n",
            "44     \t [0.86734098 0.14622375 0.46534638]. \t  0.1795843969354928 \t 3.8084053754826726\n",
            "45     \t [0.32042685 0.21356485 0.69082321]. \t  0.909072075181484 \t 3.8084053754826726\n",
            "46     \t [0.93574892 0.27989918 0.01027377]. \t  0.04031023584034215 \t 3.8084053754826726\n",
            "47     \t [0.81706479 0.81081815 0.35408109]. \t  0.1967663213350807 \t 3.8084053754826726\n",
            "48     \t [0.87105988 0.83328345 0.27765357]. \t  0.054612293237242315 \t 3.8084053754826726\n",
            "49     \t [0.80936618 0.41185184 0.98653548]. \t  1.8141680762717804 \t 3.8084053754826726\n",
            "50     \t [0.19768881 0.46814887 0.84863148]. \t  3.6004712755916812 \t 3.8084053754826726\n",
            "51     \t [0.96048891 0.86684945 0.3033962 ]. \t  0.05267070671118978 \t 3.8084053754826726\n",
            "52     \t [0.12271592 0.1474196  0.20314984]. \t  0.7305270817652886 \t 3.8084053754826726\n",
            "53     \t [0.17349131 0.99299808 0.26561668]. \t  0.20964188064412295 \t 3.8084053754826726\n",
            "54     \t [0.10685981 0.53430589 0.85641253]. \t  \u001b[92m3.823263664347254\u001b[0m \t 3.823263664347254\n",
            "55     \t [0.11058698 0.97447688 0.3993736 ]. \t  1.3143788435188235 \t 3.823263664347254\n",
            "56     \t [0.87576718 0.49481146 0.55244252]. \t  0.5009209227040916 \t 3.823263664347254\n",
            "57     \t [0.02274508 0.95137123 0.03512004]. \t  0.0009698546243275209 \t 3.823263664347254\n",
            "58     \t [0.38999471 0.77210159 0.1036472 ]. \t  0.010896907604143387 \t 3.823263664347254\n",
            "59     \t [0.21582292 0.48375857 0.98977518]. \t  2.1204190952489697 \t 3.823263664347254\n",
            "60     \t [0.73989052 0.66110466 0.04476188]. \t  0.007994844924721287 \t 3.823263664347254\n",
            "61     \t [0.6488681  0.22512697 0.23753612]. \t  0.6857983232125278 \t 3.823263664347254\n",
            "62     \t [0.14359178 0.42114215 0.06835466]. \t  0.10418286952472619 \t 3.823263664347254\n",
            "63     \t [0.00618998 0.96069648 0.70577431]. \t  1.666715409499641 \t 3.823263664347254\n",
            "64     \t [0.064971   0.79011856 0.66386657]. \t  2.5880114255305275 \t 3.823263664347254\n",
            "65     \t [0.13804909 0.39060505 0.22599701]. \t  0.3945123996524405 \t 3.823263664347254\n",
            "66     \t [0.97278609 0.91514663 0.22301319]. \t  0.01214201106358989 \t 3.823263664347254\n",
            "67     \t [0.47805976 0.44904643 0.69911163]. \t  2.1228685999717154 \t 3.823263664347254\n",
            "68     \t [0.77591688 0.99971211 0.4089057 ]. \t  0.3571896826106656 \t 3.823263664347254\n",
            "69     \t [0.20130951 0.18090192 0.01361808]. \t  0.1280031127309551 \t 3.823263664347254\n",
            "70     \t [0.17274771 0.89530659 0.84319324]. \t  1.4260445211381512 \t 3.823263664347254\n",
            "71     \t [0.44817697 0.83618646 0.21842817]. \t  0.07567909537410877 \t 3.823263664347254\n",
            "72     \t [0.63705785 0.06685462 0.37546693]. \t  0.5564580121519138 \t 3.823263664347254\n",
            "73     \t [0.34473282 0.70500744 0.23790905]. \t  0.12499189981633492 \t 3.823263664347254\n",
            "74     \t [0.01471892 0.52767853 0.83725391]. \t  3.770355627665735 \t 3.823263664347254\n",
            "75     \t [0.8195422  0.67247422 0.25780942]. \t  0.056289828958829925 \t 3.823263664347254\n",
            "76     \t [0.45033804 0.7720043  0.64231289]. \t  2.072030997389843 \t 3.823263664347254\n",
            "77     \t [0.95281713 0.4814589  0.13972786]. \t  0.05889852967393788 \t 3.823263664347254\n",
            "78     \t [0.21029326 0.08949757 0.61269778]. \t  0.24228973974732076 \t 3.823263664347254\n",
            "79     \t [0.97854859 0.24215962 0.51359038]. \t  0.17767339073826924 \t 3.823263664347254\n",
            "80     \t [0.27415217 0.58686992 0.96646935]. \t  2.6831589861332583 \t 3.823263664347254\n",
            "81     \t [0.38697907 0.29536366 0.09405804]. \t  0.29552336972369 \t 3.823263664347254\n",
            "82     \t [0.38041894 0.28113978 0.19138337]. \t  0.6436855569505461 \t 3.823263664347254\n",
            "83     \t [0.73644648 0.74409984 0.0593703 ]. \t  0.004067044093233711 \t 3.823263664347254\n",
            "84     \t [0.43974686 0.25576428 0.76727338]. \t  1.5860201622412489 \t 3.823263664347254\n",
            "85     \t [0.91716939 0.23206351 0.2338737 ]. \t  0.3441867231335853 \t 3.823263664347254\n",
            "86     \t [0.928106   0.36398451 0.2170875 ]. \t  0.19819281824603735 \t 3.823263664347254\n",
            "87     \t [0.55496943 0.67175689 0.39539542]. \t  0.5496100823873051 \t 3.823263664347254\n",
            "88     \t [0.10679858 0.88619389 0.87102049]. \t  1.436988468303008 \t 3.823263664347254\n",
            "89     \t [0.42338151 0.41695061 0.16612036]. \t  0.2995029456411814 \t 3.823263664347254\n",
            "90     \t [0.89484097 0.34203198 0.19629526]. \t  0.22656051317356457 \t 3.823263664347254\n",
            "91     \t [0.17349676 0.41012199 0.94005107]. \t  2.4938462280056983 \t 3.823263664347254\n",
            "92     \t [0.19771752 0.31239557 0.2045076 ]. \t  0.558662218988743 \t 3.823263664347254\n",
            "93     \t [0.33178305 0.02142237 0.89524347]. \t  0.24704410756571965 \t 3.823263664347254\n",
            "94     \t [0.33225438 0.53569611 0.07988812]. \t  0.06111318267713783 \t 3.823263664347254\n",
            "95     \t [0.65412864 0.03015545 0.90688896]. \t  0.2532532659952299 \t 3.823263664347254\n",
            "96     \t [0.96827564 0.09360594 0.34086681]. \t  0.28907922102817385 \t 3.823263664347254\n",
            "97     \t [0.50114949 0.9151915  0.28738893]. \t  0.21956923397295164 \t 3.823263664347254\n",
            "98     \t [0.4219465  0.87052368 0.10971162]. \t  0.007494253147581695 \t 3.823263664347254\n",
            "99     \t [0.64684064 0.68675623 0.41936934]. \t  0.5458447520385002 \t 3.823263664347254\n",
            "100    \t [0.28560638 0.15564314 0.33671172]. \t  0.8403431815373047 \t 3.823263664347254\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NqjDLo0otd9-",
        "outputId": "5331a601-f8ad-4b00-ca3a-c8685326ccb7"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 17\r\n",
        "\r\n",
        "np.random.seed(run_num_17)\r\n",
        "surrogate_loser_17 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "loser_17 = dGPGO(surrogate_loser_17, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_17.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.65182403 0.73051962 0.19336736]. \t  0.03562358530655929 \t 3.1179188940604616\n",
            "init   \t [0.51967724 0.67638327 0.80444487]. \t  3.1179188940604616 \t 3.1179188940604616\n",
            "init   \t [0.53351286 0.90803418 0.44348113]. \t  1.1984486017622078 \t 3.1179188940604616\n",
            "init   \t [0.25597221 0.27866502 0.04332186]. \t  0.1645724004847893 \t 3.1179188940604616\n",
            "init   \t [0.46897499 0.9416138  0.60040624]. \t  1.9192522196289552 \t 3.1179188940604616\n",
            "1      \t [0.24286651 0.38597556 0.96454773]. \t  1.996297956656514 \t 3.1179188940604616\n",
            "2      \t [0.78722    0.79439557 0.98615055]. \t  1.382220738619041 \t 3.1179188940604616\n",
            "3      \t [0.48019116 0.49032971 0.69126343]. \t  2.1360120328634014 \t 3.1179188940604616\n",
            "4      \t [0.4607031  0.72735048 0.97289599]. \t  2.000642401584277 \t 3.1179188940604616\n",
            "5      \t [0.00166169 0.44190378 0.82055392]. \t  \u001b[92m3.336924169425464\u001b[0m \t 3.336924169425464\n",
            "6      \t [0.84992296 0.83200048 0.62767412]. \t  0.8027149864585417 \t 3.336924169425464\n",
            "7      \t [0.0340026  0.68308541 0.48965537]. \t  1.8929631781308491 \t 3.336924169425464\n",
            "8      \t [0.02659308 0.43512057 0.74588651]. \t  2.6717059421597273 \t 3.336924169425464\n",
            "9      \t [0.05231581 0.5113275  0.93934183]. \t  3.0363767822067844 \t 3.336924169425464\n",
            "10     \t [0.78110492 0.5287478  0.92088201]. \t  3.3164288980141534 \t 3.336924169425464\n",
            "11     \t [0.7581868  0.59054205 0.89327271]. \t  \u001b[92m3.5935769448692607\u001b[0m \t 3.5935769448692607\n",
            "12     \t [0.06540998 0.93970118 0.11022951]. \t  0.0080286044197451 \t 3.5935769448692607\n",
            "13     \t [0.00917304 0.02388783 0.90208087]. \t  0.24124246128515642 \t 3.5935769448692607\n",
            "14     \t [0.76092022 0.54245962 0.7960535 ]. \t  3.430452623032548 \t 3.5935769448692607\n",
            "15     \t [0.06151538 0.96523417 0.47660316]. \t  2.287919808243954 \t 3.5935769448692607\n",
            "16     \t [0.70429838 0.55004718 0.88602409]. \t  \u001b[92m3.696554861371928\u001b[0m \t 3.696554861371928\n",
            "17     \t [0.63769391 0.60924651 0.82227901]. \t  3.571667524633763 \t 3.696554861371928\n",
            "18     \t [0.84564485 0.31890278 0.00286951]. \t  0.04128989362046553 \t 3.696554861371928\n",
            "19     \t [0.68465033 0.98359513 0.32227619]. \t  0.19492681141269716 \t 3.696554861371928\n",
            "20     \t [0.63440592 0.59892124 0.85253504]. \t  \u001b[92m3.7306976893459707\u001b[0m \t 3.7306976893459707\n",
            "21     \t [0.82399351 0.15394836 0.64722767]. \t  0.4584541049637956 \t 3.7306976893459707\n",
            "22     \t [0.00093538 0.63875692 0.84784539]. \t  3.577111350024265 \t 3.7306976893459707\n",
            "23     \t [0.13352918 0.94357418 0.30028195]. \t  0.4097630621694289 \t 3.7306976893459707\n",
            "24     \t [0.03451205 0.12497542 0.02756137]. \t  0.1274157801200702 \t 3.7306976893459707\n",
            "25     \t [0.76576681 0.43251344 0.11100496]. \t  0.111032154670387 \t 3.7306976893459707\n",
            "26     \t [0.71302946 0.65428918 0.85710419]. \t  3.4312930079457056 \t 3.7306976893459707\n",
            "27     \t [0.71440553 0.77407405 0.36272561]. \t  0.3093169795364798 \t 3.7306976893459707\n",
            "28     \t [0.70784051 0.97993163 0.67028909]. \t  0.7892816451607718 \t 3.7306976893459707\n",
            "29     \t [0.51800861 0.54551968 0.26487654]. \t  0.19936679977656882 \t 3.7306976893459707\n",
            "30     \t [0.03243126 0.94999519 0.69930194]. \t  1.8007352842172186 \t 3.7306976893459707\n",
            "31     \t [0.48660742 0.02568772 0.01930355]. \t  0.1394555186161855 \t 3.7306976893459707\n",
            "32     \t [0.78903081 0.82775419 0.1624179 ]. \t  0.009969463208157872 \t 3.7306976893459707\n",
            "33     \t [0.34639487 0.50220626 0.23128726]. \t  0.24563542914317918 \t 3.7306976893459707\n",
            "34     \t [0.73921642 0.46775549 0.67290683]. \t  1.6954635060896477 \t 3.7306976893459707\n",
            "35     \t [0.78764978 0.06042556 0.8302592 ]. \t  0.4267724380537717 \t 3.7306976893459707\n",
            "36     \t [0.70383577 0.57096565 0.17725708]. \t  0.0771285228845155 \t 3.7306976893459707\n",
            "37     \t [0.08224507 0.52206253 0.92520623]. \t  3.274999790654435 \t 3.7306976893459707\n",
            "38     \t [0.0620436  0.42801267 0.25147661]. \t  0.3106822479949355 \t 3.7306976893459707\n",
            "39     \t [0.20240843 0.223458   0.35299039]. \t  0.6751471570789034 \t 3.7306976893459707\n",
            "40     \t [0.78411066 0.99428025 0.80895205]. \t  0.5882277001490892 \t 3.7306976893459707\n",
            "41     \t [0.90256261 0.37869663 0.54763987]. \t  0.3839181168949045 \t 3.7306976893459707\n",
            "42     \t [0.82027636 0.03502204 0.24575806]. \t  0.5004688146837654 \t 3.7306976893459707\n",
            "43     \t [0.85121567 0.22893522 0.27913839]. \t  0.438480566219128 \t 3.7306976893459707\n",
            "44     \t [0.26433048 0.1597651  0.85691673]. \t  0.9205910915577684 \t 3.7306976893459707\n",
            "45     \t [0.76691622 0.51337129 0.38827815]. \t  0.19408491849855966 \t 3.7306976893459707\n",
            "46     \t [0.55298012 0.12193723 0.8363212 ]. \t  0.7153673496834476 \t 3.7306976893459707\n",
            "47     \t [0.98453676 0.40768679 0.23852634]. \t  0.13626639106252103 \t 3.7306976893459707\n",
            "48     \t [0.25856198 0.52084654 0.77467825]. \t  3.3395602090894543 \t 3.7306976893459707\n",
            "49     \t [0.45296663 0.78267689 0.08376546]. \t  0.006740911098779007 \t 3.7306976893459707\n",
            "50     \t [0.58567443 0.59245542 0.05416328]. \t  0.02355784289594982 \t 3.7306976893459707\n",
            "51     \t [0.59592109 0.97796742 0.08147182]. \t  0.0017791627784233446 \t 3.7306976893459707\n",
            "52     \t [0.02516885 0.60819578 0.81362045]. \t  3.592382920232589 \t 3.7306976893459707\n",
            "53     \t [0.81432498 0.1020756  0.43962605]. \t  0.2412751609616757 \t 3.7306976893459707\n",
            "54     \t [0.64815103 0.29806231 0.44364782]. \t  0.2969447837152171 \t 3.7306976893459707\n",
            "55     \t [0.56051781 0.51183617 0.49888347]. \t  0.5888244819252172 \t 3.7306976893459707\n",
            "56     \t [0.48517218 0.99134974 0.49422206]. \t  1.5385809578318683 \t 3.7306976893459707\n",
            "57     \t [0.05816553 0.45763475 0.69376408]. \t  2.153215054180613 \t 3.7306976893459707\n",
            "58     \t [0.26909419 0.74643037 0.04197797]. \t  0.004915713440641152 \t 3.7306976893459707\n",
            "59     \t [0.25910482 0.33170441 0.73636218]. \t  1.9550234372979356 \t 3.7306976893459707\n",
            "60     \t [0.86860015 0.22907423 0.02897167]. \t  0.07587325391055157 \t 3.7306976893459707\n",
            "61     \t [0.26429574 0.80177263 0.59003997]. \t  2.7847853706041117 \t 3.7306976893459707\n",
            "62     \t [0.3464018  0.83103848 0.63003205]. \t  2.4356311999821503 \t 3.7306976893459707\n",
            "63     \t [0.33390264 0.9444325  0.98442306]. \t  0.5888996464899308 \t 3.7306976893459707\n",
            "64     \t [0.71600419 0.11410615 0.07313597]. \t  0.22480895578597396 \t 3.7306976893459707\n",
            "65     \t [0.86916049 0.615922   0.70458205]. \t  1.9599722542591864 \t 3.7306976893459707\n",
            "66     \t [0.53336509 0.84486317 0.59531914]. \t  1.8388020734686554 \t 3.7306976893459707\n",
            "67     \t [0.73978342 0.65624186 0.80203376]. \t  3.109129084832267 \t 3.7306976893459707\n",
            "68     \t [0.46908484 0.29016108 0.48622976]. \t  0.32457236537058454 \t 3.7306976893459707\n",
            "69     \t [0.8055574  0.75264993 0.77470985]. \t  2.0918877698505494 \t 3.7306976893459707\n",
            "70     \t [0.71930082 0.69451456 0.87152847]. \t  3.121077480785742 \t 3.7306976893459707\n",
            "71     \t [0.23683334 0.00284899 0.28194082]. \t  0.8279866835289357 \t 3.7306976893459707\n",
            "72     \t [0.1040646  0.38199153 0.39295203]. \t  0.3873126590831428 \t 3.7306976893459707\n",
            "73     \t [0.30928002 0.42841219 0.01410685]. \t  0.054880482195661766 \t 3.7306976893459707\n",
            "74     \t [0.97649113 0.50950127 0.40146832]. \t  0.1005319691968133 \t 3.7306976893459707\n",
            "75     \t [0.8338393  0.74114505 0.04995728]. \t  0.002826701835849269 \t 3.7306976893459707\n",
            "76     \t [0.03921711 0.24384318 0.59904097]. \t  0.5135943190545953 \t 3.7306976893459707\n",
            "77     \t [0.66007928 0.85332333 0.76544453]. \t  1.4290146320216675 \t 3.7306976893459707\n",
            "78     \t [0.89236848 0.35997672 0.13321747]. \t  0.14218906731735056 \t 3.7306976893459707\n",
            "79     \t [0.78621447 0.86215128 0.27846026]. \t  0.07912757823474001 \t 3.7306976893459707\n",
            "80     \t [0.80024897 0.32727763 0.05355603]. \t  0.09341169596865143 \t 3.7306976893459707\n",
            "81     \t [0.93128779 0.68743986 0.38818922]. \t  0.13885246555159952 \t 3.7306976893459707\n",
            "82     \t [0.47095339 0.96251407 0.29195505]. \t  0.23654792306024855 \t 3.7306976893459707\n",
            "83     \t [0.14768116 0.94840828 0.44541929]. \t  1.9774934950818177 \t 3.7306976893459707\n",
            "84     \t [0.86850914 0.70841359 0.22112106]. \t  0.027852132025478437 \t 3.7306976893459707\n",
            "85     \t [0.3953896  0.57065299 0.56860651]. \t  1.3740985016426173 \t 3.7306976893459707\n",
            "86     \t [0.45464463 0.93392166 0.39989463]. \t  0.9872177351317891 \t 3.7306976893459707\n",
            "87     \t [0.83370591 0.80028876 0.03974119]. \t  0.0012445130735553001 \t 3.7306976893459707\n",
            "88     \t [0.3954636 0.4760783 0.9305186]. \t  3.0539262472087465 \t 3.7306976893459707\n",
            "89     \t [0.20014603 0.64918314 0.30455259]. \t  0.32378189219673037 \t 3.7306976893459707\n",
            "90     \t [0.81105369 0.1425165  0.27709311]. \t  0.5516169326058925 \t 3.7306976893459707\n",
            "91     \t [0.51933244 0.17350192 0.01046866]. \t  0.12509653075594532 \t 3.7306976893459707\n",
            "92     \t [0.20641137 0.07008326 0.74426415]. \t  0.43692938093535527 \t 3.7306976893459707\n",
            "93     \t [0.26813454 0.33962522 0.85618197]. \t  2.5218159508499016 \t 3.7306976893459707\n",
            "94     \t [0.34518094 0.31846439 0.72401148]. \t  1.7566215036092712 \t 3.7306976893459707\n",
            "95     \t [0.72152361 0.5511943  0.11487202]. \t  0.05310531285038199 \t 3.7306976893459707\n",
            "96     \t [0.88422074 0.77691097 0.8119611 ]. \t  2.122276340715377 \t 3.7306976893459707\n",
            "97     \t [0.88363888 0.3470758  0.85211001]. \t  2.540494522342128 \t 3.7306976893459707\n",
            "98     \t [0.8354768  0.44173748 0.23095813]. \t  0.17848714396589418 \t 3.7306976893459707\n",
            "99     \t [0.04158365 0.95493571 0.1420074 ]. \t  0.01712143442770344 \t 3.7306976893459707\n",
            "100    \t [0.87034875 0.38981482 0.60959142]. \t  0.8117138118498288 \t 3.7306976893459707\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jOqNmgSIteAw",
        "outputId": "616bd448-9fdf-4517-d3e3-6a3836a41463"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 18\r\n",
        "\r\n",
        "np.random.seed(run_num_18)\r\n",
        "surrogate_loser_18 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "loser_18 = dGPGO(surrogate_loser_18, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_18.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.8734294  0.96854066 0.86919454]. \t  0.722531189286755 \t 1.1210522139432408\n",
            "init   \t [0.53085569 0.23272833 0.0113988 ]. \t  0.1133646794700679 \t 1.1210522139432408\n",
            "init   \t [0.43046882 0.40235136 0.52267467]. \t  0.5232646985053151 \t 1.1210522139432408\n",
            "init   \t [0.4783918  0.55535647 0.54338602]. \t  1.0390748854353227 \t 1.1210522139432408\n",
            "init   \t [0.76089558 0.71237457 0.6196821 ]. \t  1.1210522139432408 \t 1.1210522139432408\n",
            "1      \t [0.28982083 0.95201409 0.18404427]. \t  0.04220734680078784 \t 1.1210522139432408\n",
            "2      \t [0.12883542 0.98258307 0.90180597]. \t  0.6910738837771888 \t 1.1210522139432408\n",
            "3      \t [0.49148239 0.8099037  0.6976846 ]. \t  \u001b[92m1.8373634823765186\u001b[0m \t 1.8373634823765186\n",
            "4      \t [0.46136123 0.85569499 0.88610921]. \t  1.63167944843026 \t 1.8373634823765186\n",
            "5      \t [0.49454713 0.93328953 0.56170327]. \t  \u001b[92m1.9054625615284428\u001b[0m \t 1.9054625615284428\n",
            "6      \t [0.41435422 0.93277915 0.58517174]. \t  \u001b[92m2.210826503581639\u001b[0m \t 2.210826503581639\n",
            "7      \t [0.80890325 0.0109094  0.32544314]. \t  0.45214607547986396 \t 2.210826503581639\n",
            "8      \t [0.34819219 0.99739422 0.56097608]. \t  2.1934316775664433 \t 2.210826503581639\n",
            "9      \t [0. 0. 0.]. \t  0.06797411659013229 \t 2.210826503581639\n",
            "10     \t [0.43963141 0.98143234 0.73039565]. \t  1.0945002305811715 \t 2.210826503581639\n",
            "11     \t [0.02531226 0.36418907 0.95948581]. \t  1.8848202094359148 \t 2.210826503581639\n",
            "12     \t [0.9798975  0.65782747 1.        ]. \t  1.8577503855843227 \t 2.210826503581639\n",
            "13     \t [0.99970731 0.70062909 0.0567806 ]. \t  0.0027811478146726625 \t 2.210826503581639\n",
            "14     \t [0.02495978 0.79442693 0.58527601]. \t  \u001b[92m2.9049684298182346\u001b[0m \t 2.9049684298182346\n",
            "15     \t [0.08533529 0.90729061 0.49773317]. \t  2.7067907211320232 \t 2.9049684298182346\n",
            "16     \t [0.5421836  0.01602986 0.01723476]. \t  0.12643732141156086 \t 2.9049684298182346\n",
            "17     \t [0.13495713 0.71802934 0.77968976]. \t  2.8224670581015165 \t 2.9049684298182346\n",
            "18     \t [0.03434562 0.05142528 0.46605694]. \t  0.22909651602977216 \t 2.9049684298182346\n",
            "19     \t [0.33535147 0.59732357 0.98448116]. \t  2.350249163604808 \t 2.9049684298182346\n",
            "20     \t [0.1926461  0.75814723 0.55309241]. \t  2.740424052375752 \t 2.9049684298182346\n",
            "21     \t [0.06859238 0.70957122 0.98703068]. \t  1.888144648409867 \t 2.9049684298182346\n",
            "22     \t [0.48419506 0.01549588 0.89987476]. \t  0.22772306298785155 \t 2.9049684298182346\n",
            "23     \t [0.10489585 0.81005677 0.60815803]. \t  \u001b[92m2.927816116398253\u001b[0m \t 2.927816116398253\n",
            "24     \t [0.04431098 0.71713489 0.71201417]. \t  2.5547982722760074 \t 2.927816116398253\n",
            "25     \t [0.18247832 0.87344963 0.60022047]. \t  \u001b[92m2.938319453370552\u001b[0m \t 2.938319453370552\n",
            "26     \t [0.06030413 0.11138544 0.92178117]. \t  0.49124076807692607 \t 2.938319453370552\n",
            "27     \t [0.12645112 0.80261083 0.63044693]. \t  2.800126383765289 \t 2.938319453370552\n",
            "28     \t [0.95024252 0.95266361 0.40567717]. \t  0.17505838902345658 \t 2.938319453370552\n",
            "29     \t [0.77931996 0.70884583 0.55471014]. \t  0.8143693810560588 \t 2.938319453370552\n",
            "30     \t [0.98265583 0.00159317 0.96059378]. \t  0.1290059718409104 \t 2.938319453370552\n",
            "31     \t [0.01807392 0.81368688 0.98082843]. \t  1.3266707709398322 \t 2.938319453370552\n",
            "32     \t [0.91169582 0.26398139 0.02446877]. \t  0.056762094606577974 \t 2.938319453370552\n",
            "33     \t [0.80542036 0.93073344 0.58505498]. \t  0.7409700208735187 \t 2.938319453370552\n",
            "34     \t [0.08887828 0.80400539 0.6141851 ]. \t  2.8865270941268943 \t 2.938319453370552\n",
            "35     \t [0.64262746 0.50518779 0.79424085]. \t  \u001b[92m3.4202876002328155\u001b[0m \t 3.4202876002328155\n",
            "36     \t [0.86634869 0.14229106 0.77209624]. \t  0.7956734891804487 \t 3.4202876002328155\n",
            "37     \t [0.79991674 0.35631883 0.97903321]. \t  1.5825716542459531 \t 3.4202876002328155\n",
            "38     \t [0.52897407 0.54925181 0.88068636]. \t  \u001b[92m3.7659958974425827\u001b[0m \t 3.7659958974425827\n",
            "39     \t [0.96662041 0.17014501 0.54127832]. \t  0.17694808258495845 \t 3.7659958974425827\n",
            "40     \t [0.61165389 0.07896505 0.17290464]. \t  0.6322229024122338 \t 3.7659958974425827\n",
            "41     \t [0.15815988 0.67675116 0.59988159]. \t  2.396099008710307 \t 3.7659958974425827\n",
            "42     \t [0.55307155 0.83113468 0.65867287]. \t  1.659746827776479 \t 3.7659958974425827\n",
            "43     \t [0.10993072 0.74821578 0.46707571]. \t  2.077590778069325 \t 3.7659958974425827\n",
            "44     \t [0.62939252 0.56615106 0.881394  ]. \t  3.7398318014237706 \t 3.7659958974425827\n",
            "45     \t [0.792842   0.95433978 0.03948485]. \t  0.0003512453288885632 \t 3.7659958974425827\n",
            "46     \t [0.45353138 0.15999598 0.48404493]. \t  0.29717680074648767 \t 3.7659958974425827\n",
            "47     \t [0.22384738 0.1550626  0.19208477]. \t  0.7812168459064405 \t 3.7659958974425827\n",
            "48     \t [0.48298156 0.28940149 0.37160245]. \t  0.5464451043545906 \t 3.7659958974425827\n",
            "49     \t [0.70331652 0.53653374 0.90614469]. \t  3.518286282461717 \t 3.7659958974425827\n",
            "50     \t [0.60261663 0.44378932 0.49433609]. \t  0.40890372618870746 \t 3.7659958974425827\n",
            "51     \t [0.5967522  0.98799453 0.06163179]. \t  0.0009981258980179624 \t 3.7659958974425827\n",
            "52     \t [0.71091254 0.69983393 0.23784189]. \t  0.059899164199538016 \t 3.7659958974425827\n",
            "53     \t [0.20273128 0.58872086 0.10767453]. \t  0.04954895770137224 \t 3.7659958974425827\n",
            "54     \t [0.66317934 0.93525471 0.68586192]. \t  0.9970043927416965 \t 3.7659958974425827\n",
            "55     \t [0.58764246 0.33637294 0.31484846]. \t  0.5169073769604036 \t 3.7659958974425827\n",
            "56     \t [0.86629224 0.04252853 0.05942087]. \t  0.1231852269548608 \t 3.7659958974425827\n",
            "57     \t [0.57283024 0.38759573 0.9278396 ]. \t  2.4697606835699473 \t 3.7659958974425827\n",
            "58     \t [0.27813955 0.37995999 0.75926053]. \t  2.5025501735589653 \t 3.7659958974425827\n",
            "59     \t [0.41186934 0.23513372 0.39403079]. \t  0.562511567165831 \t 3.7659958974425827\n",
            "60     \t [0.32108626 0.38704859 0.13114269]. \t  0.27577883497062416 \t 3.7659958974425827\n",
            "61     \t [0.30102575 0.18571067 0.69279238]. \t  0.7864759815082917 \t 3.7659958974425827\n",
            "62     \t [0.65089824 0.92138798 0.79668312]. \t  1.0511102182778886 \t 3.7659958974425827\n",
            "63     \t [0.0107059  0.76416342 0.50836861]. \t  2.499686990361062 \t 3.7659958974425827\n",
            "64     \t [0.77860221 0.52231521 0.39471157]. \t  0.19287222570968438 \t 3.7659958974425827\n",
            "65     \t [0.36787302 0.17657212 0.1280795 ]. \t  0.5396635722306211 \t 3.7659958974425827\n",
            "66     \t [0.79104461 0.10531206 0.92057209]. \t  0.4680412116273659 \t 3.7659958974425827\n",
            "67     \t [0.81682994 0.62089582 0.08571168]. \t  0.01656009674352917 \t 3.7659958974425827\n",
            "68     \t [0.88480013 0.54827671 0.58088944]. \t  0.6976109027858441 \t 3.7659958974425827\n",
            "69     \t [0.25229893 0.85664729 0.91213272]. \t  1.5378716112082156 \t 3.7659958974425827\n",
            "70     \t [0.50164155 0.6093058  0.23688983]. \t  0.12733227359775642 \t 3.7659958974425827\n",
            "71     \t [0.68805769 0.04971805 0.07463285]. \t  0.2312012001698145 \t 3.7659958974425827\n",
            "72     \t [0.16447824 0.05546928 0.91134554]. \t  0.31617261110468226 \t 3.7659958974425827\n",
            "73     \t [0.67234226 0.0349185  0.15057444]. \t  0.4712658901470614 \t 3.7659958974425827\n",
            "74     \t [0.3134     0.67794811 0.1465898 ]. \t  0.03974076975009795 \t 3.7659958974425827\n",
            "75     \t [0.30640295 0.92612678 0.86974016]. \t  1.119825071673948 \t 3.7659958974425827\n",
            "76     \t [0.17382888 0.31318679 0.10265048]. \t  0.26947136899597673 \t 3.7659958974425827\n",
            "77     \t [0.75990013 0.21311832 0.99415522]. \t  0.6388463637914399 \t 3.7659958974425827\n",
            "78     \t [0.89047902 0.64491292 0.32921552]. \t  0.08812881948180289 \t 3.7659958974425827\n",
            "79     \t [0.98696351 0.18431559 0.75848459]. \t  1.0022842918950319 \t 3.7659958974425827\n",
            "80     \t [0.35343701 0.18275473 0.59440178]. \t  0.3720688142423593 \t 3.7659958974425827\n",
            "81     \t [0.31539192 0.7792746  0.79955247]. \t  2.3896674916440004 \t 3.7659958974425827\n",
            "82     \t [0.14508814 0.18143484 0.64846608]. \t  0.5653094985500519 \t 3.7659958974425827\n",
            "83     \t [0.40737253 0.82678737 0.820015  ]. \t  1.9662984601658695 \t 3.7659958974425827\n",
            "84     \t [0.92766229 0.18593304 0.06503856]. \t  0.10954397157902122 \t 3.7659958974425827\n",
            "85     \t [0.32869146 0.05835603 0.47108057]. \t  0.2996102459165014 \t 3.7659958974425827\n",
            "86     \t [0.02684041 0.70539556 0.80612601]. \t  3.0104249940434444 \t 3.7659958974425827\n",
            "87     \t [0.05193446 0.06965373 0.43434944]. \t  0.3262841943372766 \t 3.7659958974425827\n",
            "88     \t [0.81004504 0.44320829 0.10766096]. \t  0.0898625192921795 \t 3.7659958974425827\n",
            "89     \t [0.61289151 0.38869684 0.43681509]. \t  0.29952453091027376 \t 3.7659958974425827\n",
            "90     \t [0.86832659 0.12322599 0.86359209]. \t  0.6746635330832248 \t 3.7659958974425827\n",
            "91     \t [0.86847025 0.1642101  0.21030744]. \t  0.4197267969060901 \t 3.7659958974425827\n",
            "92     \t [0.09465095 0.7470169  0.71808077]. \t  2.503304736154796 \t 3.7659958974425827\n",
            "93     \t [0.29236438 0.26645893 0.82895882]. \t  1.829975409816589 \t 3.7659958974425827\n",
            "94     \t [0.92417251 0.24429314 0.62430697]. \t  0.5906192293138568 \t 3.7659958974425827\n",
            "95     \t [0.55557028 0.3141471  0.73791255]. \t  1.8287978040338055 \t 3.7659958974425827\n",
            "96     \t [0.90070846 0.18634617 0.11644003]. \t  0.20613038331249936 \t 3.7659958974425827\n",
            "97     \t [0.92852018 0.18809886 0.72942986]. \t  0.9291753024647236 \t 3.7659958974425827\n",
            "98     \t [0.98478221 0.01874909 0.39303239]. \t  0.18371725118779617 \t 3.7659958974425827\n",
            "99     \t [0.5440732  0.8788577  0.07824594]. \t  0.002815235884962438 \t 3.7659958974425827\n",
            "100    \t [0.12195859 0.52278805 0.62882059]. \t  1.749290705274293 \t 3.7659958974425827\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i4HnKuoqteDo",
        "outputId": "a2a8e9d3-39df-4224-efe9-650766417111"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 19\r\n",
        "\r\n",
        "np.random.seed(run_num_19)\r\n",
        "surrogate_loser_19 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "loser_19 = dGPGO(surrogate_loser_19, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_19.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.55944558 0.76149199 0.24453155]. \t  0.09433080659504385 \t 2.524990008735946\n",
            "init   \t [0.77168947 0.35447208 0.7966974 ]. \t  2.524990008735946 \t 2.524990008735946\n",
            "init   \t [0.34600002 0.44387444 0.30330359]. \t  0.3914194506273641 \t 2.524990008735946\n",
            "init   \t [0.44211392 0.57320936 0.06566402]. \t  0.036928944981920925 \t 2.524990008735946\n",
            "init   \t [0.02408889 0.82062263 0.36513765]. \t  0.9772047942100802 \t 2.524990008735946\n",
            "1      \t [0.96499061 0.19185662 0.98563058]. \t  0.5815385407774518 \t 2.524990008735946\n",
            "2      \t [0.35021227 0.89949578 0.95419986]. \t  0.9824452595236306 \t 2.524990008735946\n",
            "3      \t [0.8364035  0.47078086 0.70932116]. \t  2.156884519592091 \t 2.524990008735946\n",
            "4      \t [0.46047313 0.25352007 0.98033154]. \t  0.9460814398415793 \t 2.524990008735946\n",
            "5      \t [0.87336482 0.09118839 0.45818428]. \t  0.17563272101768643 \t 2.524990008735946\n",
            "6      \t [0.70411181 0.42279097 0.83675882]. \t  \u001b[92m3.2477068869485466\u001b[0m \t 3.2477068869485466\n",
            "7      \t [0.74803474 0.60533411 0.93779535]. \t  3.0703841077028233 \t 3.2477068869485466\n",
            "8      \t [0.57560304 0.47085767 0.99523987]. \t  1.978350792868597 \t 3.2477068869485466\n",
            "9      \t [0.8489015  0.05888081 0.        ]. \t  0.05678630591107584 \t 3.2477068869485466\n",
            "10     \t [0.57851022 0.66686718 0.73448346]. \t  2.4468307948849746 \t 3.2477068869485466\n",
            "11     \t [0.01381987 0.98972179 0.81693901]. \t  0.8543229768829992 \t 3.2477068869485466\n",
            "12     \t [0.23320797 0.98878694 0.03639898]. \t  0.0008881824743265614 \t 3.2477068869485466\n",
            "13     \t [0.98986012 0.68494222 0.00835794]. \t  0.0016982651496915456 \t 3.2477068869485466\n",
            "14     \t [0.49101384 0.3773169  0.69597041]. \t  1.7869436007858153 \t 3.2477068869485466\n",
            "15     \t [0.55409487 0.02109967 0.87866785]. \t  0.26380236860584677 \t 3.2477068869485466\n",
            "16     \t [0.01606225 0.5779807  0.00526043]. \t  0.0106208204608719 \t 3.2477068869485466\n",
            "17     \t [0.99767452 0.26448242 0.08286597]. \t  0.08857016652594893 \t 3.2477068869485466\n",
            "18     \t [0.74305045 0.94567471 0.91399927]. \t  0.8206436917127765 \t 3.2477068869485466\n",
            "19     \t [0.00140369 0.98879627 0.22223029]. \t  0.09232469302045357 \t 3.2477068869485466\n",
            "20     \t [0.01163211 0.08638709 0.6501099 ]. \t  0.30126735795355003 \t 3.2477068869485466\n",
            "21     \t [0.79458708 0.54059858 0.95886999]. \t  2.764205728180439 \t 3.2477068869485466\n",
            "22     \t [0.68655261 0.47258864 0.84533407]. \t  \u001b[92m3.58096390901099\u001b[0m \t 3.58096390901099\n",
            "23     \t [0.09645332 0.95171827 0.70824437]. \t  1.7335812415162595 \t 3.58096390901099\n",
            "24     \t [0.29698544 0.17077213 0.37241439]. \t  0.6982054319324602 \t 3.58096390901099\n",
            "25     \t [0. 0. 0.]. \t  0.06797411659013229 \t 3.58096390901099\n",
            "26     \t [0.05937236 0.02006886 0.20927426]. \t  0.617373006092504 \t 3.58096390901099\n",
            "27     \t [0.56925757 0.90150448 0.25170707]. \t  0.10227822235163753 \t 3.58096390901099\n",
            "28     \t [0.05486895 0.31756584 0.13401811]. \t  0.2926632371936038 \t 3.58096390901099\n",
            "29     \t [0.50329508 0.22209549 0.64275971]. \t  0.6699701094542871 \t 3.58096390901099\n",
            "30     \t [0.61332035 0.09483347 0.84810284]. \t  0.563234333851149 \t 3.58096390901099\n",
            "31     \t [0.8527929  0.94871274 0.00283516]. \t  0.00011822558074294612 \t 3.58096390901099\n",
            "32     \t [0.56351103 0.61250042 0.93764616]. \t  3.0875063529706157 \t 3.58096390901099\n",
            "33     \t [0.2471381  0.01618808 0.14396125]. \t  0.547463513490101 \t 3.58096390901099\n",
            "34     \t [0.97547624 0.00351099 0.13726235]. \t  0.17554002587598122 \t 3.58096390901099\n",
            "35     \t [0.01895849 0.29888338 0.15063556]. \t  0.33152174386330074 \t 3.58096390901099\n",
            "36     \t [0.04503366 0.22594075 0.7378816 ]. \t  1.2153461880632406 \t 3.58096390901099\n",
            "37     \t [0.28985676 0.99895607 0.97563001]. \t  0.40933801513369666 \t 3.58096390901099\n",
            "38     \t [0.82309201 0.72860314 0.42102276]. \t  0.3286530780947166 \t 3.58096390901099\n",
            "39     \t [0.42547655 0.0600468  0.47533445]. \t  0.28650314809862054 \t 3.58096390901099\n",
            "40     \t [0.12523936 0.71993174 0.23134444]. \t  0.12419919234462012 \t 3.58096390901099\n",
            "41     \t [0.96160152 0.36463765 0.76840526]. \t  2.3586836952155927 \t 3.58096390901099\n",
            "42     \t [0.19076268 0.78430101 0.32881315]. \t  0.5982041585220237 \t 3.58096390901099\n",
            "43     \t [0.40813466 0.66810217 0.94607042]. \t  2.739423609810528 \t 3.58096390901099\n",
            "44     \t [0.9962501  0.90959763 0.21364914]. \t  0.009053813831887955 \t 3.58096390901099\n",
            "45     \t [0.89691926 0.09320847 0.15243687]. \t  0.2900082957339025 \t 3.58096390901099\n",
            "46     \t [0.08587334 0.74904582 0.5635006 ]. \t  2.7671860688773733 \t 3.58096390901099\n",
            "47     \t [0.08933768 0.62692057 0.7161244 ]. \t  2.717688340274245 \t 3.58096390901099\n",
            "48     \t [0.77119201 0.03859479 0.34265034]. \t  0.4890468547987831 \t 3.58096390901099\n",
            "49     \t [0.23163943 0.83329578 0.03876278]. \t  0.0021264459665956 \t 3.58096390901099\n",
            "50     \t [0.22967869 0.13702735 0.05796596]. \t  0.2523975141805462 \t 3.58096390901099\n",
            "51     \t [0.11237253 0.65966214 0.42607608]. \t  1.1995499965462453 \t 3.58096390901099\n",
            "52     \t [0.40230869 0.63370855 0.44560545]. \t  0.9785715601738311 \t 3.58096390901099\n",
            "53     \t [0.63854359 0.86124614 0.40485685]. \t  0.6643958568560102 \t 3.58096390901099\n",
            "54     \t [0.08013076 0.54133813 0.42400428]. \t  0.6870736161624138 \t 3.58096390901099\n",
            "55     \t [0.47446429 0.75289979 0.96713271]. \t  1.9044832473946198 \t 3.58096390901099\n",
            "56     \t [0.63207101 0.35509364 0.00158214]. \t  0.055427795107536884 \t 3.58096390901099\n",
            "57     \t [0.01274147 0.03203166 0.9689726 ]. \t  0.16871854597066624 \t 3.58096390901099\n",
            "58     \t [0.96799084 0.6911544  0.91876818]. \t  2.8023998550294 \t 3.58096390901099\n",
            "59     \t [0.9030704  0.69926889 0.30648406]. \t  0.06705165906432335 \t 3.58096390901099\n",
            "60     \t [0.37250422 0.30495954 0.09332783]. \t  0.28344377600126297 \t 3.58096390901099\n",
            "61     \t [0.1500338  0.2151008  0.13895462]. \t  0.48014866379633925 \t 3.58096390901099\n",
            "62     \t [0.72429142 0.87225717 0.0389365 ]. \t  0.0008069837359671329 \t 3.58096390901099\n",
            "63     \t [0.86377943 0.88216093 0.87175809]. \t  1.329082134525974 \t 3.58096390901099\n",
            "64     \t [0.52780961 0.26488685 0.19576341]. \t  0.6398531580442676 \t 3.58096390901099\n",
            "65     \t [0.60830622 0.53552546 0.48498891]. \t  0.5388088371927547 \t 3.58096390901099\n",
            "66     \t [0.43756068 0.48625546 0.57947842]. \t  1.0478624634428502 \t 3.58096390901099\n",
            "67     \t [0.5947457  0.26791511 0.43075178]. \t  0.3584817684980289 \t 3.58096390901099\n",
            "68     \t [0.3857746  0.28585579 0.81923471]. \t  2.0091479654578412 \t 3.58096390901099\n",
            "69     \t [0.36378236 0.43626916 0.27521403]. \t  0.3957733306512729 \t 3.58096390901099\n",
            "70     \t [0.31011784 0.11476409 0.94703201]. \t  0.4384406165894201 \t 3.58096390901099\n",
            "71     \t [0.96106242 0.25484353 0.2852708 ]. \t  0.28735363514336076 \t 3.58096390901099\n",
            "72     \t [0.67275161 0.90584988 0.19440821]. \t  0.024581555798376124 \t 3.58096390901099\n",
            "73     \t [0.42856543 0.24123473 0.06572751]. \t  0.2506139230144343 \t 3.58096390901099\n",
            "74     \t [0.11795803 0.82308563 0.88447371]. \t  1.9627816582310302 \t 3.58096390901099\n",
            "75     \t [0.63786628 0.39515356 0.74653427]. \t  2.4223442758118603 \t 3.58096390901099\n",
            "76     \t [0.24354281 0.26780065 0.85639567]. \t  1.8094612962111003 \t 3.58096390901099\n",
            "77     \t [0.66767182 0.18181134 0.57205183]. \t  0.28984354895333153 \t 3.58096390901099\n",
            "78     \t [0.30974426 0.6692461  0.01863442]. \t  0.007649779620783134 \t 3.58096390901099\n",
            "79     \t [0.39396999 0.01359379 0.42209846]. \t  0.44288926847849175 \t 3.58096390901099\n",
            "80     \t [0.00622778 0.70489465 0.76489267]. \t  2.78904023721274 \t 3.58096390901099\n",
            "81     \t [0.18417019 0.65820939 0.99740273]. \t  1.9631796307825014 \t 3.58096390901099\n",
            "82     \t [0.58584054 0.3550544  0.10439691]. \t  0.2224736789213607 \t 3.58096390901099\n",
            "83     \t [0.87431571 0.63954261 0.41195434]. \t  0.19574496210540726 \t 3.58096390901099\n",
            "84     \t [0.69349169 0.58534044 0.62972631]. \t  1.3302088711861788 \t 3.58096390901099\n",
            "85     \t [0.15209794 0.15202414 0.68155881]. \t  0.5943639086588272 \t 3.58096390901099\n",
            "86     \t [0.79200461 0.63794397 0.74960302]. \t  2.5584025352025037 \t 3.58096390901099\n",
            "87     \t [0.34459962 0.41262583 0.27340987]. \t  0.4449259337296729 \t 3.58096390901099\n",
            "88     \t [0.20535494 0.65975039 0.7603884 ]. \t  3.0016424961664874 \t 3.58096390901099\n",
            "89     \t [0.46426209 0.42973631 0.24545965]. \t  0.3772305907675988 \t 3.58096390901099\n",
            "90     \t [0.19736085 0.98220752 0.1427098 ]. \t  0.016314629956979514 \t 3.58096390901099\n",
            "91     \t [0.66980287 0.41687428 0.04706623]. \t  0.07243416592273004 \t 3.58096390901099\n",
            "92     \t [0.04429268 0.08927876 0.88714029]. \t  0.47583883365716306 \t 3.58096390901099\n",
            "93     \t [0.79558746 0.93614399 0.12043335]. \t  0.0028177863330248122 \t 3.58096390901099\n",
            "94     \t [0.41132162 0.13810393 0.80963589]. \t  0.8209804264571146 \t 3.58096390901099\n",
            "95     \t [0.6393976  0.57288184 0.79072809]. \t  3.3957518147380443 \t 3.58096390901099\n",
            "96     \t [0.40173052 0.16642951 0.80461023]. \t  1.0008823398027678 \t 3.58096390901099\n",
            "97     \t [0.91045104 0.13395119 0.72920788]. \t  0.6572687768834146 \t 3.58096390901099\n",
            "98     \t [0.98245798 0.35708702 0.41811415]. \t  0.12973870484146707 \t 3.58096390901099\n",
            "99     \t [0.38571383 0.86707224 0.2595399 ]. \t  0.17830519849965276 \t 3.58096390901099\n",
            "100    \t [0.00128665 0.16274988 0.01869641]. \t  0.10224037162700153 \t 3.58096390901099\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FfiDmH_bteGg",
        "outputId": "c196a799-eb08-4f35-cd4b-ebe69cc5e4a3"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 20\r\n",
        "\r\n",
        "np.random.seed(run_num_20)\r\n",
        "surrogate_loser_20 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "loser_20 = dGPGO(surrogate_loser_20, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_20.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.57051729 0.56452876 0.48844183]. \t  0.675391399411646 \t 0.675391399411646\n",
            "init   \t [0.33647775 0.37586818 0.53203587]. \t  0.5331349538596052 \t 0.675391399411646\n",
            "init   \t [0.06810629 0.58452906 0.23789776]. \t  0.14747335095307315 \t 0.675391399411646\n",
            "init   \t [0.16075658 0.15211915 0.12706922]. \t  0.48089725804912437 \t 0.675391399411646\n",
            "init   \t [0.32744117 0.69415387 0.35896647]. \t  0.6289408047543804 \t 0.675391399411646\n",
            "1      \t [0.837611   0.97862966 0.10314233]. \t  0.0013418779384681767 \t 0.675391399411646\n",
            "2      \t [0.4192146  0.99013113 0.80759551]. \t  \u001b[92m0.8040311121195758\u001b[0m \t 0.8040311121195758\n",
            "3      \t [0.58490171 0.         0.        ]. \t  0.08889155882157919 \t 0.8040311121195758\n",
            "4      \t [0.56803068 0.99098437 0.50767036]. \t  \u001b[92m1.310904866486966\u001b[0m \t 1.310904866486966\n",
            "5      \t [0.84822682 0.9788853  0.85466061]. \t  0.6702188994918671 \t 1.310904866486966\n",
            "6      \t [0.84509259 0.         0.89862994]. \t  0.19194384543565085 \t 1.310904866486966\n",
            "7      \t [0.67407812 0.99571082 0.58414561]. \t  1.010438400626982 \t 1.310904866486966\n",
            "8      \t [0.25706606 0.99173014 0.15932733]. \t  0.02275764294278305 \t 1.310904866486966\n",
            "9      \t [0.         0.         0.31158648]. \t  0.5470921834405232 \t 1.310904866486966\n",
            "10     \t [0.97947894 0.27434002 0.93560419]. \t  \u001b[92m1.395891124057239\u001b[0m \t 1.395891124057239\n",
            "11     \t [0.87557865 0.50601557 0.95614793]. \t  \u001b[92m2.7139777944255674\u001b[0m \t 2.7139777944255674\n",
            "12     \t [0.75793425 0.69772209 0.98756189]. \t  1.927540683132728 \t 2.7139777944255674\n",
            "13     \t [0.12414169 0.99832635 0.576206  ]. \t  2.566437553023501 \t 2.7139777944255674\n",
            "14     \t [0.75104891 0.2645525  0.9551541 ]. \t  1.199937865485019 \t 2.7139777944255674\n",
            "15     \t [0.97780808 0.58615943 0.81664382]. \t  \u001b[92m3.457183993275378\u001b[0m \t 3.457183993275378\n",
            "16     \t [0.98736861 0.72749542 0.80627068]. \t  2.471770357321162 \t 3.457183993275378\n",
            "17     \t [0.89238206 0.48690832 0.72463641]. \t  2.3802531328000907 \t 3.457183993275378\n",
            "18     \t [0.0105289  0.05708276 0.89134199]. \t  0.34912691446229327 \t 3.457183993275378\n",
            "19     \t [0.37352692 0.01567742 0.7968013 ]. \t  0.2930995645424861 \t 3.457183993275378\n",
            "20     \t [0.03867006 0.69475133 0.7534113 ]. \t  2.7906836037923957 \t 3.457183993275378\n",
            "21     \t [0.02486294 0.57208497 0.57980776]. \t  1.6658299552603169 \t 3.457183993275378\n",
            "22     \t [0.04885327 0.69232283 0.89481865]. \t  3.0944266830566485 \t 3.457183993275378\n",
            "23     \t [0.00853456 0.53126337 0.97882715]. \t  2.411907692500336 \t 3.457183993275378\n",
            "24     \t [0.0286142  0.81411405 0.82827108]. \t  2.131555367069342 \t 3.457183993275378\n",
            "25     \t [1.46626775e-14 2.68142046e-14 5.74875709e-14]. \t  0.06797411659020146 \t 3.457183993275378\n",
            "26     \t [0.23771443 0.50552904 0.94685943]. \t  2.927759953411534 \t 3.457183993275378\n",
            "27     \t [0.98694874 0.41002003 0.01130985]. \t  0.018869161976546287 \t 3.457183993275378\n",
            "28     \t [0.16058444 0.61398367 0.90177234]. \t  \u001b[92m3.5144420137944152\u001b[0m \t 3.5144420137944152\n",
            "29     \t [0.20739069 0.06601235 0.08339898]. \t  0.3266638276141411 \t 3.5144420137944152\n",
            "30     \t [0.09177414 0.25874783 0.46582282]. \t  0.3098176725068744 \t 3.5144420137944152\n",
            "31     \t [0.59223853 0.49019339 0.514645  ]. \t  0.5636584653373499 \t 3.5144420137944152\n",
            "32     \t [0.72667423 0.33162898 0.13623505]. \t  0.25692809763748053 \t 3.5144420137944152\n",
            "33     \t [0.82268004 0.38351431 0.59432374]. \t  0.6940951772220839 \t 3.5144420137944152\n",
            "34     \t [0.68789817 0.31596021 0.63752013]. \t  0.9215222723989691 \t 3.5144420137944152\n",
            "35     \t [0.57752401 0.80350725 0.18556134]. \t  0.0312731366325119 \t 3.5144420137944152\n",
            "36     \t [0.19787002 0.1977767  0.83017448]. \t  1.2277701990258327 \t 3.5144420137944152\n",
            "37     \t [0.16067943 0.02817582 0.75723975]. \t  0.3137567177172803 \t 3.5144420137944152\n",
            "38     \t [0.72541457 0.57787923 0.33583694]. \t  0.16900693522232732 \t 3.5144420137944152\n",
            "39     \t [0.47827067 0.1364753  0.25670492]. \t  0.9585843576107357 \t 3.5144420137944152\n",
            "40     \t [0.23343479 0.59138584 0.30642539]. \t  0.3000119022807604 \t 3.5144420137944152\n",
            "41     \t [0.43976237 0.92250638 0.44201957]. \t  1.447177776885933 \t 3.5144420137944152\n",
            "42     \t [0.01103052 0.19701301 0.73397797]. \t  1.013933980043001 \t 3.5144420137944152\n",
            "43     \t [0.24307251 0.26563776 0.48045205]. \t  0.33360810781241124 \t 3.5144420137944152\n",
            "44     \t [0.26541441 0.20442255 0.21154234]. \t  0.8182007654284428 \t 3.5144420137944152\n",
            "45     \t [0.17598597 0.39637633 0.93595256]. \t  2.4420997621465594 \t 3.5144420137944152\n",
            "46     \t [0.37043523 0.84876641 0.75157091]. \t  1.8070542759678743 \t 3.5144420137944152\n",
            "47     \t [0.65497411 0.21224035 0.76263526]. \t  1.2312315716964708 \t 3.5144420137944152\n",
            "48     \t [0.40726517 0.24250585 0.70979656]. \t  1.1742944616992397 \t 3.5144420137944152\n",
            "49     \t [0.37505632 0.11231425 0.63764472]. \t  0.3404462066127 \t 3.5144420137944152\n",
            "50     \t [0.06155421 0.86108384 0.97645898]. \t  1.069465683646719 \t 3.5144420137944152\n",
            "51     \t [0.64534578 0.18739021 0.25470415]. \t  0.7539909915086813 \t 3.5144420137944152\n",
            "52     \t [0.28313966 0.41353302 0.27735699]. \t  0.4382617694804395 \t 3.5144420137944152\n",
            "53     \t [0.38503878 0.50627476 0.87408068]. \t  \u001b[92m3.727265148396073\u001b[0m \t 3.727265148396073\n",
            "54     \t [0.65515322 0.48343052 0.95854661]. \t  2.6390659543517048 \t 3.727265148396073\n",
            "55     \t [0.35869692 0.89270219 0.01228564]. \t  0.0007101787817227387 \t 3.727265148396073\n",
            "56     \t [0.61181179 0.36417187 0.10662379]. \t  0.20987490090205413 \t 3.727265148396073\n",
            "57     \t [0.93400767 0.1910905  0.00216429]. \t  0.04407628331929119 \t 3.727265148396073\n",
            "58     \t [0.26830483 0.30305693 0.19889753]. \t  0.5988107042239119 \t 3.727265148396073\n",
            "59     \t [0.27043163 0.2608227  0.27860543]. \t  0.7938419092915375 \t 3.727265148396073\n",
            "60     \t [0.34468722 0.65659973 0.86325841]. \t  3.5060239611180912 \t 3.727265148396073\n",
            "61     \t [0.16877957 0.60131644 0.76137919]. \t  3.1874531127871046 \t 3.727265148396073\n",
            "62     \t [0.8598495  0.09687106 0.70544899]. \t  0.45449322998722325 \t 3.727265148396073\n",
            "63     \t [0.16356561 0.91951492 0.89582497]. \t  1.104444384511528 \t 3.727265148396073\n",
            "64     \t [0.09449532 0.3713226  0.38575202]. \t  0.3889635066766884 \t 3.727265148396073\n",
            "65     \t [0.95270096 0.07614477 0.80783348]. \t  0.4895141696876869 \t 3.727265148396073\n",
            "66     \t [0.97256952 0.53652074 0.09495664]. \t  0.023834490049896122 \t 3.727265148396073\n",
            "67     \t [0.04743825 0.43780258 0.59339554]. \t  1.092959659135892 \t 3.727265148396073\n",
            "68     \t [0.37743864 0.46176713 0.30895875]. \t  0.36325840663334485 \t 3.727265148396073\n",
            "69     \t [0.20933675 0.14745675 0.3301066 ]. \t  0.8199469925635752 \t 3.727265148396073\n",
            "70     \t [0.09873953 0.08344149 0.80692709]. \t  0.5296855557748921 \t 3.727265148396073\n",
            "71     \t [0.81629061 0.76645914 0.85429327]. \t  2.4207171980182767 \t 3.727265148396073\n",
            "72     \t [0.50729649 0.54241612 0.82395397]. \t  \u001b[92m3.7483005109548158\u001b[0m \t 3.7483005109548158\n",
            "73     \t [0.84389819 0.06320748 0.9842202 ]. \t  0.20065465243633962 \t 3.7483005109548158\n",
            "74     \t [0.23833763 0.14661407 0.50546705]. \t  0.25480671405943567 \t 3.7483005109548158\n",
            "75     \t [0.42865986 0.34679638 0.33070755]. \t  0.5504560083351115 \t 3.7483005109548158\n",
            "76     \t [0.49552011 0.19991211 0.82820314]. \t  1.2497044559822954 \t 3.7483005109548158\n",
            "77     \t [0.41202769 0.53309334 0.45145642]. \t  0.6403108764028697 \t 3.7483005109548158\n",
            "78     \t [0.96633121 0.2334966  0.14465115]. \t  0.19060165348368643 \t 3.7483005109548158\n",
            "79     \t [0.83820039 0.8749031  0.41691695]. \t  0.3498651032944783 \t 3.7483005109548158\n",
            "80     \t [0.86449676 0.79579375 0.91435755]. \t  1.9827797305941421 \t 3.7483005109548158\n",
            "81     \t [0.93400194 0.03894668 0.94800781]. \t  0.2100098811139738 \t 3.7483005109548158\n",
            "82     \t [0.00313698 0.46054941 0.47015367]. \t  0.5733329196058139 \t 3.7483005109548158\n",
            "83     \t [0.29671487 0.24949484 0.68979245]. \t  1.0839801349581415 \t 3.7483005109548158\n",
            "84     \t [0.76412824 0.85355953 0.26043253]. \t  0.06421503493392128 \t 3.7483005109548158\n",
            "85     \t [0.72595035 0.82629906 0.78520751]. \t  1.643169958970201 \t 3.7483005109548158\n",
            "86     \t [0.21010636 0.94073467 0.28099994]. \t  0.2948632701610779 \t 3.7483005109548158\n",
            "87     \t [0.47684429 0.79057807 0.68083692]. \t  1.937658177452781 \t 3.7483005109548158\n",
            "88     \t [7.93175689e-01 2.53034714e-01 1.34429246e-04]. \t  0.05690454852728574 \t 3.7483005109548158\n",
            "89     \t [0.20677644 0.64188221 0.66110577]. \t  2.3710065060725087 \t 3.7483005109548158\n",
            "90     \t [0.59187883 0.47656673 0.53049912]. \t  0.5999908176225892 \t 3.7483005109548158\n",
            "91     \t [0.01786845 0.04463263 0.66603289]. \t  0.24235692844368947 \t 3.7483005109548158\n",
            "92     \t [0.58293223 0.27863226 0.96653618]. \t  1.2122270629110494 \t 3.7483005109548158\n",
            "93     \t [0.86670758 0.64452272 0.24326527]. \t  0.04633779647468605 \t 3.7483005109548158\n",
            "94     \t [0.23969353 0.81773721 0.53651922]. \t  2.8242970536633676 \t 3.7483005109548158\n",
            "95     \t [0.30600453 0.74269544 0.00722497]. \t  0.002870525365569 \t 3.7483005109548158\n",
            "96     \t [0.84062088 0.07715726 0.05192499]. \t  0.1255534664247606 \t 3.7483005109548158\n",
            "97     \t [0.0562497  0.94139369 0.10719894]. \t  0.007371544767094565 \t 3.7483005109548158\n",
            "98     \t [0.10695383 0.05917109 0.42824318]. \t  0.37253454997322105 \t 3.7483005109548158\n",
            "99     \t [0.16625909 0.72276611 0.68183679]. \t  2.5378207743566707 \t 3.7483005109548158\n",
            "100    \t [0.49060885 0.6520218  0.17341417]. \t  0.05712173825425049 \t 3.7483005109548158\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9kM4bcwSteJO",
        "outputId": "cbbc7eaf-a7b4-4788-f82e-ac9531d99d81"
      },
      "source": [
        "end_lose = time.time()\r\n",
        "end_lose\r\n",
        "\r\n",
        "time_lose = end_lose - start_lose\r\n",
        "time_lose\r\n",
        "\r\n",
        "start_win = time.time()\r\n",
        "start_win"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1614281633.1854038"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8NMzLM7nteQH",
        "outputId": "6cb5697a-c05c-4180-ffc0-3ea22c1bfe68"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 1 \r\n",
        "\r\n",
        "np.random.seed(run_num_1)\r\n",
        "surrogate_winner_1 = dtStudentProcess(d_cov_func, nu = df)\r\n",
        "\r\n",
        "winner_1 = dGPGO_stp(surrogate_winner_1, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_1.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.61217018 0.16906975 0.43605902]. \t  0.37345684462559386 \t 2.3951473341797507\n",
            "init   \t [0.76926247 0.2953253  0.14916296]. \t  0.296162062221701 \t 2.3951473341797507\n",
            "init   \t [0.02247832 0.42022449 0.23868214]. \t  0.2904113287153621 \t 2.3951473341797507\n",
            "init   \t [0.33765619 0.99071246 0.23772645]. \t  0.11013785080555143 \t 2.3951473341797507\n",
            "init   \t [0.08119266 0.66960024 0.62124292]. \t  2.3951473341797507 \t 2.3951473341797507\n",
            "1      \t [0.21486674 0.88986562 0.98663162]. \t  0.839948468419951 \t 2.3951473341797507\n",
            "2      \t [0.00378919 0.56279989 0.89454642]. \t  \u001b[92m3.6399151359264543\u001b[0m \t 3.6399151359264543\n",
            "3      \t [0.00746842 0.39300358 0.94016277]. \t  2.339565033428072 \t 3.6399151359264543\n",
            "4      \t [0.02045433 0.98005749 0.77689925]. \t  1.0663122182935734 \t 3.6399151359264543\n",
            "5      \t [0.89539121 0.31310218 0.96935805]. \t  1.3903958518763 \t 3.6399151359264543\n",
            "6      \t [0.93917213 0.93424427 0.20891204]. \t  0.01055650204852788 \t 3.6399151359264543\n",
            "7      \t [0.08615544 0.56761259 0.98584963]. \t  2.3270048049786505 \t 3.6399151359264543\n",
            "8      \t [1. 1. 1.]. \t  0.3168836207041784 \t 3.6399151359264543\n",
            "9      \t [0.94996377 0.34580668 0.64566026]. \t  1.02801522109446 \t 3.6399151359264543\n",
            "10     \t [0.99765482 0.148849   0.88443506]. \t  0.7642039859210052 \t 3.6399151359264543\n",
            "11     \t [0.5895512  0.73823633 0.05002187]. \t  0.005020571170527178 \t 3.6399151359264543\n",
            "12     \t [0.         0.46299883 0.66561452]. \t  1.8338088780596098 \t 3.6399151359264543\n",
            "13     \t [1.00815954e-13 1.77479044e-13 3.90905228e-14]. \t  0.06797411659021826 \t 3.6399151359264543\n",
            "14     \t [0.75239064 0.85398881 0.49073932]. \t  0.7925207165483433 \t 3.6399151359264543\n",
            "15     \t [0.59400988 0.01941556 0.03632513]. \t  0.15759446478491274 \t 3.6399151359264543\n",
            "16     \t [0.02553586 0.72072294 0.18294087]. \t  0.05167171478418788 \t 3.6399151359264543\n",
            "17     \t [0.42185206 0.62017454 0.31015084]. \t  0.27182819578739403 \t 3.6399151359264543\n",
            "18     \t [0.00663919 0.79054384 0.70655061]. \t  2.347550389027472 \t 3.6399151359264543\n",
            "19     \t [0.02053946 0.28780228 0.04637979]. \t  0.12007600979179595 \t 3.6399151359264543\n",
            "20     \t [0.90983242 0.00916165 0.46441914]. \t  0.12719298183908073 \t 3.6399151359264543\n",
            "21     \t [0.06309767 0.65345427 0.868633  ]. \t  3.4936647227960793 \t 3.6399151359264543\n",
            "22     \t [0.97597164 0.04050317 0.09753259]. \t  0.1314998427438988 \t 3.6399151359264543\n",
            "23     \t [0.97321375 0.57690342 0.89074347]. \t  3.556158380421015 \t 3.6399151359264543\n",
            "24     \t [0.03123008 0.7118464  0.95613842]. \t  2.3061131609220547 \t 3.6399151359264543\n",
            "25     \t [0.91912131 0.97563006 0.01185975]. \t  9.040569134966132e-05 \t 3.6399151359264543\n",
            "26     \t [0.96394235 0.53258846 0.02187466]. \t  0.010109384152401478 \t 3.6399151359264543\n",
            "27     \t [0.63492443 0.995248   0.69176767]. \t  0.8410879378656316 \t 3.6399151359264543\n",
            "28     \t [0.99230239 0.52351058 0.93875269]. \t  2.993408063429225 \t 3.6399151359264543\n",
            "29     \t [0.89141638 0.67730765 0.91814553]. \t  2.9294028017004843 \t 3.6399151359264543\n",
            "30     \t [0.99226598 0.72169445 0.81052812]. \t  2.5536443803688385 \t 3.6399151359264543\n",
            "31     \t [0.47607293 0.55137508 0.80025399]. \t  3.5791095991246396 \t 3.6399151359264543\n",
            "32     \t [0.71101097 0.00901092 0.75400609]. \t  0.26117129311085635 \t 3.6399151359264543\n",
            "33     \t [0.61339416 0.60304707 0.76239782]. \t  2.991881329619878 \t 3.6399151359264543\n",
            "34     \t [0.16687669 0.54052837 0.78127724]. \t  3.434817265043173 \t 3.6399151359264543\n",
            "35     \t [0.41455414 0.66548982 0.82786019]. \t  3.3767437970657825 \t 3.6399151359264543\n",
            "36     \t [0.18766655 0.56459387 0.86167077]. \t  \u001b[92m3.8441704692993284\u001b[0m \t 3.8441704692993284\n",
            "37     \t [0.21037632 0.66255547 0.84138677]. \t  3.4669152870097353 \t 3.8441704692993284\n",
            "38     \t [0.38060646 0.58872696 0.80576582]. \t  3.604408638049759 \t 3.8441704692993284\n",
            "39     \t [0.06057252 0.59925793 0.85793751]. \t  3.762180947631306 \t 3.8441704692993284\n",
            "40     \t [0.26925578 0.6264366  0.80334051]. \t  3.481932137597609 \t 3.8441704692993284\n",
            "41     \t [0.16929082 0.54635997 0.88184755]. \t  3.762365088266183 \t 3.8441704692993284\n",
            "42     \t [0.24095087 0.58064388 0.84129767]. \t  3.823514446684429 \t 3.8441704692993284\n",
            "43     \t [0.56887786 0.3899853  0.84302951]. \t  3.0085209141724856 \t 3.8441704692993284\n",
            "44     \t [0.22766568 0.65570893 0.83679411]. \t  3.5008531366987388 \t 3.8441704692993284\n",
            "45     \t [0.13949266 0.58434698 0.80765567]. \t  3.6428429638850135 \t 3.8441704692993284\n",
            "46     \t [0.24772692 0.54937425 0.83960904]. \t  3.8432960043734097 \t 3.8441704692993284\n",
            "47     \t [0.44900851 0.5679732  0.8737006 ]. \t  3.8063101521729306 \t 3.8441704692993284\n",
            "48     \t [0.21882402 0.52831011 0.77740868]. \t  3.3826694197370877 \t 3.8441704692993284\n",
            "49     \t [0.30557226 0.63262038 0.89108654]. \t  3.5313656850323594 \t 3.8441704692993284\n",
            "50     \t [0.31616103 0.50426769 0.92262127]. \t  3.2824300092661383 \t 3.8441704692993284\n",
            "51     \t [0.45033968 0.60614362 0.85492994]. \t  3.7536889141142105 \t 3.8441704692993284\n",
            "52     \t [0.16279702 0.60143592 0.83720975]. \t  3.755347519905478 \t 3.8441704692993284\n",
            "53     \t [0.26212259 0.54450501 0.85321913]. \t  \u001b[92m3.856996655758139\u001b[0m \t 3.856996655758139\n",
            "54     \t [0.58134017 0.51852564 0.88473367]. \t  3.6847066403012727 \t 3.856996655758139\n",
            "55     \t [0.25713987 0.52916421 0.86886936]. \t  3.8085033066851004 \t 3.856996655758139\n",
            "56     \t [0.07699087 0.50775658 0.87446323]. \t  3.7004565592772956 \t 3.856996655758139\n",
            "57     \t [0.33842959 0.45937641 0.83881326]. \t  3.5491149835100964 \t 3.856996655758139\n",
            "58     \t [0.40732625 0.5830222  0.90208144]. \t  3.6022559248617005 \t 3.856996655758139\n",
            "59     \t [0.69347922 0.54470331 0.8482715 ]. \t  3.7857528348996357 \t 3.856996655758139\n",
            "60     \t [0.07414375 0.54304308 0.82810303]. \t  3.77544594612987 \t 3.856996655758139\n",
            "61     \t [0.73597311 0.56345636 0.89913417]. \t  3.5905790596044986 \t 3.856996655758139\n",
            "62     \t [0.18486575 0.52475279 0.80260635]. \t  3.614042994733518 \t 3.856996655758139\n",
            "63     \t [0.59941207 0.43948578 0.85159169]. \t  3.3992038088873127 \t 3.856996655758139\n",
            "64     \t [0.15112422 0.57169865 0.8695877 ]. \t  3.8131284252695528 \t 3.856996655758139\n",
            "65     \t [0.40410938 0.57824072 0.87792711]. \t  3.780650678970588 \t 3.856996655758139\n",
            "66     \t [0.15043155 0.60970493 0.85780318]. \t  3.7470345628240413 \t 3.856996655758139\n",
            "67     \t [0.48825941 0.54786943 0.84111149]. \t  3.825718612689603 \t 3.856996655758139\n",
            "68     \t [0.50699069 0.56642576 0.82044352]. \t  3.7180845682191843 \t 3.856996655758139\n",
            "69     \t [0.59246936 0.46778066 0.80094262]. \t  3.3692328343847766 \t 3.856996655758139\n",
            "70     \t [0.38721004 0.57612644 0.92320677]. \t  3.373790110281817 \t 3.856996655758139\n",
            "71     \t [0.87121605 0.51465957 0.86455092]. \t  3.6752722130311613 \t 3.856996655758139\n",
            "72     \t [0.57211507 0.59941331 0.88418629]. \t  3.6759366690353437 \t 3.856996655758139\n",
            "73     \t [0.23277105 0.57522965 0.8168357 ]. \t  3.726925377999931 \t 3.856996655758139\n",
            "74     \t [0.58608631 0.49842849 0.885808  ]. \t  3.6103937158594017 \t 3.856996655758139\n",
            "75     \t [0.61272645 0.59809236 0.85115259]. \t  3.7384010388840903 \t 3.856996655758139\n",
            "76     \t [0.7269977  0.57082385 0.88398796]. \t  3.695271412511484 \t 3.856996655758139\n",
            "77     \t [0.27721662 0.52994776 0.86325565]. \t  3.8268835853923 \t 3.856996655758139\n",
            "78     \t [0.67140314 0.55580732 0.86485015]. \t  3.7913509336537112 \t 3.856996655758139\n",
            "79     \t [0.18477611 0.54516117 0.86883023]. \t  3.8234605269924815 \t 3.856996655758139\n",
            "80     \t [0.63034379 0.50944513 0.85905071]. \t  3.7451775816729453 \t 3.856996655758139\n",
            "81     \t [0.43129676 0.5334239  0.82135133]. \t  3.742661626962778 \t 3.856996655758139\n",
            "82     \t [0.44806218 0.58681816 0.86771332]. \t  3.7963378766715885 \t 3.856996655758139\n",
            "83     \t [0.6270471  0.49586455 0.81796222]. \t  3.593428772869353 \t 3.856996655758139\n",
            "84     \t [0.70779713 0.6097528  0.81398907]. \t  3.4781376440932648 \t 3.856996655758139\n",
            "85     \t [0.53335572 0.46638946 0.81415275]. \t  3.470500567163201 \t 3.856996655758139\n",
            "86     \t [0.23332432 0.62877414 0.83387459]. \t  3.641109929239353 \t 3.856996655758139\n",
            "87     \t [0.83826163 0.55417167 0.85918682]. \t  3.7413436192615395 \t 3.856996655758139\n",
            "88     \t [0.89334596 0.44237723 0.86156751]. \t  3.327007719824448 \t 3.856996655758139\n",
            "89     \t [0.13984423 0.60982397 0.83636603]. \t  3.7212531230165515 \t 3.856996655758139\n",
            "90     \t [0.71995123 0.58024509 0.83664808]. \t  3.710099442293096 \t 3.856996655758139\n",
            "91     \t [0.70822321 0.5796581  0.85518904]. \t  3.7598306231850467 \t 3.856996655758139\n",
            "92     \t [0.13551095 0.6147752  0.86099234]. \t  3.7217456557246953 \t 3.856996655758139\n",
            "93     \t [0.39429329 0.5761656  0.87345389]. \t  3.8035064243724515 \t 3.856996655758139\n",
            "94     \t [0.03374698 0.59886308 0.8778744 ]. \t  3.700730318087712 \t 3.856996655758139\n",
            "95     \t [0.78403402 0.56597045 0.87503318]. \t  3.72267201044991 \t 3.856996655758139\n",
            "96     \t [0.53850851 0.46645379 0.82533036]. \t  3.5282738515065253 \t 3.856996655758139\n",
            "97     \t [0.29182275 0.54383596 0.86352091]. \t  3.84537180982511 \t 3.856996655758139\n",
            "98     \t [0.35321769 0.542296   0.81064751]. \t  3.6913378740439233 \t 3.856996655758139\n",
            "99     \t [0.05766926 0.62078636 0.81547445]. \t  3.5677144595940287 \t 3.856996655758139\n",
            "100    \t [0.33421344 0.49738651 0.83747335]. \t  3.732287776915502 \t 3.856996655758139\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBLcW6tlteS2",
        "outputId": "806c93c2-014d-4390-fade-ae2992d63d57"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 2 \r\n",
        "\r\n",
        "np.random.seed(run_num_2)\r\n",
        "surrogate_winner_2 = dtStudentProcess(d_cov_func, nu = df)\r\n",
        "\r\n",
        "winner_2 = dGPGO_stp(surrogate_winner_2, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_2.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.85198549 0.0739036  0.89493176]. \t  0.3999083566189884 \t 2.6229838112516717\n",
            "init   \t [0.43649355 0.12767773 0.57585787]. \t  0.24461577848211966 \t 2.6229838112516717\n",
            "init   \t [0.84047092 0.43512055 0.69591056]. \t  1.8897715258798413 \t 2.6229838112516717\n",
            "init   \t [0.6846381  0.70064837 0.77969426]. \t  2.6229838112516717 \t 2.6229838112516717\n",
            "init   \t [0.64274937 0.96102617 0.10846489]. \t  0.003309399496320042 \t 2.6229838112516717\n",
            "1      \t [1. 1. 1.]. \t  0.31688362070415793 \t 2.6229838112516717\n",
            "2      \t [0.14306915 0.79984146 0.95066451]. \t  1.7295279521051767 \t 2.6229838112516717\n",
            "3      \t [0.70451525 0.49940513 0.97931313]. \t  2.336108674326301 \t 2.6229838112516717\n",
            "4      \t [0.52938878 0.70483034 0.91608078]. \t  \u001b[92m2.8340286041501397\u001b[0m \t 2.8340286041501397\n",
            "5      \t [0.48958532 0.7721246  0.95156477]. \t  1.9383011154967167 \t 2.8340286041501397\n",
            "6      \t [0.04386287 0.07126412 0.99295526]. \t  0.20152402793506508 \t 2.8340286041501397\n",
            "7      \t [0.07631634 0.70720492 0.11404836]. \t  0.018441585903548774 \t 2.8340286041501397\n",
            "8      \t [0.05791664 0.44460824 0.7658505 ]. \t  \u001b[92m2.9477635921657956\u001b[0m \t 2.9477635921657956\n",
            "9      \t [0.33964339 0.51124405 0.7699875 ]. \t  \u001b[92m3.257144939777752\u001b[0m \t 3.257144939777752\n",
            "10     \t [0.78468285 0.01926407 0.0865505 ]. \t  0.20305937919587458 \t 3.257144939777752\n",
            "11     \t [0.06561569 0.44227454 0.95907602]. \t  2.4191723694205547 \t 3.257144939777752\n",
            "12     \t [0.04862388 0.9230764  0.30150178]. \t  0.42412864709945247 \t 3.257144939777752\n",
            "13     \t [0.91255754 0.88174451 0.03086672]. \t  0.000337151646282495 \t 3.257144939777752\n",
            "14     \t [1.         0.65459563 1.        ]. \t  1.8636941774637454 \t 3.257144939777752\n",
            "15     \t [0.04821426 0.61333971 0.80724208]. \t  \u001b[92m3.5405713648184833\u001b[0m \t 3.5405713648184833\n",
            "16     \t [0.23013688 0.56930877 0.82416836]. \t  \u001b[92m3.775927298881307\u001b[0m \t 3.775927298881307\n",
            "17     \t [6.78391352e-13 4.70410187e-13 7.67365744e-13]. \t  0.06797411659114577 \t 3.775927298881307\n",
            "18     \t [0.99045739 0.77053538 0.75728835]. \t  1.6722653272663857 \t 3.775927298881307\n",
            "19     \t [0.12108581 0.70000619 0.72725056]. \t  2.663824070185629 \t 3.775927298881307\n",
            "20     \t [0.00894725 0.62990125 0.83362203]. \t  3.594473031500403 \t 3.775927298881307\n",
            "21     \t [0.00296845 0.9765885  0.19537327]. \t  0.0545303625110021 \t 3.775927298881307\n",
            "22     \t [0.22148101 0.01151277 0.68876366]. \t  0.21183104577447487 \t 3.775927298881307\n",
            "23     \t [0.87817202 0.03624724 0.33951727]. \t  0.368793577665302 \t 3.775927298881307\n",
            "24     \t [0.00835958 0.63443186 0.81896001]. \t  3.509777794661265 \t 3.775927298881307\n",
            "25     \t [0.41391073 0.48383787 0.88081984]. \t  3.5963993370518437 \t 3.775927298881307\n",
            "26     \t [0.46151361 0.59570575 0.85842538]. \t  \u001b[92m3.7852098322546457\u001b[0m \t 3.7852098322546457\n",
            "27     \t [0.46980423 0.57279022 0.85820528]. \t  \u001b[92m3.832673580012459\u001b[0m \t 3.832673580012459\n",
            "28     \t [0.48322615 0.53274634 0.87943162]. \t  3.7613730518804447 \t 3.832673580012459\n",
            "29     \t [0.42384702 0.57253823 0.95212339]. \t  2.9419119539708896 \t 3.832673580012459\n",
            "30     \t [0.5295348  0.51538895 0.84890122]. \t  3.7854413687607655 \t 3.832673580012459\n",
            "31     \t [0.90888604 0.96545518 0.00619077]. \t  8.893896367979272e-05 \t 3.832673580012459\n",
            "32     \t [0.990248   0.00478566 0.02114803]. \t  0.044965321970119126 \t 3.832673580012459\n",
            "33     \t [0.64809115 0.52606371 0.8650114 ]. \t  3.7716346250730917 \t 3.832673580012459\n",
            "34     \t [0.41368658 0.49491524 0.835066  ]. \t  3.7113332277745203 \t 3.832673580012459\n",
            "35     \t [0.78057579 0.00257238 0.5764697 ]. \t  0.09897455265074313 \t 3.832673580012459\n",
            "36     \t [0.52446646 0.42538516 0.        ]. \t  0.04214935656436371 \t 3.832673580012459\n",
            "37     \t [0.4826635  0.48490075 0.86572212]. \t  3.6624075294546854 \t 3.832673580012459\n",
            "38     \t [0.38136851 0.60434273 0.83911573]. \t  3.746499840778027 \t 3.832673580012459\n",
            "39     \t [0.60710752 0.447815   0.89512203]. \t  3.2611072655211046 \t 3.832673580012459\n",
            "40     \t [0.5650307  0.54113576 0.85499284]. \t  3.823470213946395 \t 3.832673580012459\n",
            "41     \t [0.34021976 0.55717189 0.80723497]. \t  3.6681416171723944 \t 3.832673580012459\n",
            "42     \t [0.54660369 0.54401127 0.82835665]. \t  3.762389483815609 \t 3.832673580012459\n",
            "43     \t [0.49931786 0.58169755 0.79506289]. \t  3.48506939135149 \t 3.832673580012459\n",
            "44     \t [0.56725983 0.53325564 0.83080834]. \t  3.761089018971504 \t 3.832673580012459\n",
            "45     \t [0.49866366 0.54371748 0.89243872]. \t  3.6890628546991833 \t 3.832673580012459\n",
            "46     \t [0.32547558 0.54368706 0.92243827]. \t  3.3800565734848127 \t 3.832673580012459\n",
            "47     \t [0.42653635 0.57399847 0.83322413]. \t  3.7941862056015063 \t 3.832673580012459\n",
            "48     \t [0.57570794 0.53694845 0.82757433]. \t  3.746576180373713 \t 3.832673580012459\n",
            "49     \t [0.40497702 0.51265689 0.85661087]. \t  3.7940664678501945 \t 3.832673580012459\n",
            "50     \t [0.52116156 0.48450572 0.83035491]. \t  3.637817056307405 \t 3.832673580012459\n",
            "51     \t [0.34778832 0.47747675 0.84175601]. \t  3.653147028738625 \t 3.832673580012459\n",
            "52     \t [0.61117145 0.51923446 0.8728748 ]. \t  3.7413211551130447 \t 3.832673580012459\n",
            "53     \t [0.1180892  0.63526547 0.84065818]. \t  3.616422415266471 \t 3.832673580012459\n",
            "54     \t [0.37576157 0.52481793 0.86638243]. \t  3.808336985104937 \t 3.832673580012459\n",
            "55     \t [0.48226707 0.57463894 0.80868508]. \t  3.6277266968489132 \t 3.832673580012459\n",
            "56     \t [0.52111716 0.52343965 0.8653481 ]. \t  3.7923889867455336 \t 3.832673580012459\n",
            "57     \t [0.31924269 0.56143664 0.85409188]. \t  \u001b[92m3.859601314366272\u001b[0m \t 3.859601314366272\n",
            "58     \t [0.38648507 0.53032199 0.82301447]. \t  3.755133951379634 \t 3.859601314366272\n",
            "59     \t [0.47623502 0.49173133 0.88654383]. \t  3.592277807250587 \t 3.859601314366272\n",
            "60     \t [0.51590262 0.51846292 0.88452678]. \t  3.6963737244081716 \t 3.859601314366272\n",
            "61     \t [0.485576   0.53183694 0.83641151]. \t  3.7999017316108517 \t 3.859601314366272\n",
            "62     \t [0.107029   0.50665358 0.91868822]. \t  3.3180233990559285 \t 3.859601314366272\n",
            "63     \t [0.46650849 0.60592685 0.86275566]. \t  3.7473308713447935 \t 3.859601314366272\n",
            "64     \t [0.43847356 0.59793798 0.84842608]. \t  3.778173540187726 \t 3.859601314366272\n",
            "65     \t [0.42816897 0.46804587 0.90267452]. \t  3.3391451977633273 \t 3.859601314366272\n",
            "66     \t [0.42623609 0.5281078  0.85228487]. \t  3.8301411214280363 \t 3.859601314366272\n",
            "67     \t [0.46238574 0.49504401 0.83860733]. \t  3.715384799219071 \t 3.859601314366272\n",
            "68     \t [0.59784482 0.55668387 0.84448433]. \t  3.8068403139249507 \t 3.859601314366272\n",
            "69     \t [0.163209   0.56551446 0.83080524]. \t  3.803875197987305 \t 3.859601314366272\n",
            "70     \t [0.48225033 0.57478968 0.81533564]. \t  3.678399549363609 \t 3.859601314366272\n",
            "71     \t [0.52253063 0.55401161 0.85968735]. \t  3.834817036262958 \t 3.859601314366272\n",
            "72     \t [0.47405417 0.50672556 0.87833924]. \t  3.7015129744085504 \t 3.859601314366272\n",
            "73     \t [0.30557287 0.58688871 0.82214269]. \t  3.733518106487918 \t 3.859601314366272\n",
            "74     \t [0.56799897 0.60827648 0.87773216]. \t  3.6783562756702284 \t 3.859601314366272\n",
            "75     \t [0.59673538 0.62455813 0.85180945]. \t  3.6365186159590905 \t 3.859601314366272\n",
            "76     \t [0.6352499  0.59514579 0.89203151]. \t  3.6247506078288407 \t 3.859601314366272\n",
            "77     \t [0.54651454 0.49047216 0.90089626]. \t  3.461171568695036 \t 3.859601314366272\n",
            "78     \t [0.37393504 0.58367036 0.88676031]. \t  3.724345108414402 \t 3.859601314366272\n",
            "79     \t [0.16340427 0.59303396 0.85038762]. \t  3.802050459252933 \t 3.859601314366272\n",
            "80     \t [0.16875896 0.52945189 0.87197804]. \t  3.787800678708562 \t 3.859601314366272\n",
            "81     \t [0.45448935 0.58273243 0.81921351]. \t  3.69733170069422 \t 3.859601314366272\n",
            "82     \t [0.32703156 0.60536996 0.81654932]. \t  3.6453131899497673 \t 3.859601314366272\n",
            "83     \t [0.51440238 0.60125752 0.81327941]. \t  3.5852517119162064 \t 3.859601314366272\n",
            "84     \t [0.18678114 0.64228259 0.83555866]. \t  3.5751278161433344 \t 3.859601314366272\n",
            "85     \t [0.41584193 0.52439283 0.80695005]. \t  3.634021378250848 \t 3.859601314366272\n",
            "86     \t [0.26801809 0.50687879 0.85802956]. \t  3.7760472710171906 \t 3.859601314366272\n",
            "87     \t [0.31276843 0.57565174 0.84958903]. \t  3.8443582752236947 \t 3.859601314366272\n",
            "88     \t [0.59166605 0.68008235 0.81711291]. \t  3.142820562465115 \t 3.859601314366272\n",
            "89     \t [0.50742804 0.53740165 0.87708214]. \t  3.7767955376161644 \t 3.859601314366272\n",
            "90     \t [0.51224068 0.54862888 0.89317643]. \t  3.6855394575354756 \t 3.859601314366272\n",
            "91     \t [0.37261464 0.51629931 0.86573894]. \t  3.7893139803170963 \t 3.859601314366272\n",
            "92     \t [0.48135873 0.55328054 0.85352339]. \t  3.84487718168799 \t 3.859601314366272\n",
            "93     \t [0.41922338 0.49405009 0.81753639]. \t  3.6292697590157914 \t 3.859601314366272\n",
            "94     \t [0.41504909 0.48712331 0.84323848]. \t  3.69678938202429 \t 3.859601314366272\n",
            "95     \t [0.61679254 0.45459377 0.85515913]. \t  3.4937685066485953 \t 3.859601314366272\n",
            "96     \t [0.62602202 0.54016026 0.88660207]. \t  3.704678447571813 \t 3.859601314366272\n",
            "97     \t [0.63912846 0.52727427 0.9035032 ]. \t  3.542494426485578 \t 3.859601314366272\n",
            "98     \t [0.43153917 0.60656983 0.83676732]. \t  3.7216121003839344 \t 3.859601314366272\n",
            "99     \t [0.61089737 0.50316034 0.87007051]. \t  3.703223225274818 \t 3.859601314366272\n",
            "100    \t [0.4813337  0.54814764 0.86210394]. \t  3.8376879206328613 \t 3.859601314366272\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "emgjXwfeuvRY",
        "outputId": "c304fd37-e62e-498a-84ea-fc1e5bf82897"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 3 \r\n",
        "\r\n",
        "np.random.seed(run_num_3)\r\n",
        "surrogate_winner_3 = dtStudentProcess(d_cov_func, nu = df)\r\n",
        "\r\n",
        "winner_3 = dGPGO_stp(surrogate_winner_3, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_3.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.75157561 0.10925128 0.48612128]. \t  0.19395982560567754 \t 0.5647137279144399\n",
            "init   \t [0.49983118 0.65711    0.23588471]. \t  0.10633623603471726 \t 0.5647137279144399\n",
            "init   \t [0.61279489 0.1196524  0.7122023 ]. \t  0.5647137279144399 \t 0.5647137279144399\n",
            "init   \t [0.37256054 0.52476827 0.26328708]. \t  0.24645583811739444 \t 0.5647137279144399\n",
            "init   \t [0.62163122 0.44909976 0.21906361]. \t  0.2633149120115605 \t 0.5647137279144399\n",
            "1      \t [0.11772068 0.14251025 0.82235537]. \t  \u001b[92m0.839257643081402\u001b[0m \t 0.839257643081402\n",
            "2      \t [0.39729708 0.30636754 0.99169899]. \t  \u001b[92m1.1547534875345482\u001b[0m \t 1.1547534875345482\n",
            "3      \t [1. 1. 1.]. \t  0.31688362070415665 \t 1.1547534875345482\n",
            "4      \t [0.23348226 0.41862732 0.97880473]. \t  \u001b[92m1.9978017168921371\u001b[0m \t 1.9978017168921371\n",
            "5      \t [0.05018267 0.95018324 0.96278164]. \t  0.6517797142407744 \t 1.9978017168921371\n",
            "6      \t [0.49266047 0.61433502 0.72285707]. \t  \u001b[92m2.5711510847438284\u001b[0m \t 2.5711510847438284\n",
            "7      \t [0.82308242 0.5629948  0.71751724]. \t  2.3019715434085493 \t 2.5711510847438284\n",
            "8      \t [0.0032138  0.48914681 0.85835288]. \t  \u001b[92m3.656660563088443\u001b[0m \t 3.656660563088443\n",
            "9      \t [0.00478938 0.57274751 0.60456592]. \t  1.78786636368351 \t 3.656660563088443\n",
            "10     \t [0.92447327 0.12892628 0.98241231]. \t  0.3667543063655288 \t 3.656660563088443\n",
            "11     \t [0.62340847 0.58373195 0.87663164]. \t  \u001b[92m3.739128332453949\u001b[0m \t 3.739128332453949\n",
            "12     \t [5.74009238e-14 9.65114246e-14 5.59736546e-14]. \t  0.06797411659021728 \t 3.739128332453949\n",
            "13     \t [0.58365483 0.65365599 0.95116192]. \t  2.724840274774978 \t 3.739128332453949\n",
            "14     \t [0.63951321 0.97013475 0.01417377]. \t  0.00026435776501290296 \t 3.739128332453949\n",
            "15     \t [0.48175283 0.0226142  0.05551754]. \t  0.22927233240664766 \t 3.739128332453949\n",
            "16     \t [0.03306194 0.82507298 0.04533608]. \t  0.0022807747427808956 \t 3.739128332453949\n",
            "17     \t [0.71874866 0.52230578 0.95817827]. \t  2.7596541737687836 \t 3.739128332453949\n",
            "18     \t [0.         0.30492384 0.0030914 ]. \t  0.05753369702275296 \t 3.739128332453949\n",
            "19     \t [0.44396543 0.62342187 0.00789836]. \t  0.010195777004606782 \t 3.739128332453949\n",
            "20     \t [0.69790727 0.99544456 0.73990806]. \t  0.6431117247097781 \t 3.739128332453949\n",
            "21     \t [0.03599419 0.43840359 0.95317763]. \t  2.480276463897923 \t 3.739128332453949\n",
            "22     \t [0.         0.09014052 0.4359023 ]. \t  0.29748429702985335 \t 3.739128332453949\n",
            "23     \t [0.7686757  0.03464826 0.04456144]. \t  0.13059536094003113 \t 3.739128332453949\n",
            "24     \t [0.63628287 0.43140225 0.82744579]. \t  3.301945503280332 \t 3.739128332453949\n",
            "25     \t [0.         0.73824289 0.82867601]. \t  2.8083788014280247 \t 3.739128332453949\n",
            "26     \t [0.66365884 0.56526884 0.79014635]. \t  3.3906402724144247 \t 3.739128332453949\n",
            "27     \t [0.92360991 0.97062721 0.62057901]. \t  0.42522114195613847 \t 3.739128332453949\n",
            "28     \t [0.       0.993069 0.      ]. \t  0.0002818551441934357 \t 3.739128332453949\n",
            "29     \t [0.02060651 0.42121952 0.85480121]. \t  3.237330901345551 \t 3.739128332453949\n",
            "30     \t [0.61826532 0.56253058 0.80753451]. \t  3.5867945205593967 \t 3.739128332453949\n",
            "31     \t [0.61552297 0.53965676 0.81942936]. \t  3.688020232674545 \t 3.739128332453949\n",
            "32     \t [0.60484571 0.53381928 0.80261062]. \t  3.55475110360348 \t 3.739128332453949\n",
            "33     \t [0.65527565 0.55350443 0.81852482]. \t  3.667627425161005 \t 3.739128332453949\n",
            "34     \t [0.70674863 0.55930282 0.84412267]. \t  \u001b[92m3.7706888619389622\u001b[0m \t 3.7706888619389622\n",
            "35     \t [0.62790561 0.53784135 0.8391928 ]. \t  \u001b[92m3.7805772248058744\u001b[0m \t 3.7805772248058744\n",
            "36     \t [0.60576319 0.51363866 0.80548959]. \t  3.5537296992483385 \t 3.7805772248058744\n",
            "37     \t [0.66658929 0.56721458 0.85074559]. \t  \u001b[92m3.7890497977152933\u001b[0m \t 3.7890497977152933\n",
            "38     \t [0.7520253  0.69189053 0.87578147]. \t  3.1232032563899685 \t 3.7890497977152933\n",
            "39     \t [0.60664403 0.54047743 0.87619714]. \t  3.766065604050793 \t 3.7890497977152933\n",
            "40     \t [0.72008024 0.58090337 0.82157947]. \t  3.6243384069749434 \t 3.7890497977152933\n",
            "41     \t [0.65350674 0.54769173 0.84953269]. \t  \u001b[92m3.800652356667139\u001b[0m \t 3.800652356667139\n",
            "42     \t [0.65099623 0.55291202 0.84681187]. \t  3.797329145055375 \t 3.800652356667139\n",
            "43     \t [0.54550224 0.52587804 0.83994678]. \t  3.7890004634846552 \t 3.800652356667139\n",
            "44     \t [0.6581031  0.5208937  0.88359422]. \t  3.6815363396012843 \t 3.800652356667139\n",
            "45     \t [0.45820569 0.49409701 0.88371908]. \t  3.6229391009548833 \t 3.800652356667139\n",
            "46     \t [0.97249381 0.         0.23997123]. \t  0.2858837084200273 \t 3.800652356667139\n",
            "47     \t [0.54351245 0.58962165 0.86740751]. \t  3.773309703394615 \t 3.800652356667139\n",
            "48     \t [0.25166014 0.51340591 0.87558378]. \t  3.7412096681875378 \t 3.800652356667139\n",
            "49     \t [0.57408266 0.57053759 0.83503565]. \t  3.7704975982396274 \t 3.800652356667139\n",
            "50     \t [0.25969364 0.52902961 0.84973823]. \t  \u001b[92m3.8369322853351155\u001b[0m \t 3.8369322853351155\n",
            "51     \t [0.11901834 0.59311894 0.83297125]. \t  3.758383339855081 \t 3.8369322853351155\n",
            "52     \t [0.06911114 0.52506993 0.89631325]. \t  3.602499283983059 \t 3.8369322853351155\n",
            "53     \t [0.24357704 0.61567184 0.88748768]. \t  3.6276003335145175 \t 3.8369322853351155\n",
            "54     \t [0.56686837 0.51289356 0.86660674]. \t  3.7542199582470954 \t 3.8369322853351155\n",
            "55     \t [0.59164521 0.57171477 0.87538924]. \t  3.770807359195711 \t 3.8369322853351155\n",
            "56     \t [0.36566355 0.53503986 0.85623199]. \t  \u001b[92m3.8449055294153487\u001b[0m \t 3.8449055294153487\n",
            "57     \t [0.24498004 0.59009332 0.87105141]. \t  3.788053641930685 \t 3.8449055294153487\n",
            "58     \t [0.56361574 0.56628537 0.845863  ]. \t  3.8118181713787456 \t 3.8449055294153487\n",
            "59     \t [0.28321513 0.50327565 0.86146394]. \t  3.7581642824722348 \t 3.8449055294153487\n",
            "60     \t [0.17950318 0.53593237 0.85858793]. \t  3.836189063703364 \t 3.8449055294153487\n",
            "61     \t [0.18432046 0.49270155 0.86234314]. \t  3.7051817907753026 \t 3.8449055294153487\n",
            "62     \t [0.21936996 0.5403341  0.90089973]. \t  3.6147193934223467 \t 3.8449055294153487\n",
            "63     \t [0.06976174 0.50321961 0.85339569]. \t  3.737688030401247 \t 3.8449055294153487\n",
            "64     \t [0.54873875 0.62619073 0.86346418]. \t  3.643008410697196 \t 3.8449055294153487\n",
            "65     \t [0.58338851 0.55725328 0.83633025]. \t  3.7860594558599807 \t 3.8449055294153487\n",
            "66     \t [0.61469236 0.56887713 0.87586421]. \t  3.766561504800875 \t 3.8449055294153487\n",
            "67     \t [0.47264715 0.55820682 0.87495748]. \t  3.8031944329508915 \t 3.8449055294153487\n",
            "68     \t [0.58936299 0.56308638 0.83495747]. \t  3.7745742853486774 \t 3.8449055294153487\n",
            "69     \t [0.25254703 0.53448438 0.8865151 ]. \t  3.727228051161397 \t 3.8449055294153487\n",
            "70     \t [0.52061396 0.57164109 0.82043551]. \t  3.7069417928063464 \t 3.8449055294153487\n",
            "71     \t [0.33328028 0.51139489 0.87065843]. \t  3.7594481466784977 \t 3.8449055294153487\n",
            "72     \t [0.22675221 0.56487659 0.85008996]. \t  \u001b[92m3.854844672106845\u001b[0m \t 3.854844672106845\n",
            "73     \t [0.48060546 0.64944957 0.85212793]. \t  3.5307626273104047 \t 3.854844672106845\n",
            "74     \t [0.09378274 0.61594291 0.87005848]. \t  3.6884097863203675 \t 3.854844672106845\n",
            "75     \t [0.1710871  0.46475473 0.86325729]. \t  3.5563538661481977 \t 3.854844672106845\n",
            "76     \t [0.57787263 0.57622032 0.81891335]. \t  3.670129280307974 \t 3.854844672106845\n",
            "77     \t [0.57525693 0.53439203 0.84825767]. \t  3.810615060114226 \t 3.854844672106845\n",
            "78     \t [0.1602778  0.55694057 0.83317456]. \t  3.816512120450125 \t 3.854844672106845\n",
            "79     \t [0.43553713 0.54754313 0.89048178]. \t  3.713805599922156 \t 3.854844672106845\n",
            "80     \t [0.12125842 0.59471263 0.86922824]. \t  3.766861277872451 \t 3.854844672106845\n",
            "81     \t [0.37133005 0.50762501 0.87410252]. \t  3.7322761984047457 \t 3.854844672106845\n",
            "82     \t [0.52337678 0.572788   0.9018212 ]. \t  3.6059766250623935 \t 3.854844672106845\n",
            "83     \t [0.16394604 0.61591689 0.85099492]. \t  3.726265183042934 \t 3.854844672106845\n",
            "84     \t [0.62750072 0.53773188 0.86484648]. \t  3.7948660038303808 \t 3.854844672106845\n",
            "85     \t [0.50115509 0.52666646 0.83816215]. \t  3.7946438266424045 \t 3.854844672106845\n",
            "86     \t [0.53166189 0.54440169 0.84133887]. \t  3.815828962281895 \t 3.854844672106845\n",
            "87     \t [0.50603792 0.52781481 0.85813076]. \t  3.8159391365068585 \t 3.854844672106845\n",
            "88     \t [0.60913984 0.50312324 0.89107168]. \t  3.5866404249682646 \t 3.854844672106845\n",
            "89     \t [0.12867569 0.6140194  0.89088936]. \t  3.596919807667831 \t 3.854844672106845\n",
            "90     \t [0.22236928 0.55235116 0.84878743]. \t  \u001b[92m3.856926445186091\u001b[0m \t 3.856926445186091\n",
            "91     \t [0.05076518 0.55714795 0.84886811]. \t  3.8255613419801833 \t 3.856926445186091\n",
            "92     \t [0.45347071 0.4992793  0.84919542]. \t  3.7471969204788125 \t 3.856926445186091\n",
            "93     \t [0.61053591 0.53856788 0.84349207]. \t  3.7975788903607492 \t 3.856926445186091\n",
            "94     \t [0.6304023  0.50538065 0.89664818]. \t  3.545445536524114 \t 3.856926445186091\n",
            "95     \t [0.03090641 0.56959179 0.84052233]. \t  3.799928659466585 \t 3.856926445186091\n",
            "96     \t [0.51004299 0.60296536 0.81282797]. \t  3.577596081551628 \t 3.856926445186091\n",
            "97     \t [0.62339006 0.52798632 0.87084512]. \t  3.765006092701534 \t 3.856926445186091\n",
            "98     \t [0.17207363 0.51920498 0.85439072]. \t  3.8064695194881484 \t 3.856926445186091\n",
            "99     \t [0.21977544 0.57941329 0.847677  ]. \t  3.835044023551469 \t 3.856926445186091\n",
            "100    \t [0.13229407 0.60139431 0.84149862]. \t  3.7614527533135584 \t 3.856926445186091\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8riJpBBKuvT4",
        "outputId": "59f2d351-a873-4847-fb0c-7cc0ac21041c"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 4 \r\n",
        "\r\n",
        "np.random.seed(run_num_4)\r\n",
        "surrogate_winner_4 = dtStudentProcess(d_cov_func, nu = df)\r\n",
        "\r\n",
        "winner_4 = dGPGO_stp(surrogate_winner_4, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_4.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.83936105 0.82129497 0.64220716]. \t  0.8796727111154451 \t 1.9592421489197056\n",
            "init   \t [0.66722262 0.03913991 0.06567239]. \t  0.21283931850025994 \t 1.9592421489197056\n",
            "init   \t [0.27650064 0.3163605  0.19359241]. \t  0.5590312289203825 \t 1.9592421489197056\n",
            "init   \t [0.19709288 0.84920429 0.84235809]. \t  1.818649590001362 \t 1.9592421489197056\n",
            "init   \t [0.34736473 0.7397423  0.49340914]. \t  1.9592421489197056 \t 1.9592421489197056\n",
            "1      \t [0.47647277 0.92393606 0.15672912]. \t  0.017844770068977132 \t 1.9592421489197056\n",
            "2      \t [0.62669901 0.00726163 0.89138442]. \t  0.21709467881616828 \t 1.9592421489197056\n",
            "3      \t [0.00577284 0.65866019 0.65878781]. \t  \u001b[92m2.3839373501852625\u001b[0m \t 2.3839373501852625\n",
            "4      \t [0.07213246 0.95861543 0.39059898]. \t  1.2397133678239307 \t 2.3839373501852625\n",
            "5      \t [0.26929835 0.45317323 0.7189162 ]. \t  \u001b[92m2.450859292838306\u001b[0m \t 2.450859292838306\n",
            "6      \t [0.00769853 0.46667349 0.73545673]. \t  \u001b[92m2.6840062457678893\u001b[0m \t 2.6840062457678893\n",
            "7      \t [0.03841704 0.03599917 0.67043243]. \t  0.2324559504688195 \t 2.6840062457678893\n",
            "8      \t [0.24176347 0.4651597  0.92960141]. \t  \u001b[92m3.0028884758028953\u001b[0m \t 3.0028884758028953\n",
            "9      \t [0.04681794 0.56517096 0.99093683]. \t  2.2288293321682677 \t 3.0028884758028953\n",
            "10     \t [0.80976234 0.53938253 0.97530895]. \t  2.4743499879148376 \t 3.0028884758028953\n",
            "11     \t [0.69059538 0.88801551 0.9878112 ]. \t  0.8300210142211342 \t 3.0028884758028953\n",
            "12     \t [0.95812486 0.29223318 0.07273536]. \t  0.08339625301849192 \t 3.0028884758028953\n",
            "13     \t [1.17893283e-14 2.24470538e-15 2.95420920e-14]. \t  0.06797411659016658 \t 3.0028884758028953\n",
            "14     \t [0.61255918 0.52208875 0.99963714]. \t  2.041540973187679 \t 3.0028884758028953\n",
            "15     \t [0.96978735 0.2939813  0.96027593]. \t  1.335065938568058 \t 3.0028884758028953\n",
            "16     \t [0.99338067 0.74763339 0.99347948]. \t  1.5409311203104863 \t 3.0028884758028953\n",
            "17     \t [0.06021121 0.48513292 0.03397306]. \t  0.0380359887693118 \t 3.0028884758028953\n",
            "18     \t [0.57179646 0.2388695  0.01650496]. \t  0.11544904783707856 \t 3.0028884758028953\n",
            "19     \t [0.28866373 0.04051792 0.93933776]. \t  0.23265025596374495 \t 3.0028884758028953\n",
            "20     \t [0.99883873 0.39347614 0.70061501]. \t  1.7707009570386418 \t 3.0028884758028953\n",
            "21     \t [0.22934553 0.64604589 0.77403917]. \t  \u001b[92m3.1659500609028797\u001b[0m \t 3.1659500609028797\n",
            "22     \t [0.05012095 0.4915082  0.87160813]. \t  \u001b[92m3.6424220797210607\u001b[0m \t 3.6424220797210607\n",
            "23     \t [0.87698206 0.03553835 0.5090948 ]. \t  0.10843267047381704 \t 3.6424220797210607\n",
            "24     \t [0.97913111 0.74667696 0.02899248]. \t  0.001195883839156326 \t 3.6424220797210607\n",
            "25     \t [0.15438836 0.         0.        ]. \t  0.08906286407512769 \t 3.6424220797210607\n",
            "26     \t [0.02415485 0.40795749 0.88236207]. \t  3.022914884934102 \t 3.6424220797210607\n",
            "27     \t [0.94175874 0.49212269 0.2399686 ]. \t  0.09408504982178667 \t 3.6424220797210607\n",
            "28     \t [0.18322597 0.59983894 0.71929542]. \t  2.756344038295855 \t 3.6424220797210607\n",
            "29     \t [0.06035345 0.52719872 0.86019436]. \t  \u001b[92m3.7944098406070497\u001b[0m \t 3.7944098406070497\n",
            "30     \t [0.45579397 1.         0.75134638]. \t  0.8949034041139163 \t 3.7944098406070497\n",
            "31     \t [0.13998293 0.56200535 0.80580691]. \t  3.6561290462256792 \t 3.7944098406070497\n",
            "32     \t [0.10842962 0.66505445 0.79527955]. \t  3.233937187903374 \t 3.7944098406070497\n",
            "33     \t [0.16048217 0.62757336 0.84816656]. \t  3.672492119943112 \t 3.7944098406070497\n",
            "34     \t [0.00507357 0.53594707 0.83557673]. \t  3.7747671570811914 \t 3.7944098406070497\n",
            "35     \t [0.48237038 0.68573463 0.83853371]. \t  3.2464143647034645 \t 3.7944098406070497\n",
            "36     \t [0.51143861 0.62619542 0.79414846]. \t  3.322430600947632 \t 3.7944098406070497\n",
            "37     \t [6.18937631e-04 5.61939383e-01 7.87825423e-01]. \t  3.463230550639951 \t 3.7944098406070497\n",
            "38     \t [0.345101   0.64849164 0.80411743]. \t  3.363586792989034 \t 3.7944098406070497\n",
            "39     \t [0.4198417  0.59995917 0.84089014]. \t  3.7596224936499025 \t 3.7944098406070497\n",
            "40     \t [0.16914052 0.6399334  0.81078637]. \t  3.4736044572993032 \t 3.7944098406070497\n",
            "41     \t [0.14815752 0.55355574 0.84428607]. \t  \u001b[92m3.842927317947709\u001b[0m \t 3.842927317947709\n",
            "42     \t [0.17737815 0.63360898 0.86178758]. \t  3.640950692577504 \t 3.842927317947709\n",
            "43     \t [0.20091864 0.54388982 0.87296181]. \t  3.8090430531455284 \t 3.842927317947709\n",
            "44     \t [0.1773096  0.58426096 0.8605727 ]. \t  3.819443589101862 \t 3.842927317947709\n",
            "45     \t [0.04766799 0.56052349 0.86187567]. \t  3.816730851560058 \t 3.842927317947709\n",
            "46     \t [0.27805725 0.59470274 0.85747296]. \t  3.8051821097914003 \t 3.842927317947709\n",
            "47     \t [0.57425335 0.571097   0.83936996]. \t  3.7857018947828336 \t 3.842927317947709\n",
            "48     \t [0.41977168 0.58104903 0.89339207]. \t  3.678877292987399 \t 3.842927317947709\n",
            "49     \t [0.33557114 0.53364053 0.9002346 ]. \t  3.617387431618054 \t 3.842927317947709\n",
            "50     \t [0.41930735 0.56710037 0.84365213]. \t  3.8364358065875943 \t 3.842927317947709\n",
            "51     \t [0.25020804 0.60018656 0.79296863]. \t  3.491157945225277 \t 3.842927317947709\n",
            "52     \t [0.17701413 0.67187957 0.82934215]. \t  3.3716206169167524 \t 3.842927317947709\n",
            "53     \t [0.51740682 0.41843914 0.84222125]. \t  3.2540879130775586 \t 3.842927317947709\n",
            "54     \t [0.34305701 0.60170077 0.89404034]. \t  3.6310623203659165 \t 3.842927317947709\n",
            "55     \t [0.02419428 0.50930589 0.84433735]. \t  3.7423666095989754 \t 3.842927317947709\n",
            "56     \t [0.00361544 0.61760212 0.85461232]. \t  3.6813321324021167 \t 3.842927317947709\n",
            "57     \t [0.13622884 0.60497684 0.85241207]. \t  3.763016107758704 \t 3.842927317947709\n",
            "58     \t [0.43775782 0.56235632 0.84066936]. \t  3.830470981307728 \t 3.842927317947709\n",
            "59     \t [0.52390433 0.57116564 0.84955575]. \t  3.8215643864887063 \t 3.842927317947709\n",
            "60     \t [0.0455202  0.58940035 0.85480938]. \t  3.785686204162433 \t 3.842927317947709\n",
            "61     \t [0.48633686 0.62352666 0.80665594]. \t  3.456061225541313 \t 3.842927317947709\n",
            "62     \t [0.10814925 0.58528168 0.84437839]. \t  3.803888267383758 \t 3.842927317947709\n",
            "63     \t [0.1396996 0.5275263 0.8484843]. \t  3.819842137758962 \t 3.842927317947709\n",
            "64     \t [0.1411614  0.62783927 0.84423651]. \t  3.663620184328794 \t 3.842927317947709\n",
            "65     \t [0.29638078 0.46240392 0.86898714]. \t  3.535117609690714 \t 3.842927317947709\n",
            "66     \t [0.35955414 0.51797118 0.82401589]. \t  3.7412080885452865 \t 3.842927317947709\n",
            "67     \t [0.45618312 0.50450913 0.85197971]. \t  3.765776905597936 \t 3.842927317947709\n",
            "68     \t [0.03434395 0.61503725 0.84049015]. \t  3.6869655957680747 \t 3.842927317947709\n",
            "69     \t [0.40807211 0.6017446  0.16536546]. \t  0.0811633230047678 \t 3.842927317947709\n",
            "70     \t [0.96913938 0.95179477 0.20167911]. \t  0.0075758744813107124 \t 3.842927317947709\n",
            "71     \t [0.3149807  0.603116   0.86713189]. \t  3.764423102212528 \t 3.842927317947709\n",
            "72     \t [0.03088413 0.60827554 0.85780866]. \t  3.7244194856172266 \t 3.842927317947709\n",
            "73     \t [0.34249521 0.49549684 0.85045221]. \t  3.7407205817697693 \t 3.842927317947709\n",
            "74     \t [0.36679955 0.57648375 0.82818887]. \t  3.777892794677525 \t 3.842927317947709\n",
            "75     \t [0.41148259 0.54683101 0.85695995]. \t  \u001b[92m3.851987105943082\u001b[0m \t 3.851987105943082\n",
            "76     \t [0.45203417 0.53567756 0.8016389 ]. \t  3.5944493182566726 \t 3.851987105943082\n",
            "77     \t [0.63058655 0.49573635 0.84742422]. \t  3.700093029028868 \t 3.851987105943082\n",
            "78     \t [0.31708664 0.59040173 0.839278  ]. \t  3.7960391732963865 \t 3.851987105943082\n",
            "79     \t [0.43664338 0.5805793  0.84447845]. \t  3.8154392002067197 \t 3.851987105943082\n",
            "80     \t [0.57744466 0.60118229 0.8675968 ]. \t  3.7318203638374747 \t 3.851987105943082\n",
            "81     \t [0.30980606 0.57104419 0.88094912]. \t  3.777578981236617 \t 3.851987105943082\n",
            "82     \t [0.37968994 0.6045637  0.84693333]. \t  3.7635430607716973 \t 3.851987105943082\n",
            "83     \t [0.23978245 0.58549277 0.83966287]. \t  3.810005107189879 \t 3.851987105943082\n",
            "84     \t [0.72757711 0.56533062 0.84228534]. \t  3.7527493061738038 \t 3.851987105943082\n",
            "85     \t [0.37562798 0.52557347 0.84284278]. \t  3.8211723693056974 \t 3.851987105943082\n",
            "86     \t [0.67781563 0.45205189 0.85020698]. \t  3.4675062880668777 \t 3.851987105943082\n",
            "87     \t [0.4744085  0.55961409 0.85541898]. \t  3.844668927177353 \t 3.851987105943082\n",
            "88     \t [0.5341241  0.5686226  0.83691554]. \t  3.7915215009773213 \t 3.851987105943082\n",
            "89     \t [0.11626632 0.63601205 0.87260692]. \t  3.592441242631219 \t 3.851987105943082\n",
            "90     \t [0.68412909 0.49262305 0.85760465]. \t  3.6737581133762682 \t 3.851987105943082\n",
            "91     \t [0.39933524 0.6255051  0.8745057 ]. \t  3.6472043516629586 \t 3.851987105943082\n",
            "92     \t [0.62228295 0.56596856 0.84842284]. \t  3.80056136965575 \t 3.851987105943082\n",
            "93     \t [0.03680639 0.57418346 0.87739763]. \t  3.7527839854085934 \t 3.851987105943082\n",
            "94     \t [0.35697267 0.49298132 0.81787027]. \t  3.6338928690176866 \t 3.851987105943082\n",
            "95     \t [0.36220971 0.5266912  0.82820184]. \t  3.777955357581823 \t 3.851987105943082\n",
            "96     \t [0.64993523 0.48152374 0.83693551]. \t  3.6175306142691954 \t 3.851987105943082\n",
            "97     \t [0.64640884 0.45045481 0.8186956 ]. \t  3.384623877365808 \t 3.851987105943082\n",
            "98     \t [0.0485262  0.62554318 0.82240134]. \t  3.581926018421969 \t 3.851987105943082\n",
            "99     \t [0.05970461 0.53394763 0.87106289]. \t  3.7765887954417665 \t 3.851987105943082\n",
            "100    \t [0.61173629 0.59357303 0.82404951]. \t  3.6487401394613777 \t 3.851987105943082\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mrvkDXP0uvWU",
        "outputId": "e1cec378-771d-4322-f2c2-098c5d16a083"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 5 \r\n",
        "\r\n",
        "np.random.seed(run_num_5)\r\n",
        "surrogate_winner_5 = dtStudentProcess(d_cov_func, nu = df)\r\n",
        "\r\n",
        "winner_5 = dGPGO_stp(surrogate_winner_5, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_5.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.55735327 0.54176063 0.56042378]. \t  0.9599610140567721 \t 0.9810564697651996\n",
            "init   \t [0.38204809 0.11909418 0.84599912]. \t  0.6913364116577386 \t 0.9810564697651996\n",
            "init   \t [0.73974161 0.93235756 0.85932801]. \t  0.9810564697651996 \t 0.9810564697651996\n",
            "init   \t [0.70810463 0.68096875 0.4086023 ]. \t  0.4017443293787304 \t 0.9810564697651996\n",
            "init   \t [0.77458462 0.9844762  0.96548486]. \t  0.4769494155906823 \t 0.9810564697651996\n",
            "1      \t [0.0655006  0.90661895 0.81035589]. \t  \u001b[92m1.403888408720423\u001b[0m \t 1.403888408720423\n",
            "2      \t [0.19464317 0.98816634 0.61681324]. \t  \u001b[92m2.3709418876142516\u001b[0m \t 2.3709418876142516\n",
            "3      \t [0.02311239 0.85121629 0.01672445]. \t  0.000979175972205631 \t 2.3709418876142516\n",
            "4      \t [0.48250701 0.96713222 0.64026967]. \t  1.5877782501640503 \t 2.3709418876142516\n",
            "5      \t [0.87084589 0.09247934 0.94679426]. \t  0.353707208137631 \t 2.3709418876142516\n",
            "6      \t [0.02696058 0.07840881 0.38188613]. \t  0.4731045438129057 \t 2.3709418876142516\n",
            "7      \t [0.04739328 0.92232176 0.3811972 ]. \t  1.175417890835531 \t 2.3709418876142516\n",
            "8      \t [0.9288332  0.06863405 0.08101552]. \t  0.13465766405489876 \t 2.3709418876142516\n",
            "9      \t [0.20373356 0.         0.        ]. \t  0.09421206088440288 \t 2.3709418876142516\n",
            "10     \t [0.78455302 0.00373135 0.52721638]. \t  0.10390003353747995 \t 2.3709418876142516\n",
            "11     \t [0.96938496 0.54837528 0.76980595]. \t  \u001b[92m3.011308783092942\u001b[0m \t 3.011308783092942\n",
            "12     \t [0.96892689 0.54072357 0.76898711]. \t  3.0068094564462617 \t 3.011308783092942\n",
            "13     \t [0.98807957 0.45784469 0.4473364 ]. \t  0.12847389649892899 \t 3.011308783092942\n",
            "14     \t [0.99791449 0.57534855 0.97766359]. \t  2.3990967251404487 \t 3.011308783092942\n",
            "15     \t [1.         1.         0.88794661]. \t  0.5358004484596874 \t 3.011308783092942\n",
            "16     \t [0.36014358 0.89402807 0.00757341]. \t  0.0006265479450828218 \t 3.011308783092942\n",
            "17     \t [0.08628409 0.16019415 0.85775701]. \t  0.9141776354738065 \t 3.011308783092942\n",
            "18     \t [0.02820986 0.0106109  0.6507289 ]. \t  0.1650994313028501 \t 3.011308783092942\n",
            "19     \t [0.0072136  0.96410013 0.67003456]. \t  1.998398303282005 \t 3.011308783092942\n",
            "20     \t [0.88390866 0.48087642 0.82299181]. \t  \u001b[92m3.4818597706872505\u001b[0m \t 3.4818597706872505\n",
            "21     \t [2.27472747e-12 7.80569980e-12 5.14627458e-12]. \t  0.06797411659732641 \t 3.4818597706872505\n",
            "22     \t [0.79376456 0.35674385 0.78195552]. \t  2.4476370164713472 \t 3.4818597706872505\n",
            "23     \t [0.57350707 0.41175336 0.99261932]. \t  1.7496484246582502 \t 3.4818597706872505\n",
            "24     \t [0.0148067  0.42265294 0.018071  ]. \t  0.04191070937733527 \t 3.4818597706872505\n",
            "25     \t [0.94608193 0.46690999 0.72274414]. \t  2.2971339807957656 \t 3.4818597706872505\n",
            "26     \t [0.00283593 0.97434653 0.98676217]. \t  0.4547206903766693 \t 3.4818597706872505\n",
            "27     \t [0.90411442 0.60304658 0.84879234]. \t  \u001b[92m3.607154254180408\u001b[0m \t 3.607154254180408\n",
            "28     \t [0.97160033 0.77770122 0.25395824]. \t  0.023822375083256013 \t 3.607154254180408\n",
            "29     \t [0.02286773 0.44673075 0.55611908]. \t  0.887660605482282 \t 3.607154254180408\n",
            "30     \t [0.77849986 0.70802806 0.83620698]. \t  2.932133741537937 \t 3.607154254180408\n",
            "31     \t [0.81164724 0.59891304 0.85640043]. \t  \u001b[92m3.6714963641016114\u001b[0m \t 3.6714963641016114\n",
            "32     \t [0.81162262 0.6090705  0.82299695]. \t  3.5023898687987574 \t 3.6714963641016114\n",
            "33     \t [0.7602426  0.62921037 0.80377715]. \t  3.2712660387142543 \t 3.6714963641016114\n",
            "34     \t [0.45637316 0.01803572 0.9804964 ]. \t  0.13408915496068086 \t 3.6714963641016114\n",
            "35     \t [0.89344645 0.57471898 0.86897525]. \t  \u001b[92m3.688275084669724\u001b[0m \t 3.688275084669724\n",
            "36     \t [0.92174537 0.5914533  0.8027055 ]. \t  3.3402879953448137 \t 3.688275084669724\n",
            "37     \t [0.84993793 0.4978271  0.85281292]. \t  3.6427856974186152 \t 3.688275084669724\n",
            "38     \t [0.81002022 0.63617804 0.82510157]. \t  3.383506159389367 \t 3.688275084669724\n",
            "39     \t [0.87563649 0.54587517 0.86958951]. \t  \u001b[92m3.7095604353307214\u001b[0m \t 3.7095604353307214\n",
            "40     \t [0.92531684 0.60065322 0.84231725]. \t  3.5871775871524103 \t 3.7095604353307214\n",
            "41     \t [0.88014199 0.60896599 0.83595635]. \t  3.5487593716896586 \t 3.7095604353307214\n",
            "42     \t [0.9268305  0.52291586 0.90862034]. \t  3.3969169508233197 \t 3.7095604353307214\n",
            "43     \t [0.88191411 0.59086158 0.83676283]. \t  3.616331074454239 \t 3.7095604353307214\n",
            "44     \t [0.86325237 0.61129982 0.83127713]. \t  3.522051354946078 \t 3.7095604353307214\n",
            "45     \t [0.86000152 0.60967761 0.80757485]. \t  3.350070472744214 \t 3.7095604353307214\n",
            "46     \t [0.8184308  0.61896215 0.92959706]. \t  3.1220877313202333 \t 3.7095604353307214\n",
            "47     \t [0.81690748 0.57290101 0.82892665]. \t  3.6477269001551793 \t 3.7095604353307214\n",
            "48     \t [0.86105055 0.61344107 0.84964834]. \t  3.5861889840072236 \t 3.7095604353307214\n",
            "49     \t [0.94937363 0.30806137 0.00509099]. \t  0.032114526776805874 \t 3.7095604353307214\n",
            "50     \t [0.85141289 0.58830576 0.8885907 ]. \t  3.5978967712159693 \t 3.7095604353307214\n",
            "51     \t [0.84160445 0.61467685 0.81414651]. \t  3.395832181860647 \t 3.7095604353307214\n",
            "52     \t [0.94552184 0.58556907 0.8313379 ]. \t  3.574994138750172 \t 3.7095604353307214\n",
            "53     \t [0.86158426 0.62921236 0.9073933 ]. \t  3.3118303179401987 \t 3.7095604353307214\n",
            "54     \t [0.87694618 0.64469372 0.82128397]. \t  3.275247782927709 \t 3.7095604353307214\n",
            "55     \t [0.68913829 0.45728796 0.86127606]. \t  3.4847848102758077 \t 3.7095604353307214\n",
            "56     \t [0.82871617 0.61039187 0.83011112]. \t  3.5346660628031783 \t 3.7095604353307214\n",
            "57     \t [0.74213563 0.57851967 0.8253487 ]. \t  3.6457011761976323 \t 3.7095604353307214\n",
            "58     \t [0.9162584  0.58488263 0.82908129]. \t  3.57751936043747 \t 3.7095604353307214\n",
            "59     \t [0.74196048 0.53800498 0.82967496]. \t  3.704058502431782 \t 3.7095604353307214\n",
            "60     \t [0.86508941 0.53371121 0.85200699]. \t  \u001b[92m3.720289893209919\u001b[0m \t 3.720289893209919\n",
            "61     \t [0.65074648 0.54860063 0.88167849]. \t  \u001b[92m3.7351235383874797\u001b[0m \t 3.7351235383874797\n",
            "62     \t [0.82490678 0.5189799  0.83829608]. \t  3.6855727676536727 \t 3.7351235383874797\n",
            "63     \t [0.73339024 0.56129009 0.80457214]. \t  3.5140746040642146 \t 3.7351235383874797\n",
            "64     \t [0.7051219  0.54793919 0.89625321]. \t  3.6212829907243425 \t 3.7351235383874797\n",
            "65     \t [0.84795184 0.59995047 0.80827706]. \t  3.400935597111914 \t 3.7351235383874797\n",
            "66     \t [0.67776442 0.49248373 0.85177576]. \t  3.677648547128336 \t 3.7351235383874797\n",
            "67     \t [0.80534219 0.5825008  0.83695036]. \t  3.6725186506084895 \t 3.7351235383874797\n",
            "68     \t [0.81281975 0.54964765 0.86170043]. \t  \u001b[92m3.7493539914105813\u001b[0m \t 3.7493539914105813\n",
            "69     \t [0.8422598  0.57071494 0.79679998]. \t  3.370701265323577 \t 3.7493539914105813\n",
            "70     \t [0.98217014 0.56173761 0.79848605]. \t  3.3378919819317927 \t 3.7493539914105813\n",
            "71     \t [0.48620735 0.45773621 0.84542803]. \t  3.5366543296645885 \t 3.7493539914105813\n",
            "72     \t [0.00868111 0.99328083 0.05013461]. \t  0.0012906571215203404 \t 3.7493539914105813\n",
            "73     \t [0.6439138  0.5257653  0.83685086]. \t  \u001b[92m3.7538387283683234\u001b[0m \t 3.7538387283683234\n",
            "74     \t [0.84456955 0.66688763 0.83920572]. \t  3.2458458195276965 \t 3.7538387283683234\n",
            "75     \t [0.64609746 0.55084923 0.84020769]. \t  \u001b[92m3.782804589725961\u001b[0m \t 3.782804589725961\n",
            "76     \t [0.88217378 0.5240695  0.81945079]. \t  3.574933571371056 \t 3.782804589725961\n",
            "77     \t [0.62233522 0.5170035  0.80870654]. \t  3.581271820689804 \t 3.782804589725961\n",
            "78     \t [0.98408581 0.52759336 0.89806879]. \t  3.4830702198668257 \t 3.782804589725961\n",
            "79     \t [0.48747185 0.51787539 0.88924274]. \t  3.6660860758892184 \t 3.782804589725961\n",
            "80     \t [0.9176533  0.74346029 0.01234191]. \t  0.0011909798642173906 \t 3.782804589725961\n",
            "81     \t [0.69942403 0.56800144 0.86484828]. \t  3.775263513702334 \t 3.782804589725961\n",
            "82     \t [0.99194538 0.59858744 0.80083694]. \t  3.261704538193528 \t 3.782804589725961\n",
            "83     \t [0.51757383 0.53352718 0.83084139]. \t  3.773780998894142 \t 3.782804589725961\n",
            "84     \t [0.70429795 0.56449392 0.83743811]. \t  3.745325535625585 \t 3.782804589725961\n",
            "85     \t [0.70315372 0.56390855 0.77904578]. \t  3.2430808588451843 \t 3.782804589725961\n",
            "86     \t [0.62299063 0.6285273  0.822756  ]. \t  3.492418605923472 \t 3.782804589725961\n",
            "87     \t [0.48501865 0.5825682  0.83946504]. \t  \u001b[92m3.7885320123088384\u001b[0m \t 3.7885320123088384\n",
            "88     \t [0.60455173 0.53300249 0.79838786]. \t  3.514508486216223 \t 3.7885320123088384\n",
            "89     \t [0.73652685 0.60689971 0.81067557]. \t  3.4485784999511853 \t 3.7885320123088384\n",
            "90     \t [0.94585348 0.54744085 0.87309181]. \t  3.6706426701273407 \t 3.7885320123088384\n",
            "91     \t [0.7537782  0.55045172 0.81144631]. \t  3.5763582244177963 \t 3.7885320123088384\n",
            "92     \t [0.75203283 0.60532633 0.81919063]. \t  3.5166826973413396 \t 3.7885320123088384\n",
            "93     \t [0.73956008 0.57673408 0.83125337]. \t  3.684551351364524 \t 3.7885320123088384\n",
            "94     \t [0.5110653  0.4925732  0.82948548]. \t  3.668783324154873 \t 3.7885320123088384\n",
            "95     \t [0.72370151 0.5154064  0.88829475]. \t  3.62007234293138 \t 3.7885320123088384\n",
            "96     \t [0.65458847 0.5768877  0.87168664]. \t  3.7606797622726296 \t 3.7885320123088384\n",
            "97     \t [0.73929426 0.58911903 0.84382759]. \t  3.7064899281311368 \t 3.7885320123088384\n",
            "98     \t [0.66687693 0.59593538 0.87857486]. \t  3.6899758230194353 \t 3.7885320123088384\n",
            "99     \t [0.61780982 0.48014342 0.865671  ]. \t  3.6175434218403115 \t 3.7885320123088384\n",
            "100    \t [0.60735812 0.57921968 0.83455747]. \t  3.742914138385889 \t 3.7885320123088384\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cO3I_9cbuvY2",
        "outputId": "70d0469c-0a9d-421a-890b-64b4d24187f9"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 6 \r\n",
        "\r\n",
        "np.random.seed(run_num_6)\r\n",
        "surrogate_winner_6 = dtStudentProcess(d_cov_func, nu = df)\r\n",
        "\r\n",
        "winner_6 = dGPGO_stp(surrogate_winner_6, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_6.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.89286015 0.33197981 0.82122912]. \t  2.3879127825536575 \t 2.5106636917702634\n",
            "init   \t [0.04169663 0.10765668 0.59505206]. \t  0.2321934630074273 \t 2.5106636917702634\n",
            "init   \t [0.52981736 0.41880743 0.33540785]. \t  0.37989557867087576 \t 2.5106636917702634\n",
            "init   \t [0.62251943 0.43814143 0.73588211]. \t  2.5106636917702634 \t 2.5106636917702634\n",
            "init   \t [0.51803641 0.5788586  0.6453551 ]. \t  1.7289203948034264 \t 2.5106636917702634\n",
            "1      \t [0.46426722 0.31959039 0.90909418]. \t  2.0488314609076865 \t 2.5106636917702634\n",
            "2      \t [1. 1. 1.]. \t  0.31688362070415665 \t 2.5106636917702634\n",
            "3      \t [0.67938996 0.01798911 0.67590895]. \t  0.20764042083180906 \t 2.5106636917702634\n",
            "4      \t [0.85573281 0.61381496 0.75539061]. \t  \u001b[92m2.6989513169745067\u001b[0m \t 2.6989513169745067\n",
            "5      \t [0.91098307 0.51241791 0.91040191]. \t  \u001b[92m3.356149832199529\u001b[0m \t 3.356149832199529\n",
            "6      \t [0.82762613 0.51322514 0.9423355 ]. \t  2.9660960227381628 \t 3.356149832199529\n",
            "7      \t [0.97311854 0.60304551 0.92068359]. \t  3.227679728733692 \t 3.356149832199529\n",
            "8      \t [0.22445394 0.957169   0.11039609]. \t  0.007583627613987315 \t 3.356149832199529\n",
            "9      \t [0.9274213  0.9825399  0.00315874]. \t  6.582583868350753e-05 \t 3.356149832199529\n",
            "10     \t [0.58735989 0.18512662 0.96554991]. \t  0.6698514332792584 \t 3.356149832199529\n",
            "11     \t [0.22101853 0.01593724 0.05694876]. \t  0.22421092754204433 \t 3.356149832199529\n",
            "12     \t [0.10786687 0.92349636 0.81860333]. \t  1.2686604154522199 \t 3.356149832199529\n",
            "13     \t [0.04717438 0.16192433 0.96409854]. \t  0.5638654741204204 \t 3.356149832199529\n",
            "14     \t [0.00960474 0.73501483 0.97670187]. \t  1.8789565432346857 \t 3.356149832199529\n",
            "15     \t [0.53497861 0.01629817 0.02246142]. \t  0.13772020549933148 \t 3.356149832199529\n",
            "16     \t [0.93424642 0.45202638 0.91325746]. \t  3.0341089952274363 \t 3.356149832199529\n",
            "17     \t [0.04680842 0.57980482 0.01852415]. \t  0.013661551244930953 \t 3.356149832199529\n",
            "18     \t [0.00294542 0.63500913 0.53134342]. \t  1.8206158742631189 \t 3.356149832199529\n",
            "19     \t [0.90023516 0.54524372 0.89234731]. \t  \u001b[92m3.5859921455142323\u001b[0m \t 3.5859921455142323\n",
            "20     \t [0.48654508 0.3766493  0.        ]. \t  0.057332179234442074 \t 3.5859921455142323\n",
            "21     \t [0.94877316 0.56109367 0.91275331]. \t  3.3863522279094265 \t 3.5859921455142323\n",
            "22     \t [0.96116158 0.5549063  0.83287276]. \t  \u001b[92m3.627659758585901\u001b[0m \t 3.627659758585901\n",
            "23     \t [0.96139373 0.50075738 0.77625066]. \t  3.0833305445867207 \t 3.627659758585901\n",
            "24     \t [0.93271599 0.52092713 0.86353337]. \t  \u001b[92m3.666914164697461\u001b[0m \t 3.666914164697461\n",
            "25     \t [0.20184176 0.38346112 0.40719298]. \t  0.41077437382377124 \t 3.666914164697461\n",
            "26     \t [0.95221501 0.56830162 0.81665166]. \t  3.510791758895265 \t 3.666914164697461\n",
            "27     \t [0.99547027 0.56038914 0.87281449]. \t  3.648334711071155 \t 3.666914164697461\n",
            "28     \t [0.4789933  0.71934025 0.03552675]. \t  0.005595854701017595 \t 3.666914164697461\n",
            "29     \t [0.00250009 0.8709827  0.38929028]. \t  1.2796815519156026 \t 3.666914164697461\n",
            "30     \t [0.86706812 0.44037167 0.82224797]. \t  3.2759678362961275 \t 3.666914164697461\n",
            "31     \t [0.95130545 0.50948377 0.82645721]. \t  3.562317720269443 \t 3.666914164697461\n",
            "32     \t [0.9986785  0.56526755 0.8426733 ]. \t  3.6397044553639812 \t 3.666914164697461\n",
            "33     \t [0.80718346 0.45885907 0.82883455]. \t  3.4316450909652385 \t 3.666914164697461\n",
            "34     \t [0.94605256 0.55770071 0.84674955]. \t  \u001b[92m3.6827342480077787\u001b[0m \t 3.6827342480077787\n",
            "35     \t [0.98105687 0.58063848 0.85667344]. \t  3.648536242045816 \t 3.6827342480077787\n",
            "36     \t [0.69579807 0.60863893 0.84158244]. \t  3.649438037138281 \t 3.6827342480077787\n",
            "37     \t [0.33254032 0.89414231 0.98713051]. \t  0.8152211980472256 \t 3.6827342480077787\n",
            "38     \t [0.97518926 0.46607209 0.86919091]. \t  3.421381827184204 \t 3.6827342480077787\n",
            "39     \t [0.56103336 0.59270668 0.87506679]. \t  \u001b[92m3.738997733030838\u001b[0m \t 3.738997733030838\n",
            "40     \t [0.47157778 0.60535434 0.90342657]. \t  3.5316138790699934 \t 3.738997733030838\n",
            "41     \t [0.69401689 0.56966071 0.84593568]. \t  \u001b[92m3.768598362175937\u001b[0m \t 3.768598362175937\n",
            "42     \t [0.59071116 0.5297534  0.88891718]. \t  3.680394064636645 \t 3.768598362175937\n",
            "43     \t [0.40431759 0.68042075 0.88075125]. \t  3.2817648268101194 \t 3.768598362175937\n",
            "44     \t [0.78097968 0.57712636 0.8788575 ]. \t  3.6949545296059174 \t 3.768598362175937\n",
            "45     \t [0.75843432 0.56457707 0.872986  ]. \t  3.7395441029631895 \t 3.768598362175937\n",
            "46     \t [0.74704238 0.6652529  0.84093996]. \t  3.3063355637435476 \t 3.768598362175937\n",
            "47     \t [0.68136073 0.55117465 0.89569466]. \t  3.633741663292128 \t 3.768598362175937\n",
            "48     \t [0.65150751 0.5742525  0.88807152]. \t  3.687641987324419 \t 3.768598362175937\n",
            "49     \t [0.60641196 0.5010556  0.87327551]. \t  3.684284703329765 \t 3.768598362175937\n",
            "50     \t [0.6568369  0.50919711 0.90592846]. \t  3.4655590854603995 \t 3.768598362175937\n",
            "51     \t [0.6308542  0.6262356  0.85617803]. \t  3.6215497049472547 \t 3.768598362175937\n",
            "52     \t [0.6037664  0.56650284 0.88868589]. \t  3.7020353513119693 \t 3.768598362175937\n",
            "53     \t [0.5832193  0.57517387 0.90445595]. \t  3.568773472419458 \t 3.768598362175937\n",
            "54     \t [0.74206799 0.48308165 0.87508094]. \t  3.5671126609610395 \t 3.768598362175937\n",
            "55     \t [0.54486518 0.57354071 0.91384258]. \t  3.477429763552247 \t 3.768598362175937\n",
            "56     \t [0.50671567 0.59326382 0.87247358]. \t  3.7574240204802685 \t 3.768598362175937\n",
            "57     \t [0.76830656 0.48045019 0.86053456]. \t  3.5918664019327746 \t 3.768598362175937\n",
            "58     \t [0.82399055 0.57862384 0.82451902]. \t  3.605720443980166 \t 3.768598362175937\n",
            "59     \t [0.72109583 0.53680716 0.86749696]. \t  3.7618325453923807 \t 3.768598362175937\n",
            "60     \t [0.94110359 0.5453443  0.86936518]. \t  3.6827174215641483 \t 3.768598362175937\n",
            "61     \t [0.55754041 0.70596566 0.89554473]. \t  2.976277709125238 \t 3.768598362175937\n",
            "62     \t [0.03897193 0.91315916 0.0434837 ]. \t  0.0014285457406390313 \t 3.768598362175937\n",
            "63     \t [0.89592049 0.53688688 0.86027643]. \t  3.710997654111199 \t 3.768598362175937\n",
            "64     \t [0.48409484 0.56947038 0.87169164]. \t  \u001b[92m3.8073700461042934\u001b[0m \t 3.8073700461042934\n",
            "65     \t [0.20845479 0.00664926 0.99985623]. \t  0.09840269603332967 \t 3.8073700461042934\n",
            "66     \t [0.02340301 0.05485303 0.12448871]. \t  0.36475133132944865 \t 3.8073700461042934\n",
            "67     \t [0.56282455 0.56861776 0.85587803]. \t  \u001b[92m3.8194619755629216\u001b[0m \t 3.8194619755629216\n",
            "68     \t [0.8837183  0.62029022 0.86006054]. \t  3.5582931879704587 \t 3.8194619755629216\n",
            "69     \t [0.806186   0.56061552 0.82739041]. \t  3.661055976232804 \t 3.8194619755629216\n",
            "70     \t [0.74088194 0.61838477 0.86747277]. \t  3.616797789388893 \t 3.8194619755629216\n",
            "71     \t [0.72197129 0.55525425 0.90335304]. \t  3.556948864329744 \t 3.8194619755629216\n",
            "72     \t [0.63164198 0.55399409 0.87126053]. \t  3.7861665498282226 \t 3.8194619755629216\n",
            "73     \t [0.62404903 0.54566393 0.88034571]. \t  3.746780603147911 \t 3.8194619755629216\n",
            "74     \t [0.57050678 0.56294731 0.85560262]. \t  \u001b[92m3.822895895292492\u001b[0m \t 3.822895895292492\n",
            "75     \t [0.74503285 0.56880916 0.8308415 ]. \t  3.6943158519498698 \t 3.822895895292492\n",
            "76     \t [0.38994468 0.530838   0.86894034]. \t  3.811626926351229 \t 3.822895895292492\n",
            "77     \t [0.91325573 0.50615927 0.88364141]. \t  3.5586733141023648 \t 3.822895895292492\n",
            "78     \t [0.5342162  0.56698691 0.91437993]. \t  3.477084605665842 \t 3.822895895292492\n",
            "79     \t [0.29173637 0.62642299 0.87337638]. \t  3.6530814287682287 \t 3.822895895292492\n",
            "80     \t [0.21907196 0.5912246  0.87634261]. \t  3.7626535679017623 \t 3.822895895292492\n",
            "81     \t [0.69477317 0.5140604  0.8804754 ]. \t  3.672045034768339 \t 3.822895895292492\n",
            "82     \t [0.75978713 0.55848755 0.85137054]. \t  3.7660299169440634 \t 3.822895895292492\n",
            "83     \t [0.22837816 0.68862868 0.8673478 ]. \t  3.2717592193394003 \t 3.822895895292492\n",
            "84     \t [0.72274966 0.60418876 0.84785163]. \t  3.6743572725667364 \t 3.822895895292492\n",
            "85     \t [0.50307084 0.49125072 0.86759585]. \t  3.683048288889263 \t 3.822895895292492\n",
            "86     \t [0.99407258 0.48880575 0.0139181 ]. \t  0.011332328490271945 \t 3.822895895292492\n",
            "87     \t [0.61064988 0.53798648 0.8447356 ]. \t  3.7997027311050107 \t 3.822895895292492\n",
            "88     \t [0.24411435 0.58265298 0.89160333]. \t  3.6904805725194496 \t 3.822895895292492\n",
            "89     \t [0.61020157 0.55807904 0.84488465]. \t  3.8035871494468525 \t 3.822895895292492\n",
            "90     \t [0.60588703 0.56373552 0.85289278]. \t  3.812298244651906 \t 3.822895895292492\n",
            "91     \t [0.56603825 0.57592325 0.84234892]. \t  3.7893012359594076 \t 3.822895895292492\n",
            "92     \t [0.58957406 0.53642829 0.89528868]. \t  3.6429824605953867 \t 3.822895895292492\n",
            "93     \t [0.48922866 0.56872584 0.83990068]. \t  3.812129055824739 \t 3.822895895292492\n",
            "94     \t [0.65954685 0.48516204 0.84579136]. \t  3.6481414871498457 \t 3.822895895292492\n",
            "95     \t [0.59236612 0.5529449  0.8707261 ]. \t  3.7971839676314367 \t 3.822895895292492\n",
            "96     \t [0.60226808 0.56527437 0.90002825]. \t  3.6142165519154896 \t 3.822895895292492\n",
            "97     \t [0.28897243 0.46675476 0.88104014]. \t  3.501913836265158 \t 3.822895895292492\n",
            "98     \t [0.04275798 0.55625251 0.83663675]. \t  3.8012224314742102 \t 3.822895895292492\n",
            "99     \t [0.76586006 0.55485107 0.85431554]. \t  3.7679018648531404 \t 3.822895895292492\n",
            "100    \t [0.37780608 0.5186722  0.85024246]. \t  3.814021793663204 \t 3.822895895292492\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8rQbLZD8uvbI",
        "outputId": "3cee50bc-3c20-4599-d03b-84db153798a4"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 7 \r\n",
        "\r\n",
        "np.random.seed(run_num_7)\r\n",
        "surrogate_winner_7 = dtStudentProcess(d_cov_func, nu = df)\r\n",
        "\r\n",
        "winner_7 = dGPGO_stp(surrogate_winner_7, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_7.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.15266373 0.30235661 0.06203641]. \t  0.1742288077468587 \t 1.6237282255098657\n",
            "init   \t [0.45986034 0.83525338 0.92699705]. \t  1.6237282255098657 \t 1.6237282255098657\n",
            "init   \t [0.72698898 0.76849622 0.26920507]. \t  0.08405715787489784 \t 1.6237282255098657\n",
            "init   \t [0.64402929 0.09337326 0.07968589]. \t  0.2756481743251193 \t 1.6237282255098657\n",
            "init   \t [0.58961375 0.34334054 0.98887615]. \t  1.4020548914621052 \t 1.6237282255098657\n",
            "1      \t [0.14505021 0.54653559 0.9534044 ]. \t  \u001b[92m2.8989260962375925\u001b[0m \t 2.8989260962375925\n",
            "2      \t [0.05924653 0.10640852 0.86190162]. \t  0.5962477305175093 \t 2.8989260962375925\n",
            "3      \t [0.08306623 0.99668859 0.81981745]. \t  0.8193325731144557 \t 2.8989260962375925\n",
            "4      \t [0.29068609 0.68706315 0.99569586]. \t  1.8820564675206315 \t 2.8989260962375925\n",
            "5      \t [1. 1. 1.]. \t  0.31688362070415665 \t 2.8989260962375925\n",
            "6      \t [0.09339782 0.49211829 0.58138287]. \t  1.2605373980972707 \t 2.8989260962375925\n",
            "7      \t [0.99247569 0.07531813 0.7594942 ]. \t  0.4605729057121457 \t 2.8989260962375925\n",
            "8      \t [0.03685289 0.49116928 0.9040006 ]. \t  \u001b[92m3.4062663378427063\u001b[0m \t 3.4062663378427063\n",
            "9      \t [0.87151726 0.70992739 0.7091363 ]. \t  1.6390907330747977 \t 3.4062663378427063\n",
            "10     \t [0.61912308 0.98684101 0.51859637]. \t  1.17703949879159 \t 3.4062663378427063\n",
            "11     \t [0.03425619 0.65019213 0.88615961]. \t  \u001b[92m3.432285289001223\u001b[0m \t 3.432285289001223\n",
            "12     \t [0.0308816  0.4218521  0.92101588]. \t  2.7942207106850754 \t 3.432285289001223\n",
            "13     \t [0. 0. 0.]. \t  0.06797411659013229 \t 3.432285289001223\n",
            "14     \t [0.91176493 0.99057367 0.12105107]. \t  0.001447648642711683 \t 3.432285289001223\n",
            "15     \t [0.43138314 0.02971717 0.56764555]. \t  0.14111024626918536 \t 3.432285289001223\n",
            "16     \t [0.35601513 0.99529101 0.03233783]. \t  0.0006848054034144275 \t 3.432285289001223\n",
            "17     \t [0.52903547 0.46688446 0.        ]. \t  0.03195075461751697 \t 3.432285289001223\n",
            "18     \t [0.99697033 0.05149246 0.33198442]. \t  0.2594678878495174 \t 3.432285289001223\n",
            "19     \t [0.98567311 0.1337062  0.01089334]. \t  0.044318838551756924 \t 3.432285289001223\n",
            "20     \t [0.0598641  0.58070449 0.828455  ]. \t  \u001b[92m3.753260236227722\u001b[0m \t 3.753260236227722\n",
            "21     \t [0.12750938 0.98768044 0.00164378]. \t  0.00032358601775224575 \t 3.753260236227722\n",
            "22     \t [0.00783809 0.6273612  0.80822611]. \t  3.478293576846896 \t 3.753260236227722\n",
            "23     \t [0.29859978 0.488486   0.8192966 ]. \t  3.6268813088266794 \t 3.753260236227722\n",
            "24     \t [0.10405698 0.52492668 0.83190752]. \t  \u001b[92m3.7735270106131686\u001b[0m \t 3.7735270106131686\n",
            "25     \t [0.2228459  0.43322724 0.82918053]. \t  3.3503519290440407 \t 3.7735270106131686\n",
            "26     \t [0.13429218 0.66188541 0.80385391]. \t  3.3098760063963963 \t 3.7735270106131686\n",
            "27     \t [0.04915677 0.55882355 0.83269751]. \t  \u001b[92m3.789756006498905\u001b[0m \t 3.789756006498905\n",
            "28     \t [0.33859138 0.53118564 0.79872933]. \t  3.5869304515581546 \t 3.789756006498905\n",
            "29     \t [0.47644971 0.56381223 0.83091101]. \t  3.7862817440883623 \t 3.789756006498905\n",
            "30     \t [0.61012311 0.56100689 0.7979294 ]. \t  3.5016672067475625 \t 3.789756006498905\n",
            "31     \t [0.45761943 0.58404875 0.8157108 ]. \t  3.6698929589726186 \t 3.789756006498905\n",
            "32     \t [0.43964341 0.54523935 0.84459381]. \t  \u001b[92m3.840759035281851\u001b[0m \t 3.840759035281851\n",
            "33     \t [0.25439511 0.52253949 0.82947413]. \t  3.778749172512148 \t 3.840759035281851\n",
            "34     \t [0.00325271 0.47631089 0.83089351]. \t  3.573745631027366 \t 3.840759035281851\n",
            "35     \t [0.94720938 0.59727373 0.95922376]. \t  2.702615978066122 \t 3.840759035281851\n",
            "36     \t [0.05497372 0.57447141 0.83832525]. \t  3.796059548685059 \t 3.840759035281851\n",
            "37     \t [0.64365169 0.57690721 0.92273907]. \t  3.349650601527945 \t 3.840759035281851\n",
            "38     \t [0.22929174 0.59708915 0.81862745]. \t  3.691381162239595 \t 3.840759035281851\n",
            "39     \t [0.45243761 0.54180626 0.84596945]. \t  3.8389456281776404 \t 3.840759035281851\n",
            "40     \t [0.32081453 0.57275146 0.82306375]. \t  3.7627871362142002 \t 3.840759035281851\n",
            "41     \t [0.32479722 0.61457548 0.84399243]. \t  3.726773783446501 \t 3.840759035281851\n",
            "42     \t [0.58345865 0.57782172 0.80868192]. \t  3.585757233851634 \t 3.840759035281851\n",
            "43     \t [0.18185167 0.56471054 0.82863324]. \t  3.7977763370130146 \t 3.840759035281851\n",
            "44     \t [0.2943338  0.55316802 0.8124425 ]. \t  3.7127444662843594 \t 3.840759035281851\n",
            "45     \t [0.11448407 0.53152779 0.85202158]. \t  3.822788892169682 \t 3.840759035281851\n",
            "46     \t [0.26382495 0.53500616 0.84169212]. \t  3.836845472858277 \t 3.840759035281851\n",
            "47     \t [0.11116267 0.59244953 0.83505507]. \t  3.765547941316544 \t 3.840759035281851\n",
            "48     \t [0.16402997 0.57296229 0.89286956]. \t  3.6841042219971585 \t 3.840759035281851\n",
            "49     \t [0.56130765 0.54723701 0.81051315]. \t  3.6413822065383235 \t 3.840759035281851\n",
            "50     \t [0.05086427 0.58321731 0.86421043]. \t  3.7887127885135228 \t 3.840759035281851\n",
            "51     \t [0.54005149 0.70606945 0.85580958]. \t  3.0889547060940945 \t 3.840759035281851\n",
            "52     \t [0.36152321 0.59487623 0.83404954]. \t  3.76231330761637 \t 3.840759035281851\n",
            "53     \t [0.29655134 0.57038306 0.88168823]. \t  3.773861095470603 \t 3.840759035281851\n",
            "54     \t [0.05947958 0.55743713 0.87273006]. \t  3.7890870954133016 \t 3.840759035281851\n",
            "55     \t [0.33206304 0.5753393  0.77804475]. \t  3.378030640735082 \t 3.840759035281851\n",
            "56     \t [0.08589965 0.54618806 0.84581799]. \t  3.8295175294605515 \t 3.840759035281851\n",
            "57     \t [0.23742044 0.494024   0.82216589]. \t  3.661767021544173 \t 3.840759035281851\n",
            "58     \t [0.00088453 0.41622981 0.01949253]. \t  0.043178806652781974 \t 3.840759035281851\n",
            "59     \t [0.30983272 0.54263698 0.86647763]. \t  3.8372314706779087 \t 3.840759035281851\n",
            "60     \t [0.27358529 0.60648725 0.82194975]. \t  3.6792920873113713 \t 3.840759035281851\n",
            "61     \t [0.03923948 0.4733607  0.81445333]. \t  3.4971440755371828 \t 3.840759035281851\n",
            "62     \t [0.44746032 0.59521023 0.81404089]. \t  3.632123289276927 \t 3.840759035281851\n",
            "63     \t [0.59292118 0.57480815 0.77910824]. \t  3.281589494910895 \t 3.840759035281851\n",
            "64     \t [0.52723274 0.54418994 0.88793044]. \t  3.718497233491366 \t 3.840759035281851\n",
            "65     \t [0.0195905  0.62550834 0.85034942]. \t  3.6513535175893184 \t 3.840759035281851\n",
            "66     \t [0.00445148 0.63033172 0.86583584]. \t  3.6111546918967408 \t 3.840759035281851\n",
            "67     \t [0.17555335 0.62157021 0.8381856 ]. \t  3.683950582762198 \t 3.840759035281851\n",
            "68     \t [0.13557666 0.57246697 0.84958892]. \t  3.8359509142377797 \t 3.840759035281851\n",
            "69     \t [0.43048141 0.59224318 0.90225983]. \t  3.5812824004322286 \t 3.840759035281851\n",
            "70     \t [0.19505953 0.48744783 0.84462585]. \t  3.696555384798323 \t 3.840759035281851\n",
            "71     \t [0.24705592 0.56351367 0.88810695]. \t  3.73508451867244 \t 3.840759035281851\n",
            "72     \t [0.57250307 0.48977778 0.89327841]. \t  3.52096337039173 \t 3.840759035281851\n",
            "73     \t [2.88692016e-04 5.87595891e-01 8.25566716e-01]. \t  3.706802856704247 \t 3.840759035281851\n",
            "74     \t [0.40986115 0.49504028 0.85982356]. \t  3.7277209455692164 \t 3.840759035281851\n",
            "75     \t [0.5751882  0.43545154 0.83292366]. \t  3.357950365021516 \t 3.840759035281851\n",
            "76     \t [0.0619389  0.59488187 0.84954218]. \t  3.775640773021753 \t 3.840759035281851\n",
            "77     \t [0.22166039 0.51123772 0.87489964]. \t  3.7349744599112262 \t 3.840759035281851\n",
            "78     \t [0.50979905 0.51869445 0.89015176]. \t  3.6589329744007264 \t 3.840759035281851\n",
            "79     \t [0.20404968 0.56419299 0.89716352]. \t  3.659470181310452 \t 3.840759035281851\n",
            "81     \t [0.09154616 0.60733696 0.80746646]. \t  3.574061014522551 \t 3.840759035281851\n",
            "82     \t [0.08212368 0.61876487 0.82552276]. \t  3.634539855655828 \t 3.840759035281851\n",
            "83     \t [0.23854961 0.54897707 0.90258429]. \t  3.6093468010861702 \t 3.840759035281851\n",
            "84     \t [0.22343934 0.49960566 0.82614201]. \t  3.6994351844813487 \t 3.840759035281851\n",
            "85     \t [0.20601977 0.66581318 0.83807222]. \t  3.4382841043578454 \t 3.840759035281851\n",
            "86     \t [0.5420357  0.6186406  0.86292977]. \t  3.680409839373773 \t 3.840759035281851\n",
            "87     \t [0.5126533  0.51873554 0.88156769]. \t  3.7154949835122375 \t 3.840759035281851\n",
            "88     \t [0.3552446  0.59906634 0.85134439]. \t  3.789204011969339 \t 3.840759035281851\n",
            "89     \t [0.36005245 0.58110128 0.7937848 ]. \t  3.5206217755023914 \t 3.840759035281851\n",
            "90     \t [0.4739443  0.48791514 0.81130547]. \t  3.5579963393176484 \t 3.840759035281851\n",
            "91     \t [0.31999881 0.64126366 0.84543475]. \t  3.599968030079599 \t 3.840759035281851\n",
            "92     \t [0.44420319 0.54416481 0.91724318]. \t  3.444264475422064 \t 3.840759035281851\n",
            "93     \t [0.24867409 0.6400979  0.82316739]. \t  3.5398801199553382 \t 3.840759035281851\n",
            "94     \t [0.14141847 0.54592449 0.84317179]. \t  3.8373448774525922 \t 3.840759035281851\n",
            "95     \t [0.15981566 0.55222133 0.86913439]. \t  3.8228451972729314 \t 3.840759035281851\n",
            "96     \t [0.38164947 0.6227622  0.88162782]. \t  3.630615361188895 \t 3.840759035281851\n",
            "97     \t [0.29531631 0.59319113 0.81554321]. \t  3.679480612979479 \t 3.840759035281851\n",
            "98     \t [0.20157469 0.43880872 0.89807821]. \t  3.181983910264284 \t 3.840759035281851\n",
            "99     \t [0.37279213 0.56240402 0.82791702]. \t  3.7925300356224994 \t 3.840759035281851\n",
            "100    \t [0.30798799 0.50163144 0.88752886]. \t  3.631868531334823 \t 3.840759035281851\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6YRio_skuvd2",
        "outputId": "6a19b2c7-7356-4953-e4cb-5a4166daf8de"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 8 \r\n",
        "\r\n",
        "np.random.seed(run_num_8)\r\n",
        "surrogate_winner_8 = dtStudentProcess(d_cov_func, nu = df)\r\n",
        "\r\n",
        "winner_8 = dGPGO_stp(surrogate_winner_8, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_8.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.10949987 0.04221501 0.59969258]. \t  0.15732379460832624 \t 0.8830091449513892\n",
            "init   \t [0.29987071 0.79661178 0.36619613]. \t  0.8830091449513892 \t 0.8830091449513892\n",
            "init   \t [0.88060834 0.29784163 0.32910326]. \t  0.2992873424905509 \t 0.8830091449513892\n",
            "init   \t [0.56871692 0.74392742 0.05385289]. \t  0.0051668955191694075 \t 0.8830091449513892\n",
            "init   \t [0.60207437 0.42931858 0.20547361]. \t  0.2908249205255991 \t 0.8830091449513892\n",
            "1      \t [0.19816249 0.92572195 0.98825083]. \t  0.653164991364588 \t 0.8830091449513892\n",
            "2      \t [0.04465769 0.93281371 0.1511552 ]. \t  0.02223163867041527 \t 0.8830091449513892\n",
            "3      \t [0.91371011 0.93067908 0.92923527]. \t  0.8488687568071137 \t 0.8830091449513892\n",
            "4      \t [0.69508631 0.44186519 0.80775517]. \t  \u001b[92m3.256754654499839\u001b[0m \t 3.256754654499839\n",
            "5      \t [0.48230321 0.25643606 0.99076421]. \t  0.8816868796381114 \t 3.256754654499839\n",
            "6      \t [0.94012977 0.22216948 0.93753243]. \t  1.0144224194762295 \t 3.256754654499839\n",
            "7      \t [0.95648587 0.63293029 0.66692766]. \t  1.3616465148913897 \t 3.256754654499839\n",
            "8      \t [0.62378547 0.64097979 0.82206409]. \t  \u001b[92m3.4196965646963235\u001b[0m \t 3.4196965646963235\n",
            "9      \t [0.71033454 0.59438989 0.8428629 ]. \t  \u001b[92m3.6989213954535574\u001b[0m \t 3.6989213954535574\n",
            "10     \t [0.6940569  0.57031931 0.91700011]. \t  3.41472867585017 \t 3.6989213954535574\n",
            "11     \t [0.40638733 0.96987401 0.47781898]. \t  1.7633806895118607 \t 3.6989213954535574\n",
            "12     \t [0.70644211 0.69901829 0.78871905]. \t  2.7052170786118785 \t 3.6989213954535574\n",
            "13     \t [0.02298079 0.05297391 0.89388603]. \t  0.33264238562026655 \t 3.6989213954535574\n",
            "14     \t [0.93917583 0.01477412 0.0975133 ]. \t  0.14299196623894228 \t 3.6989213954535574\n",
            "15     \t [0.15447103 0.97323768 0.53337813]. \t  2.6839696061353777 \t 3.6989213954535574\n",
            "16     \t [0. 0. 0.]. \t  0.06797411659013229 \t 3.6989213954535574\n",
            "17     \t [1.         0.80444176 1.        ]. \t  1.1545116441976433 \t 3.6989213954535574\n",
            "18     \t [0.94795061 0.29004187 0.07754015]. \t  0.09204718877231219 \t 3.6989213954535574\n",
            "19     \t [0.35151655 0.98505762 0.00995079]. \t  0.0003750671388777951 \t 3.6989213954535574\n",
            "20     \t [0.007884   0.52215323 0.6692032 ]. \t  2.081995527310208 \t 3.6989213954535574\n",
            "21     \t [0.27706233 0.54983763 0.7878948 ]. \t  3.5044810884807664 \t 3.6989213954535574\n",
            "22     \t [0.9970784  0.52082012 0.92767776]. \t  3.139458986763932 \t 3.6989213954535574\n",
            "23     \t [0.00126691 0.45695769 0.96438921]. \t  2.404355804942458 \t 3.6989213954535574\n",
            "24     \t [0.32545615 0.52779325 0.86753382]. \t  \u001b[92m3.8123273217462206\u001b[0m \t 3.8123273217462206\n",
            "25     \t [0.31624676 0.55082983 0.978043  ]. \t  2.4830399060761623 \t 3.8123273217462206\n",
            "26     \t [0.19922913 0.408345   0.00124971]. \t  0.04698472326972861 \t 3.8123273217462206\n",
            "27     \t [0.20492117 0.34349926 0.86622308]. \t  2.5261239669462405 \t 3.8123273217462206\n",
            "28     \t [2.97082443e-01 1.81334775e-02 1.21063361e-15]. \t  0.1046958096691158 \t 3.8123273217462206\n",
            "29     \t [0.26849154 1.         0.77861882]. \t  0.9322962262622467 \t 3.8123273217462206\n",
            "30     \t [0.94559261 0.97587159 0.06065383]. \t  0.00028265138280272 \t 3.8123273217462206\n",
            "31     \t [0.10899073 0.89949944 0.87819854]. \t  1.3089587945916827 \t 3.8123273217462206\n",
            "32     \t [0.44896216 0.52483245 0.84885026]. \t  \u001b[92m3.819753370426918\u001b[0m \t 3.819753370426918\n",
            "33     \t [0.49803819 0.49764172 0.84083305]. \t  3.724579629129354 \t 3.819753370426918\n",
            "34     \t [0.51379427 0.50457598 0.88459535]. \t  3.652663324782548 \t 3.819753370426918\n",
            "35     \t [0.98758503 0.00602577 0.39085808]. \t  0.1795978309370378 \t 3.819753370426918\n",
            "36     \t [0.42435948 0.55044522 0.72259995]. \t  2.6702881942657504 \t 3.819753370426918\n",
            "37     \t [0.41629355 0.0014918  0.8189517 ]. \t  0.25248700366527815 \t 3.819753370426918\n",
            "38     \t [0.39705221 0.6676233  0.82114337]. \t  3.335709307199239 \t 3.819753370426918\n",
            "39     \t [0.54029298 0.54312406 0.84757137]. \t  \u001b[92m3.825587796777769\u001b[0m \t 3.825587796777769\n",
            "40     \t [0.6657756  0.51350699 0.83329975]. \t  3.7102047954050734 \t 3.825587796777769\n",
            "41     \t [0.62003798 0.51572221 0.84739172]. \t  3.7646298935859397 \t 3.825587796777769\n",
            "42     \t [0.16682143 0.99746151 0.32756439]. \t  0.5419101104966457 \t 3.825587796777769\n",
            "43     \t [1.         1.         0.65186811]. \t  0.2920827846996008 \t 3.825587796777769\n",
            "44     \t [0.64852678 0.20730756 0.00422202]. \t  0.09140908766231316 \t 3.825587796777769\n",
            "45     \t [0.56477686 0.50479133 0.83245303]. \t  3.710586112713423 \t 3.825587796777769\n",
            "46     \t [0.01221449 0.97073354 0.46677346]. \t  2.109169363829101 \t 3.825587796777769\n",
            "47     \t [0.45564106 0.57293319 0.83650464]. \t  3.8025464071923736 \t 3.825587796777769\n",
            "48     \t [0.68726006 0.44689031 0.90034126]. \t  3.1953192278438944 \t 3.825587796777769\n",
            "49     \t [0.52085054 0.51083229 0.86540489]. \t  3.7593625691248604 \t 3.825587796777769\n",
            "50     \t [0.60911088 0.52083487 0.84283232]. \t  3.7711856231486562 \t 3.825587796777769\n",
            "51     \t [0.55443066 0.50912909 0.85695543]. \t  3.762841951252306 \t 3.825587796777769\n",
            "52     \t [0.56662513 0.52445094 0.843686  ]. \t  3.790334270654168 \t 3.825587796777769\n",
            "53     \t [0.48026632 0.56001074 0.86662852]. \t  \u001b[92m3.829108806473154\u001b[0m \t 3.829108806473154\n",
            "54     \t [0.53320416 0.55230529 0.89890299]. \t  3.636476334191801 \t 3.829108806473154\n",
            "55     \t [0.53156797 0.52746553 0.84684239]. \t  3.808478548696369 \t 3.829108806473154\n",
            "56     \t [0.51866544 0.54593426 0.83646687]. \t  3.8045439228280196 \t 3.829108806473154\n",
            "57     \t [0.55480589 0.49092082 0.88741311]. \t  3.5729652953829993 \t 3.829108806473154\n",
            "58     \t [0.50930285 0.521515   0.87977249]. \t  3.733041332534874 \t 3.829108806473154\n",
            "59     \t [0.58774774 0.51006287 0.8932883 ]. \t  3.5972280101622296 \t 3.829108806473154\n",
            "60     \t [0.56863767 0.53273742 0.84006942]. \t  3.7939122509017196 \t 3.829108806473154\n",
            "61     \t [0.46961504 0.56497193 0.82178919]. \t  3.73812541818326 \t 3.829108806473154\n",
            "62     \t [0.53178931 0.52451403 0.86803822]. \t  3.7856057811574217 \t 3.829108806473154\n",
            "63     \t [0.60561655 0.5156508  0.88554349]. \t  3.6670222944849185 \t 3.829108806473154\n",
            "64     \t [0.67271374 0.52706591 0.8801307 ]. \t  3.7110615764052812 \t 3.829108806473154\n",
            "65     \t [0.48472802 0.56438738 0.8310891 ]. \t  3.784634138267628 \t 3.829108806473154\n",
            "66     \t [0.6129135  0.51538551 0.8464689 ]. \t  3.7644141500029895 \t 3.829108806473154\n",
            "67     \t [0.52371485 0.53424295 0.8385382 ]. \t  3.8017078495795915 \t 3.829108806473154\n",
            "68     \t [0.5204596  0.50799081 0.8180384 ]. \t  3.6568751661480388 \t 3.829108806473154\n",
            "69     \t [0.57092985 0.5403814  0.90364998]. \t  3.575917742320477 \t 3.829108806473154\n",
            "70     \t [0.64253129 0.49496888 0.82627573]. \t  3.632311014239813 \t 3.829108806473154\n",
            "71     \t [0.5772822  0.53633596 0.84730615]. \t  3.8110577367604677 \t 3.829108806473154\n",
            "72     \t [0.54450322 0.53640462 0.86520315]. \t  3.8108416821145576 \t 3.829108806473154\n",
            "73     \t [0.50101555 0.55610461 0.80538265]. \t  3.615106373848873 \t 3.829108806473154\n",
            "74     \t [0.57415296 0.537779   0.84324021]. \t  3.805810796023201 \t 3.829108806473154\n",
            "75     \t [0.46123966 0.55294037 0.87900214]. \t  3.785712473810526 \t 3.829108806473154\n",
            "76     \t [0.55172021 0.53909541 0.82461112]. \t  3.7387079003992323 \t 3.829108806473154\n",
            "77     \t [0.60460778 0.51537917 0.82029864]. \t  3.6663228387144993 \t 3.829108806473154\n",
            "78     \t [0.52334387 0.54884318 0.89283197]. \t  3.686800842434322 \t 3.829108806473154\n",
            "79     \t [0.5437866  0.51976168 0.8516507 ]. \t  3.7946868190739798 \t 3.829108806473154\n",
            "80     \t [0.48449423 0.50569442 0.87314253]. \t  3.721354955441691 \t 3.829108806473154\n",
            "81     \t [0.6119333  0.52565985 0.82319109]. \t  3.6995555431852063 \t 3.829108806473154\n",
            "82     \t [0.52152287 0.53828193 0.82927238]. \t  3.7700967360245614 \t 3.829108806473154\n",
            "83     \t [0.49572062 0.55242679 0.85105821]. \t  \u001b[92m3.84121017380115\u001b[0m \t 3.84121017380115\n",
            "84     \t [0.53252917 0.55575549 0.89079564]. \t  3.70340382911983 \t 3.84121017380115\n",
            "85     \t [0.61382434 0.50697501 0.86493536]. \t  3.7306944137999802 \t 3.84121017380115\n",
            "86     \t [0.50079545 0.57211675 0.82560825]. \t  3.7431512387027466 \t 3.84121017380115\n",
            "87     \t [0.60630508 0.51940765 0.86094901]. \t  3.7750096203133126 \t 3.84121017380115\n",
            "88     \t [0.53441106 0.53157291 0.82467811]. \t  3.73701064792531 \t 3.84121017380115\n",
            "89     \t [0.52335068 0.53960203 0.86228955]. \t  3.824129962378937 \t 3.84121017380115\n",
            "90     \t [0.52750513 0.52105421 0.85023524]. \t  3.800108039613571 \t 3.84121017380115\n",
            "91     \t [0.59367062 0.56631654 0.92229035]. \t  3.3717838756434366 \t 3.84121017380115\n",
            "92     \t [0.54366608 0.52881508 0.85570775]. \t  3.8124495364250004 \t 3.84121017380115\n",
            "93     \t [0.6711269  0.50448757 0.8588763 ]. \t  3.7194463750685696 \t 3.84121017380115\n",
            "94     \t [0.51855321 0.54691522 0.84891749]. \t  3.8337323753746695 \t 3.84121017380115\n",
            "95     \t [0.58463044 0.53454028 0.85171797]. \t  3.8115584128754274 \t 3.84121017380115\n",
            "96     \t [0.56083266 0.54968458 0.80542792]. \t  3.5984154954937937 \t 3.84121017380115\n",
            "97     \t [0.51187134 0.62032997 0.77483314]. \t  3.1467353498603146 \t 3.84121017380115\n",
            "98     \t [0.38652501 0.57123039 0.89047568]. \t  3.715035487814495 \t 3.84121017380115\n",
            "99     \t [0.50375102 0.49822219 0.87230309]. \t  3.694726081080244 \t 3.84121017380115\n",
            "100    \t [0.53043289 0.54201701 0.89932037]. \t  3.6250224173898578 \t 3.84121017380115\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejw6v-Ihuvf_",
        "outputId": "cbfa3443-bede-4e47-921e-908d7e24aca3"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 9 \r\n",
        "\r\n",
        "np.random.seed(run_num_9)\r\n",
        "surrogate_winner_9 = dtStudentProcess(d_cov_func, nu = df)\r\n",
        "\r\n",
        "winner_9 = dGPGO_stp(surrogate_winner_9, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_9.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.67227856 0.4880784  0.82549517]. \t  3.595021899183128 \t 3.595021899183128\n",
            "init   \t [0.03144639 0.80804996 0.56561742]. \t  2.9633561694281085 \t 3.595021899183128\n",
            "init   \t [0.2976225  0.04669572 0.9906274 ]. \t  0.16382388103073592 \t 3.595021899183128\n",
            "init   \t [0.00682573 0.76979303 0.7467671 ]. \t  2.382987807172393 \t 3.595021899183128\n",
            "init   \t [0.37743894 0.49414745 0.92894839]. \t  3.1588932929069533 \t 3.595021899183128\n",
            "1      \t [0.71850638 0.95798    0.01578222]. \t  0.00023878367703005274 \t 3.595021899183128\n",
            "2      \t [1. 1. 1.]. \t  0.31688362070415665 \t 3.595021899183128\n",
            "3      \t [0.05966058 0.19924882 0.10169277]. \t  0.3081744858038323 \t 3.595021899183128\n",
            "4      \t [0.37942171 0.72603532 0.60985522]. \t  2.2310078708273684 \t 3.595021899183128\n",
            "5      \t [0.91816595 0.02645044 0.95803148]. \t  0.17202834392072675 \t 3.595021899183128\n",
            "6      \t [0.0422304  0.93936415 0.09868535]. \t  0.005890517292476375 \t 3.595021899183128\n",
            "7      \t [0.02348389 0.3095753  0.55398448]. \t  0.46934889171781835 \t 3.595021899183128\n",
            "8      \t [0.99523721 0.34606977 0.13988078]. \t  0.11216690562547638 \t 3.595021899183128\n",
            "9      \t [0.98075579 0.53275479 0.92111818]. \t  3.254186124673051 \t 3.595021899183128\n",
            "10     \t [0.52022186 0.38735098 0.93189633]. \t  2.4251332821948144 \t 3.595021899183128\n",
            "11     \t [0.96844349 0.83295132 0.20288508]. \t  0.009650400345747998 \t 3.595021899183128\n",
            "12     \t [0. 0. 0.]. \t  0.06797411659013229 \t 3.595021899183128\n",
            "13     \t [0.56411437 0.80911011 0.96168261]. \t  1.5492839993810303 \t 3.595021899183128\n",
            "14     \t [0.82696694 0.03193888 0.37319571]. \t  0.3559971348125631 \t 3.595021899183128\n",
            "15     \t [0.01157148 0.55638309 0.26113589]. \t  0.17959368218039257 \t 3.595021899183128\n",
            "16     \t [0.66889478 0.17285726 0.        ]. \t  0.08675564340200939 \t 3.595021899183128\n",
            "17     \t [0.931535   0.26263724 0.78190525]. \t  1.6507628020773408 \t 3.595021899183128\n",
            "18     \t [0.82842744 0.65805564 0.81362867]. \t  3.1535570691442927 \t 3.595021899183128\n",
            "19     \t [0.301199   0.01677904 0.66299792]. \t  0.1933106086798974 \t 3.595021899183128\n",
            "20     \t [0.91595622 0.69628659 0.99247028]. \t  1.8335501503661484 \t 3.595021899183128\n",
            "21     \t [0.94850439 0.05664093 0.01777956]. \t  0.05436319875233393 \t 3.595021899183128\n",
            "22     \t [0.99777832 0.60655517 0.79961257]. \t  3.21486004354509 \t 3.595021899183128\n",
            "23     \t [0.02067804 0.91099811 0.59912263]. \t  2.8424606783464554 \t 3.595021899183128\n",
            "24     \t [0.30703308 0.6366763  0.74618744]. \t  2.9170093755511513 \t 3.595021899183128\n",
            "25     \t [0.35968177 0.01309435 0.09902123]. \t  0.3837580129619906 \t 3.595021899183128\n",
            "26     \t [0.6044126  0.59037707 0.71811025]. \t  2.4434198088486365 \t 3.595021899183128\n",
            "27     \t [0.9467061  0.98499965 0.70867012]. \t  0.4133180705940646 \t 3.595021899183128\n",
            "28     \t [0.01851577 0.56911107 0.96464903]. \t  2.6939275512535708 \t 3.595021899183128\n",
            "29     \t [0.03852561 0.01919826 0.19020956]. \t  0.5480952127143827 \t 3.595021899183128\n",
            "30     \t [0.03878991 0.97647059 0.3580824 ]. \t  0.833801833779583 \t 3.595021899183128\n",
            "31     \t [0.04006812 0.07805251 0.93993866]. \t  0.3276717741631027 \t 3.595021899183128\n",
            "32     \t [0.975598   0.59443828 0.05704385]. \t  0.009091315097897658 \t 3.595021899183128\n",
            "33     \t [0.22146959 0.73061112 0.96620503]. \t  2.069009350785855 \t 3.595021899183128\n",
            "34     \t [0.18558133 0.88545985 0.66613081]. \t  2.3908006775181585 \t 3.595021899183128\n",
            "35     \t [0.25230514 0.59785399 0.02693644]. \t  0.01711307868526705 \t 3.595021899183128\n",
            "36     \t [0.91943184 0.4783374  0.93626946]. \t  2.896657974684566 \t 3.595021899183128\n",
            "37     \t [0.79251709 0.50321893 0.85454196]. \t  \u001b[92m3.6814335084928747\u001b[0m \t 3.6814335084928747\n",
            "38     \t [0.76550542 0.53838408 0.84154396]. \t  \u001b[92m3.7430682600013\u001b[0m \t 3.7430682600013\n",
            "39     \t [0.51678055 0.63624234 0.98010404]. \t  2.3326321855298104 \t 3.7430682600013\n",
            "40     \t [0.79905997 0.52200804 0.81341428]. \t  3.5624532433997214 \t 3.7430682600013\n",
            "41     \t [0.55642169 0.56607551 0.88048217]. \t  \u001b[92m3.760422081936679\u001b[0m \t 3.760422081936679\n",
            "42     \t [0.01795892 0.33146497 0.99647587]. \t  1.2331661936137746 \t 3.760422081936679\n",
            "43     \t [0.83983383 0.57746511 0.84596561]. \t  3.7005108046434687 \t 3.760422081936679\n",
            "44     \t [0.74203218 0.55304828 0.8255053 ]. \t  3.681269909841385 \t 3.760422081936679\n",
            "45     \t [0.95066611 0.95894178 0.04782396]. \t  0.00022090388164608667 \t 3.760422081936679\n",
            "46     \t [0.6301264  0.53153638 0.84394551]. \t  \u001b[92m3.785750334109432\u001b[0m \t 3.785750334109432\n",
            "47     \t [0.66714771 0.5822516  0.86304469]. \t  3.7652042232877947 \t 3.785750334109432\n",
            "48     \t [0.86406741 0.57103875 0.8409801 ]. \t  3.6868720778524597 \t 3.785750334109432\n",
            "49     \t [0.92045053 0.49669506 0.84042738]. \t  3.5947053076881565 \t 3.785750334109432\n",
            "50     \t [0.89784726 0.48786628 0.77990078]. \t  3.1263184566572733 \t 3.785750334109432\n",
            "51     \t [0.91386188 0.60285578 0.80972943]. \t  3.3717285564811856 \t 3.785750334109432\n",
            "52     \t [0.54941352 0.67248508 0.77382263]. \t  2.854685902237762 \t 3.785750334109432\n",
            "53     \t [0.71016369 0.60944902 0.91084556]. \t  3.4013471021311474 \t 3.785750334109432\n",
            "54     \t [0.90804407 0.50067217 0.83705436]. \t  3.6028853011234223 \t 3.785750334109432\n",
            "55     \t [0.74961592 0.54785096 0.91333955]. \t  3.4418402467398233 \t 3.785750334109432\n",
            "56     \t [0.86859572 0.61560732 0.83685212]. \t  3.5293642831300764 \t 3.785750334109432\n",
            "57     \t [0.8709551  0.55606838 0.88069007]. \t  3.671601123460579 \t 3.785750334109432\n",
            "58     \t [0.83856288 0.54880025 0.86012893]. \t  3.7408435272176157 \t 3.785750334109432\n",
            "59     \t [0.71589021 0.45645852 0.84589166]. \t  3.483945774794558 \t 3.785750334109432\n",
            "60     \t [0.7141952  0.60631736 0.91901071]. \t  3.3208066853988787 \t 3.785750334109432\n",
            "61     \t [0.64872154 0.59287084 0.85416425]. \t  3.746322488511991 \t 3.785750334109432\n",
            "62     \t [0.7376153  0.46176345 0.88203598]. \t  3.4164626630363477 \t 3.785750334109432\n",
            "63     \t [0.81322779 0.35092311 0.88917619]. \t  2.4469482746582196 \t 3.785750334109432\n",
            "64     \t [0.70725012 0.57121295 0.87027457]. \t  3.757392454482895 \t 3.785750334109432\n",
            "65     \t [0.58124421 0.5797453  0.90268673]. \t  3.580175109998657 \t 3.785750334109432\n",
            "66     \t [0.26952321 0.63343594 0.83922822]. \t  3.632926454633575 \t 3.785750334109432\n",
            "67     \t [0.66795683 0.60805505 0.82353949]. \t  3.5723724690182346 \t 3.785750334109432\n",
            "68     \t [0.50058449 0.5769403  0.86676115]. \t  \u001b[92m3.808951488060277\u001b[0m \t 3.808951488060277\n",
            "69     \t [0.94155018 0.64389679 0.84920877]. \t  3.3908231999591356 \t 3.808951488060277\n",
            "70     \t [0.57748717 0.55745493 0.89468057]. \t  3.666051588035015 \t 3.808951488060277\n",
            "71     \t [0.82239928 0.64670149 0.85927963]. \t  3.4396333652868574 \t 3.808951488060277\n",
            "72     \t [0.51982837 0.49132132 0.88432192]. \t  3.600376688012345 \t 3.808951488060277\n",
            "73     \t [0.62050802 0.58403401 0.82412412]. \t  3.67219176598753 \t 3.808951488060277\n",
            "74     \t [0.70674759 0.60393268 0.88223666]. \t  3.6372469030174113 \t 3.808951488060277\n",
            "75     \t [0.55229547 0.55205597 0.8691889 ]. \t  \u001b[92m3.8102603029652844\u001b[0m \t 3.8102603029652844\n",
            "76     \t [0.37961307 0.56776273 0.82182918]. \t  3.7542105046006142 \t 3.8102603029652844\n",
            "77     \t [0.761864   0.60707749 0.81926702]. \t  3.5059262052844944 \t 3.8102603029652844\n",
            "78     \t [0.40354831 0.59055156 0.88115317]. \t  3.7409500418405397 \t 3.8102603029652844\n",
            "79     \t [0.48124089 0.61513958 0.83707528]. \t  3.676775532050938 \t 3.8102603029652844\n",
            "80     \t [0.55347369 0.56568787 0.8204301 ]. \t  3.704858272070446 \t 3.8102603029652844\n",
            "81     \t [0.28492441 0.60088021 0.85594906]. \t  3.7878149282481894 \t 3.8102603029652844\n",
            "82     \t [0.72756417 0.61578283 0.86521007]. \t  3.6357977854922403 \t 3.8102603029652844\n",
            "83     \t [0.83736595 0.59424924 0.87826174]. \t  3.6400431323928437 \t 3.8102603029652844\n",
            "84     \t [0.39434821 0.52328496 0.84209109]. \t  \u001b[92m3.8136406151019893\u001b[0m \t 3.8136406151019893\n",
            "85     \t [0.26626314 0.5967651  0.80477552]. \t  3.5964573859045097 \t 3.8136406151019893\n",
            "86     \t [0.43868834 0.54777793 0.82255852]. \t  3.7572454188354656 \t 3.8136406151019893\n",
            "87     \t [0.26059517 0.52464409 0.79982935]. \t  3.5934754271935816 \t 3.8136406151019893\n",
            "88     \t [0.20699384 0.58922662 0.77277184]. \t  3.324580127988196 \t 3.8136406151019893\n",
            "89     \t [0.42501252 0.64145814 0.82271212]. \t  3.4966551978742686 \t 3.8136406151019893\n",
            "90     \t [0.37091889 0.584266   0.83262199]. \t  3.7819915827236112 \t 3.8136406151019893\n",
            "91     \t [0.17866222 0.69459712 0.83812824]. \t  3.2268185444431587 \t 3.8136406151019893\n",
            "92     \t [0.75606547 0.51797537 0.88539366]. \t  3.636895208638631 \t 3.8136406151019893\n",
            "93     \t [0.40110135 0.57232548 0.88431382]. \t  3.754170892017967 \t 3.8136406151019893\n",
            "94     \t [0.51808753 0.63460426 0.84094733]. \t  3.5848913244749436 \t 3.8136406151019893\n",
            "95     \t [0.45269046 0.62898878 0.82375827]. \t  3.559507908331981 \t 3.8136406151019893\n",
            "96     \t [0.50782101 0.49603556 0.81465066]. \t  3.6025319397577764 \t 3.8136406151019893\n",
            "97     \t [0.8857606  0.51839001 0.89410913]. \t  3.5333588363545485 \t 3.8136406151019893\n",
            "98     \t [0.50772457 0.53934594 0.88284166]. \t  3.7486082055621224 \t 3.8136406151019893\n",
            "99     \t [0.33868437 0.58515045 0.84729561]. \t  \u001b[92m3.823184944915744\u001b[0m \t 3.823184944915744\n",
            "100    \t [0.57165835 0.65089202 0.89220416]. \t  3.394374575159 \t 3.823184944915744\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N4xG5dbuuvig",
        "outputId": "173f8cdc-0080-4626-9b0c-1fdb1c054df3"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 10 \r\n",
        "\r\n",
        "np.random.seed(run_num_10)\r\n",
        "surrogate_winner_10 = dtStudentProcess(d_cov_func, nu = df)\r\n",
        "\r\n",
        "winner_10 = dGPGO_stp(surrogate_winner_10, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_10.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.65358959 0.11500694 0.95028286]. \t  0.42730315147591735 \t 1.1029187088185965\n",
            "init   \t [0.4821914  0.87247454 0.21233268]. \t  0.06161964400032635 \t 1.1029187088185965\n",
            "init   \t [0.04070962 0.39719446 0.2331322 ]. \t  0.33269334660262956 \t 1.1029187088185965\n",
            "init   \t [0.84174072 0.20708234 0.74246953]. \t  1.1029187088185965 \t 1.1029187088185965\n",
            "init   \t [0.39215413 0.18225652 0.74353941]. \t  0.9779763535009853 \t 1.1029187088185965\n",
            "1      \t [0.65784615 0.04582828 0.06191159]. \t  0.20874301192282538 \t 1.1029187088185965\n",
            "2      \t [1. 1. 1.]. \t  0.31688362070415665 \t 1.1029187088185965\n",
            "3      \t [0.03722426 0.76271424 0.89011521]. \t  \u001b[92m2.5039656647591935\u001b[0m \t 2.5039656647591935\n",
            "4      \t [0.0068223  0.9162458  0.71242354]. \t  1.8230948036136974 \t 2.5039656647591935\n",
            "5      \t [0.18507279 0.78686552 0.95730744]. \t  1.76705662715257 \t 2.5039656647591935\n",
            "6      \t [0.02218954 0.56584252 0.99967923]. \t  2.067141380028605 \t 2.5039656647591935\n",
            "7      \t [0.99331851 0.56370661 0.47098301]. \t  0.1790629989671529 \t 2.5039656647591935\n",
            "8      \t [0.04224025 0.59256866 0.76081573]. \t  \u001b[92m3.1731953164847555\u001b[0m \t 3.1731953164847555\n",
            "9      \t [0.11687494 0.61830051 0.64177725]. \t  2.2392500656750944 \t 3.1731953164847555\n",
            "10     \t [0.03402321 0.08383914 0.53254351]. \t  0.16204279934957974 \t 3.1731953164847555\n",
            "11     \t [0.82238358 0.61622669 0.01512494]. \t  0.006678719482431739 \t 3.1731953164847555\n",
            "12     \t [0.93595995 0.99391835 0.49926782]. \t  0.31351760048081456 \t 3.1731953164847555\n",
            "13     \t [0.07764217 0.86489954 0.01260723]. \t  0.0008555924984510947 \t 3.1731953164847555\n",
            "14     \t [0.65213913 0.55978111 0.80589177]. \t  \u001b[92m3.5618417284658936\u001b[0m \t 3.5618417284658936\n",
            "15     \t [0.72180329 0.39224491 0.93833704]. \t  2.363538697312913 \t 3.5618417284658936\n",
            "16     \t [0.68587616 0.74194627 0.86264977]. \t  2.7121333458643346 \t 3.5618417284658936\n",
            "17     \t [3.12613728e-14 1.51064755e-13 3.82964304e-13]. \t  0.06797411659057852 \t 3.5618417284658936\n",
            "18     \t [0.66633289 0.55549312 0.72974174]. \t  2.597214248666466 \t 3.5618417284658936\n",
            "19     \t [0.81355348 0.93523956 0.0123949 ]. \t  0.00019327965368300627 \t 3.5618417284658936\n",
            "20     \t [0.9388895  0.21176718 0.05105017]. \t  0.08480814612397433 \t 3.5618417284658936\n",
            "21     \t [1.         0.76314907 0.82770901]. \t  2.289054615540258 \t 3.5618417284658936\n",
            "22     \t [0.38968095 0.54059421 0.94347831]. \t  3.069620756925648 \t 3.5618417284658936\n",
            "23     \t [0.3695644  0.34764843 0.00124357]. \t  0.07027463872817954 \t 3.5618417284658936\n",
            "24     \t [0.51576788 0.48546897 0.8668884 ]. \t  \u001b[92m3.6573795148458603\u001b[0m \t 3.6573795148458603\n",
            "25     \t [0.40622508 0.59006741 0.88755387]. \t  \u001b[92m3.704283306235746\u001b[0m \t 3.704283306235746\n",
            "26     \t [0.4731873  0.65051288 0.80004621]. \t  3.2699626536301647 \t 3.704283306235746\n",
            "27     \t [0.43492958 0.4827162  0.78143005]. \t  3.2837363955734293 \t 3.704283306235746\n",
            "28     \t [0.39938337 0.57245689 0.90595281]. \t  3.5777981935867045 \t 3.704283306235746\n",
            "29     \t [0.5185865  0.48313793 0.88135415]. \t  3.580510347967646 \t 3.704283306235746\n",
            "30     \t [0.64465315 0.51422062 0.82179674]. \t  3.6610696047649243 \t 3.704283306235746\n",
            "31     \t [0.08914815 0.98322095 0.18393165]. \t  0.04330544919669205 \t 3.704283306235746\n",
            "32     \t [0.43686906 0.54250747 0.82856585]. \t  \u001b[92m3.787292977871507\u001b[0m \t 3.787292977871507\n",
            "33     \t [0.40814347 0.51764754 0.89023743]. \t  3.664469896238432 \t 3.787292977871507\n",
            "34     \t [0.99866554 0.01293789 0.08625557]. \t  0.10213978141038657 \t 3.787292977871507\n",
            "35     \t [0.03004536 0.15848571 0.87299746]. \t  0.8678338688687262 \t 3.787292977871507\n",
            "36     \t [0.50309387 0.44907166 0.82520034]. \t  3.4343735219199574 \t 3.787292977871507\n",
            "37     \t [0.4089475  0.65217135 0.83184148]. \t  3.481319385310845 \t 3.787292977871507\n",
            "38     \t [0.49219803 0.53887326 0.83121864]. \t  3.7860449901461948 \t 3.787292977871507\n",
            "39     \t [0.4419578  0.51549194 0.8937807 ]. \t  3.628250135717829 \t 3.787292977871507\n",
            "40     \t [0.29856494 0.61617932 0.83271558]. \t  3.690225832380969 \t 3.787292977871507\n",
            "41     \t [0.20142537 0.57145111 0.83705087]. \t  \u001b[92m3.823840880564537\u001b[0m \t 3.823840880564537\n",
            "42     \t [0.15076231 0.57503785 0.80178503]. \t  3.6142648401837913 \t 3.823840880564537\n",
            "43     \t [0.50211385 0.47079957 0.9200523 ]. \t  3.157777205088061 \t 3.823840880564537\n",
            "44     \t [0.48792659 0.55223613 0.83794642]. \t  3.8169791170980565 \t 3.823840880564537\n",
            "45     \t [0.29079395 0.52639388 0.86664913]. \t  3.811596739689298 \t 3.823840880564537\n",
            "46     \t [0.33604344 0.50145119 0.88810291]. \t  3.6276731937787106 \t 3.823840880564537\n",
            "47     \t [0.1288282  0.52863163 0.79327494]. \t  3.5331095362215263 \t 3.823840880564537\n",
            "48     \t [0.1130231  0.5081149  0.84465645]. \t  3.762130344598458 \t 3.823840880564537\n",
            "49     \t [0.23245356 0.45897958 0.87874785]. \t  3.4631284725239295 \t 3.823840880564537\n",
            "50     \t [0.23047771 0.63145343 0.81224128]. \t  3.5237405093365375 \t 3.823840880564537\n",
            "51     \t [0.5783509  0.49008829 0.81913902]. \t  3.5934784878492065 \t 3.823840880564537\n",
            "52     \t [0.09535569 0.51745243 0.84154358]. \t  3.7803675489248865 \t 3.823840880564537\n",
            "53     \t [0.50902559 0.56761325 0.82655929]. \t  3.752360388277689 \t 3.823840880564537\n",
            "54     \t [0.65790481 0.50206077 0.85102675]. \t  3.7179961892947286 \t 3.823840880564537\n",
            "55     \t [0.51884018 0.57083199 0.85833946]. \t  \u001b[92m3.825950075211796\u001b[0m \t 3.825950075211796\n",
            "56     \t [0.16058745 0.57811519 0.78911339]. \t  3.501259627525074 \t 3.825950075211796\n",
            "57     \t [0.01821824 0.47428464 0.81365178]. \t  3.4907336854999853 \t 3.825950075211796\n",
            "58     \t [0.22846334 0.58820708 0.82333703]. \t  3.7399428189077066 \t 3.825950075211796\n",
            "59     \t [0.56150275 0.53237619 0.85852006]. \t  3.8125537030915897 \t 3.825950075211796\n",
            "60     \t [0.02253709 0.46072794 0.89874421]. \t  3.2904526310808944 \t 3.825950075211796\n",
            "61     \t [0.64710234 0.50404027 0.83054467]. \t  3.679403151122495 \t 3.825950075211796\n",
            "62     \t [0.49492225 0.52206329 0.88230578]. \t  3.721936438594028 \t 3.825950075211796\n",
            "63     \t [0.45929229 0.5882154  0.8312586 ]. \t  3.7498610200516396 \t 3.825950075211796\n",
            "64     \t [0.51350584 0.52024994 0.86124307]. \t  3.7948337215412185 \t 3.825950075211796\n",
            "65     \t [0.77183169 0.5156257  0.84182923]. \t  3.7081193777919514 \t 3.825950075211796\n",
            "66     \t [0.5596224  0.66058046 0.81387635]. \t  3.2724999955434573 \t 3.825950075211796\n",
            "67     \t [0.52199797 0.56106488 0.83557094]. \t  3.7971847022169953 \t 3.825950075211796\n",
            "68     \t [0.51341549 0.51389707 0.84835842]. \t  3.784084189308428 \t 3.825950075211796\n",
            "69     \t [0.31550074 0.57188946 0.83286142]. \t  3.8101825047235316 \t 3.825950075211796\n",
            "70     \t [0.35171343 0.48156892 0.87819285]. \t  3.6015197319732932 \t 3.825950075211796\n",
            "71     \t [0.36689723 0.46891745 0.80815362]. \t  3.4681345143155995 \t 3.825950075211796\n",
            "72     \t [0.55213179 0.52639722 0.87147551]. \t  3.774550876297647 \t 3.825950075211796\n",
            "73     \t [0.32078452 0.5653035  0.91972557]. \t  3.4249850246036213 \t 3.825950075211796\n",
            "74     \t [0.22235745 0.53273405 0.87146467]. \t  3.802651745039733 \t 3.825950075211796\n",
            "75     \t [0.51330774 0.54342134 0.83184149]. \t  3.786758241896764 \t 3.825950075211796\n",
            "76     \t [0.217102   0.53385741 0.77223569]. \t  3.3333990743870485 \t 3.825950075211796\n",
            "77     \t [0.09452708 0.53454606 0.82899835]. \t  3.7757699659686046 \t 3.825950075211796\n",
            "78     \t [0.37233617 0.60712217 0.8870054 ]. \t  3.662042686639273 \t 3.825950075211796\n",
            "79     \t [0.03511375 0.56339281 0.77719412]. \t  3.3698550738750597 \t 3.825950075211796\n",
            "80     \t [0.19079304 0.5492814  0.85276274]. \t  \u001b[92m3.8540176131000927\u001b[0m \t 3.8540176131000927\n",
            "81     \t [0.08117841 0.56041158 0.87319062]. \t  3.7924535243529225 \t 3.8540176131000927\n",
            "82     \t [0.15421961 0.58138463 0.86315443]. \t  3.8170253126461455 \t 3.8540176131000927\n",
            "83     \t [0.37579014 0.56459625 0.89313639]. \t  3.6997044489529882 \t 3.8540176131000927\n",
            "84     \t [0.46896919 0.55727394 0.83665081]. \t  3.8152719035542932 \t 3.8540176131000927\n",
            "85     \t [0.03127939 0.60986704 0.8462831 ]. \t  3.7158909199603967 \t 3.8540176131000927\n",
            "86     \t [0.03326941 0.44343024 0.84275344]. \t  3.4110461466072315 \t 3.8540176131000927\n",
            "87     \t [0.269181   0.53514101 0.8557854 ]. \t  3.846088389834165 \t 3.8540176131000927\n",
            "88     \t [0.07475194 0.62305933 0.86358838]. \t  3.6698797803508656 \t 3.8540176131000927\n",
            "89     \t [0.52443718 0.54372326 0.79887095]. \t  3.5512242427520704 \t 3.8540176131000927\n",
            "90     \t [0.73039592 0.53077173 0.83070651]. \t  3.707337743834635 \t 3.8540176131000927\n",
            "91     \t [0.56135936 0.59474134 0.83085318]. \t  3.7008042404130235 \t 3.8540176131000927\n",
            "92     \t [0.24782861 0.472361   0.83995492]. \t  3.6218761795245475 \t 3.8540176131000927\n",
            "93     \t [0.52212212 0.55121476 0.84680281]. \t  3.8310197853560313 \t 3.8540176131000927\n",
            "94     \t [0.44746658 0.50324358 0.81834391]. \t  3.660009966223251 \t 3.8540176131000927\n",
            "95     \t [0.2326055  0.52617439 0.8524771 ]. \t  3.8300230674427915 \t 3.8540176131000927\n",
            "96     \t [0.64688986 0.56633895 0.81046338]. \t  3.595854693074133 \t 3.8540176131000927\n",
            "97     \t [0.7693017  0.49337861 0.83756582]. \t  3.631897401597447 \t 3.8540176131000927\n",
            "98     \t [0.11961054 0.60777968 0.86856183]. \t  3.7287361670492998 \t 3.8540176131000927\n",
            "99     \t [0.19322018 0.63427637 0.85924469]. \t  3.6422929290593777 \t 3.8540176131000927\n",
            "100    \t [0.26077252 0.54266976 0.83838354]. \t  3.8370553962811376 \t 3.8540176131000927\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pSj_CQIAuvk3",
        "outputId": "8997540e-e142-4cbd-f9aa-4414dd4a62ca"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 11 \r\n",
        "\r\n",
        "np.random.seed(run_num_11)\r\n",
        "surrogate_winner_11 = dtStudentProcess(d_cov_func, nu = df)\r\n",
        "\r\n",
        "winner_11 = dGPGO_stp(surrogate_winner_11, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_11.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.98103566 0.82124785 0.64589361]. \t  0.6570240879762975 \t 0.687459437576373\n",
            "init   \t [0.42368801 0.20231098 0.49190677]. \t  0.29901296656621285 \t 0.687459437576373\n",
            "init   \t [0.13855833 0.45252104 0.11373463]. \t  0.1378633538075966 \t 0.687459437576373\n",
            "init   \t [0.00292449 0.52342617 0.18997116]. \t  0.12299763462541902 \t 0.687459437576373\n",
            "init   \t [0.14171979 0.33586637 0.58369219]. \t  0.687459437576373 \t 0.687459437576373\n",
            "1      \t [0.13253886 0.83967104 0.95737971]. \t  \u001b[92m1.3677915979569943\u001b[0m \t 1.3677915979569943\n",
            "2      \t [0.68849746 1.         1.        ]. \t  0.32793236595209807 \t 1.3677915979569943\n",
            "3      \t [0.86329302 0.95392965 0.17714534]. \t  0.007430275313978778 \t 1.3677915979569943\n",
            "4      \t [0.10922994 0.17736885 0.99587964]. \t  0.4885993482312173 \t 1.3677915979569943\n",
            "5      \t [0.03431813 0.5534249  0.8649314 ]. \t  \u001b[92m3.8065905322499787\u001b[0m \t 3.8065905322499787\n",
            "6      \t [0.94561973 0.50050163 0.0278312 ]. \t  0.015184956042492256 \t 3.8065905322499787\n",
            "7      \t [0.87810196 0.44177777 0.92649034]. \t  2.8331763959111425 \t 3.8065905322499787\n",
            "8      \t [0.29630583 0.53830535 0.99365443]. \t  2.184692840765682 \t 3.8065905322499787\n",
            "9      \t [0.98738774 0.06784079 0.71599439]. \t  0.37605082365730585 \t 3.8065905322499787\n",
            "10     \t [1. 1. 1.]. \t  0.31688362070447745 \t 3.8065905322499787\n",
            "11     \t [0.20969008 0.97403676 0.07037021]. \t  0.0024965636354188007 \t 3.8065905322499787\n",
            "12     \t [0.01103206 0.38516658 0.95715505]. \t  2.064382960815838 \t 3.8065905322499787\n",
            "13     \t [0.391226   0.76745853 0.47062692]. \t  1.747148761526802 \t 3.8065905322499787\n",
            "14     \t [0.0176432  0.8816692  0.60416436]. \t  2.87872321929492 \t 3.8065905322499787\n",
            "15     \t [0.56394398 0.28264998 0.98985344]. \t  1.0316343050968653 \t 3.8065905322499787\n",
            "16     \t [0.41524468 0.00910236 0.07325746]. \t  0.28580849744571485 \t 3.8065905322499787\n",
            "17     \t [3.29448114e-04 6.59240035e-01 5.79078191e-01]. \t  2.186631247896338 \t 3.8065905322499787\n",
            "18     \t [5.27059027e-13 1.16359023e-13 1.90912256e-12]. \t  0.06797411659231137 \t 3.8065905322499787\n",
            "19     \t [0.62128212 0.61773443 0.79468743]. \t  3.3080237871630596 \t 3.8065905322499787\n",
            "20     \t [0.49729937 0.9290295  0.67888564]. \t  1.4689642410686714 \t 3.8065905322499787\n",
            "21     \t [0.0307018  0.79637601 0.05002823]. \t  0.003037760897730993 \t 3.8065905322499787\n",
            "22     \t [0.84878393 0.02989029 0.15365266]. \t  0.3153086987690147 \t 3.8065905322499787\n",
            "23     \t [0.96080264 0.6956864  0.95629173]. \t  2.342958601127812 \t 3.8065905322499787\n",
            "24     \t [0.76649393 0.45108174 0.71782591]. \t  2.260297202385502 \t 3.8065905322499787\n",
            "25     \t [0.01456894 0.76119951 0.89066542]. \t  2.5105357781051945 \t 3.8065905322499787\n",
            "26     \t [0.24154346 0.62200074 0.80466269]. \t  3.5130173796498574 \t 3.8065905322499787\n",
            "27     \t [0.84566145 0.01748764 0.89370982]. \t  0.234935177882342 \t 3.8065905322499787\n",
            "28     \t [2.64978369e-01 2.37299300e-10 7.23745816e-01]. \t  0.22204025043483752 \t 3.8065905322499787\n",
            "29     \t [0.64163909 0.42401567 0.06438764]. \t  0.09075766160034404 \t 3.8065905322499787\n",
            "30     \t [0.1153977  0.51038284 0.81519275]. \t  3.660990817349257 \t 3.8065905322499787\n",
            "31     \t [0.18968288 0.69150806 0.78243774]. \t  3.000978262908105 \t 3.8065905322499787\n",
            "32     \t [0.01213219 0.99473636 0.46684949]. \t  2.000812779809574 \t 3.8065905322499787\n",
            "33     \t [0.31889489 0.53729722 0.78955898]. \t  3.5097464013549926 \t 3.8065905322499787\n",
            "34     \t [0.64092338 0.75789543 0.05514189]. \t  0.004044415097615545 \t 3.8065905322499787\n",
            "35     \t [0.01814484 0.61864898 0.78791613]. \t  3.3612266266419457 \t 3.8065905322499787\n",
            "36     \t [0.20473233 0.50010379 0.81141712]. \t  3.6189078667805643 \t 3.8065905322499787\n",
            "37     \t [0.20679387 0.47798666 0.83178897]. \t  3.628093064672107 \t 3.8065905322499787\n",
            "38     \t [0.4536071  0.4453087  0.84934569]. \t  3.4602636027459113 \t 3.8065905322499787\n",
            "39     \t [0.31499738 0.5369413  0.8732597 ]. \t  \u001b[92m3.8070259008488474\u001b[0m \t 3.8070259008488474\n",
            "40     \t [0.43271385 0.41239457 0.79751445]. \t  3.0427707920600886 \t 3.8070259008488474\n",
            "41     \t [0.34725732 0.5515831  0.7871173 ]. \t  3.4854833507749023 \t 3.8070259008488474\n",
            "42     \t [0.00086147 0.61131219 0.8133503 ]. \t  3.5709476777449263 \t 3.8070259008488474\n",
            "43     \t [0.59042349 0.47739167 0.85134519]. \t  3.630401790391966 \t 3.8070259008488474\n",
            "44     \t [0.38667955 0.50533768 0.8959391 ]. \t  3.579415980196567 \t 3.8070259008488474\n",
            "45     \t [0.50657016 0.49089221 0.85138261]. \t  3.7073324946270843 \t 3.8070259008488474\n",
            "46     \t [0.56252896 0.6229269  0.86368163]. \t  3.654935507629283 \t 3.8070259008488474\n",
            "47     \t [0.31061669 0.54694307 0.88556352]. \t  3.751739589628985 \t 3.8070259008488474\n",
            "48     \t [0.02737576 0.99577143 0.71777246]. \t  1.3969425077586213 \t 3.8070259008488474\n",
            "49     \t [0.30264295 0.60745111 0.87784554]. \t  3.712429481677972 \t 3.8070259008488474\n",
            "50     \t [0.37567097 0.48150563 0.85948196]. \t  3.668843467976787 \t 3.8070259008488474\n",
            "51     \t [0.34103848 0.47763601 0.87454306]. \t  3.5996859777604984 \t 3.8070259008488474\n",
            "52     \t [0.63100562 0.56537282 0.92980877]. \t  3.265081333905771 \t 3.8070259008488474\n",
            "53     \t [0.44500673 0.64195557 0.82712318]. \t  3.5106983372946 \t 3.8070259008488474\n",
            "54     \t [0.22615737 0.51281611 0.86805747]. \t  3.7679735044481095 \t 3.8070259008488474\n",
            "55     \t [0.30831062 0.64179392 0.82991041]. \t  3.55449600899634 \t 3.8070259008488474\n",
            "56     \t [0.08529122 0.57462002 0.82896882]. \t  3.771717212178942 \t 3.8070259008488474\n",
            "57     \t [0.10093131 0.51082038 0.83336637]. \t  3.74370540421398 \t 3.8070259008488474\n",
            "58     \t [0.14848189 0.60892087 0.81055274]. \t  3.5998652645706692 \t 3.8070259008488474\n",
            "59     \t [0.23816069 0.45177043 0.87778609]. \t  3.4217542917011885 \t 3.8070259008488474\n",
            "60     \t [0.25617899 0.63084939 0.77904037]. \t  3.2648291267315503 \t 3.8070259008488474\n",
            "61     \t [0.48421656 0.493728   0.85377748]. \t  3.7216099548700554 \t 3.8070259008488474\n",
            "62     \t [0.44433344 0.52467376 0.85528406]. \t  \u001b[92m3.820860150300385\u001b[0m \t 3.820860150300385\n",
            "63     \t [0.41501054 0.4572178  0.85456926]. \t  3.537466378760792 \t 3.820860150300385\n",
            "64     \t [0.57531549 0.62266199 0.89199189]. \t  3.5419572040112346 \t 3.820860150300385\n",
            "65     \t [0.03033621 0.60607188 0.8731761 ]. \t  3.6973188378660997 \t 3.820860150300385\n",
            "66     \t [0.38311766 0.57716223 0.84585103]. \t  \u001b[92m3.832003952745011\u001b[0m \t 3.832003952745011\n",
            "67     \t [0.28671914 0.50649623 0.89419485]. \t  3.5971526667745213 \t 3.832003952745011\n",
            "68     \t [0.46530289 0.56618438 0.82141748]. \t  3.7356284054236553 \t 3.832003952745011\n",
            "69     \t [0.48262862 0.5823131  0.89142261]. \t  3.6850633646565942 \t 3.832003952745011\n",
            "70     \t [0.24643853 0.57801793 0.87130892]. \t  3.8102488871675555 \t 3.832003952745011\n",
            "71     \t [0.50859518 0.64723696 0.84347109]. \t  3.5218274178611217 \t 3.832003952745011\n",
            "72     \t [0.69001989 0.57526862 0.82176998]. \t  3.6498634348882186 \t 3.832003952745011\n",
            "73     \t [0.56908667 0.61819378 0.82188794]. \t  3.558240565940396 \t 3.832003952745011\n",
            "74     \t [0.29175292 0.55872521 0.88714516]. \t  3.7450895310223586 \t 3.832003952745011\n",
            "75     \t [0.41443338 0.60581294 0.863799  ]. \t  3.7542537274948486 \t 3.832003952745011\n",
            "76     \t [0.3815899  0.59862108 0.87130661]. \t  3.7629194843194065 \t 3.832003952745011\n",
            "77     \t [0.23359401 0.57713205 0.76057222]. \t  3.205893918060222 \t 3.832003952745011\n",
            "78     \t [0.37441405 0.50152668 0.89356769]. \t  3.585492038419422 \t 3.832003952745011\n",
            "79     \t [0.44004829 0.57021048 0.83858703]. \t  3.8162505296339466 \t 3.832003952745011\n",
            "80     \t [0.50480804 0.49181214 0.84365538]. \t  3.7059923415412097 \t 3.832003952745011\n",
            "81     \t [0.52142803 0.65028994 0.88420784]. \t  3.453188627310547 \t 3.832003952745011\n",
            "82     \t [0.3768818  0.58418398 0.82541667]. \t  3.747688861624937 \t 3.832003952745011\n",
            "83     \t [0.02741008 0.6231477  0.79859315]. \t  3.4346218132316038 \t 3.832003952745011\n",
            "84     \t [0.08201242 0.4989633  0.78819076]. \t  3.4086009625642144 \t 3.832003952745011\n",
            "85     \t [0.41301166 0.54243128 0.91044301]. \t  3.5237623779308946 \t 3.832003952745011\n",
            "86     \t [0.3508875  0.60968053 0.8876112 ]. \t  3.6503589841860644 \t 3.832003952745011\n",
            "87     \t [0.46605663 0.5061019  0.83114273]. \t  3.7289548022278396 \t 3.832003952745011\n",
            "88     \t [0.38814997 0.61027652 0.87802501]. \t  3.69788546688918 \t 3.832003952745011\n",
            "89     \t [0.50940047 0.57562178 0.85740413]. \t  3.821596914301206 \t 3.832003952745011\n",
            "90     \t [3.36437373e-04 5.92900455e-01 8.76078804e-01]. \t  3.714162858140928 \t 3.832003952745011\n",
            "91     \t [0.20497773 0.61640727 0.84554322]. \t  3.7232781621578592 \t 3.832003952745011\n",
            "92     \t [0.41503592 0.54752483 0.84909805]. \t  \u001b[92m3.8511165079641128\u001b[0m \t 3.8511165079641128\n",
            "93     \t [0.44511862 0.57354454 0.84122054]. \t  3.8183099316297167 \t 3.8511165079641128\n",
            "94     \t [0.47540019 0.50247134 0.84723974]. \t  3.7544355481009624 \t 3.8511165079641128\n",
            "95     \t [0.51249217 0.58451519 0.85032931]. \t  3.8012016614135224 \t 3.8511165079641128\n",
            "96     \t [0.27678326 0.47274306 0.90379055]. \t  3.353352378745186 \t 3.8511165079641128\n",
            "97     \t [0.60779283 0.53786754 0.85966835]. \t  3.807832102975487 \t 3.8511165079641128\n",
            "98     \t [0.39260071 0.51814607 0.78806398]. \t  3.4566820804395504 \t 3.8511165079641128\n",
            "99     \t [0.57318609 0.58854266 0.84364229]. \t  3.7632444022136338 \t 3.8511165079641128\n",
            "100    \t [0.19969712 0.61923166 0.87975831]. \t  3.6533042261629225 \t 3.8511165079641128\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l98Nt7Tguvna",
        "outputId": "36eb943c-a935-4127-e90a-53cda7749ea1"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 12\r\n",
        "\r\n",
        "np.random.seed(run_num_12)\r\n",
        "surrogate_winner_12 = dtStudentProcess(d_cov_func, nu = df)\r\n",
        "\r\n",
        "winner_12 = dGPGO_stp(surrogate_winner_12, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_12.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.19151945 0.62210877 0.43772774]. \t  1.1006281843679786 \t 1.6482992955272024\n",
            "init   \t [0.78535858 0.77997581 0.27259261]. \t  0.07153771680480671 \t 1.6482992955272024\n",
            "init   \t [0.27646426 0.80187218 0.95813935]. \t  1.6482992955272024 \t 1.6482992955272024\n",
            "init   \t [0.87593263 0.35781727 0.50099513]. \t  0.2282556248207173 \t 1.6482992955272024\n",
            "init   \t [0.68346294 0.71270203 0.37025075]. \t  0.33032494760932407 \t 1.6482992955272024\n",
            "1      \t [0.14644767 0.4817297  0.9709433 ]. \t  \u001b[92m2.4306906358044365\u001b[0m \t 2.4306906358044365\n",
            "2      \t [0.02211118 0.60598834 0.96479652]. \t  \u001b[92m2.6499602294128\u001b[0m \t 2.6499602294128\n",
            "3      \t [0.03494356 0.11089546 0.89536938]. \t  0.5535687532914807 \t 2.6499602294128\n",
            "4      \t [0.18676015 0.59648467 0.9337563 ]. \t  \u001b[92m3.188902569223007\u001b[0m \t 3.188902569223007\n",
            "5      \t [1.         0.77463447 0.79843042]. \t  1.9934143631411911 \t 3.188902569223007\n",
            "6      \t [0.         0.43463635 0.61288509]. \t  1.2168291700915608 \t 3.188902569223007\n",
            "7      \t [0.56830605 0.03356502 0.97684007]. \t  0.16226669597537768 \t 3.188902569223007\n",
            "8      \t [0.01888274 0.90468212 0.53045009]. \t  2.8879684708430005 \t 3.188902569223007\n",
            "9      \t [0.1113436  0.75189209 0.72891858]. \t  2.492694753824666 \t 3.188902569223007\n",
            "10     \t [1.90227801e-13 2.53212745e-13 2.44143043e-13]. \t  0.06797411659046734 \t 3.188902569223007\n",
            "11     \t [0.37069885 0.88708842 0.04566153]. \t  0.0016346298588470634 \t 3.188902569223007\n",
            "12     \t [0.81164891 0.51784942 0.94750604]. \t  2.902596511865748 \t 3.188902569223007\n",
            "13     \t [0.2733825  0.01619619 0.52963335]. \t  0.15345356349589626 \t 3.188902569223007\n",
            "14     \t [0.55841211 0.53516634 0.85110357]. \t  \u001b[92m3.8179923846839823\u001b[0m \t 3.8179923846839823\n",
            "15     \t [0.01941552 0.81179704 0.14320322]. \t  0.021040703040039674 \t 3.8179923846839823\n",
            "16     \t [0.9902054  0.68965352 0.55495843]. \t  0.44345730601579786 \t 3.8179923846839823\n",
            "17     \t [0.7835869  0.11158772 0.09292909]. \t  0.23970929548373915 \t 3.8179923846839823\n",
            "18     \t [0.80262025 0.32588038 0.00879151]. \t  0.049521158831366395 \t 3.8179923846839823\n",
            "19     \t [0.99999999 0.99999996 1.        ]. \t  0.3168837453169505 \t 3.8179923846839823\n",
            "20     \t [0.70123189 0.66933085 0.88630613]. \t  3.2736654849161244 \t 3.8179923846839823\n",
            "21     \t [0.49950331 0.57952711 0.97944743]. \t  2.4558440791788083 \t 3.8179923846839823\n",
            "22     \t [0.29995525 0.         0.        ]. \t  0.10079892850930157 \t 3.8179923846839823\n",
            "23     \t [0.81596508 0.98370171 0.01148573]. \t  0.0001269638144386121 \t 3.8179923846839823\n",
            "24     \t [0.89098967 0.00699013 0.5104566 ]. \t  0.09339085810634216 \t 3.8179923846839823\n",
            "25     \t [0.18151696 0.97699022 0.27394658]. \t  0.24977437477251194 \t 3.8179923846839823\n",
            "26     \t [0.42974315 0.35034957 0.72573434]. \t  1.9699192796787997 \t 3.8179923846839823\n",
            "27     \t [0.77861286 0.57415148 0.8240048 ]. \t  3.6311074577983256 \t 3.8179923846839823\n",
            "28     \t [0.95578339 0.17389833 0.89491419]. \t  0.8902547759736333 \t 3.8179923846839823\n",
            "29     \t [0.68817762 0.54296212 0.82470698]. \t  3.697841073890367 \t 3.8179923846839823\n",
            "30     \t [0.62503436 0.54291131 0.86372041]. \t  3.8024175208735684 \t 3.8179923846839823\n",
            "31     \t [0.9691119  0.60084512 0.00139054]. \t  0.003930478273440829 \t 3.8179923846839823\n",
            "32     \t [0.73729503 0.49862286 0.81924638]. \t  3.5755989601628673 \t 3.8179923846839823\n",
            "33     \t [0.98935949 0.00797188 0.18589577]. \t  0.22933865993000419 \t 3.8179923846839823\n",
            "34     \t [0.02499349 0.00605118 0.2681429 ]. \t  0.6202589039399784 \t 3.8179923846839823\n",
            "35     \t [0.83177502 0.55702533 0.82642377]. \t  3.64798863554443 \t 3.8179923846839823\n",
            "36     \t [0.73019187 0.5689658  0.80318858]. \t  3.490861703504535 \t 3.8179923846839823\n",
            "37     \t [0.66162984 0.56269575 0.78562375]. \t  3.343799306392473 \t 3.8179923846839823\n",
            "38     \t [0.80158295 0.57421955 0.85277126]. \t  3.7341896613628496 \t 3.8179923846839823\n",
            "39     \t [0.81077892 0.51472141 0.81437685]. \t  3.5537825437203283 \t 3.8179923846839823\n",
            "40     \t [0.02401995 0.96170179 0.84719456]. \t  0.936001386847024 \t 3.8179923846839823\n",
            "41     \t [0.8278846  0.54248537 0.80598251]. \t  3.4996119830966324 \t 3.8179923846839823\n",
            "42     \t [0.66599287 0.56923211 0.82289758]. \t  3.6771098404821294 \t 3.8179923846839823\n",
            "43     \t [0.54717543 0.49466091 0.8478098 ]. \t  3.714437796517699 \t 3.8179923846839823\n",
            "44     \t [0.48447973 0.5187844  0.87026998]. \t  3.772011767882904 \t 3.8179923846839823\n",
            "45     \t [0.77057261 0.54354588 0.88410394]. \t  3.6842580321099994 \t 3.8179923846839823\n",
            "46     \t [0.65639962 0.56702326 0.83436864]. \t  3.746229755088729 \t 3.8179923846839823\n",
            "47     \t [0.5128049  0.52886013 0.82626386]. \t  3.7472675977302186 \t 3.8179923846839823\n",
            "48     \t [0.55079216 0.54824794 0.85521141]. \t  \u001b[92m3.8305734170702337\u001b[0m \t 3.8305734170702337\n",
            "49     \t [0.93808094 0.57876935 0.88350822]. \t  3.610385017043364 \t 3.8305734170702337\n",
            "50     \t [0.5387238  0.53872715 0.8849611 ]. \t  3.7302185889088415 \t 3.8305734170702337\n",
            "51     \t [0.45367452 0.56989417 0.88421859]. \t  3.752344453008608 \t 3.8305734170702337\n",
            "52     \t [0.97360337 0.5001345  0.86784386]. \t  3.581366682231876 \t 3.8305734170702337\n",
            "53     \t [0.99180312 0.58630999 0.82381189]. \t  3.503796464525915 \t 3.8305734170702337\n",
            "54     \t [0.91604482 0.52663071 0.85613386]. \t  3.6911474767633883 \t 3.8305734170702337\n",
            "55     \t [0.68086447 0.52533207 0.8523517 ]. \t  3.7724088338317596 \t 3.8305734170702337\n",
            "56     \t [0.44559479 0.50870512 0.85916776]. \t  3.7752895037527896 \t 3.8305734170702337\n",
            "57     \t [0.87759164 0.52590311 0.8259716 ]. \t  3.6199283099995228 \t 3.8305734170702337\n",
            "58     \t [0.41785364 0.56149216 0.77829244]. \t  3.366958203431307 \t 3.8305734170702337\n",
            "59     \t [0.51688836 0.56900757 0.83937028]. \t  3.8037326865716565 \t 3.8305734170702337\n",
            "60     \t [0.60324661 0.52437432 0.89685261]. \t  3.604416165798052 \t 3.8305734170702337\n",
            "61     \t [0.32065486 0.59586172 0.83741202]. \t  3.7755992713714672 \t 3.8305734170702337\n",
            "62     \t [0.8143449  0.60667573 0.8102249 ]. \t  3.4088401201524023 \t 3.8305734170702337\n",
            "63     \t [0.58944968 0.57585921 0.80923671]. \t  3.5920404013013236 \t 3.8305734170702337\n",
            "64     \t [0.99790846 0.57526805 0.87863631]. \t  3.6116212137969326 \t 3.8305734170702337\n",
            "65     \t [0.27283437 0.57237221 0.8614404 ]. \t  \u001b[92m3.8442348173474845\u001b[0m \t 3.8442348173474845\n",
            "66     \t [0.99382924 0.5379479  0.85526552]. \t  3.670098913582027 \t 3.8442348173474845\n",
            "67     \t [0.52596381 0.54759133 0.85425475]. \t  3.835587618501904 \t 3.8442348173474845\n",
            "68     \t [0.43277081 0.53783218 0.83852069]. \t  3.822193152198139 \t 3.8442348173474845\n",
            "69     \t [0.71104901 0.59549274 0.9188557 ]. \t  3.351599454103028 \t 3.8442348173474845\n",
            "70     \t [0.56073328 0.53334316 0.90431632]. \t  3.5603443325378863 \t 3.8442348173474845\n",
            "71     \t [0.65872291 0.54375859 0.81604195]. \t  3.6505928781491175 \t 3.8442348173474845\n",
            "72     \t [0.43690332 0.54231653 0.86322396]. \t  3.8377951490455957 \t 3.8442348173474845\n",
            "73     \t [0.30911351 0.61021119 0.84350423]. \t  3.744171598132753 \t 3.8442348173474845\n",
            "74     \t [0.8435284  0.52420099 0.83799086]. \t  3.686411563113091 \t 3.8442348173474845\n",
            "75     \t [0.54119347 0.60390854 0.81667808]. \t  3.591115205586129 \t 3.8442348173474845\n",
            "76     \t [0.89842098 0.48460779 0.79403221]. \t  3.2677956389999006 \t 3.8442348173474845\n",
            "77     \t [0.50190079 0.51824682 0.84792374]. \t  3.7966109740494156 \t 3.8442348173474845\n",
            "78     \t [0.38289272 0.53742975 0.81764202]. \t  3.73183088221676 \t 3.8442348173474845\n",
            "79     \t [0.51945475 0.48536123 0.78092337]. \t  3.2663671154781664 \t 3.8442348173474845\n",
            "80     \t [0.28539529 0.59765743 0.85838605]. \t  3.7961479415927757 \t 3.8442348173474845\n",
            "81     \t [0.43750944 0.61952838 0.88054085]. \t  3.64434155707522 \t 3.8442348173474845\n",
            "82     \t [0.90546565 0.62517113 0.83918745]. \t  3.4772716383133044 \t 3.8442348173474845\n",
            "83     \t [0.74549632 0.48758222 0.83934489]. \t  3.622144384359814 \t 3.8442348173474845\n",
            "84     \t [0.93026722 0.49453764 0.82859896]. \t  3.540845001888303 \t 3.8442348173474845\n",
            "85     \t [0.54097084 0.53840715 0.84683867]. \t  3.8207993738291206 \t 3.8442348173474845\n",
            "86     \t [0.60832238 0.55700038 0.81874419]. \t  3.683568939368378 \t 3.8442348173474845\n",
            "87     \t [0.70108693 0.57325593 0.87006523]. \t  3.7571656181765887 \t 3.8442348173474845\n",
            "88     \t [0.43448432 0.6082266  0.85670623]. \t  3.7488417782281496 \t 3.8442348173474845\n",
            "89     \t [0.32432703 0.52778817 0.87755112]. \t  3.771366313912431 \t 3.8442348173474845\n",
            "90     \t [0.96806927 0.58717482 0.8810427 ]. \t  3.5927379188737927 \t 3.8442348173474845\n",
            "91     \t [0.66546093 0.55096427 0.89342966]. \t  3.6555230477411857 \t 3.8442348173474845\n",
            "92     \t [0.40862247 0.60480433 0.86182413]. \t  3.761340461850924 \t 3.8442348173474845\n",
            "93     \t [0.59580775 0.54706917 0.89548333]. \t  3.6520509150556584 \t 3.8442348173474845\n",
            "94     \t [0.36016406 0.56044179 0.87076428]. \t  3.828787703864702 \t 3.8442348173474845\n",
            "95     \t [0.58511978 0.58863354 0.81864114]. \t  3.6368960621733866 \t 3.8442348173474845\n",
            "96     \t [0.12726301 0.52402271 0.88224719]. \t  3.716830777144037 \t 3.8442348173474845\n",
            "97     \t [0.07109439 0.61629693 0.86431961]. \t  3.6962269894442352 \t 3.8442348173474845\n",
            "98     \t [0.33723774 0.58684078 0.84895181]. \t  3.8214814789930904 \t 3.8442348173474845\n",
            "99     \t [0.44548727 0.57210299 0.90885498]. \t  3.5449301084269456 \t 3.8442348173474845\n",
            "100    \t [0.39414779 0.57429291 0.81946759]. \t  3.7286489485700822 \t 3.8442348173474845\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Bpn-kmNuvqC",
        "outputId": "d62e600f-dd1f-4fe2-a2ea-043ecb878e75"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 13 \r\n",
        "\r\n",
        "np.random.seed(run_num_13)\r\n",
        "surrogate_winner_13 = dtStudentProcess(d_cov_func, nu = df)\r\n",
        "\r\n",
        "winner_13 = dGPGO_stp(surrogate_winner_13, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_13.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.66064431 0.41360065 0.66810256]. \t  1.5457680899590245 \t 2.6697919207500047\n",
            "init   \t [0.22994342 0.80767834 0.63681846]. \t  2.6697919207500047 \t 2.6697919207500047\n",
            "init   \t [0.17219385 0.26038587 0.91531999]. \t  1.4685599870430508 \t 2.6697919207500047\n",
            "init   \t [0.46281551 0.12019095 0.88088551]. \t  0.639297028201682 \t 2.6697919207500047\n",
            "init   \t [0.22621895 0.81144033 0.44587892]. \t  1.960293029067079 \t 2.6697919207500047\n",
            "1      \t [1. 1. 1.]. \t  0.31688362070415665 \t 2.6697919207500047\n",
            "2      \t [0.37038883 0.74884883 0.93919259]. \t  2.261581652300019 \t 2.6697919207500047\n",
            "3      \t [0.05541546 0.94065901 0.80220177]. \t  1.1967277289563476 \t 2.6697919207500047\n",
            "4      \t [0.59243586 0.83374429 0.56648219]. \t  1.5891756993759176 \t 2.6697919207500047\n",
            "5      \t [0. 0. 0.]. \t  0.06797411659013229 \t 2.6697919207500047\n",
            "6      \t [0.0878313  0.39505626 0.5645263 ]. \t  0.7575555308759292 \t 2.6697919207500047\n",
            "7      \t [0.92107496 0.00171591 0.01397905]. \t  0.051162923260407445 \t 2.6697919207500047\n",
            "8      \t [0.9572096  0.69423713 0.92260859]. \t  \u001b[92m2.7477207101744305\u001b[0m \t 2.7477207101744305\n",
            "9      \t [0.96160336 0.22537985 0.90545458]. \t  1.2081833030583835 \t 2.7477207101744305\n",
            "10     \t [0.73361101 0.53759357 0.98574732]. \t  2.2999361966564225 \t 2.7477207101744305\n",
            "11     \t [0.26894409 0.68659066 0.6662963 ]. \t  2.41123194309757 \t 2.7477207101744305\n",
            "12     \t [0.28295395 0.99213626 0.55559161]. \t  2.407164167175514 \t 2.7477207101744305\n",
            "13     \t [0.90969676 0.59783804 0.81549803]. \t  \u001b[92m3.4429320931011254\u001b[0m \t 3.4429320931011254\n",
            "14     \t [0.99325398 0.62676208 0.55356945]. \t  0.4473890126884814 \t 3.4429320931011254\n",
            "15     \t [0.08000187 0.93009615 0.12745989]. \t  0.012709563575657093 \t 3.4429320931011254\n",
            "16     \t [0.58592153 0.93457908 0.84146957]. \t  1.018575507198293 \t 3.4429320931011254\n",
            "17     \t [0.92417766 0.01373158 0.43630291]. \t  0.15817465484947205 \t 3.4429320931011254\n",
            "18     \t [0.96668661 0.82565299 0.75526639]. \t  1.2882465488174721 \t 3.4429320931011254\n",
            "19     \t [0.01167944 0.05627394 0.58432351]. \t  0.15317057585631572 \t 3.4429320931011254\n",
            "20     \t [0.05164106 0.47835413 0.92653091]. \t  3.085727059380435 \t 3.4429320931011254\n",
            "21     \t [0.91636208 0.49973171 0.83299363]. \t  \u001b[92m3.5819737860024294\u001b[0m \t 3.5819737860024294\n",
            "22     \t [0.03727308 0.75980191 0.61098118]. \t  2.751007224507623 \t 3.5819737860024294\n",
            "23     \t [0.03540947 0.67053996 0.88954764]. \t  3.2832619333268993 \t 3.5819737860024294\n",
            "24     \t [0.99284591 0.50389619 0.75883264]. \t  2.847762527773266 \t 3.5819737860024294\n",
            "25     \t [0.6773124  0.89807632 0.00949013]. \t  0.000381052456340945 \t 3.5819737860024294\n",
            "26     \t [0.79935368 0.52437316 0.85993148]. \t  \u001b[92m3.7303293495529086\u001b[0m \t 3.7303293495529086\n",
            "27     \t [0.77367897 0.57696369 0.84008623]. \t  3.7098763762296043 \t 3.7303293495529086\n",
            "28     \t [0.8355407  0.54201892 0.79050956]. \t  3.3371793454631535 \t 3.7303293495529086\n",
            "29     \t [0.93628178 0.98787552 0.00269434]. \t  5.987355022400528e-05 \t 3.7303293495529086\n",
            "30     \t [0.77149934 0.52785607 0.86154714]. \t  \u001b[92m3.744277225919486\u001b[0m \t 3.744277225919486\n",
            "31     \t [0.7971665  0.5857516  0.82086021]. \t  3.574277488249529 \t 3.744277225919486\n",
            "32     \t [0.79195283 0.57542313 0.82931341]. \t  3.6555088695138447 \t 3.744277225919486\n",
            "33     \t [0.77806685 0.54180798 0.83173046]. \t  3.7023888829894176 \t 3.744277225919486\n",
            "34     \t [0.76380059 0.57861178 0.81589222]. \t  3.569062223951734 \t 3.744277225919486\n",
            "35     \t [0.32384319 0.99745114 0.15010572]. \t  0.016773042861865987 \t 3.744277225919486\n",
            "36     \t [0.         0.29037988 0.07194566]. \t  0.1567352063882219 \t 3.744277225919486\n",
            "37     \t [0.95977548 0.3249872  0.0368473 ]. \t  0.046276376052106855 \t 3.744277225919486\n",
            "38     \t [0.52445385 0.60658742 0.87886934]. \t  3.6885330508068095 \t 3.744277225919486\n",
            "39     \t [0.72227587 0.50631441 0.84827681]. \t  3.711259627792981 \t 3.744277225919486\n",
            "40     \t [0.62275378 0.51343046 0.83472413]. \t  3.7277128729442444 \t 3.744277225919486\n",
            "41     \t [0.74832124 0.5103337  0.8605389 ]. \t  3.7131111585339536 \t 3.744277225919486\n",
            "42     \t [0.80315006 0.51641923 0.82751503]. \t  3.6431501524926055 \t 3.744277225919486\n",
            "43     \t [0.75482341 0.48518578 0.86304788]. \t  3.613213064864391 \t 3.744277225919486\n",
            "44     \t [0.01250337 0.57029065 0.85463448]. \t  \u001b[92m3.807181352426551\u001b[0m \t 3.807181352426551\n",
            "45     \t [0.61289549 0.51118428 0.85326417]. \t  3.757925708155324 \t 3.807181352426551\n",
            "46     \t [0.59817841 0.62119151 0.81597091]. \t  3.491864160305358 \t 3.807181352426551\n",
            "47     \t [0.61958351 0.4939924  0.84039451]. \t  3.6850626468682974 \t 3.807181352426551\n",
            "48     \t [0.96838512 0.66142807 0.00765085]. \t  0.0023498468654898582 \t 3.807181352426551\n",
            "49     \t [0.67561109 0.5865782  0.82393387]. \t  3.6435926212984096 \t 3.807181352426551\n",
            "50     \t [0.54712854 0.53652095 0.86499489]. \t  \u001b[92m3.810949235451461\u001b[0m \t 3.810949235451461\n",
            "51     \t [0.61940137 0.58288993 0.85798217]. \t  3.7811266917900666 \t 3.810949235451461\n",
            "52     \t [0.7278839  0.59730401 0.83369632]. \t  3.6460408404160263 \t 3.810949235451461\n",
            "53     \t [0.00353274 0.5490535  0.84900383]. \t  3.809578189645349 \t 3.810949235451461\n",
            "54     \t [0.72718594 0.55950215 0.82801563]. \t  3.6970431013441125 \t 3.810949235451461\n",
            "55     \t [0.63718031 0.59047105 0.84627657]. \t  3.744890206279476 \t 3.810949235451461\n",
            "56     \t [0.11486888 0.58061736 0.88393872]. \t  3.7278945411623896 \t 3.810949235451461\n",
            "57     \t [0.43716771 0.56761097 0.84334066]. \t  \u001b[92m3.8324085998930824\u001b[0m \t 3.8324085998930824\n",
            "58     \t [0.02886444 0.54303362 0.84591433]. \t  3.8114128628187203 \t 3.8324085998930824\n",
            "59     \t [0.26930254 0.62650913 0.87115116]. \t  3.6599982023762028 \t 3.8324085998930824\n",
            "60     \t [0.21666309 0.56097286 0.84792437]. \t  \u001b[92m3.8546717904178536\u001b[0m \t 3.8546717904178536\n",
            "61     \t [0.13795406 0.57666117 0.81167866]. \t  3.683676595833264 \t 3.8546717904178536\n",
            "62     \t [0.75486122 0.49483091 0.86494863]. \t  3.6498061460416165 \t 3.8546717904178536\n",
            "63     \t [0.55203254 0.50446802 0.86424908]. \t  3.736353562002889 \t 3.8546717904178536\n",
            "64     \t [0.14448146 0.54374792 0.85951187]. \t  3.838195885923537 \t 3.8546717904178536\n",
            "65     \t [0.35438514 0.56209063 0.84625159]. \t  3.8523875348847474 \t 3.8546717904178536\n",
            "66     \t [0.78859085 0.51374215 0.85434311]. \t  3.7133049672386838 \t 3.8546717904178536\n",
            "67     \t [0.08226152 0.56855097 0.8359652 ]. \t  3.803524493201402 \t 3.8546717904178536\n",
            "68     \t [0.39491499 0.56397357 0.84322694]. \t  3.841675347530251 \t 3.8546717904178536\n",
            "69     \t [0.2377863  0.56616633 0.85128193]. \t  \u001b[92m3.855130855141171\u001b[0m \t 3.855130855141171\n",
            "70     \t [0.34462105 0.53994175 0.86581474]. \t  3.835908803609491 \t 3.855130855141171\n",
            "71     \t [0.17103082 0.54548333 0.8511846 ]. \t  3.8492289955847547 \t 3.855130855141171\n",
            "72     \t [0.60949432 0.57545537 0.83868024]. \t  3.765749236890471 \t 3.855130855141171\n",
            "73     \t [0.2722175  0.53618736 0.8320273 ]. \t  3.810849937761577 \t 3.855130855141171\n",
            "74     \t [0.37209426 0.5429103  0.90198913]. \t  3.6139406933838685 \t 3.855130855141171\n",
            "75     \t [0.71549866 0.50605584 0.82322694]. \t  3.6274475387801277 \t 3.855130855141171\n",
            "76     \t [0.0740286  0.54526975 0.87613399]. \t  3.772729815788237 \t 3.855130855141171\n",
            "77     \t [0.04239457 0.51352406 0.83837758]. \t  3.7497393111107886 \t 3.855130855141171\n",
            "78     \t [0.31905212 0.53997738 0.8958157 ]. \t  3.666771169651225 \t 3.855130855141171\n",
            "79     \t [0.21267988 0.56704595 0.82835256]. \t  3.7972811197769056 \t 3.855130855141171\n",
            "80     \t [0.68936741 0.52373295 0.88161947]. \t  3.691735517376497 \t 3.855130855141171\n",
            "81     \t [0.31875601 0.53478931 0.86653553]. \t  3.82749501822794 \t 3.855130855141171\n",
            "82     \t [0.62949945 0.54667522 0.86157449]. \t  3.8067586634530994 \t 3.855130855141171\n",
            "83     \t [0.45094234 0.5592204  0.79823549]. \t  3.564270725212454 \t 3.855130855141171\n",
            "84     \t [0.07572605 0.57707146 0.82997371]. \t  3.769909484770413 \t 3.855130855141171\n",
            "85     \t [0.18627296 0.55046063 0.83011921]. \t  3.808191794064511 \t 3.855130855141171\n",
            "86     \t [0.28671347 0.52451552 0.84196564]. \t  3.820461647820789 \t 3.855130855141171\n",
            "87     \t [0.45458558 0.55732375 0.86320295]. \t  3.840820160074265 \t 3.855130855141171\n",
            "88     \t [0.54716635 0.53896991 0.86557054]. \t  3.8124831432712556 \t 3.855130855141171\n",
            "89     \t [0.1575673  0.52013075 0.83031895]. \t  3.767760526255228 \t 3.855130855141171\n",
            "90     \t [0.67080602 0.46777955 0.84486321]. \t  3.559281917806858 \t 3.855130855141171\n",
            "91     \t [0.53765772 0.55259465 0.85440826]. \t  3.8341023124015146 \t 3.855130855141171\n",
            "92     \t [0.2058539  0.58969559 0.82907726]. \t  3.7627655429835833 \t 3.855130855141171\n",
            "93     \t [0.42107909 0.53067711 0.87058366]. \t  3.803486650815236 \t 3.855130855141171\n",
            "94     \t [0.34849882 0.59629712 0.89851465]. \t  3.609177655256145 \t 3.855130855141171\n",
            "95     \t [0.48904417 0.57651288 0.85369525]. \t  3.8242154868625153 \t 3.855130855141171\n",
            "96     \t [0.16249921 0.60775603 0.85821565]. \t  3.7554653078705575 \t 3.855130855141171\n",
            "97     \t [0.14612594 0.52361832 0.83348779]. \t  3.7845163446748114 \t 3.855130855141171\n",
            "98     \t [0.26398144 0.54654488 0.89585008]. \t  3.6713338503903223 \t 3.855130855141171\n",
            "99     \t [0.71115145 0.51880666 0.85869989]. \t  3.7485512563717407 \t 3.855130855141171\n",
            "100    \t [0.3639437  0.49296533 0.84844836]. \t  3.7294242837998492 \t 3.855130855141171\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9NdFRXtPuvsP",
        "outputId": "d06e0e43-f918-48d7-a44a-fd4323b009c5"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 14 \r\n",
        "\r\n",
        "np.random.seed(run_num_14)\r\n",
        "surrogate_winner_14 = dtStudentProcess(d_cov_func, nu = df)\r\n",
        "\r\n",
        "winner_14 = dGPGO_stp(surrogate_winner_14, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_14.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.64755105 0.50714969 0.52834138]. \t  0.5963212059988954 \t 2.610000357863649\n",
            "init   \t [0.8962852  0.69999119 0.7142971 ]. \t  1.7197848290620104 \t 2.610000357863649\n",
            "init   \t [0.71733838 0.22281946 0.17515452]. \t  0.48166052848103497 \t 2.610000357863649\n",
            "init   \t [0.45684149 0.92873843 0.00988589]. \t  0.0004588015757462679 \t 2.610000357863649\n",
            "init   \t [0.08992219 0.85020027 0.48562106]. \t  2.610000357863649 \t 2.610000357863649\n",
            "1      \t [0.25116351 1.         1.        ]. \t  0.3342674438919039 \t 2.610000357863649\n",
            "2      \t [2.56309830e-15 1.67813454e-15 2.51391463e-15]. \t  0.06797411659013565 \t 2.610000357863649\n",
            "3      \t [0.05593593 0.92782293 0.12748656]. \t  0.012672955321826789 \t 2.610000357863649\n",
            "4      \t [0.95952385 0.93006517 0.49119923]. \t  0.30873207474460657 \t 2.610000357863649\n",
            "5      \t [0.02359998 0.43118547 0.86765735]. \t  \u001b[92m3.2820236967203504\u001b[0m \t 3.2820236967203504\n",
            "6      \t [0.85439944 0.22164859 0.97546702]. \t  0.7868381068897362 \t 3.2820236967203504\n",
            "7      \t [1. 1. 1.]. \t  0.3168836207041991 \t 3.2820236967203504\n",
            "8      \t [0.95459697 0.64959917 0.01973276]. \t  0.003370989979419414 \t 3.2820236967203504\n",
            "9      \t [0.0695667  0.74300789 0.70099192]. \t  2.5104373261954422 \t 3.2820236967203504\n",
            "10     \t [0.07652137 0.05994366 0.73743115]. \t  0.3904159524914682 \t 3.2820236967203504\n",
            "11     \t [0.01890103 0.3976229  0.95418299]. \t  2.1938238611786636 \t 3.2820236967203504\n",
            "12     \t [0.98525594 0.01392002 0.61492877]. \t  0.12310742142191286 \t 3.2820236967203504\n",
            "13     \t [0.00666272 0.25069271 0.43751302]. \t  0.30645503300254684 \t 3.2820236967203504\n",
            "14     \t [0.80116048 0.0240518  0.04793146]. \t  0.12361098927616539 \t 3.2820236967203504\n",
            "15     \t [0.2875125  0.99999    0.44199543]. \t  1.5888397545622637 \t 3.2820236967203504\n",
            "16     \t [0.37271771 0.35811434 0.00244768]. \t  0.06818435932504385 \t 3.2820236967203504\n",
            "17     \t [0.98528618 0.53752133 0.68445238]. \t  1.7512208833960212 \t 3.2820236967203504\n",
            "18     \t [0.00253235 0.94578515 0.53593367]. \t  2.7525142736706707 \t 3.2820236967203504\n",
            "19     \t [0.00548917 0.48757827 0.69224509]. \t  2.2266009843474586 \t 3.2820236967203504\n",
            "20     \t [0.36028116 0.0027522  0.16222424]. \t  0.6300511795386963 \t 3.2820236967203504\n",
            "21     \t [0.36214764 0.49508642 0.97420567]. \t  2.436937436063722 \t 3.2820236967203504\n",
            "22     \t [0.9766765  0.49872696 0.92787128]. \t  3.0769973779643953 \t 3.2820236967203504\n",
            "23     \t [0.19871041 0.5049178  0.77519831]. \t  \u001b[92m3.3111470475946843\u001b[0m \t 3.3111470475946843\n",
            "24     \t [0.82412032 0.63412567 0.97976738]. \t  2.3015288100618085 \t 3.3111470475946843\n",
            "25     \t [0.99604397 0.75140738 0.92382502]. \t  2.273882525180712 \t 3.3111470475946843\n",
            "26     \t [0.56510913 0.69529888 0.13907009]. \t  0.02577418659148989 \t 3.3111470475946843\n",
            "27     \t [0.83061968 0.35436511 0.84979341]. \t  2.626224557206659 \t 3.3111470475946843\n",
            "28     \t [0.10669258 0.99151665 0.49429856]. \t  2.346550660445885 \t 3.3111470475946843\n",
            "29     \t [0.25667675 0.24888519 0.78770493]. \t  1.6051189045359937 \t 3.3111470475946843\n",
            "30     \t [0.97174717 0.91931037 0.07123248]. \t  0.0004536356146057022 \t 3.3111470475946843\n",
            "31     \t [0.39660742 0.58387269 0.87390921]. \t  \u001b[92m3.788683712335529\u001b[0m \t 3.788683712335529\n",
            "32     \t [0.4695558  0.79510968 0.86047394]. \t  2.2600123155000955 \t 3.788683712335529\n",
            "33     \t [0.30572222 0.61346314 0.85193119]. \t  3.742099343815536 \t 3.788683712335529\n",
            "34     \t [0.49208105 0.53372339 0.84250279]. \t  \u001b[92m3.8177491867498174\u001b[0m \t 3.8177491867498174\n",
            "35     \t [0.50465642 0.51478251 0.84238877]. \t  3.779482655093178 \t 3.8177491867498174\n",
            "36     \t [0.50351658 0.50053329 0.84522877]. \t  3.7412828108242495 \t 3.8177491867498174\n",
            "37     \t [0.51282755 0.51685489 0.82031252]. \t  3.6933498284260287 \t 3.8177491867498174\n",
            "38     \t [0.53443502 0.52318607 0.81999395]. \t  3.697952843148424 \t 3.8177491867498174\n",
            "39     \t [0.99083013 0.32223447 0.84712035]. \t  2.2811798820046 \t 3.8177491867498174\n",
            "40     \t [0.28633976 0.66606413 0.81504458]. \t  3.3418968433453946 \t 3.8177491867498174\n",
            "41     \t [0.99824301 0.0517753  0.19672696]. \t  0.25154756338580997 \t 3.8177491867498174\n",
            "42     \t [0.45976738 0.44619901 0.81428277]. \t  3.3707826263232015 \t 3.8177491867498174\n",
            "43     \t [0.48200109 0.62917962 0.84300631]. \t  3.629325657265867 \t 3.8177491867498174\n",
            "44     \t [0.41284237 0.62561895 0.89811227]. \t  3.5072327446647256 \t 3.8177491867498174\n",
            "45     \t [0.33421936 0.48937789 0.77946801]. \t  3.3034549842167067 \t 3.8177491867498174\n",
            "46     \t [0.58366769 0.58697916 0.91691618]. \t  3.4180589928433815 \t 3.8177491867498174\n",
            "47     \t [0.47653156 0.48498034 0.87682501]. \t  3.6191104813688106 \t 3.8177491867498174\n",
            "48     \t [0.45496638 0.49036459 0.83690933]. \t  3.693718672301491 \t 3.8177491867498174\n",
            "49     \t [0.41279524 0.54997278 0.77463984]. \t  3.331588832827835 \t 3.8177491867498174\n",
            "50     \t [0.33126451 0.5228497  0.77344274]. \t  3.31942465291854 \t 3.8177491867498174\n",
            "51     \t [0.56078263 0.55378067 0.84058232]. \t  3.8077156747551255 \t 3.8177491867498174\n",
            "52     \t [0.55329568 0.63472467 0.85244609]. \t  3.5976166393084066 \t 3.8177491867498174\n",
            "53     \t [0.46357161 0.50992248 0.75956185]. \t  3.093638706916895 \t 3.8177491867498174\n",
            "54     \t [0.38986295 0.52899791 0.81242769]. \t  3.6863635906832815 \t 3.8177491867498174\n",
            "55     \t [0.50838164 0.55489979 0.8483617 ]. \t  \u001b[92m3.8358527854838926\u001b[0m \t 3.8358527854838926\n",
            "56     \t [0.60819018 0.54642738 0.8069941 ]. \t  3.595990513596801 \t 3.8358527854838926\n",
            "57     \t [0.45722529 0.61733784 0.82283267]. \t  3.606682167011595 \t 3.8358527854838926\n",
            "58     \t [0.44660623 0.59127942 0.79722123]. \t  3.50355846561738 \t 3.8358527854838926\n",
            "59     \t [0.44822414 0.63257401 0.88971241]. \t  3.5316279287265844 \t 3.8358527854838926\n",
            "60     \t [0.50101441 0.54323325 0.84493525]. \t  3.829651663142151 \t 3.8358527854838926\n",
            "61     \t [0.38508672 0.52065936 0.88729083]. \t  3.6950105899075676 \t 3.8358527854838926\n",
            "62     \t [0.55469864 0.54897745 0.85721751]. \t  3.829366782964977 \t 3.8358527854838926\n",
            "63     \t [0.46269481 0.55776063 0.8927635 ]. \t  3.6978466257368146 \t 3.8358527854838926\n",
            "64     \t [0.5715699  0.51593108 0.88840278]. \t  3.654746910651372 \t 3.8358527854838926\n",
            "65     \t [0.44354699 0.53893234 0.83841434]. \t  3.8212528769565957 \t 3.8358527854838926\n",
            "66     \t [0.47594698 0.57926026 0.80768011]. \t  3.613066791665288 \t 3.8358527854838926\n",
            "67     \t [0.53487074 0.6193715  0.8917969 ]. \t  3.5650995816739344 \t 3.8358527854838926\n",
            "68     \t [0.42624913 0.65698891 0.8413997 ]. \t  3.4773825240005642 \t 3.8358527854838926\n",
            "69     \t [0.24907094 0.6030287  0.88781508]. \t  3.6696272680487736 \t 3.8358527854838926\n",
            "70     \t [0.5376927  0.53833243 0.87901367]. \t  3.763710914154632 \t 3.8358527854838926\n",
            "71     \t [0.68127102 0.53148825 0.87833292]. \t  3.7259410531100765 \t 3.8358527854838926\n",
            "72     \t [0.38493682 0.54553001 0.8293334 ]. \t  3.8002342540313494 \t 3.8358527854838926\n",
            "73     \t [0.48589792 0.49934155 0.80402128]. \t  3.5391447659837207 \t 3.8358527854838926\n",
            "74     \t [0.62515085 0.48929445 0.81303064]. \t  3.5391582881710772 \t 3.8358527854838926\n",
            "75     \t [0.45506725 0.49871699 0.86237276]. \t  3.7334216244740768 \t 3.8358527854838926\n",
            "76     \t [0.62685663 0.51753644 0.88366382]. \t  3.679616845301837 \t 3.8358527854838926\n",
            "77     \t [0.1210697  0.61982659 0.91000782]. \t  3.406296017219336 \t 3.8358527854838926\n",
            "78     \t [0.08402846 0.71125903 0.83018841]. \t  3.064449642784458 \t 3.8358527854838926\n",
            "79     \t [0.35891441 0.62312404 0.89028172]. \t  3.5795366516268494 \t 3.8358527854838926\n",
            "80     \t [0.71116253 0.50817909 0.9124865 ]. \t  3.3795338815414753 \t 3.8358527854838926\n",
            "81     \t [0.28747049 0.6318224  0.80671528]. \t  3.4794408996579995 \t 3.8358527854838926\n",
            "82     \t [0.59398141 0.58524397 0.81988854]. \t  3.651142935339099 \t 3.8358527854838926\n",
            "83     \t [0.42943351 0.68024654 0.90695442]. \t  3.111910087822498 \t 3.8358527854838926\n",
            "84     \t [0.41121517 0.56393779 0.80639645]. \t  3.6430346049690847 \t 3.8358527854838926\n",
            "85     \t [0.63870131 0.47938099 0.87346065]. \t  3.581565175723094 \t 3.8358527854838926\n",
            "86     \t [0.47767675 0.58035015 0.79405771]. \t  3.4863598359039996 \t 3.8358527854838926\n",
            "87     \t [0.2491679  0.56829007 0.86205731]. \t  \u001b[92m3.8463719539915955\u001b[0m \t 3.8463719539915955\n",
            "88     \t [0.27822126 0.59208323 0.85911403]. \t  3.810695735479138 \t 3.8463719539915955\n",
            "89     \t [0.58636834 0.4497918  0.79863066]. \t  3.262298872859489 \t 3.8463719539915955\n",
            "90     \t [0.14941355 0.52339563 0.84834833]. \t  3.813038856550463 \t 3.8463719539915955\n",
            "91     \t [0.42561506 0.54511276 0.81686928]. \t  3.7238911163332777 \t 3.8463719539915955\n",
            "92     \t [0.58915779 0.61827511 0.85944557]. \t  3.67202858198469 \t 3.8463719539915955\n",
            "93     \t [0.5565746  0.5663825  0.81790186]. \t  3.6858712880419424 \t 3.8463719539915955\n",
            "94     \t [0.53823229 0.58285806 0.78559631]. \t  3.366631092584579 \t 3.8463719539915955\n",
            "95     \t [0.59996432 0.4988854  0.8278061 ]. \t  3.6640349961844114 \t 3.8463719539915955\n",
            "96     \t [0.08611753 0.63182653 0.90385526]. \t  3.4091460584705024 \t 3.8463719539915955\n",
            "97     \t [0.3252372  0.52599685 0.84822982]. \t  3.83131094218598 \t 3.8463719539915955\n",
            "98     \t [0.54013694 0.55060387 0.7634288 ]. \t  3.1401959393499324 \t 3.8463719539915955\n",
            "99     \t [0.40615135 0.61296527 0.89524144]. \t  3.5816332706834864 \t 3.8463719539915955\n",
            "100    \t [0.65785427 0.50215243 0.83164268]. \t  3.675231729604789 \t 3.8463719539915955\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d86panpOuvum",
        "outputId": "4d467932-0326-4450-971d-2ab587fa24bf"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 15 \r\n",
        "\r\n",
        "np.random.seed(run_num_15)\r\n",
        "surrogate_winner_15 = dtStudentProcess(d_cov_func, nu = df)\r\n",
        "\r\n",
        "winner_15 = dGPGO_stp(surrogate_winner_15, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_15.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.57788186 0.25550133 0.25252687]. \t  0.7217338409961529 \t 1.540625560354162\n",
            "init   \t [0.70990435 0.44755236 0.22694296]. \t  0.23199690080875204 \t 1.540625560354162\n",
            "init   \t [0.40170957 0.88225774 0.43721347]. \t  1.540625560354162 \t 1.540625560354162\n",
            "init   \t [0.87842518 0.78052775 0.53421429]. \t  0.5653855252190516 \t 1.540625560354162\n",
            "init   \t [0.01173301 0.26575648 0.3311941 ]. \t  0.5019682819961726 \t 1.540625560354162\n",
            "1      \t [0.45870705 0.97826428 0.97993811]. \t  0.46749494069699443 \t 1.540625560354162\n",
            "2      \t [0.61363169 0.18622958 0.87189166]. \t  1.0703600264488446 \t 1.540625560354162\n",
            "3      \t [0.02973734 0.87604562 0.30017952]. \t  0.42389001967224293 \t 1.540625560354162\n",
            "4      \t [0.23088942 0.01569806 0.70806062]. \t  0.2408675136196685 \t 1.540625560354162\n",
            "5      \t [0. 0. 0.]. \t  0.06797411659013229 \t 1.540625560354162\n",
            "6      \t [0.45428525 0.55430998 0.64975248]. \t  \u001b[92m1.8106275929872717\u001b[0m \t 1.8106275929872717\n",
            "7      \t [0.40336825 0.56666324 0.51136198]. \t  1.0480675528568812 \t 1.8106275929872717\n",
            "8      \t [0.86486807 0.09954537 0.01253691]. \t  0.06800843173916662 \t 1.8106275929872717\n",
            "9      \t [0.96729669 0.01805335 0.42207395]. \t  0.15612185887703656 \t 1.8106275929872717\n",
            "10     \t [0.04241685 0.57427846 0.86643881]. \t  \u001b[92m3.794734222480046\u001b[0m \t 3.794734222480046\n",
            "11     \t [0.02606356 0.81415336 0.86896079]. \t  2.0830929571977084 \t 3.794734222480046\n",
            "12     \t [0.50456537 0.96872057 0.06889843]. \t  0.0016471827755629707 \t 3.794734222480046\n",
            "13     \t [0.15330848 0.45967043 0.93516617]. \t  2.8855711761089777 \t 3.794734222480046\n",
            "14     \t [0.78828738 0.70854775 0.97963349]. \t  1.981807888396912 \t 3.794734222480046\n",
            "15     \t [5.88172875e-12 4.10510897e-02 8.99203805e-01]. \t  0.2893129338696584 \t 3.794734222480046\n",
            "16     \t [0.00115517 0.46339706 0.63232021]. \t  1.5012642582306461 \t 3.794734222480046\n",
            "17     \t [0.02069974 0.54473709 0.97905639]. \t  2.428119806769981 \t 3.794734222480046\n",
            "18     \t [0.31648677 0.22179841 0.00795279]. \t  0.11814330809230826 \t 3.794734222480046\n",
            "19     \t [0.9502359  0.12856656 0.93192707]. \t  0.525852056534787 \t 3.794734222480046\n",
            "20     \t [0.99832916 0.99763281 0.96919066]. \t  0.405733798612385 \t 3.794734222480046\n",
            "21     \t [0.0275235  0.53099556 0.06581822]. \t  0.038277618041477134 \t 3.794734222480046\n",
            "22     \t [0.96231802 0.92590781 0.06258099]. \t  0.00037212675080182183 \t 3.794734222480046\n",
            "23     \t [0.29341445 0.74549957 0.82015381]. \t  2.7440258497452454 \t 3.794734222480046\n",
            "24     \t [0.98706971 0.46131778 0.29691262]. \t  0.10298696188741949 \t 3.794734222480046\n",
            "25     \t [0.73503528 1.         0.85891383]. \t  0.5878474478387443 \t 3.794734222480046\n",
            "26     \t [0.03832024 0.99118353 0.06993527]. \t  0.0023379782679471516 \t 3.794734222480046\n",
            "27     \t [0.93902116 0.98300982 0.40163048]. \t  0.16799325736308818 \t 3.794734222480046\n",
            "28     \t [0.91282357 0.40114937 0.92610915]. \t  2.5367223436282753 \t 3.794734222480046\n",
            "29     \t [0.15078069 0.68752581 0.83583848]. \t  3.274666270210281 \t 3.794734222480046\n",
            "30     \t [0.02709807 0.65161177 0.8208225 ]. \t  3.435282285959145 \t 3.794734222480046\n",
            "31     \t [0.08009088 0.40086646 0.87027048]. \t  3.0352615420068654 \t 3.794734222480046\n",
            "32     \t [0.09283606 0.75937049 0.77778068]. \t  2.532627244174635 \t 3.794734222480046\n",
            "33     \t [0.05511272 0.6331861  0.79495542]. \t  3.37586611797502 \t 3.794734222480046\n",
            "34     \t [0.09581381 0.68056366 0.8464236 ]. \t  3.3343297551070563 \t 3.794734222480046\n",
            "35     \t [0.89083065 0.53777201 0.88013396]. \t  3.6569951792480837 \t 3.794734222480046\n",
            "36     \t [0.99803458 0.59461844 0.85557812]. \t  3.6034985586388446 \t 3.794734222480046\n",
            "37     \t [0.99903874 0.31477372 0.73342077]. \t  1.7185658135170376 \t 3.794734222480046\n",
            "38     \t [0.00726023 0.56452066 0.85872284]. \t  \u001b[92m3.8070936830420665\u001b[0m \t 3.8070936830420665\n",
            "39     \t [0.19262466 0.53550011 0.89645248]. \t  3.6450235178470405 \t 3.8070936830420665\n",
            "40     \t [0.12506523 0.65453934 0.81127786]. \t  3.3941285082748776 \t 3.8070936830420665\n",
            "41     \t [0.94387458 0.57437965 0.85969073]. \t  3.677929895594283 \t 3.8070936830420665\n",
            "42     \t [0.27604951 0.00172387 0.17270767]. \t  0.6523647529935233 \t 3.8070936830420665\n",
            "43     \t [0.14352415 0.6248338  0.86020607]. \t  3.681098306042842 \t 3.8070936830420665\n",
            "44     \t [0.74641919 0.59413886 0.91045895]. \t  3.438299832487469 \t 3.8070936830420665\n",
            "45     \t [0.97185909 0.51494552 0.809043  ]. \t  3.4437245140234114 \t 3.8070936830420665\n",
            "46     \t [0.87741965 0.52813993 0.77282651]. \t  3.103154566961942 \t 3.8070936830420665\n",
            "47     \t [0.5372194  0.50889033 0.85462277]. \t  3.7668389890770153 \t 3.8070936830420665\n",
            "48     \t [0.4686338  0.59602153 0.81653159]. \t  3.6408742923456883 \t 3.8070936830420665\n",
            "49     \t [0.28138774 0.65520074 0.89104074]. \t  3.4064835405661165 \t 3.8070936830420665\n",
            "50     \t [0.39808317 0.58473696 0.87071514]. \t  3.7980590078017955 \t 3.8070936830420665\n",
            "51     \t [0.1472674  0.98972022 0.3078402 ]. \t  0.4195452465766425 \t 3.8070936830420665\n",
            "52     \t [0.32309444 0.53452377 0.90314782]. \t  3.590482170858553 \t 3.8070936830420665\n",
            "53     \t [0.60761974 0.54853422 0.92598776]. \t  3.3165551449810304 \t 3.8070936830420665\n",
            "54     \t [0.70467028 0.58529209 0.8618072 ]. \t  3.7479889032826903 \t 3.8070936830420665\n",
            "55     \t [0.6475716  0.56123357 0.8738434 ]. \t  3.771593754565109 \t 3.8070936830420665\n",
            "56     \t [0.56629941 0.53770403 0.89575589]. \t  3.645057147841716 \t 3.8070936830420665\n",
            "57     \t [0.54134615 0.55111158 0.84553353]. \t  \u001b[92m3.8245307409040867\u001b[0m \t 3.8245307409040867\n",
            "58     \t [0.05781196 0.551295   0.82573539]. \t  3.76389243224169 \t 3.8245307409040867\n",
            "59     \t [0.96798107 0.58848768 0.86128692]. \t  3.6366721712767007 \t 3.8245307409040867\n",
            "60     \t [0.59383933 0.68972853 0.84874547]. \t  3.2004845071096732 \t 3.8245307409040867\n",
            "61     \t [0.41541258 0.63862868 0.82726934]. \t  3.53841171844108 \t 3.8245307409040867\n",
            "62     \t [0.68446677 0.51279333 0.81229842]. \t  3.580839860297626 \t 3.8245307409040867\n",
            "63     \t [0.21906173 0.55249405 0.85174238]. \t  \u001b[92m3.8579257205959188\u001b[0m \t 3.8579257205959188\n",
            "64     \t [0.44371013 0.56619009 0.89048092]. \t  3.7145971595194816 \t 3.8579257205959188\n",
            "65     \t [0.50210634 0.60926039 0.86352875]. \t  3.7272121535147305 \t 3.8579257205959188\n",
            "66     \t [0.33177714 0.55356155 0.91439503]. \t  3.488977311645044 \t 3.8579257205959188\n",
            "67     \t [0.4776729  0.63141189 0.83429465]. \t  3.590361715172583 \t 3.8579257205959188\n",
            "68     \t [4.05513818e-04 1.14259023e-02 4.31560977e-01]. \t  0.2719460142554362 \t 3.8579257205959188\n",
            "69     \t [0.3201446  0.5908342  0.85144838]. \t  3.814875201979761 \t 3.8579257205959188\n",
            "70     \t [0.28032826 0.57527832 0.86296312]. \t  3.838106071072329 \t 3.8579257205959188\n",
            "71     \t [0.05642039 0.63382161 0.88610023]. \t  3.5268189009529167 \t 3.8579257205959188\n",
            "72     \t [0.29494063 0.61369802 0.87519607]. \t  3.700815701671898 \t 3.8579257205959188\n",
            "73     \t [0.36628882 0.62360585 0.84029504]. \t  3.6738708764145627 \t 3.8579257205959188\n",
            "74     \t [0.46888243 0.58452035 0.85491183]. \t  3.8130122330966314 \t 3.8579257205959188\n",
            "75     \t [0.4871254  0.52984346 0.82683505]. \t  3.7571558209719513 \t 3.8579257205959188\n",
            "76     \t [0.17915104 0.56889618 0.85508269]. \t  3.8469985935468345 \t 3.8579257205959188\n",
            "77     \t [0.99265024 0.6230631  0.84316818]. \t  3.463104194208259 \t 3.8579257205959188\n",
            "78     \t [0.5112043  0.58251777 0.83922448]. \t  3.781534659367854 \t 3.8579257205959188\n",
            "79     \t [0.23971741 0.57924803 0.87058798]. \t  3.8104223088830134 \t 3.8579257205959188\n",
            "80     \t [0.36969825 0.62193064 0.88236617]. \t  3.631351212969145 \t 3.8579257205959188\n",
            "81     \t [0.00223087 0.62100847 0.84302958]. \t  3.6566365583229445 \t 3.8579257205959188\n",
            "82     \t [0.07960634 0.54178004 0.8414267 ]. \t  3.817773291246787 \t 3.8579257205959188\n",
            "83     \t [0.86849217 0.5815052  0.86750985]. \t  3.6892206450215186 \t 3.8579257205959188\n",
            "84     \t [0.45554368 0.56453581 0.87103353]. \t  3.817640020246683 \t 3.8579257205959188\n",
            "85     \t [0.31987897 0.57677953 0.8572652 ]. \t  3.8429444041621084 \t 3.8579257205959188\n",
            "86     \t [0.56965328 0.53517017 0.84099421]. \t  3.7989218302072594 \t 3.8579257205959188\n",
            "87     \t [0.54219151 0.57182538 0.82481149]. \t  3.7271150611425483 \t 3.8579257205959188\n",
            "88     \t [0.19333838 0.52943567 0.84441377]. \t  3.8275476279235434 \t 3.8579257205959188\n",
            "89     \t [0.22523989 0.46307685 0.86479607]. \t  3.5485143101141867 \t 3.8579257205959188\n",
            "90     \t [0.14392481 0.55259473 0.8598797 ]. \t  3.842481915551602 \t 3.8579257205959188\n",
            "91     \t [0.44238414 0.61678217 0.83564966]. \t  3.674435388391718 \t 3.8579257205959188\n",
            "92     \t [0.66882374 0.62941179 0.81375748]. \t  3.402796766106826 \t 3.8579257205959188\n",
            "93     \t [0.35598689 0.58590171 0.85961466]. \t  3.8222957128095865 \t 3.8579257205959188\n",
            "94     \t [0.27914867 0.56880976 0.88635103]. \t  3.746064445618946 \t 3.8579257205959188\n",
            "95     \t [0.18900847 0.54422556 0.87385392]. \t  3.8040838966988013 \t 3.8579257205959188\n",
            "96     \t [0.00722489 0.51789397 0.82600296]. \t  3.7075680855585524 \t 3.8579257205959188\n",
            "97     \t [0.02767127 0.54582882 0.8676113 ]. \t  3.79351208036115 \t 3.8579257205959188\n",
            "98     \t [0.72665915 0.48520817 0.85434783]. \t  3.6328765874134885 \t 3.8579257205959188\n",
            "99     \t [0.61987901 0.50286908 0.85537619]. \t  3.729787091818289 \t 3.8579257205959188\n",
            "100    \t [0.7484816  0.568986   0.88041113]. \t  3.708972976074263 \t 3.8579257205959188\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "any0xrgYuvxA",
        "outputId": "cca7e706-347d-4a37-f11d-3d21f8fd15c8"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 16 \r\n",
        "\r\n",
        "np.random.seed(run_num_16)\r\n",
        "surrogate_winner_16 = dtStudentProcess(d_cov_func, nu = df)\r\n",
        "\r\n",
        "winner_16 = dGPGO_stp(surrogate_winner_16, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_16.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.86394833 0.316366   0.67215078]. \t  1.1947633822631252 \t 3.8084053754826726\n",
            "init   \t [0.50791167 0.78166043 0.28368168]. \t  0.20052765717741963 \t 3.8084053754826726\n",
            "init   \t [0.23370878 0.56379969 0.87502436]. \t  3.8084053754826726 \t 3.8084053754826726\n",
            "init   \t [0.71894292 0.18213174 0.24380041]. \t  0.6532826728302027 \t 3.8084053754826726\n",
            "init   \t [0.08605673 0.53424539 0.83969965]. \t  3.8072565344663847 \t 3.8084053754826726\n",
            "1      \t [0.26431958 0.04731623 0.9519732 ]. \t  0.22754937654248866 \t 3.8084053754826726\n",
            "2      \t [0.43751288 0.99716585 0.9184722 ]. \t  0.5658696288532565 \t 3.8084053754826726\n",
            "3      \t [0.01538211 0.38798846 0.        ]. \t  0.03869488301260809 \t 3.8084053754826726\n",
            "4      \t [0.06501847 0.73100853 0.95963938]. \t  2.1366485370200565 \t 3.8084053754826726\n",
            "5      \t [0.24509488 0.41118968 0.4793939 ]. \t  0.49233333986793765 \t 3.8084053754826726\n",
            "6      \t [1. 1. 1.]. \t  0.3168836207046828 \t 3.8084053754826726\n",
            "7      \t [0.9420988  0.81466355 0.01029373]. \t  0.0004455515635946987 \t 3.8084053754826726\n",
            "8      \t [0.00969798 0.26850713 0.85801565]. \t  1.7886650539287932 \t 3.8084053754826726\n",
            "9      \t [0.97684885 0.16625467 0.10343961]. \t  0.14391422782213323 \t 3.8084053754826726\n",
            "10     \t [0.92273034 0.11962526 0.99238758]. \t  0.3098794296166971 \t 3.8084053754826726\n",
            "11     \t [0.88017221 0.96252011 0.48521611]. \t  0.412375387348048 \t 3.8084053754826726\n",
            "12     \t [0.5908906  0.31875476 0.90632621]. \t  2.054831274750908 \t 3.8084053754826726\n",
            "13     \t [0.05790357 0.97831053 0.0286539 ]. \t  0.0007424991674577035 \t 3.8084053754826726\n",
            "14     \t [0.28721841 0.62812964 0.76267312]. \t  3.111287439882954 \t 3.8084053754826726\n",
            "15     \t [0.14121568 0.39698074 0.9027641 ]. \t  2.7983101233010212 \t 3.8084053754826726\n",
            "16     \t [0.87041744 0.00477875 0.62333708]. \t  0.12625234886042935 \t 3.8084053754826726\n",
            "17     \t [0.7217923  0.69593641 1.        ]. \t  1.7541232150812196 \t 3.8084053754826726\n",
            "18     \t [3.52543048e-11 1.86706618e-11 4.67060204e-12]. \t  0.06797411660349793 \t 3.8084053754826726\n",
            "19     \t [0.5896618  0.04833873 0.01014623]. \t  0.113359520110876 \t 3.8084053754826726\n",
            "20     \t [0.00906456 0.62832628 0.58966033]. \t  2.0391866047901805 \t 3.8084053754826726\n",
            "21     \t [0.08809203 0.55699565 0.78663036]. \t  3.481208067261094 \t 3.8084053754826726\n",
            "22     \t [0.30000853 0.62087246 0.91771824]. \t  3.335445843329138 \t 3.8084053754826726\n",
            "23     \t [0.03940306 0.54296476 0.84926213]. \t  \u001b[92m3.817404427872387\u001b[0m \t 3.817404427872387\n",
            "24     \t [0.03125578 0.55220064 0.86148998]. \t  3.8126159779597892 \t 3.817404427872387\n",
            "25     \t [0.0417258  0.61890468 0.77434079]. \t  3.2486372732797646 \t 3.817404427872387\n",
            "26     \t [0.82871384 0.45189561 0.07667343]. \t  0.05819015526481757 \t 3.817404427872387\n",
            "27     \t [0.41718313 0.9947402  0.00295114]. \t  0.0002653942709024769 \t 3.817404427872387\n",
            "28     \t [0.16850752 0.5746383  0.93601311]. \t  3.183628649466312 \t 3.817404427872387\n",
            "29     \t [0.41089285 0.55838164 0.83017599]. \t  3.799705455932137 \t 3.817404427872387\n",
            "30     \t [0.6586057  0.59357028 0.68994361]. \t  2.020969034789335 \t 3.817404427872387\n",
            "31     \t [0.38982813 0.55892185 0.8928207 ]. \t  3.7028719455774333 \t 3.817404427872387\n",
            "32     \t [0.         0.67831574 0.00166356]. \t  0.003632622370107462 \t 3.817404427872387\n",
            "33     \t [0.88399073 0.99488082 0.21409159]. \t  0.013340296088961688 \t 3.817404427872387\n",
            "34     \t [0.29809944 0.44133172 0.85280723]. \t  3.43409263715234 \t 3.817404427872387\n",
            "35     \t [0.15686661 0.59993793 0.86049972]. \t  3.777654263614589 \t 3.817404427872387\n",
            "36     \t [0.01616735 0.46541101 0.84623397]. \t  3.5466088481018785 \t 3.817404427872387\n",
            "37     \t [0.99833552 0.6457594  0.81291773]. \t  3.142536305436217 \t 3.817404427872387\n",
            "38     \t [1.         0.48559092 0.88105925]. \t  3.4573593471446395 \t 3.817404427872387\n",
            "39     \t [4.54070231e-04 5.75025344e-01 8.25803869e-01]. \t  3.730582608014279 \t 3.817404427872387\n",
            "40     \t [0.22132964 0.53237987 0.8508239 ]. \t  \u001b[92m3.8402230041191117\u001b[0m \t 3.8402230041191117\n",
            "41     \t [0.01479808 0.55473223 0.82138463]. \t  3.728242192303498 \t 3.8402230041191117\n",
            "42     \t [0.00911721 0.54863518 0.86372086]. \t  3.7997364616316402 \t 3.8402230041191117\n",
            "43     \t [0.45164502 0.56499692 0.88088795]. \t  3.775037807418519 \t 3.8402230041191117\n",
            "44     \t [0.11111847 0.54131336 0.78629137]. \t  3.4779454910693604 \t 3.8402230041191117\n",
            "45     \t [1.         0.5112263  0.92663573]. \t  3.1260662029619732 \t 3.8402230041191117\n",
            "46     \t [0.39030874 0.59975528 0.84104263]. \t  3.765751562188532 \t 3.8402230041191117\n",
            "47     \t [0.23189138 0.62661371 0.77507175]. \t  3.248739363945851 \t 3.8402230041191117\n",
            "48     \t [0.16849159 0.495371   0.86944956]. \t  3.692391244908925 \t 3.8402230041191117\n",
            "49     \t [0.39114293 0.5877849  0.82687739]. \t  3.744438083979967 \t 3.8402230041191117\n",
            "50     \t [0.34863003 0.56349366 0.8610763 ]. \t  \u001b[92m3.8517798424016876\u001b[0m \t 3.8517798424016876\n",
            "51     \t [0.18559942 0.597246   0.84581105]. \t  3.7888985187473727 \t 3.8517798424016876\n",
            "52     \t [0.32723912 0.49038783 0.8155521 ]. \t  3.612290123440908 \t 3.8517798424016876\n",
            "53     \t [0.05848135 0.47135776 0.81994148]. \t  3.5213504013127426 \t 3.8517798424016876\n",
            "54     \t [0.05226739 0.58760802 0.82449243]. \t  3.7191823574139526 \t 3.8517798424016876\n",
            "55     \t [0.27543777 0.52180978 0.86941213]. \t  3.7916217332174766 \t 3.8517798424016876\n",
            "56     \t [0.50735776 0.55230575 0.89256634]. \t  3.692878413932031 \t 3.8517798424016876\n",
            "57     \t [0.27441811 0.47819168 0.87699424]. \t  3.5881099216936856 \t 3.8517798424016876\n",
            "58     \t [0.38231263 0.58522219 0.77704949]. \t  3.3357533657981326 \t 3.8517798424016876\n",
            "59     \t [0.49924878 0.7390864  0.81379363]. \t  2.689941384919297 \t 3.8517798424016876\n",
            "60     \t [0.50550233 0.57053106 0.93435864]. \t  3.215860610813447 \t 3.8517798424016876\n",
            "61     \t [0.15021869 0.5483191  0.8571351 ]. \t  3.8452854417497084 \t 3.8517798424016876\n",
            "62     \t [0.36749204 0.53530764 0.87486016]. \t  3.797084062988681 \t 3.8517798424016876\n",
            "63     \t [0.09705901 0.49377569 0.76694622]. \t  3.1761007950803837 \t 3.8517798424016876\n",
            "64     \t [0.26842215 0.59480591 0.93331984]. \t  3.205850012641074 \t 3.8517798424016876\n",
            "65     \t [0.31829413 0.51272436 0.83250718]. \t  3.7671602916835734 \t 3.8517798424016876\n",
            "66     \t [0.06259587 0.48840153 0.85312155]. \t  3.676326922774931 \t 3.8517798424016876\n",
            "67     \t [0.27029749 0.52471957 0.91005931]. \t  3.494748952671511 \t 3.8517798424016876\n",
            "68     \t [0.0796643  0.536108   0.88064967]. \t  3.7395446042988794 \t 3.8517798424016876\n",
            "69     \t [0.20600723 0.50626523 0.86669346]. \t  3.7488531223102814 \t 3.8517798424016876\n",
            "70     \t [0.23796527 0.56620633 0.84659903]. \t  3.851497363231072 \t 3.8517798424016876\n",
            "71     \t [0.27929903 0.58105179 0.77917433]. \t  3.3941882772916716 \t 3.8517798424016876\n",
            "72     \t [0.25025845 0.49817928 0.80930314]. \t  3.6007170439397296 \t 3.8517798424016876\n",
            "73     \t [0.03225177 0.62648606 0.8549786 ]. \t  3.6515483433083067 \t 3.8517798424016876\n",
            "74     \t [0.05736842 0.4879553  0.82102697]. \t  3.602568948532407 \t 3.8517798424016876\n",
            "75     \t [0.40831827 0.54854693 0.85296935]. \t  \u001b[92m3.8542203730996047\u001b[0m \t 3.8542203730996047\n",
            "76     \t [0.51989284 0.59820405 0.81655192]. \t  3.6173031530058912 \t 3.8542203730996047\n",
            "77     \t [0.43155624 0.44628891 0.7964772 ]. \t  3.2544022430150754 \t 3.8542203730996047\n",
            "78     \t [0.31570183 0.58666343 0.88680729]. \t  3.7196564196630884 \t 3.8542203730996047\n",
            "79     \t [0.37749011 0.61607498 0.84184692]. \t  3.709426144464372 \t 3.8542203730996047\n",
            "80     \t [0.08592806 0.64802937 0.85836576]. \t  3.549999912819504 \t 3.8542203730996047\n",
            "81     \t [0.50582512 0.56983854 0.90053025]. \t  3.622777063012947 \t 3.8542203730996047\n",
            "82     \t [0.45538382 0.54676807 0.85461981]. \t  3.8474941601927766 \t 3.8542203730996047\n",
            "83     \t [0.37207106 0.44318633 0.87599332]. \t  3.3770456911204327 \t 3.8542203730996047\n",
            "84     \t [0.06029067 0.65060979 0.78171803]. \t  3.198429706617403 \t 3.8542203730996047\n",
            "85     \t [0.12333698 0.57166102 0.86684617]. \t  3.816340464469965 \t 3.8542203730996047\n",
            "86     \t [0.73550378 0.57306883 0.88608319]. \t  3.678165752837509 \t 3.8542203730996047\n",
            "87     \t [0.32014186 0.4927334  0.85450152]. \t  3.7278381942572056 \t 3.8542203730996047\n",
            "88     \t [0.13044128 0.58809781 0.87017835]. \t  3.781361577743517 \t 3.8542203730996047\n",
            "89     \t [0.07412993 0.60184887 0.84501701]. \t  3.753669566548071 \t 3.8542203730996047\n",
            "90     \t [0.48633202 0.62899906 0.83131332]. \t  3.587345717466445 \t 3.8542203730996047\n",
            "91     \t [0.00438534 0.51190907 0.90846124]. \t  3.4306866467335144 \t 3.8542203730996047\n",
            "92     \t [0.40161686 0.51766142 0.84997913]. \t  3.8096405055789813 \t 3.8542203730996047\n",
            "93     \t [0.23573251 0.61253724 0.82943516]. \t  3.694392041556439 \t 3.8542203730996047\n",
            "94     \t [0.00911478 0.56900627 0.84767889]. \t  3.804983609960365 \t 3.8542203730996047\n",
            "95     \t [0.1835834  0.52926003 0.8687968 ]. \t  3.8014392016328675 \t 3.8542203730996047\n",
            "96     \t [0.48824113 0.60249332 0.84361449]. \t  3.743528949203096 \t 3.8542203730996047\n",
            "97     \t [0.25198224 0.59437978 0.79597744]. \t  3.531933722481984 \t 3.8542203730996047\n",
            "98     \t [0.60075337 0.60321411 0.81045252]. \t  3.521705392593244 \t 3.8542203730996047\n",
            "99     \t [0.00884745 0.63816378 0.87347363]. \t  3.550952547398052 \t 3.8542203730996047\n",
            "100    \t [0.21354005 0.5963514  0.86288677]. \t  3.7909808894764567 \t 3.8542203730996047\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "reLyKt6Quvzx",
        "outputId": "bdaf26bb-a220-4300-bfad-51f4a6166197"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 17 \r\n",
        "\r\n",
        "np.random.seed(run_num_17)\r\n",
        "surrogate_winner_17 = dtStudentProcess(d_cov_func, nu = df)\r\n",
        "\r\n",
        "winner_17 = dGPGO_stp(surrogate_winner_17, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_17.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.65182403 0.73051962 0.19336736]. \t  0.03562358530655929 \t 3.1179188940604616\n",
            "init   \t [0.51967724 0.67638327 0.80444487]. \t  3.1179188940604616 \t 3.1179188940604616\n",
            "init   \t [0.53351286 0.90803418 0.44348113]. \t  1.1984486017622078 \t 3.1179188940604616\n",
            "init   \t [0.25597221 0.27866502 0.04332186]. \t  0.1645724004847893 \t 3.1179188940604616\n",
            "init   \t [0.46897499 0.9416138  0.60040624]. \t  1.9192522196289552 \t 3.1179188940604616\n",
            "1      \t [0.41089971 0.1354948  0.94682916]. \t  0.5225133233501791 \t 3.1179188940604616\n",
            "2      \t [1. 1. 1.]. \t  0.3168836207041842 \t 3.1179188940604616\n",
            "3      \t [0.0434631  0.65910314 0.84197486]. \t  \u001b[92m3.46354892023729\u001b[0m \t 3.46354892023729\n",
            "4      \t [0.03280843 0.75151925 0.98857466]. \t  1.627424858411729 \t 3.46354892023729\n",
            "5      \t [0.03278838 0.4084311  0.53771179]. \t  0.6600064169515342 \t 3.46354892023729\n",
            "6      \t [0.03834814 0.83156825 0.20642271]. \t  0.08024905393318603 \t 3.46354892023729\n",
            "7      \t [0.83093373 0.00874913 0.12872981]. \t  0.2635131463663533 \t 3.46354892023729\n",
            "8      \t [0.9434293  0.43778931 0.97623477]. \t  2.082522771067082 \t 3.46354892023729\n",
            "9      \t [0.26225724 0.64589953 0.63528002]. \t  2.2318725685733307 \t 3.46354892023729\n",
            "10     \t [0.99859934 0.87251952 0.51939709]. \t  0.31075737909655843 \t 3.46354892023729\n",
            "11     \t [0.04476045 0.43900197 0.83108343]. \t  3.364954169685313 \t 3.46354892023729\n",
            "12     \t [0.48070056 0.         0.        ]. \t  0.09848354235518049 \t 3.46354892023729\n",
            "13     \t [0.00240499 0.42898317 0.90288537]. \t  3.0298527764554186 \t 3.46354892023729\n",
            "14     \t [0.94958751 0.89865208 0.05733709]. \t  0.00042949352685820623 \t 3.46354892023729\n",
            "15     \t [0.30422027 0.59570156 0.90268439]. \t  \u001b[92m3.5729336882528866\u001b[0m \t 3.5729336882528866\n",
            "16     \t [0.50229052 0.86368592 0.01970752]. \t  0.0009229103962309357 \t 3.5729336882528866\n",
            "17     \t [0.93210548 0.03390371 0.03382528]. \t  0.07023116666378118 \t 3.5729336882528866\n",
            "18     \t [0.64484947 0.36942707 0.87858389]. \t  2.713597742963363 \t 3.5729336882528866\n",
            "19     \t [0.14510632 0.6668167  0.80405517]. \t  3.283221431788283 \t 3.5729336882528866\n",
            "20     \t [0.3007847  0.51758437 0.85904791]. \t  \u001b[92m3.8078560040723284\u001b[0m \t 3.8078560040723284\n",
            "21     \t [0.07952307 0.59771241 0.82052197]. \t  3.681165534617595 \t 3.8078560040723284\n",
            "22     \t [0.16687244 0.54708591 0.83440357]. \t  \u001b[92m3.8202852515460846\u001b[0m \t 3.8202852515460846\n",
            "23     \t [0.23495692 0.49615309 0.88935808]. \t  3.5915474472828834 \t 3.8202852515460846\n",
            "24     \t [0.98820412 0.15454864 0.9482944 ]. \t  0.5819556500877513 \t 3.8202852515460846\n",
            "25     \t [0.44398877 0.51198579 0.79855975]. \t  3.5340000545811208 \t 3.8202852515460846\n",
            "26     \t [0.02908431 0.59144948 0.83674283]. \t  3.751250007593029 \t 3.8202852515460846\n",
            "27     \t [0.3102521  0.59177831 0.83499804]. \t  3.7796467477628597 \t 3.8202852515460846\n",
            "28     \t [0.26400987 0.46476742 0.89638377]. \t  3.3754373326652765 \t 3.8202852515460846\n",
            "29     \t [0.21673413 0.57692406 0.83346038]. \t  3.805793583219982 \t 3.8202852515460846\n",
            "30     \t [0.22168342 0.4574946  0.82285756]. \t  3.48670323566566 \t 3.8202852515460846\n",
            "31     \t [0.18647439 0.58008549 0.89218516]. \t  3.6840839668881964 \t 3.8202852515460846\n",
            "32     \t [0.29622705 0.54547218 0.85997046]. \t  \u001b[92m3.8531850400776015\u001b[0m \t 3.8531850400776015\n",
            "33     \t [0.40104817 0.55546444 0.87453942]. \t  3.8125314630142766 \t 3.8531850400776015\n",
            "34     \t [0.02988762 0.59984077 0.02320157]. \t  0.011811116101299894 \t 3.8531850400776015\n",
            "35     \t [0.6009811  0.35546047 0.02102685]. \t  0.07812190702761938 \t 3.8531850400776015\n",
            "36     \t [0. 0. 0.]. \t  0.06797411659013229 \t 3.8531850400776015\n",
            "37     \t [0.14943156 0.53208956 0.89095743]. \t  3.67780720229086 \t 3.8531850400776015\n",
            "38     \t [0.06639408 0.99444495 0.04231428]. \t  0.0010534060687619564 \t 3.8531850400776015\n",
            "39     \t [0.998478   0.16668825 0.51450662]. \t  0.13834068589443638 \t 3.8531850400776015\n",
            "40     \t [4.66028239e-01 2.02698881e-10 5.76415622e-01]. \t  0.11688328955654971 \t 3.8531850400776015\n",
            "41     \t [0.16218737 0.53608207 0.84926115]. \t  3.837850895498655 \t 3.8531850400776015\n",
            "42     \t [0.03428499 0.58728494 0.74344392]. \t  2.9907997136086957 \t 3.8531850400776015\n",
            "43     \t [0.25140769 0.51130458 0.87311042]. \t  3.7459985895395294 \t 3.8531850400776015\n",
            "44     \t [0.23727795 0.45830681 0.86128709]. \t  3.529693084065228 \t 3.8531850400776015\n",
            "45     \t [0.10537744 0.5676065  0.85087542]. \t  3.835408283015269 \t 3.8531850400776015\n",
            "46     \t [0.10378828 0.61037249 0.83148333]. \t  3.696335198193361 \t 3.8531850400776015\n",
            "47     \t [0.17087231 0.6011248  0.78514262]. \t  3.4211893925005654 \t 3.8531850400776015\n",
            "48     \t [0.42189845 0.56006365 0.80108647]. \t  3.5980324098402194 \t 3.8531850400776015\n",
            "49     \t [0.00943834 0.63017781 0.89394128]. \t  3.4793464334354227 \t 3.8531850400776015\n",
            "50     \t [0.19215346 0.60724419 0.83094981]. \t  3.718096935435949 \t 3.8531850400776015\n",
            "51     \t [0.28134029 0.48174873 0.85899166]. \t  3.670356201297278 \t 3.8531850400776015\n",
            "52     \t [0.35348394 0.54054037 0.79326966]. \t  3.5423436991380184 \t 3.8531850400776015\n",
            "53     \t [0.33754054 0.54393935 0.84659631]. \t  3.853014892331622 \t 3.8531850400776015\n",
            "54     \t [0.35816366 0.58616054 0.8524589 ]. \t  3.823826743363244 \t 3.8531850400776015\n",
            "55     \t [0.28770883 0.49612646 0.86651767]. \t  3.717414031517436 \t 3.8531850400776015\n",
            "56     \t [0.06932805 0.47782073 0.89014707]. \t  3.470998875923565 \t 3.8531850400776015\n",
            "57     \t [0.32655411 0.55591243 0.89680469]. \t  3.6707418119572557 \t 3.8531850400776015\n",
            "58     \t [0.20657033 0.55838524 0.91945795]. \t  3.420645882363831 \t 3.8531850400776015\n",
            "59     \t [0.11293304 0.57314507 0.85437512]. \t  3.83141849877468 \t 3.8531850400776015\n",
            "60     \t [0.39374785 0.46831984 0.86580634]. \t  3.581651949296692 \t 3.8531850400776015\n",
            "61     \t [0.25838956 0.58887201 0.83989378]. \t  3.8033291294789806 \t 3.8531850400776015\n",
            "62     \t [0.39508293 0.43799761 0.86591078]. \t  3.381484197398616 \t 3.8531850400776015\n",
            "63     \t [0.40862017 0.65896549 0.87992518]. \t  3.436072296611867 \t 3.8531850400776015\n",
            "64     \t [0.47588641 0.59984021 0.82139536]. \t  3.658582483835624 \t 3.8531850400776015\n",
            "65     \t [0.1637473  0.61947206 0.80975277]. \t  3.5578897868967374 \t 3.8531850400776015\n",
            "66     \t [0.44866529 0.4865279  0.83824745]. \t  3.6816315286701915 \t 3.8531850400776015\n",
            "67     \t [0.27469395 0.5151826  0.79396623]. \t  3.5231506172565226 \t 3.8531850400776015\n",
            "68     \t [0.04620454 0.62053944 0.85232001]. \t  3.6824656382064456 \t 3.8531850400776015\n",
            "69     \t [0.34851111 0.57723018 0.81293542]. \t  3.688662487338992 \t 3.8531850400776015\n",
            "70     \t [0.03043002 0.55365845 0.84809944]. \t  3.8189294254843715 \t 3.8531850400776015\n",
            "71     \t [0.27613841 0.52266476 0.84539379]. \t  3.8215528906049494 \t 3.8531850400776015\n",
            "72     \t [0.07071546 0.5930985  0.89179162]. \t  3.6410632043700657 \t 3.8531850400776015\n",
            "73     \t [0.33833847 0.47539656 0.87150038]. \t  3.6012942631832976 \t 3.8531850400776015\n",
            "74     \t [0.16257091 0.49381998 0.87354921]. \t  3.6678535323464017 \t 3.8531850400776015\n",
            "75     \t [0.50626715 0.57877581 0.83649483]. \t  3.78094997838649 \t 3.8531850400776015\n",
            "76     \t [0.03019914 0.59662302 0.8389748 ]. \t  3.7441545005130275 \t 3.8531850400776015\n",
            "77     \t [0.30059602 0.53064969 0.88354545]. \t  3.742487605612661 \t 3.8531850400776015\n",
            "78     \t [0.06564478 0.59197935 0.85117855]. \t  3.7850225644814333 \t 3.8531850400776015\n",
            "79     \t [0.30038298 0.50797898 0.82421797]. \t  3.7197480979920616 \t 3.8531850400776015\n",
            "80     \t [0.42036627 0.55923238 0.79450359]. \t  3.537765520141484 \t 3.8531850400776015\n",
            "81     \t [0.1144593  0.56449723 0.87695647]. \t  3.781897647489056 \t 3.8531850400776015\n",
            "82     \t [0.0331559  0.58421934 0.86927744]. \t  3.768459023469747 \t 3.8531850400776015\n",
            "83     \t [0.27535181 0.61161338 0.86852323]. \t  3.7311063164554366 \t 3.8531850400776015\n",
            "84     \t [0.19684817 0.49145922 0.89566458]. \t  3.5157751234964527 \t 3.8531850400776015\n",
            "85     \t [0.32919673 0.52477739 0.88090148]. \t  3.7466542328048353 \t 3.8531850400776015\n",
            "86     \t [0.30725429 0.64655565 0.90342   ]. \t  3.3622532207929194 \t 3.8531850400776015\n",
            "87     \t [0.12000152 0.55615164 0.88423222]. \t  3.744010544696965 \t 3.8531850400776015\n",
            "88     \t [0.07414746 0.60314013 0.89546524]. \t  3.5871718570580025 \t 3.8531850400776015\n",
            "89     \t [0.25595859 0.54700763 0.84137806]. \t  3.846683130046673 \t 3.8531850400776015\n",
            "90     \t [0.46895797 0.55730648 0.86723602]. \t  3.82997130122261 \t 3.8531850400776015\n",
            "91     \t [0.40658931 0.50425379 0.87391844]. \t  3.7197932272770498 \t 3.8531850400776015\n",
            "92     \t [0.41844025 0.47817072 0.84328128]. \t  3.654724941888539 \t 3.8531850400776015\n",
            "93     \t [0.16582816 0.56720486 0.8550023 ]. \t  3.8466422264021882 \t 3.8531850400776015\n",
            "94     \t [0.07423351 0.64133595 0.82406164]. \t  3.5205027917729064 \t 3.8531850400776015\n",
            "95     \t [0.48456103 0.52159347 0.89803485]. \t  3.604645697446997 \t 3.8531850400776015\n",
            "96     \t [0.36644585 0.50467095 0.88795653]. \t  3.6407934696635307 \t 3.8531850400776015\n",
            "97     \t [0.38988631 0.63819153 0.85532246]. \t  3.6166627679741516 \t 3.8531850400776015\n",
            "98     \t [0.08318329 0.59994938 0.85910978]. \t  3.764799256868108 \t 3.8531850400776015\n",
            "99     \t [0.48974764 0.52558597 0.83322474]. \t  3.7783765368404603 \t 3.8531850400776015\n",
            "100    \t [0.21134706 0.51846553 0.87067687]. \t  3.773172213283738 \t 3.8531850400776015\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cI3FtfYSuv2u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66fd5e73-1584-421b-f9bd-0480f394370b"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 18 \r\n",
        "\r\n",
        "np.random.seed(run_num_18)\r\n",
        "surrogate_winner_18 = dtStudentProcess(d_cov_func, nu = df)\r\n",
        "\r\n",
        "winner_18 = dGPGO_stp(surrogate_winner_18, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_18.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.8734294  0.96854066 0.86919454]. \t  0.722531189286755 \t 1.1210522139432408\n",
            "init   \t [0.53085569 0.23272833 0.0113988 ]. \t  0.1133646794700679 \t 1.1210522139432408\n",
            "init   \t [0.43046882 0.40235136 0.52267467]. \t  0.5232646985053151 \t 1.1210522139432408\n",
            "init   \t [0.4783918  0.55535647 0.54338602]. \t  1.0390748854353227 \t 1.1210522139432408\n",
            "init   \t [0.76089558 0.71237457 0.6196821 ]. \t  1.1210522139432408 \t 1.1210522139432408\n",
            "1      \t [0.28982083 0.95201409 0.18404427]. \t  0.04220734680078784 \t 1.1210522139432408\n",
            "2      \t [0.00482474 0.64977529 0.94137026]. \t  \u001b[92m2.8634726533487465\u001b[0m \t 2.8634726533487465\n",
            "3      \t [0.12931895 0.62449796 0.99986347]. \t  2.0131717971615943 \t 2.8634726533487465\n",
            "4      \t [0.04377814 0.94603183 0.8490757 ]. \t  1.034546770545397 \t 2.8634726533487465\n",
            "5      \t [0.94527434 0.7027213  0.16011708]. \t  0.0110459359196211 \t 2.8634726533487465\n",
            "6      \t [0.00806648 0.63086884 0.22273634]. \t  0.10481434203252675 \t 2.8634726533487465\n",
            "7      \t [0.02415347 0.53154945 0.79057782]. \t  \u001b[92m3.4855888364055527\u001b[0m \t 3.4855888364055527\n",
            "8      \t [0.01644977 0.04218135 0.84017573]. \t  0.35497034147653084 \t 3.4855888364055527\n",
            "9      \t [0.9993393  0.0086611  0.23114407]. \t  0.25953555543334106 \t 3.4855888364055527\n",
            "10     \t [0.0227948  0.48662174 0.77123941]. \t  3.179872758642781 \t 3.4855888364055527\n",
            "11     \t [0.78115207 0.14036239 0.97255015]. \t  0.44365735032860515 \t 3.4855888364055527\n",
            "12     \t [0.94049098 0.49695016 0.94841749]. \t  2.7877422251401383 \t 3.4855888364055527\n",
            "13     \t [0.03134313 0.81924117 0.58082107]. \t  2.980369024305593 \t 3.4855888364055527\n",
            "14     \t [0.07346516 0.69622683 0.71595881]. \t  2.6212081247905252 \t 3.4855888364055527\n",
            "15     \t [0.36837903 0.3235421  0.98355355]. \t  1.3446589738603891 \t 3.4855888364055527\n",
            "16     \t [0.86068398 0.36106247 0.02140388]. \t  0.043500407291218596 \t 3.4855888364055527\n",
            "17     \t [0.01455595 0.35007925 0.98435586]. \t  1.4830911516129277 \t 3.4855888364055527\n",
            "18     \t [0.69728534 0.55163792 0.97417233]. \t  2.527804257890841 \t 3.4855888364055527\n",
            "19     \t [1.5761469e-11 1.5761469e-11 1.5761469e-11]. \t  0.06797411661219338 \t 3.4855888364055527\n",
            "20     \t [1. 1. 1.]. \t  0.31688362070415665 \t 3.4855888364055527\n",
            "21     \t [0.9661124  0.96885384 0.05739277]. \t  0.00024576747613142507 \t 3.4855888364055527\n",
            "22     \t [0.92552914 0.98817306 0.45703493]. \t  0.2713736587017947 \t 3.4855888364055527\n",
            "23     \t [0.03698564 0.61355088 0.64158739]. \t  2.1952902977783557 \t 3.4855888364055527\n",
            "24     \t [0.94250153 0.22609141 0.65220785]. \t  0.6968150717248845 \t 3.4855888364055527\n",
            "25     \t [0.09934439 0.91429262 0.06700685]. \t  0.0027674436069959176 \t 3.4855888364055527\n",
            "26     \t [0.94653089 0.0038093  0.7698587 ]. \t  0.25053481441742076 \t 3.4855888364055527\n",
            "27     \t [0.64276268 0.00426067 0.09295942]. \t  0.2825389446547606 \t 3.4855888364055527\n",
            "28     \t [0.52982514 0.97501336 0.61380985]. \t  1.5193216487885988 \t 3.4855888364055527\n",
            "29     \t [0.10841019 0.99082148 0.57022494]. \t  2.630743937149305 \t 3.4855888364055527\n",
            "30     \t [0.03422976 0.96949666 0.17117367]. \t  0.03296997170140389 \t 3.4855888364055527\n",
            "31     \t [0.67037743 0.6147903  0.04016887]. \t  0.013804529371493507 \t 3.4855888364055527\n",
            "32     \t [0.03370431 0.03183527 0.55620525]. \t  0.1236537428641728 \t 3.4855888364055527\n",
            "33     \t [0.52283919 0.02989733 0.65690621]. \t  0.20757706038091797 \t 3.4855888364055527\n",
            "34     \t [0.04204555 0.53062352 0.        ]. \t  0.015467579831572919 \t 3.4855888364055527\n",
            "35     \t [0.43594365 0.43260834 0.84291666]. \t  3.3705095448276152 \t 3.4855888364055527\n",
            "36     \t [0.59953466 0.45280825 0.83012981]. \t  3.457809422256522 \t 3.4855888364055527\n",
            "37     \t [0.44927728 0.46107172 0.90207797]. \t  3.3022233331158506 \t 3.4855888364055527\n",
            "38     \t [0.54378865 0.38426976 0.81171051]. \t  2.8907422885427625 \t 3.4855888364055527\n",
            "39     \t [0.49958773 0.46518172 0.84654659]. \t  \u001b[92m3.579485450634112\u001b[0m \t 3.579485450634112\n",
            "40     \t [0.49058664 0.50273698 0.80403078]. \t  3.547830457378981 \t 3.579485450634112\n",
            "41     \t [0.99635096 0.81655256 0.78234648]. \t  1.529342703506713 \t 3.579485450634112\n",
            "42     \t [0.99910289 0.37496468 0.92089504]. \t  2.3568029183256014 \t 3.579485450634112\n",
            "43     \t [0.45652582 0.57168554 0.82351417]. \t  \u001b[92m3.743260555518294\u001b[0m \t 3.743260555518294\n",
            "44     \t [0.51265615 0.6418573  0.83629353]. \t  3.5300007274291967 \t 3.743260555518294\n",
            "45     \t [0.49824809 0.43240244 0.82979054]. \t  3.3398832498157724 \t 3.743260555518294\n",
            "46     \t [0.36493399 0.59387193 0.87952896]. \t  \u001b[92m3.743676159546551\u001b[0m \t 3.743676159546551\n",
            "47     \t [0.371435   0.62471747 0.89342495]. \t  3.549953782230856 \t 3.743676159546551\n",
            "48     \t [0.43361894 0.70970756 0.80014408]. \t  2.89299602192006 \t 3.743676159546551\n",
            "49     \t [0.06038355 0.48710185 0.85396555]. \t  3.6692168800923985 \t 3.743676159546551\n",
            "50     \t [0.23082112 0.51981109 0.85882598]. \t  \u001b[92m3.810184033282763\u001b[0m \t 3.810184033282763\n",
            "51     \t [0.2725925  0.4927411  0.86053732]. \t  3.717853887739766 \t 3.810184033282763\n",
            "52     \t [0.52975261 0.51457915 0.82669434]. \t  3.7193690713322805 \t 3.810184033282763\n",
            "53     \t [0.24872389 0.49160939 0.75965023]. \t  3.0965149602215822 \t 3.810184033282763\n",
            "54     \t [0.22286344 0.50408234 0.91519907]. \t  3.3683668149254045 \t 3.810184033282763\n",
            "55     \t [0.23800071 0.58577722 0.86217081]. \t  \u001b[92m3.819724598382984\u001b[0m \t 3.819724598382984\n",
            "56     \t [0.28436177 0.53660732 0.83885416]. \t  \u001b[92m3.832828444936715\u001b[0m \t 3.832828444936715\n",
            "57     \t [0.00617518 0.66230776 0.78486587]. \t  3.145119079879026 \t 3.832828444936715\n",
            "58     \t [0.38149589 0.55683354 0.84962487]. \t  \u001b[92m3.8559948262445496\u001b[0m \t 3.8559948262445496\n",
            "59     \t [0.12387215 0.58448286 0.80479577]. \t  3.618902791368914 \t 3.8559948262445496\n",
            "60     \t [0.43826398 0.52544814 0.87044209]. \t  3.7922448560247566 \t 3.8559948262445496\n",
            "61     \t [0.50944693 0.50949936 0.87404043]. \t  3.72698968627705 \t 3.8559948262445496\n",
            "62     \t [0.12583932 0.53821064 0.85233859]. \t  3.834641830318196 \t 3.8559948262445496\n",
            "63     \t [0.29354694 0.65383855 0.78275189]. \t  3.1862808679827697 \t 3.8559948262445496\n",
            "64     \t [0.03691833 0.47470475 0.87852472]. \t  3.52239984557158 \t 3.8559948262445496\n",
            "65     \t [0.5839216  0.52908568 0.84047264]. \t  3.7863194440448975 \t 3.8559948262445496\n",
            "66     \t [0.31467733 0.52551692 0.90174005]. \t  3.585897295131483 \t 3.8559948262445496\n",
            "67     \t [0.40350825 0.48734523 0.83457705]. \t  3.680336037524838 \t 3.8559948262445496\n",
            "68     \t [0.1716311  0.62662635 0.82786013]. \t  3.625879908018528 \t 3.8559948262445496\n",
            "69     \t [0.90696403 0.50357807 0.89089319]. \t  3.504736111864369 \t 3.8559948262445496\n",
            "70     \t [0.58686081 0.51102316 0.89141407]. \t  3.6151468785904393 \t 3.8559948262445496\n",
            "71     \t [0.02625201 0.55786441 0.84645561]. \t  3.815787390721942 \t 3.8559948262445496\n",
            "72     \t [0.24878524 0.56947782 0.86426663]. \t  3.840916042002484 \t 3.8559948262445496\n",
            "73     \t [0.62051778 0.46815577 0.85552276]. \t  3.5731675758501926 \t 3.8559948262445496\n",
            "74     \t [0.49819374 0.58290071 0.89149726]. \t  3.681408318949563 \t 3.8559948262445496\n",
            "75     \t [0.03065976 0.57910002 0.82451358]. \t  3.7287670570183122 \t 3.8559948262445496\n",
            "76     \t [0.21774654 0.60955027 0.83948732]. \t  3.738703500869203 \t 3.8559948262445496\n",
            "77     \t [0.39069303 0.49608959 0.8277798 ]. \t  3.6918588863349417 \t 3.8559948262445496\n",
            "78     \t [0.26065029 0.59573368 0.8698221 ]. \t  3.778615657348264 \t 3.8559948262445496\n",
            "79     \t [0.55826089 0.55911208 0.84474276]. \t  3.8166867121639347 \t 3.8559948262445496\n",
            "80     \t [0.07743487 0.47597234 0.7850551 ]. \t  3.293803015906214 \t 3.8559948262445496\n",
            "81     \t [0.28141624 0.55189009 0.81134512]. \t  3.705841149851199 \t 3.8559948262445496\n",
            "82     \t [0.61132594 0.52952952 0.85533323]. \t  3.7986056067203675 \t 3.8559948262445496\n",
            "83     \t [0.24625681 0.58182187 0.85524631]. \t  3.8349769243952587 \t 3.8559948262445496\n",
            "84     \t [0.16883855 0.53646462 0.82004888]. \t  3.7482387844043465 \t 3.8559948262445496\n",
            "85     \t [0.23323829 0.00158268 0.10768351]. \t  0.38568395642124553 \t 3.8559948262445496\n",
            "86     \t [0.41170721 0.53357721 0.8776474 ]. \t  3.7788738505433788 \t 3.8559948262445496\n",
            "87     \t [0.53229927 0.51335823 0.84437338]. \t  3.7741955453342726 \t 3.8559948262445496\n",
            "88     \t [0.56335103 0.63177929 0.83477455]. \t  3.5628077247782612 \t 3.8559948262445496\n",
            "89     \t [0.30152842 0.55281078 0.82864814]. \t  3.805872527480709 \t 3.8559948262445496\n",
            "90     \t [0.71764436 0.50120913 0.89630177]. \t  3.513416336552514 \t 3.8559948262445496\n",
            "91     \t [0.06977406 0.60739298 0.83193071]. \t  3.7003125434280713 \t 3.8559948262445496\n",
            "92     \t [0.21862552 0.47465422 0.85162343]. \t  3.6377182461511834 \t 3.8559948262445496\n",
            "93     \t [0.35394243 0.63044838 0.86791602]. \t  3.647153655182984 \t 3.8559948262445496\n",
            "94     \t [0.52897407 0.54925181 0.88068636]. \t  3.7659958974425827 \t 3.8559948262445496\n",
            "95     \t [0.59117268 0.45423849 0.87454504]. \t  3.4400787358527585 \t 3.8559948262445496\n",
            "96     \t [0.92077285 0.47923521 0.92493803]. \t  3.0517824588803872 \t 3.8559948262445496\n",
            "97     \t [0.63461924 0.53905751 0.8603979 ]. \t  3.801479079028581 \t 3.8559948262445496\n",
            "98     \t [0.66415748 0.48039688 0.77650462]. \t  3.1550381327648083 \t 3.8559948262445496\n",
            "99     \t [0.28029021 0.61315091 0.84074469]. \t  3.72830397719656 \t 3.8559948262445496\n",
            "100    \t [0.23913699 0.53010755 0.83224022]. \t  3.802341092036368 \t 3.8559948262445496\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9YT-CgKvuv4q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95f9b01c-926b-44e9-d48e-0ea0c1451936"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 19 \r\n",
        "\r\n",
        "np.random.seed(run_num_19)\r\n",
        "surrogate_winner_19 = dtStudentProcess(d_cov_func, nu = df)\r\n",
        "\r\n",
        "winner_19 = dGPGO_stp(surrogate_winner_19, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_19.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.55944558 0.76149199 0.24453155]. \t  0.09433080659504385 \t 2.524990008735946\n",
            "init   \t [0.77168947 0.35447208 0.7966974 ]. \t  2.524990008735946 \t 2.524990008735946\n",
            "init   \t [0.34600002 0.44387444 0.30330359]. \t  0.3914194506273641 \t 2.524990008735946\n",
            "init   \t [0.44211392 0.57320936 0.06566402]. \t  0.036928944981920925 \t 2.524990008735946\n",
            "init   \t [0.02408889 0.82062263 0.36513765]. \t  0.9772047942100802 \t 2.524990008735946\n",
            "1      \t [0.96499061 0.19185662 0.98563058]. \t  0.5815385407774518 \t 2.524990008735946\n",
            "2      \t [1. 1. 1.]. \t  0.3168836207041624 \t 2.524990008735946\n",
            "3      \t [0.03364059 0.33501849 0.98410297]. \t  1.3937191686454828 \t 2.524990008735946\n",
            "4      \t [0.31985762 0.78245129 0.89645038]. \t  2.301400686098107 \t 2.524990008735946\n",
            "5      \t [0.98063477 0.07486764 0.1985829 ]. \t  0.2774821572854841 \t 2.524990008735946\n",
            "6      \t [0.6492824  0.63015609 0.97118431]. \t  2.4893665446826185 \t 2.524990008735946\n",
            "7      \t [0.06546839 0.67685517 0.98076297]. \t  2.1459718934538365 \t 2.524990008735946\n",
            "8      \t [0.46664877 0.45397971 0.81849397]. \t  \u001b[92m3.43871523256551\u001b[0m \t 3.43871523256551\n",
            "9      \t [0.56532792 0.04329219 0.86685748]. \t  0.3404214327163485 \t 3.43871523256551\n",
            "10     \t [0.24083791 0.57357524 0.80517333]. \t  \u001b[92m3.6471515038220588\u001b[0m \t 3.6471515038220588\n",
            "11     \t [0.39574733 0.51361815 0.79266067]. \t  3.4926715143265223 \t 3.6471515038220588\n",
            "12     \t [0.95706381 0.89112844 0.03656602]. \t  0.0002890144264835248 \t 3.6471515038220588\n",
            "13     \t [0.04742278 0.58693213 0.71824962]. \t  2.7300113629335696 \t 3.6471515038220588\n",
            "14     \t [0.42833722 0.43462256 0.84393691]. \t  3.386328950903048 \t 3.6471515038220588\n",
            "15     \t [0.99514663 0.66559978 0.69347922]. \t  1.543548541088649 \t 3.6471515038220588\n",
            "16     \t [0.24541234 0.56703429 0.75810662]. \t  3.182178566432365 \t 3.6471515038220588\n",
            "17     \t [0. 0. 0.]. \t  0.06797411659013229 \t 3.6471515038220588\n",
            "18     \t [0.87706805 0.98367244 0.71204394]. \t  0.4839672795306256 \t 3.6471515038220588\n",
            "19     \t [5.68836918e-01 4.84097686e-04 9.57517327e-02]. \t  0.3202851758829711 \t 3.6471515038220588\n",
            "20     \t [0.05030671 0.78839594 0.06623977]. \t  0.004569358810027998 \t 3.6471515038220588\n",
            "21     \t [0.44498135 0.49248846 0.917723  ]. \t  3.296519112624257 \t 3.6471515038220588\n",
            "22     \t [0.36089253 0.53556271 0.84884829]. \t  \u001b[92m3.8457523011513746\u001b[0m \t 3.8457523011513746\n",
            "23     \t [0.7242184  0.02557229 0.67694735]. \t  0.22210574348758572 \t 3.8457523011513746\n",
            "24     \t [0.89923773 0.4061386  0.05121515]. \t  0.04595924247841664 \t 3.8457523011513746\n",
            "25     \t [0.30544661 0.5490748  0.83968374]. \t  3.843833827992947 \t 3.8457523011513746\n",
            "26     \t [0.27008144 0.45585562 0.92026046]. \t  3.070157495617653 \t 3.8457523011513746\n",
            "27     \t [0.50143923 0.57427884 0.83984742]. \t  3.8013973418547105 \t 3.8457523011513746\n",
            "28     \t [0.4754612  0.57180821 0.76417735]. \t  3.165338188867101 \t 3.8457523011513746\n",
            "29     \t [0.23320797 0.98878694 0.03639898]. \t  0.0008881824750883105 \t 3.8457523011513746\n",
            "30     \t [0.41113285 0.50850822 0.89512425]. \t  3.5966752637428776 \t 3.8457523011513746\n",
            "31     \t [0.16968813 0.00334689 0.77148546]. \t  0.2550221791977182 \t 3.8457523011513746\n",
            "32     \t [0.34934634 0.54855366 0.9227411 ]. \t  3.38135635415166 \t 3.8457523011513746\n",
            "33     \t [0.5132625  0.52681916 0.88096984]. \t  3.7378755820064624 \t 3.8457523011513746\n",
            "34     \t [0.4692865  0.65520536 0.84797288]. \t  3.4917492675237334 \t 3.8457523011513746\n",
            "35     \t [0.3318371  0.60553692 0.84836368]. \t  3.7672068645486303 \t 3.8457523011513746\n",
            "36     \t [0.45364824 0.49789786 0.8436698 ]. \t  3.73691732916868 \t 3.8457523011513746\n",
            "37     \t [0.38323423 0.59150146 0.80068825]. \t  3.5544795271372194 \t 3.8457523011513746\n",
            "38     \t [0.95583291 0.58027727 0.89937525]. \t  3.4950104632622248 \t 3.8457523011513746\n",
            "39     \t [0.13204123 0.61299282 0.85631703]. \t  3.7326384598283133 \t 3.8457523011513746\n",
            "40     \t [0.30414134 0.58632436 0.84665655]. \t  3.821683047998385 \t 3.8457523011513746\n",
            "41     \t [0.54760242 0.53264453 0.87867755]. \t  3.755496029425511 \t 3.8457523011513746\n",
            "42     \t [0.31401142 0.59437876 0.90958465]. \t  3.506941630347282 \t 3.8457523011513746\n",
            "43     \t [0.55878565 0.54428248 0.84350672]. \t  3.814603785171686 \t 3.8457523011513746\n",
            "44     \t [0.33960879 0.50008536 0.83292582]. \t  3.728825751313649 \t 3.8457523011513746\n",
            "45     \t [0.25031258 0.63449735 0.8702777 ]. \t  3.6227320250339003 \t 3.8457523011513746\n",
            "46     \t [0.40567078 0.57232314 0.8497858 ]. \t  3.841510565754008 \t 3.8457523011513746\n",
            "47     \t [0.55416329 0.52154156 0.88732382]. \t  3.6800513704254585 \t 3.8457523011513746\n",
            "48     \t [0.45122524 0.54792073 0.87688662]. \t  3.795012005262345 \t 3.8457523011513746\n",
            "49     \t [0.03606038 0.69548653 0.83154651]. \t  3.180625835097338 \t 3.8457523011513746\n",
            "50     \t [0.33378795 0.53145584 0.92089508]. \t  3.381271924580193 \t 3.8457523011513746\n",
            "51     \t [0.53932062 0.56268954 0.85233877]. \t  3.8293542739506665 \t 3.8457523011513746\n",
            "52     \t [0.34114834 0.64986748 0.82210122]. \t  3.467360996036892 \t 3.8457523011513746\n",
            "53     \t [0.46559321 0.57329696 0.83902826]. \t  3.808348499447201 \t 3.8457523011513746\n",
            "54     \t [0.52393454 0.55130395 0.81695381]. \t  3.700512022152781 \t 3.8457523011513746\n",
            "55     \t [0.33125404 0.53246604 0.88153084]. \t  3.7585910760960095 \t 3.8457523011513746\n",
            "56     \t [0.62872083 0.62836927 0.85052057]. \t  3.605307610281071 \t 3.8457523011513746\n",
            "57     \t [0.43952428 0.55492472 0.87988208]. \t  3.7839057652981767 \t 3.8457523011513746\n",
            "58     \t [0.99332101 0.68120747 0.93436395]. \t  2.697460660231673 \t 3.8457523011513746\n",
            "59     \t [0.31415272 0.52555688 0.88290289]. \t  3.73625918930163 \t 3.8457523011513746\n",
            "60     \t [0.5463764  0.53324602 0.82716137]. \t  3.7489705285709802 \t 3.8457523011513746\n",
            "61     \t [0.49445744 0.52434759 0.79529934]. \t  3.5122602665240987 \t 3.8457523011513746\n",
            "62     \t [0.0325982  0.52476701 0.80330545]. \t  3.5881809754482696 \t 3.8457523011513746\n",
            "63     \t [0.6281898  0.56253518 0.84712661]. \t  3.799848447752897 \t 3.8457523011513746\n",
            "64     \t [0.26345614 0.45403693 0.83321725]. \t  3.503332877541431 \t 3.8457523011513746\n",
            "65     \t [0.69434869 0.54799769 0.85393242]. \t  3.7919149480043925 \t 3.8457523011513746\n",
            "66     \t [0.16889282 0.53371454 0.83922725]. \t  3.821543422535262 \t 3.8457523011513746\n",
            "67     \t [0.49078108 0.55344081 0.87847797]. \t  3.7846278601519625 \t 3.8457523011513746\n",
            "68     \t [0.46443771 0.54865492 0.86747242]. \t  3.8286977712915276 \t 3.8457523011513746\n",
            "69     \t [0.52703508 0.55805325 0.85409469]. \t  3.835302615055836 \t 3.8457523011513746\n",
            "70     \t [0.76223488 0.53912389 0.89791206]. \t  3.5829450789956474 \t 3.8457523011513746\n",
            "71     \t [0.51493988 0.52404194 0.84245175]. \t  3.7981129561950606 \t 3.8457523011513746\n",
            "72     \t [0.53969711 0.54594332 0.86785772]. \t  3.814044457165606 \t 3.8457523011513746\n",
            "73     \t [0.36063581 0.54424985 0.88223465]. \t  3.7700355262267764 \t 3.8457523011513746\n",
            "74     \t [0.95162273 0.         0.        ]. \t  0.03691764881925055 \t 3.8457523011513746\n",
            "75     \t [0.21349641 0.58057924 0.86324725]. \t  3.825716731424189 \t 3.8457523011513746\n",
            "76     \t [0.01680609 0.53401419 0.88409765]. \t  3.698566170960916 \t 3.8457523011513746\n",
            "77     \t [0.52020588 0.54195887 0.80191963]. \t  3.580568749267905 \t 3.8457523011513746\n",
            "78     \t [0.03184441 0.51773954 0.8752082 ]. \t  3.715845653517257 \t 3.8457523011513746\n",
            "79     \t [0.55860998 0.55909702 0.85236153]. \t  3.8270810954668057 \t 3.8457523011513746\n",
            "80     \t [0.006454   0.53308555 0.87277185]. \t  3.7529342661293286 \t 3.8457523011513746\n",
            "81     \t [0.42782637 0.54729056 0.87264788]. \t  3.815103568701952 \t 3.8457523011513746\n",
            "82     \t [0.32382865 0.52370181 0.87469713]. \t  3.77614318351931 \t 3.8457523011513746\n",
            "83     \t [0.31143479 0.57466033 0.85725906]. \t  \u001b[92m3.846238360871764\u001b[0m \t 3.846238360871764\n",
            "84     \t [0.40491551 0.61507781 0.82466201]. \t  3.640612026142874 \t 3.846238360871764\n",
            "85     \t [0.15866546 0.55275814 0.86312897]. \t  3.839158327450672 \t 3.846238360871764\n",
            "86     \t [0.56052688 0.52587973 0.83196776]. \t  3.7580573160090616 \t 3.846238360871764\n",
            "87     \t [0.50990504 0.50798049 0.84600198]. \t  3.7654845659765326 \t 3.846238360871764\n",
            "88     \t [0.48091724 0.58901157 0.80987201]. \t  3.607018879447114 \t 3.846238360871764\n",
            "89     \t [0.82693786 0.54162117 0.91390789]. \t  3.4067139007902187 \t 3.846238360871764\n",
            "90     \t [0.65301349 0.55285517 0.80905556]. \t  3.594863726551615 \t 3.846238360871764\n",
            "91     \t [0.43101863 0.52885257 0.85430421]. \t  3.830718262299848 \t 3.846238360871764\n",
            "92     \t [0.81014129 0.60786318 0.86603949]. \t  3.637166809322262 \t 3.846238360871764\n",
            "93     \t [0.06351166 0.5521649  0.86748768]. \t  3.807522639746976 \t 3.846238360871764\n",
            "94     \t [0.62576323 0.51226192 0.84455157]. \t  3.750242278998649 \t 3.846238360871764\n",
            "95     \t [0.58482981 0.50036073 0.83175755]. \t  3.689350764336959 \t 3.846238360871764\n",
            "96     \t [0.61142173 0.47833651 0.85100096]. \t  3.6307155309069885 \t 3.846238360871764\n",
            "97     \t [0.07172067 0.57194263 0.85674237]. \t  3.8218390775866355 \t 3.846238360871764\n",
            "98     \t [0.49108169 0.56039321 0.80981592]. \t  3.651998819692031 \t 3.846238360871764\n",
            "99     \t [0.46443963 0.54704141 0.86130741]. \t  3.8408960922270126 \t 3.846238360871764\n",
            "100    \t [0.32549712 0.46916938 0.90100188]. \t  3.3625193003391223 \t 3.846238360871764\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHz_Jg2_uv7E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1dee355c-15b2-48a3-ef59-9dee28cdf7eb"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 20 \r\n",
        "\r\n",
        "np.random.seed(run_num_20)\r\n",
        "surrogate_winner_20 = dtStudentProcess(d_cov_func, nu = df)\r\n",
        "\r\n",
        "winner_20 = dGPGO_stp(surrogate_winner_20, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_20.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.57051729 0.56452876 0.48844183]. \t  0.675391399411646 \t 0.675391399411646\n",
            "init   \t [0.33647775 0.37586818 0.53203587]. \t  0.5331349538596052 \t 0.675391399411646\n",
            "init   \t [0.06810629 0.58452906 0.23789776]. \t  0.14747335095307315 \t 0.675391399411646\n",
            "init   \t [0.16075658 0.15211915 0.12706922]. \t  0.48089725804912437 \t 0.675391399411646\n",
            "init   \t [0.32744117 0.69415387 0.35896647]. \t  0.6289408047543804 \t 0.675391399411646\n",
            "1      \t [0.57649302 0.98758952 1.        ]. \t  0.3670189101767849 \t 0.675391399411646\n",
            "2      \t [0.70037193 0.92655252 0.12231107]. \t  0.0042900165483392835 \t 0.675391399411646\n",
            "3      \t [0.5785137  0.20260817 0.0069126 ]. \t  0.10655124559279107 \t 0.675391399411646\n",
            "4      \t [0.36290861 0.89083502 0.4922098 ]. \t  \u001b[92m2.2148014243227467\u001b[0m \t 2.2148014243227467\n",
            "5      \t [0.04282238 0.98714938 0.99662789]. \t  0.3795921540393995 \t 2.2148014243227467\n",
            "6      \t [0.92033399 0.0074506  0.66564945]. \t  0.17245586676139496 \t 2.2148014243227467\n",
            "7      \t [0.60708299 0.9941949  0.55753696]. \t  1.2603013243800587 \t 2.2148014243227467\n",
            "8      \t [0.17302385 0.99136649 0.05150956]. \t  0.0014003871883348138 \t 2.2148014243227467\n",
            "9      \t [2.68104908e-15 3.36154677e-15 5.85161697e-15]. \t  0.06797411659013954 \t 2.2148014243227467\n",
            "10     \t [0.88533267 0.00227171 0.15635272]. \t  0.27225890779944084 \t 2.2148014243227467\n",
            "11     \t [0.10647859 0.98092739 0.46525789]. \t  2.1066673716276823 \t 2.2148014243227467\n",
            "12     \t [0.31721173 0.93594391 0.60743021]. \t  \u001b[92m2.442054500069666\u001b[0m \t 2.442054500069666\n",
            "13     \t [0.53192111 0.4270498  0.98622016]. \t  1.9302759658553803 \t 2.442054500069666\n",
            "14     \t [0.27246661 0.0111521  0.9823936 ]. \t  0.12238272002595811 \t 2.442054500069666\n",
            "15     \t [0.86248491 0.12271965 0.99133771]. \t  0.3232730606762705 \t 2.442054500069666\n",
            "16     \t [0.00937073 0.02897897 0.49221871]. \t  0.16312679389772336 \t 2.442054500069666\n",
            "17     \t [0.53724711 0.00294871 0.70164692]. \t  0.20910167908844243 \t 2.442054500069666\n",
            "18     \t [0.40301045 0.60554451 0.93470541]. \t  \u001b[92m3.1630403192371355\u001b[0m \t 3.1630403192371355\n",
            "19     \t [0.1167668  0.7034293  0.91663743]. \t  2.8429779387886462 \t 3.1630403192371355\n",
            "20     \t [0.27762849 0.75444346 0.9876322 ]. \t  1.6396872333894295 \t 3.1630403192371355\n",
            "21     \t [0.97881018 0.60563277 0.96128586]. \t  2.6429708219534245 \t 3.1630403192371355\n",
            "22     \t [0.74576519 0.68484156 0.87554805]. \t  \u001b[92m3.182178619891659\u001b[0m \t 3.182178619891659\n",
            "23     \t [0.42313791 0.00520794 0.03070825]. \t  0.16315287020397942 \t 3.182178619891659\n",
            "24     \t [0.66974431 0.6356458  0.90559059]. \t  \u001b[92m3.353972331184178\u001b[0m \t 3.353972331184178\n",
            "25     \t [0.         0.13393778 0.8947666 ]. \t  0.6676458845671615 \t 3.353972331184178\n",
            "26     \t [0.00772517 0.86296697 0.72977519]. \t  1.9550610945236682 \t 3.353972331184178\n",
            "27     \t [0.91952289 0.34097614 0.0681412 ]. \t  0.07420958078727719 \t 3.353972331184178\n",
            "28     \t [0.59645353 0.6711146  0.82935694]. \t  3.2765805867278397 \t 3.353972331184178\n",
            "29     \t [0.114978   0.51908126 0.79807242]. \t  \u001b[92m3.5560106746221587\u001b[0m \t 3.5560106746221587\n",
            "30     \t [0.06555592 0.59177126 0.77876539]. \t  3.3638382690174 \t 3.5560106746221587\n",
            "31     \t [0.02299746 0.57932969 0.86094885]. \t  \u001b[92m3.7930223750832366\u001b[0m \t 3.7930223750832366\n",
            "32     \t [0.05176222 0.45888546 0.98299612]. \t  2.11918944474665 \t 3.7930223750832366\n",
            "33     \t [0.1637544  0.59464698 0.83040775]. \t  3.7518677373552696 \t 3.7930223750832366\n",
            "34     \t [0.49274337 0.62433543 0.88043092]. \t  3.615335559215147 \t 3.7930223750832366\n",
            "35     \t [0.93458229 0.03306316 0.01945012]. \t  0.05651049060365551 \t 3.7930223750832366\n",
            "36     \t [0.10479787 0.57130297 0.86786336]. \t  \u001b[92m3.809945790880117\u001b[0m \t 3.809945790880117\n",
            "37     \t [0.92958736 0.79853746 0.99471636]. \t  1.250192899540528 \t 3.809945790880117\n",
            "38     \t [0.30837873 0.99525651 0.28900805]. \t  0.27649700471756156 \t 3.809945790880117\n",
            "39     \t [0.82761619 0.84147683 0.00492818]. \t  0.0004272018245921742 \t 3.809945790880117\n",
            "40     \t [0.96483886 0.86910586 0.23879751]. \t  0.01789904327325809 \t 3.809945790880117\n",
            "41     \t [0.20829501 0.65104079 0.81651671]. \t  3.4478208061262636 \t 3.809945790880117\n",
            "42     \t [0.04743262 0.38834427 0.81576907]. \t  2.924125514387116 \t 3.809945790880117\n",
            "43     \t [0.51749586 0.74622797 0.79126358]. \t  2.4965177246462047 \t 3.809945790880117\n",
            "44     \t [0.75450574 0.52484828 0.96691322]. \t  2.6089104437262423 \t 3.809945790880117\n",
            "45     \t [0.08103709 0.57039454 0.02447685]. \t  0.01728364626340532 \t 3.809945790880117\n",
            "46     \t [0.94105601 0.56803485 0.80433378]. \t  3.4068522705455466 \t 3.809945790880117\n",
            "47     \t [0.22533639 0.51855868 0.8803211 ]. \t  3.728732317136682 \t 3.809945790880117\n",
            "48     \t [0.99233224 0.45367171 0.79390988]. \t  3.1089146530040583 \t 3.809945790880117\n",
            "49     \t [0.38300977 0.60710909 0.81577577]. \t  3.6230649033630797 \t 3.809945790880117\n",
            "50     \t [0.15236472 0.55274551 0.86965429]. \t  \u001b[92m3.820029667217732\u001b[0m \t 3.820029667217732\n",
            "51     \t [0.15121873 0.47938618 0.78537694]. \t  3.324686878195748 \t 3.820029667217732\n",
            "52     \t [0.21951205 0.55564684 0.8667471 ]. \t  \u001b[92m3.838512278536112\u001b[0m \t 3.838512278536112\n",
            "53     \t [0.10339931 0.6664858  0.81699028]. \t  3.3485685362341395 \t 3.838512278536112\n",
            "54     \t [0.04019255 0.56151669 0.86524869]. \t  3.807067472726131 \t 3.838512278536112\n",
            "55     \t [0.07670264 0.61363776 0.84043449]. \t  3.704402461197554 \t 3.838512278536112\n",
            "56     \t [0.03362637 0.53816054 0.8645663 ]. \t  3.79578845338876 \t 3.838512278536112\n",
            "57     \t [0.72299324 0.53572423 0.8769911 ]. \t  3.7268555197090376 \t 3.838512278536112\n",
            "58     \t [0.73466907 0.48155124 0.82659685]. \t  3.555275742255817 \t 3.838512278536112\n",
            "59     \t [0.33566021 0.57681387 0.84819008]. \t  \u001b[92m3.8401138413354348\u001b[0m \t 3.8401138413354348\n",
            "60     \t [0.02384854 0.55888663 0.86491059]. \t  3.8036298266311954 \t 3.8401138413354348\n",
            "61     \t [0.23341733 0.64051695 0.81569272]. \t  3.499564467583094 \t 3.8401138413354348\n",
            "62     \t [0.02302265 0.5575992  0.85307503]. \t  3.8182835792861987 \t 3.8401138413354348\n",
            "63     \t [0.2762281  0.6460176  0.83408985]. \t  3.549104848709927 \t 3.8401138413354348\n",
            "64     \t [0.2228989  0.5925615  0.86417633]. \t  3.799315818059108 \t 3.8401138413354348\n",
            "65     \t [0.34129467 0.65034085 0.81731469]. \t  3.4388971867111247 \t 3.8401138413354348\n",
            "66     \t [0.21495804 0.60982321 0.84502789]. \t  3.7488135925207304 \t 3.8401138413354348\n",
            "67     \t [0.00265707 0.58348541 0.87143208]. \t  3.7531525688448735 \t 3.8401138413354348\n",
            "68     \t [0.82892334 0.48742159 0.8470391 ]. \t  3.607123876714047 \t 3.8401138413354348\n",
            "69     \t [0.63821709 0.55635012 0.86257742]. \t  3.804088258153546 \t 3.8401138413354348\n",
            "70     \t [0.16058403 0.61398209 0.90177002]. \t  3.5144685796264987 \t 3.8401138413354348\n",
            "71     \t [0.88547236 0.59021276 0.88291723]. \t  3.6115311830253103 \t 3.8401138413354348\n",
            "72     \t [0.15357368 0.57156244 0.80843218]. \t  3.6696620773083417 \t 3.8401138413354348\n",
            "73     \t [0.06558108 0.60683762 0.81033467]. \t  3.5884923082253715 \t 3.8401138413354348\n",
            "74     \t [0.08042055 0.5185713  0.83791381]. \t  3.771973238497603 \t 3.8401138413354348\n",
            "75     \t [0.4537319  0.58623293 0.86229391]. \t  3.807215344965617 \t 3.8401138413354348\n",
            "76     \t [0.81772393 0.49901856 0.8648822 ]. \t  3.6454774381009196 \t 3.8401138413354348\n",
            "77     \t [0.00530683 0.62859845 0.86995306]. \t  3.609218064149419 \t 3.8401138413354348\n",
            "78     \t [0.19715738 0.50336483 0.92125754]. \t  3.287622556071226 \t 3.8401138413354348\n",
            "79     \t [0.39725423 0.5261547  0.77963125]. \t  3.3783721916373786 \t 3.8401138413354348\n",
            "80     \t [0.01097528 0.88784014 0.08071432]. \t  0.004069435627051742 \t 3.8401138413354348\n",
            "81     \t [0.19144907 0.54173317 0.82521164]. \t  3.7826514903721855 \t 3.8401138413354348\n",
            "82     \t [0.26937765 0.46633095 0.8520106 ]. \t  3.5952512754577874 \t 3.8401138413354348\n",
            "83     \t [0.53236811 0.59647274 0.87525663]. \t  3.733906031019176 \t 3.8401138413354348\n",
            "84     \t [0.42804764 0.57731811 0.85816272]. \t  3.8323348136584388 \t 3.8401138413354348\n",
            "85     \t [0.05664048 0.55932333 0.84186255]. \t  3.8174045836538544 \t 3.8401138413354348\n",
            "86     \t [0.29655427 0.57018886 0.78510469]. \t  3.465094361491423 \t 3.8401138413354348\n",
            "87     \t [0.06502968 0.47875452 0.83555344]. \t  3.61639623640796 \t 3.8401138413354348\n",
            "88     \t [0.30745788 0.65592484 0.78629888]. \t  3.1998843366221172 \t 3.8401138413354348\n",
            "89     \t [0.24871024 0.61735135 0.79403939]. \t  3.446945146297706 \t 3.8401138413354348\n",
            "90     \t [0.2247422  0.59112813 0.82992123]. \t  3.7637348816884715 \t 3.8401138413354348\n",
            "91     \t [0.21732874 0.55758111 0.84414664]. \t  \u001b[92m3.8510374076303444\u001b[0m \t 3.8510374076303444\n",
            "92     \t [0.53914154 0.57982081 0.76219394]. \t  3.093104073815641 \t 3.8510374076303444\n",
            "93     \t [0.88018044 0.5182676  0.79801921]. \t  3.3873615019469794 \t 3.8510374076303444\n",
            "94     \t [0.60402896 0.68133915 0.82989748]. \t  3.1998498529152797 \t 3.8510374076303444\n",
            "95     \t [0.20372773 0.51560341 0.80428992]. \t  3.6105815437954325 \t 3.8510374076303444\n",
            "96     \t [0.01065858 0.70069972 0.79811263]. \t  2.9987180188212332 \t 3.8510374076303444\n",
            "97     \t [0.18174606 0.60886634 0.81560438]. \t  3.635167422482123 \t 3.8510374076303444\n",
            "98     \t [0.28094583 0.56704615 0.85348783]. \t  \u001b[92m3.8562383580084028\u001b[0m \t 3.8562383580084028\n",
            "99     \t [0.06873299 0.50326222 0.81432401]. \t  3.6240365559136496 \t 3.8562383580084028\n",
            "100    \t [0.37655113 0.57882966 0.80215087]. \t  3.5958014059591705 \t 3.8562383580084028\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUnhsKpCuv9o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3642156-8496-41f5-9757-e3f63580e49c"
      },
      "source": [
        "end_win = time.time()\r\n",
        "end_win\r\n",
        "\r\n",
        "time_win = end_win - start_win\r\n",
        "time_win"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16631.375532865524"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJG0SLpwuwAL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14c8f020-0348-4e2d-f81e-8ca3ed124f59"
      },
      "source": [
        "### Training regret minimization: run number = 1\r\n",
        "\r\n",
        "loser_output_1 = np.append(np.max(loser_1.GP.y[0:n_init]),loser_1.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_1 = np.append(np.max(winner_1.GP.y[0:n_init]),winner_1.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_1 = np.log(y_global_orig - loser_output_1)\r\n",
        "regret_winner_1 = np.log(y_global_orig - winner_output_1)\r\n",
        "\r\n",
        "train_regret_loser_1 = min_max_array(regret_loser_1)\r\n",
        "train_regret_winner_1 = min_max_array(regret_winner_1)\r\n",
        "\r\n",
        "min_train_regret_loser_1 = min(train_regret_loser_1)\r\n",
        "min_train_regret_winner_1 = min(train_regret_winner_1)\r\n",
        "\r\n",
        "min_train_regret_loser_1, min_train_regret_winner_1"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-3.282774727971788, -5.152773175058427)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lA9eZf0uwCx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fea8747c-062b-4a85-cb50-e4f0eaaee05a"
      },
      "source": [
        "### Training regret minimization: run number = 2\r\n",
        "\r\n",
        "loser_output_2 = np.append(np.max(loser_2.GP.y[0:n_init]),loser_2.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_2 = np.append(np.max(winner_2.GP.y[0:n_init]),winner_2.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_2 = np.log(y_global_orig - loser_output_2)\r\n",
        "regret_winner_2 = np.log(y_global_orig - winner_output_2)\r\n",
        "\r\n",
        "train_regret_loser_2 = min_max_array(regret_loser_2)\r\n",
        "train_regret_winner_2 = min_max_array(regret_winner_2)\r\n",
        "\r\n",
        "min_train_regret_loser_2 = min(train_regret_loser_2)\r\n",
        "min_train_regret_winner_2 = min(train_regret_winner_2)\r\n",
        "\r\n",
        "min_train_regret_loser_2, min_train_regret_winner_2"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-1.8749236392611095, -5.751287490358574)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glrTGcpAuwFa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57c1267c-b7bb-4af8-92a7-e28dd4400503"
      },
      "source": [
        "### Training regret minimization: run number = 3\r\n",
        "\r\n",
        "loser_output_3 = np.append(np.max(loser_3.GP.y[0:n_init]),loser_3.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_3 = np.append(np.max(winner_3.GP.y[0:n_init]),winner_3.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_3 = np.log(y_global_orig - loser_output_3)\r\n",
        "regret_winner_3 = np.log(y_global_orig - winner_output_3)\r\n",
        "\r\n",
        "train_regret_loser_3 = min_max_array(regret_loser_3)\r\n",
        "train_regret_winner_3 = min_max_array(regret_winner_3)\r\n",
        "\r\n",
        "min_train_regret_loser_3 = min(train_regret_loser_3)\r\n",
        "min_train_regret_winner_3 = min(train_regret_winner_3)\r\n",
        "\r\n",
        "min_train_regret_loser_3, min_train_regret_winner_3"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-2.6160962144747533, -5.140706141792039)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LaRwfbxeuwIR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7001ff7-352f-4aec-e0a7-ae5b89a673cd"
      },
      "source": [
        "### Training regret minimization: run number = 4\r\n",
        "\r\n",
        "loser_output_4 = np.append(np.max(loser_4.GP.y[0:n_init]),loser_4.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_4 = np.append(np.max(winner_4.GP.y[0:n_init]),winner_4.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_4 = np.log(y_global_orig - loser_output_4)\r\n",
        "regret_winner_4 = np.log(y_global_orig - winner_output_4)\r\n",
        "\r\n",
        "train_regret_loser_4 = min_max_array(regret_loser_4)\r\n",
        "train_regret_winner_4 = min_max_array(regret_winner_4)\r\n",
        "\r\n",
        "min_train_regret_loser_4 = min(train_regret_loser_4)\r\n",
        "min_train_regret_winner_4 = min(train_regret_winner_4)\r\n",
        "\r\n",
        "min_train_regret_loser_4, min_train_regret_winner_4"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-2.854269340274048, -4.528867319093919)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Nb5NkfyuwKp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78f544a1-2d28-4b19-c28d-fe412f4238f9"
      },
      "source": [
        "### Training regret minimization: run number = 5\r\n",
        "\r\n",
        "loser_output_5 = np.append(np.max(loser_5.GP.y[0:n_init]),loser_5.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_5 = np.append(np.max(winner_5.GP.y[0:n_init]),winner_5.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_5 = np.log(y_global_orig - loser_output_5)\r\n",
        "regret_winner_5 = np.log(y_global_orig - winner_output_5)\r\n",
        "\r\n",
        "train_regret_loser_5 = min_max_array(regret_loser_5)\r\n",
        "train_regret_winner_5 = min_max_array(regret_winner_5)\r\n",
        "\r\n",
        "min_train_regret_loser_5 = min(train_regret_loser_5)\r\n",
        "min_train_regret_winner_5 = min(train_regret_winner_5)\r\n",
        "\r\n",
        "min_train_regret_loser_5, min_train_regret_winner_5"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-2.9864402258065152, -2.6003446034691278)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0Q-WfXbuwNg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d32c77f-e18b-4c0c-a564-f3f22916300c"
      },
      "source": [
        "### Training regret minimization: run number = 6\r\n",
        "\r\n",
        "loser_output_6 = np.append(np.max(loser_6.GP.y[0:n_init]),loser_6.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_6 = np.append(np.max(winner_6.GP.y[0:n_init]),winner_6.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_6 = np.log(y_global_orig - loser_output_6)\r\n",
        "regret_winner_6 = np.log(y_global_orig - winner_output_6)\r\n",
        "\r\n",
        "train_regret_loser_6 = min_max_array(regret_loser_6)\r\n",
        "train_regret_winner_6 = min_max_array(regret_winner_6)\r\n",
        "\r\n",
        "min_train_regret_loser_6 = min(train_regret_loser_6)\r\n",
        "min_train_regret_winner_6 = min(train_regret_winner_6)\r\n",
        "\r\n",
        "min_train_regret_loser_6, min_train_regret_winner_6"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-2.652528982265508, -3.2217774127179686)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqqS7VLcuwPy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd0655af-c5ad-4ba9-eb8d-71c06046d218"
      },
      "source": [
        "### Training regret minimization: run number = 7\r\n",
        "\r\n",
        "loser_output_7 = np.append(np.max(loser_7.GP.y[0:n_init]),loser_7.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_7 = np.append(np.max(winner_7.GP.y[0:n_init]),winner_7.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_7 = np.log(y_global_orig - loser_output_7)\r\n",
        "regret_winner_7 = np.log(y_global_orig - winner_output_7)\r\n",
        "\r\n",
        "train_regret_loser_7 = min_max_array(regret_loser_7)\r\n",
        "train_regret_winner_7 = min_max_array(regret_winner_7)\r\n",
        "\r\n",
        "min_train_regret_loser_7 = min(train_regret_loser_7)\r\n",
        "min_train_regret_winner_7 = min(train_regret_winner_7)\r\n",
        "\r\n",
        "min_train_regret_loser_7, min_train_regret_winner_7"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-2.427527665591056, -3.815760337650508)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOi5iX8guwSS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f88c0a85-7fb0-45a9-d974-74aa96a0705e"
      },
      "source": [
        "### Training regret minimization: run number = 8\r\n",
        "\r\n",
        "loser_output_8 = np.append(np.max(loser_8.GP.y[0:n_init]),loser_8.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_8 = np.append(np.max(winner_8.GP.y[0:n_init]),winner_8.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_8 = np.log(y_global_orig - loser_output_8)\r\n",
        "regret_winner_8 = np.log(y_global_orig - winner_output_8)\r\n",
        "\r\n",
        "train_regret_loser_8 = min_max_array(regret_loser_8)\r\n",
        "train_regret_winner_8 = min_max_array(regret_winner_8)\r\n",
        "\r\n",
        "min_train_regret_loser_8 = min(train_regret_loser_8)\r\n",
        "min_train_regret_winner_8 = min(train_regret_winner_8)\r\n",
        "\r\n",
        "min_train_regret_loser_8, min_train_regret_winner_8"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-2.405697629363659, -3.8364598761541124)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFVApeazuwU5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bf1938c-f2f3-445b-b9fb-db918441a3b5"
      },
      "source": [
        "### Training regret minimization: run number = 9\r\n",
        "\r\n",
        "loser_output_9 = np.append(np.max(loser_9.GP.y[0:n_init]),loser_9.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_9 = np.append(np.max(winner_9.GP.y[0:n_init]),winner_9.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_9 = np.log(y_global_orig - loser_output_9)\r\n",
        "regret_winner_9 = np.log(y_global_orig - winner_output_9)\r\n",
        "\r\n",
        "train_regret_loser_9 = min_max_array(regret_loser_9)\r\n",
        "train_regret_winner_9 = min_max_array(regret_winner_9)\r\n",
        "\r\n",
        "min_train_regret_loser_9 = min(train_regret_loser_9)\r\n",
        "min_train_regret_winner_9 = min(train_regret_winner_9)\r\n",
        "\r\n",
        "min_train_regret_loser_9, min_train_regret_winner_9"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-3.732056583522499, -3.2290510401285055)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g92jk9WJuwXb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "adc8b629-4c9e-48a2-f57c-a5ed347987f7"
      },
      "source": [
        "### Training regret minimization: run number = 10\r\n",
        "\r\n",
        "loser_output_10 = np.append(np.max(loser_10.GP.y[0:n_init]),loser_10.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_10 = np.append(np.max(winner_10.GP.y[0:n_init]),winner_10.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_10 = np.log(y_global_orig - loser_output_10)\r\n",
        "regret_winner_10 = np.log(y_global_orig - winner_output_10)\r\n",
        "\r\n",
        "train_regret_loser_10 = min_max_array(regret_loser_10)\r\n",
        "train_regret_winner_10 = min_max_array(regret_winner_10)\r\n",
        "\r\n",
        "min_train_regret_loser_10 = min(train_regret_loser_10)\r\n",
        "min_train_regret_winner_10 = min(train_regret_winner_10)\r\n",
        "\r\n",
        "min_train_regret_loser_10, min_train_regret_winner_10"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-3.301444590661275, -4.737286933990644)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmcF1x-NuwZz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ca30278-f35e-4eeb-aab1-8c67e50804c7"
      },
      "source": [
        "### Training regret minimization: run number = 11\r\n",
        "\r\n",
        "loser_output_11 = np.append(np.max(loser_11.GP.y[0:n_init]),loser_11.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_11 = np.append(np.max(winner_11.GP.y[0:n_init]),winner_11.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_11 = np.log(y_global_orig - loser_output_11)\r\n",
        "regret_winner_11 = np.log(y_global_orig - winner_output_11)\r\n",
        "\r\n",
        "train_regret_loser_11 = min_max_array(regret_loser_11)\r\n",
        "train_regret_winner_11 = min_max_array(regret_winner_11)\r\n",
        "\r\n",
        "min_train_regret_loser_11 = min(train_regret_loser_11)\r\n",
        "min_train_regret_winner_11 = min(train_regret_winner_11)\r\n",
        "\r\n",
        "min_train_regret_loser_11, min_train_regret_winner_11"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-4.451291654399481, -4.451291654399481)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8axVhb6uwcc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2a4577e-e8e0-4ee6-98f1-4e69aa7c74b3"
      },
      "source": [
        "### Training regret minimization: run number = 12\r\n",
        "\r\n",
        "loser_output_12 = np.append(np.max(loser_12.GP.y[0:n_init]),loser_12.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_12 = np.append(np.max(winner_12.GP.y[0:n_init]),winner_12.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_12 = np.log(y_global_orig - loser_output_12)\r\n",
        "regret_winner_12 = np.log(y_global_orig - winner_output_12)\r\n",
        "\r\n",
        "train_regret_loser_12 = min_max_array(regret_loser_12)\r\n",
        "train_regret_winner_12 = min_max_array(regret_winner_12)\r\n",
        "\r\n",
        "min_train_regret_loser_12 = min(train_regret_loser_12)\r\n",
        "min_train_regret_winner_12 = min(train_regret_winner_12)\r\n",
        "\r\n",
        "min_train_regret_loser_12, min_train_regret_winner_12"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-3.1007531149503116, -3.987545218937542)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzlrL8XFuwfB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "582b6479-6eb0-44a9-eb28-088fb6ea1807"
      },
      "source": [
        "### Training regret minimization: run number = 13\r\n",
        "\r\n",
        "loser_output_13 = np.append(np.max(loser_13.GP.y[0:n_init]),loser_13.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_13 = np.append(np.max(winner_13.GP.y[0:n_init]),winner_13.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_13 = np.log(y_global_orig - loser_output_13)\r\n",
        "regret_winner_13 = np.log(y_global_orig - winner_output_13)\r\n",
        "\r\n",
        "train_regret_loser_13 = min_max_array(regret_loser_13)\r\n",
        "train_regret_winner_13 = min_max_array(regret_winner_13)\r\n",
        "\r\n",
        "min_train_regret_loser_13 = min(train_regret_loser_13)\r\n",
        "min_train_regret_winner_13 = min(train_regret_winner_13)\r\n",
        "\r\n",
        "min_train_regret_loser_13, min_train_regret_winner_13"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-1.9578012996071412, -4.873161420551539)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3uZlLJ1quwh6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91bf0d04-d1d6-4c99-872f-6d06efcf390e"
      },
      "source": [
        "### Training regret minimization: run number = 14\r\n",
        "\r\n",
        "loser_output_14 = np.append(np.max(loser_14.GP.y[0:n_init]),loser_14.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_14 = np.append(np.max(winner_14.GP.y[0:n_init]),winner_14.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_14 = np.log(y_global_orig - loser_output_14)\r\n",
        "regret_winner_14 = np.log(y_global_orig - winner_output_14)\r\n",
        "\r\n",
        "train_regret_loser_14 = min_max_array(regret_loser_14)\r\n",
        "train_regret_winner_14 = min_max_array(regret_winner_14)\r\n",
        "\r\n",
        "min_train_regret_loser_14 = min(train_regret_loser_14)\r\n",
        "min_train_regret_winner_14 = min(train_regret_winner_14)\r\n",
        "\r\n",
        "min_train_regret_loser_14, min_train_regret_winner_14"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-1.4423529446398398, -4.1099834541932925)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVBcHRiMuwjx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "031145b3-fcb9-4fab-c8cd-7b3f35ebced1"
      },
      "source": [
        "### Training regret minimization: run number = 15\r\n",
        "\r\n",
        "loser_output_15 = np.append(np.max(loser_15.GP.y[0:n_init]),loser_15.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_15 = np.append(np.max(winner_15.GP.y[0:n_init]),winner_15.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_15 = np.log(y_global_orig - loser_output_15)\r\n",
        "regret_winner_15 = np.log(y_global_orig - winner_output_15)\r\n",
        "\r\n",
        "train_regret_loser_15 = min_max_array(regret_loser_15)\r\n",
        "train_regret_winner_15 = min_max_array(regret_winner_15)\r\n",
        "\r\n",
        "min_train_regret_loser_15 = min(train_regret_loser_15)\r\n",
        "min_train_regret_winner_15 = min(train_regret_winner_15)\r\n",
        "\r\n",
        "min_train_regret_loser_15, min_train_regret_winner_15"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-2.431852117724113, -5.32789461171894)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8nZ_DrKuwnA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "255858fb-5d39-44be-dff4-12a935409cbc"
      },
      "source": [
        "### Training regret minimization: run number = 16\r\n",
        "\r\n",
        "loser_output_16 = np.append(np.max(loser_16.GP.y[0:n_init]),loser_16.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_16 = np.append(np.max(winner_16.GP.y[0:n_init]),winner_16.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_16 = np.log(y_global_orig - loser_output_16)\r\n",
        "regret_winner_16 = np.log(y_global_orig - winner_output_16)\r\n",
        "\r\n",
        "train_regret_loser_16 = min_max_array(regret_loser_16)\r\n",
        "train_regret_winner_16 = min_max_array(regret_winner_16)\r\n",
        "\r\n",
        "min_train_regret_loser_16 = min(train_regret_loser_16)\r\n",
        "min_train_regret_winner_16 = min(train_regret_winner_16)\r\n",
        "\r\n",
        "min_train_regret_loser_16, min_train_regret_winner_16"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-3.2310411317387597, -4.760698676180839)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eg6qzzQFuwpU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8681da4c-5809-417a-c15c-a85631fe0bab"
      },
      "source": [
        "### Training regret minimization: run number = 17\r\n",
        "\r\n",
        "loser_output_17 = np.append(np.max(loser_17.GP.y[0:n_init]),loser_17.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_17 = np.append(np.max(winner_17.GP.y[0:n_init]),winner_17.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_17 = np.log(y_global_orig - loser_output_17)\r\n",
        "regret_winner_17 = np.log(y_global_orig - winner_output_17)\r\n",
        "\r\n",
        "train_regret_loser_17 = min_max_array(regret_loser_17)\r\n",
        "train_regret_winner_17 = min_max_array(regret_winner_17)\r\n",
        "\r\n",
        "min_train_regret_loser_17 = min(train_regret_loser_17)\r\n",
        "min_train_regret_winner_17 = min(train_regret_winner_17)\r\n",
        "\r\n",
        "min_train_regret_loser_17, min_train_regret_winner_17"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-2.02432998517116, -4.646517326456852)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrRtDLMFuwsB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa1402fe-6358-4a0e-ce2c-33cf7c3a58e8"
      },
      "source": [
        "### Training regret minimization: run number = 18\r\n",
        "\r\n",
        "loser_output_18 = np.append(np.max(loser_18.GP.y[0:n_init]),loser_18.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_18 = np.append(np.max(winner_18.GP.y[0:n_init]),winner_18.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_18 = np.log(y_global_orig - loser_output_18)\r\n",
        "regret_winner_18 = np.log(y_global_orig - winner_output_18)\r\n",
        "\r\n",
        "train_regret_loser_18 = min_max_array(regret_loser_18)\r\n",
        "train_regret_winner_18 = min_max_array(regret_winner_18)\r\n",
        "\r\n",
        "min_train_regret_loser_18 = min(train_regret_loser_18)\r\n",
        "min_train_regret_winner_18 = min(train_regret_winner_18)\r\n",
        "\r\n",
        "min_train_regret_loser_18, min_train_regret_winner_18"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-2.3352725279655404, -4.993015377261186)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkmSu4CUuwuh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ab8db10-e451-4b9f-a4d9-ac822e438a9d"
      },
      "source": [
        "### Training regret minimization: run number = 19\r\n",
        "\r\n",
        "loser_output_19 = np.append(np.max(loser_19.GP.y[0:n_init]),loser_19.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_19 = np.append(np.max(winner_19.GP.y[0:n_init]),winner_19.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_19 = np.log(y_global_orig - loser_output_19)\r\n",
        "regret_winner_19 = np.log(y_global_orig - winner_output_19)\r\n",
        "\r\n",
        "train_regret_loser_19 = min_max_array(regret_loser_19)\r\n",
        "train_regret_winner_19 = min_max_array(regret_winner_19)\r\n",
        "\r\n",
        "min_train_regret_loser_19 = min(train_regret_loser_19)\r\n",
        "min_train_regret_winner_19 = min(train_regret_winner_19)\r\n",
        "\r\n",
        "min_train_regret_loser_19, min_train_regret_winner_19"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-1.2665005804060472, -4.101874493436068)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eymecjwkuwxb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18f6e0e1-1fd2-4ade-ac11-987fc4d2c3bd"
      },
      "source": [
        "### Training regret minimization: run number = 20\r\n",
        "\r\n",
        "loser_output_20 = np.append(np.max(loser_20.GP.y[0:n_init]),loser_20.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_20 = np.append(np.max(winner_20.GP.y[0:n_init]),winner_20.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_20 = np.log(y_global_orig - loser_output_20)\r\n",
        "regret_winner_20 = np.log(y_global_orig - winner_output_20)\r\n",
        "\r\n",
        "train_regret_loser_20 = min_max_array(regret_loser_20)\r\n",
        "train_regret_winner_20 = min_max_array(regret_winner_20)\r\n",
        "\r\n",
        "min_train_regret_loser_20 = min(train_regret_loser_20)\r\n",
        "min_train_regret_winner_20 = min(train_regret_winner_20)\r\n",
        "\r\n",
        "min_train_regret_loser_20, min_train_regret_winner_20"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-2.167359607010987, -5.029567075974041)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5P6kq2Kuw0d"
      },
      "source": [
        "# Iteration1 :\r\n",
        "\r\n",
        "slice1 = 0\r\n",
        "\r\n",
        "loser1 = [train_regret_loser_1[slice1],\r\n",
        "       train_regret_loser_2[slice1],\r\n",
        "       train_regret_loser_3[slice1],\r\n",
        "       train_regret_loser_4[slice1],\r\n",
        "       train_regret_loser_5[slice1],\r\n",
        "       train_regret_loser_6[slice1],\r\n",
        "       train_regret_loser_7[slice1],\r\n",
        "       train_regret_loser_8[slice1],\r\n",
        "       train_regret_loser_9[slice1],\r\n",
        "       train_regret_loser_10[slice1],\r\n",
        "       train_regret_loser_11[slice1],\r\n",
        "       train_regret_loser_12[slice1],\r\n",
        "       train_regret_loser_13[slice1],\r\n",
        "       train_regret_loser_14[slice1],\r\n",
        "       train_regret_loser_15[slice1],\r\n",
        "       train_regret_loser_16[slice1],\r\n",
        "       train_regret_loser_17[slice1],\r\n",
        "       train_regret_loser_18[slice1],\r\n",
        "       train_regret_loser_19[slice1],\r\n",
        "       train_regret_loser_20[slice1]]\r\n",
        "\r\n",
        "winner1 = [train_regret_winner_1[slice1],\r\n",
        "       train_regret_winner_2[slice1],\r\n",
        "       train_regret_winner_3[slice1],\r\n",
        "       train_regret_winner_4[slice1],\r\n",
        "       train_regret_winner_5[slice1],\r\n",
        "       train_regret_winner_6[slice1],\r\n",
        "       train_regret_winner_7[slice1],\r\n",
        "       train_regret_winner_8[slice1],\r\n",
        "       train_regret_winner_9[slice1],\r\n",
        "       train_regret_winner_10[slice1],\r\n",
        "       train_regret_winner_11[slice1],\r\n",
        "       train_regret_winner_12[slice1],\r\n",
        "       train_regret_winner_13[slice1],\r\n",
        "       train_regret_winner_14[slice1],\r\n",
        "       train_regret_winner_15[slice1],\r\n",
        "       train_regret_winner_16[slice1],\r\n",
        "       train_regret_winner_17[slice1],\r\n",
        "       train_regret_winner_18[slice1],\r\n",
        "       train_regret_winner_19[slice1],\r\n",
        "       train_regret_winner_20[slice1]]\r\n",
        "\r\n",
        "loser1_results = pd.DataFrame(loser1).sort_values(by=[0], ascending=False)\r\n",
        "winner1_results = pd.DataFrame(winner1).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser1 = np.asarray(loser1_results[4:5][0])[0]\r\n",
        "median_loser1 = np.asarray(loser1_results[9:10][0])[0]\r\n",
        "upper_loser1 = np.asarray(loser1_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner1 = np.asarray(winner1_results[4:5][0])[0]\r\n",
        "median_winner1 = np.asarray(winner1_results[9:10][0])[0]\r\n",
        "upper_winner1 = np.asarray(winner1_results[14:15][0])[0]"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXcnj2pNuw3X"
      },
      "source": [
        "# Iteration11 :\r\n",
        "\r\n",
        "slice11 = 10\r\n",
        "\r\n",
        "loser11 = [train_regret_loser_1[slice11],\r\n",
        "       train_regret_loser_2[slice11],\r\n",
        "       train_regret_loser_3[slice11],\r\n",
        "       train_regret_loser_4[slice11],\r\n",
        "       train_regret_loser_5[slice11],\r\n",
        "       train_regret_loser_6[slice11],\r\n",
        "       train_regret_loser_7[slice11],\r\n",
        "       train_regret_loser_8[slice11],\r\n",
        "       train_regret_loser_9[slice11],\r\n",
        "       train_regret_loser_10[slice11],\r\n",
        "       train_regret_loser_11[slice11],\r\n",
        "       train_regret_loser_12[slice11],\r\n",
        "       train_regret_loser_13[slice11],\r\n",
        "       train_regret_loser_14[slice11],\r\n",
        "       train_regret_loser_15[slice11],\r\n",
        "       train_regret_loser_16[slice11],\r\n",
        "       train_regret_loser_17[slice11],\r\n",
        "       train_regret_loser_18[slice11],\r\n",
        "       train_regret_loser_19[slice11],\r\n",
        "       train_regret_loser_20[slice11]]\r\n",
        "\r\n",
        "winner11 = [train_regret_winner_1[slice11],\r\n",
        "       train_regret_winner_2[slice11],\r\n",
        "       train_regret_winner_3[slice11],\r\n",
        "       train_regret_winner_4[slice11],\r\n",
        "       train_regret_winner_5[slice11],\r\n",
        "       train_regret_winner_6[slice11],\r\n",
        "       train_regret_winner_7[slice11],\r\n",
        "       train_regret_winner_8[slice11],\r\n",
        "       train_regret_winner_9[slice11],\r\n",
        "       train_regret_winner_10[slice11],\r\n",
        "       train_regret_winner_11[slice11],\r\n",
        "       train_regret_winner_12[slice11],\r\n",
        "       train_regret_winner_13[slice11],\r\n",
        "       train_regret_winner_14[slice11],\r\n",
        "       train_regret_winner_15[slice11],\r\n",
        "       train_regret_winner_16[slice11],\r\n",
        "       train_regret_winner_17[slice11],\r\n",
        "       train_regret_winner_18[slice11],\r\n",
        "       train_regret_winner_19[slice11],\r\n",
        "       train_regret_winner_20[slice11]]\r\n",
        "\r\n",
        "loser11_results = pd.DataFrame(loser11).sort_values(by=[0], ascending=False)\r\n",
        "winner11_results = pd.DataFrame(winner11).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser11 = np.asarray(loser11_results[4:5][0])[0]\r\n",
        "median_loser11 = np.asarray(loser11_results[9:10][0])[0]\r\n",
        "upper_loser11 = np.asarray(loser11_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner11 = np.asarray(winner11_results[4:5][0])[0]\r\n",
        "median_winner11 = np.asarray(winner11_results[9:10][0])[0]\r\n",
        "upper_winner11 = np.asarray(winner11_results[14:15][0])[0]"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxQa6BAVuw6L"
      },
      "source": [
        "# Iteration21 :\r\n",
        "\r\n",
        "slice21 = 20\r\n",
        "\r\n",
        "loser21 = [train_regret_loser_1[slice21],\r\n",
        "       train_regret_loser_2[slice21],\r\n",
        "       train_regret_loser_3[slice21],\r\n",
        "       train_regret_loser_4[slice21],\r\n",
        "       train_regret_loser_5[slice21],\r\n",
        "       train_regret_loser_6[slice21],\r\n",
        "       train_regret_loser_7[slice21],\r\n",
        "       train_regret_loser_8[slice21],\r\n",
        "       train_regret_loser_9[slice21],\r\n",
        "       train_regret_loser_10[slice21],\r\n",
        "       train_regret_loser_11[slice21],\r\n",
        "       train_regret_loser_12[slice21],\r\n",
        "       train_regret_loser_13[slice21],\r\n",
        "       train_regret_loser_14[slice21],\r\n",
        "       train_regret_loser_15[slice21],\r\n",
        "       train_regret_loser_16[slice21],\r\n",
        "       train_regret_loser_17[slice21],\r\n",
        "       train_regret_loser_18[slice21],\r\n",
        "       train_regret_loser_19[slice21],\r\n",
        "       train_regret_loser_20[slice21]]\r\n",
        "\r\n",
        "winner21 = [train_regret_winner_1[slice21],\r\n",
        "       train_regret_winner_2[slice21],\r\n",
        "       train_regret_winner_3[slice21],\r\n",
        "       train_regret_winner_4[slice21],\r\n",
        "       train_regret_winner_5[slice21],\r\n",
        "       train_regret_winner_6[slice21],\r\n",
        "       train_regret_winner_7[slice21],\r\n",
        "       train_regret_winner_8[slice21],\r\n",
        "       train_regret_winner_9[slice21],\r\n",
        "       train_regret_winner_10[slice21],\r\n",
        "       train_regret_winner_11[slice21],\r\n",
        "       train_regret_winner_12[slice21],\r\n",
        "       train_regret_winner_13[slice21],\r\n",
        "       train_regret_winner_14[slice21],\r\n",
        "       train_regret_winner_15[slice21],\r\n",
        "       train_regret_winner_16[slice21],\r\n",
        "       train_regret_winner_17[slice21],\r\n",
        "       train_regret_winner_18[slice21],\r\n",
        "       train_regret_winner_19[slice21],\r\n",
        "       train_regret_winner_20[slice21]]\r\n",
        "\r\n",
        "loser21_results = pd.DataFrame(loser21).sort_values(by=[0], ascending=False)\r\n",
        "winner21_results = pd.DataFrame(winner21).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser21 = np.asarray(loser21_results[4:5][0])[0]\r\n",
        "median_loser21 = np.asarray(loser21_results[9:10][0])[0]\r\n",
        "upper_loser21 = np.asarray(loser21_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner21 = np.asarray(winner21_results[4:5][0])[0]\r\n",
        "median_winner21 = np.asarray(winner21_results[9:10][0])[0]\r\n",
        "upper_winner21 = np.asarray(winner21_results[14:15][0])[0]"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wdJdIsWuw8r"
      },
      "source": [
        "# Iteration31 :\r\n",
        "\r\n",
        "slice31 = 30\r\n",
        "\r\n",
        "loser31 = [train_regret_loser_1[slice31],\r\n",
        "       train_regret_loser_2[slice31],\r\n",
        "       train_regret_loser_3[slice31],\r\n",
        "       train_regret_loser_4[slice31],\r\n",
        "       train_regret_loser_5[slice31],\r\n",
        "       train_regret_loser_6[slice31],\r\n",
        "       train_regret_loser_7[slice31],\r\n",
        "       train_regret_loser_8[slice31],\r\n",
        "       train_regret_loser_9[slice31],\r\n",
        "       train_regret_loser_10[slice31],\r\n",
        "       train_regret_loser_11[slice31],\r\n",
        "       train_regret_loser_12[slice31],\r\n",
        "       train_regret_loser_13[slice31],\r\n",
        "       train_regret_loser_14[slice31],\r\n",
        "       train_regret_loser_15[slice31],\r\n",
        "       train_regret_loser_16[slice31],\r\n",
        "       train_regret_loser_17[slice31],\r\n",
        "       train_regret_loser_18[slice31],\r\n",
        "       train_regret_loser_19[slice31],\r\n",
        "       train_regret_loser_20[slice31]]\r\n",
        "\r\n",
        "winner31 = [train_regret_winner_1[slice31],\r\n",
        "       train_regret_winner_2[slice31],\r\n",
        "       train_regret_winner_3[slice31],\r\n",
        "       train_regret_winner_4[slice31],\r\n",
        "       train_regret_winner_5[slice31],\r\n",
        "       train_regret_winner_6[slice31],\r\n",
        "       train_regret_winner_7[slice31],\r\n",
        "       train_regret_winner_8[slice31],\r\n",
        "       train_regret_winner_9[slice31],\r\n",
        "       train_regret_winner_10[slice31],\r\n",
        "       train_regret_winner_11[slice31],\r\n",
        "       train_regret_winner_12[slice31],\r\n",
        "       train_regret_winner_13[slice31],\r\n",
        "       train_regret_winner_14[slice31],\r\n",
        "       train_regret_winner_15[slice31],\r\n",
        "       train_regret_winner_16[slice31],\r\n",
        "       train_regret_winner_17[slice31],\r\n",
        "       train_regret_winner_18[slice31],\r\n",
        "       train_regret_winner_19[slice31],\r\n",
        "       train_regret_winner_20[slice31]]\r\n",
        "\r\n",
        "loser31_results = pd.DataFrame(loser31).sort_values(by=[0], ascending=False)\r\n",
        "winner31_results = pd.DataFrame(winner31).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser31 = np.asarray(loser31_results[4:5][0])[0]\r\n",
        "median_loser31 = np.asarray(loser31_results[9:10][0])[0]\r\n",
        "upper_loser31 = np.asarray(loser31_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner31 = np.asarray(winner31_results[4:5][0])[0]\r\n",
        "median_winner31 = np.asarray(winner31_results[9:10][0])[0]\r\n",
        "upper_winner31 = np.asarray(winner31_results[14:15][0])[0]"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vWZBDufuw_Z"
      },
      "source": [
        "# Iteration41 :\r\n",
        "\r\n",
        "slice41 = 40\r\n",
        "\r\n",
        "loser41 = [train_regret_loser_1[slice41],\r\n",
        "       train_regret_loser_2[slice41],\r\n",
        "       train_regret_loser_3[slice41],\r\n",
        "       train_regret_loser_4[slice41],\r\n",
        "       train_regret_loser_5[slice41],\r\n",
        "       train_regret_loser_6[slice41],\r\n",
        "       train_regret_loser_7[slice41],\r\n",
        "       train_regret_loser_8[slice41],\r\n",
        "       train_regret_loser_9[slice41],\r\n",
        "       train_regret_loser_10[slice41],\r\n",
        "       train_regret_loser_11[slice41],\r\n",
        "       train_regret_loser_12[slice41],\r\n",
        "       train_regret_loser_13[slice41],\r\n",
        "       train_regret_loser_14[slice41],\r\n",
        "       train_regret_loser_15[slice41],\r\n",
        "       train_regret_loser_16[slice41],\r\n",
        "       train_regret_loser_17[slice41],\r\n",
        "       train_regret_loser_18[slice41],\r\n",
        "       train_regret_loser_19[slice41],\r\n",
        "       train_regret_loser_20[slice41]]\r\n",
        "\r\n",
        "winner41 = [train_regret_winner_1[slice41],\r\n",
        "       train_regret_winner_2[slice41],\r\n",
        "       train_regret_winner_3[slice41],\r\n",
        "       train_regret_winner_4[slice41],\r\n",
        "       train_regret_winner_5[slice41],\r\n",
        "       train_regret_winner_6[slice41],\r\n",
        "       train_regret_winner_7[slice41],\r\n",
        "       train_regret_winner_8[slice41],\r\n",
        "       train_regret_winner_9[slice41],\r\n",
        "       train_regret_winner_10[slice41],\r\n",
        "       train_regret_winner_11[slice41],\r\n",
        "       train_regret_winner_12[slice41],\r\n",
        "       train_regret_winner_13[slice41],\r\n",
        "       train_regret_winner_14[slice41],\r\n",
        "       train_regret_winner_15[slice41],\r\n",
        "       train_regret_winner_16[slice41],\r\n",
        "       train_regret_winner_17[slice41],\r\n",
        "       train_regret_winner_18[slice41],\r\n",
        "       train_regret_winner_19[slice41],\r\n",
        "       train_regret_winner_20[slice41]]\r\n",
        "\r\n",
        "loser41_results = pd.DataFrame(loser41).sort_values(by=[0], ascending=False)\r\n",
        "winner41_results = pd.DataFrame(winner41).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser41 = np.asarray(loser41_results[4:5][0])[0]\r\n",
        "median_loser41 = np.asarray(loser41_results[9:10][0])[0]\r\n",
        "upper_loser41 = np.asarray(loser41_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner41 = np.asarray(winner41_results[4:5][0])[0]\r\n",
        "median_winner41 = np.asarray(winner41_results[9:10][0])[0]\r\n",
        "upper_winner41 = np.asarray(winner41_results[14:15][0])[0]"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwi_GzjiuxB6"
      },
      "source": [
        "# Iteration51 :\r\n",
        "\r\n",
        "slice51 = 50\r\n",
        "\r\n",
        "loser51 = [train_regret_loser_1[slice51],\r\n",
        "       train_regret_loser_2[slice51],\r\n",
        "       train_regret_loser_3[slice51],\r\n",
        "       train_regret_loser_4[slice51],\r\n",
        "       train_regret_loser_5[slice51],\r\n",
        "       train_regret_loser_6[slice51],\r\n",
        "       train_regret_loser_7[slice51],\r\n",
        "       train_regret_loser_8[slice51],\r\n",
        "       train_regret_loser_9[slice51],\r\n",
        "       train_regret_loser_10[slice51],\r\n",
        "       train_regret_loser_11[slice51],\r\n",
        "       train_regret_loser_12[slice51],\r\n",
        "       train_regret_loser_13[slice51],\r\n",
        "       train_regret_loser_14[slice51],\r\n",
        "       train_regret_loser_15[slice51],\r\n",
        "       train_regret_loser_16[slice51],\r\n",
        "       train_regret_loser_17[slice51],\r\n",
        "       train_regret_loser_18[slice51],\r\n",
        "       train_regret_loser_19[slice51],\r\n",
        "       train_regret_loser_20[slice51]]\r\n",
        "\r\n",
        "winner51 = [train_regret_winner_1[slice51],\r\n",
        "       train_regret_winner_2[slice51],\r\n",
        "       train_regret_winner_3[slice51],\r\n",
        "       train_regret_winner_4[slice51],\r\n",
        "       train_regret_winner_5[slice51],\r\n",
        "       train_regret_winner_6[slice51],\r\n",
        "       train_regret_winner_7[slice51],\r\n",
        "       train_regret_winner_8[slice51],\r\n",
        "       train_regret_winner_9[slice51],\r\n",
        "       train_regret_winner_10[slice51],\r\n",
        "       train_regret_winner_11[slice51],\r\n",
        "       train_regret_winner_12[slice51],\r\n",
        "       train_regret_winner_13[slice51],\r\n",
        "       train_regret_winner_14[slice51],\r\n",
        "       train_regret_winner_15[slice51],\r\n",
        "       train_regret_winner_16[slice51],\r\n",
        "       train_regret_winner_17[slice51],\r\n",
        "       train_regret_winner_18[slice51],\r\n",
        "       train_regret_winner_19[slice51],\r\n",
        "       train_regret_winner_20[slice51]]\r\n",
        "\r\n",
        "loser51_results = pd.DataFrame(loser51).sort_values(by=[0], ascending=False)\r\n",
        "winner51_results = pd.DataFrame(winner51).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser51 = np.asarray(loser51_results[4:5][0])[0]\r\n",
        "median_loser51 = np.asarray(loser51_results[9:10][0])[0]\r\n",
        "upper_loser51 = np.asarray(loser51_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner51 = np.asarray(winner51_results[4:5][0])[0]\r\n",
        "median_winner51 = np.asarray(winner51_results[9:10][0])[0]\r\n",
        "upper_winner51 = np.asarray(winner51_results[14:15][0])[0]"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wk9nlS6puxFZ"
      },
      "source": [
        "# Iteration61 :\r\n",
        "\r\n",
        "slice61 = 60\r\n",
        "\r\n",
        "loser61 = [train_regret_loser_1[slice61],\r\n",
        "       train_regret_loser_2[slice61],\r\n",
        "       train_regret_loser_3[slice61],\r\n",
        "       train_regret_loser_4[slice61],\r\n",
        "       train_regret_loser_5[slice61],\r\n",
        "       train_regret_loser_6[slice61],\r\n",
        "       train_regret_loser_7[slice61],\r\n",
        "       train_regret_loser_8[slice61],\r\n",
        "       train_regret_loser_9[slice61],\r\n",
        "       train_regret_loser_10[slice61],\r\n",
        "       train_regret_loser_11[slice61],\r\n",
        "       train_regret_loser_12[slice61],\r\n",
        "       train_regret_loser_13[slice61],\r\n",
        "       train_regret_loser_14[slice61],\r\n",
        "       train_regret_loser_15[slice61],\r\n",
        "       train_regret_loser_16[slice61],\r\n",
        "       train_regret_loser_17[slice61],\r\n",
        "       train_regret_loser_18[slice61],\r\n",
        "       train_regret_loser_19[slice61],\r\n",
        "       train_regret_loser_20[slice61]]\r\n",
        "\r\n",
        "winner61 = [train_regret_winner_1[slice61],\r\n",
        "       train_regret_winner_2[slice61],\r\n",
        "       train_regret_winner_3[slice61],\r\n",
        "       train_regret_winner_4[slice61],\r\n",
        "       train_regret_winner_5[slice61],\r\n",
        "       train_regret_winner_6[slice61],\r\n",
        "       train_regret_winner_7[slice61],\r\n",
        "       train_regret_winner_8[slice61],\r\n",
        "       train_regret_winner_9[slice61],\r\n",
        "       train_regret_winner_10[slice61],\r\n",
        "       train_regret_winner_11[slice61],\r\n",
        "       train_regret_winner_12[slice61],\r\n",
        "       train_regret_winner_13[slice61],\r\n",
        "       train_regret_winner_14[slice61],\r\n",
        "       train_regret_winner_15[slice61],\r\n",
        "       train_regret_winner_16[slice61],\r\n",
        "       train_regret_winner_17[slice61],\r\n",
        "       train_regret_winner_18[slice61],\r\n",
        "       train_regret_winner_19[slice61],\r\n",
        "       train_regret_winner_20[slice61]]\r\n",
        "\r\n",
        "loser61_results = pd.DataFrame(loser61).sort_values(by=[0], ascending=False)\r\n",
        "winner61_results = pd.DataFrame(winner61).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser61 = np.asarray(loser61_results[4:5][0])[0]\r\n",
        "median_loser61 = np.asarray(loser61_results[9:10][0])[0]\r\n",
        "upper_loser61 = np.asarray(loser61_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner61 = np.asarray(winner61_results[4:5][0])[0]\r\n",
        "median_winner61 = np.asarray(winner61_results[9:10][0])[0]\r\n",
        "upper_winner61 = np.asarray(winner61_results[14:15][0])[0]"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMus9vDpuxHN"
      },
      "source": [
        "# Iteration71 :\r\n",
        "\r\n",
        "slice71 = 70\r\n",
        "\r\n",
        "loser71 = [train_regret_loser_1[slice71],\r\n",
        "       train_regret_loser_2[slice71],\r\n",
        "       train_regret_loser_3[slice71],\r\n",
        "       train_regret_loser_4[slice71],\r\n",
        "       train_regret_loser_5[slice71],\r\n",
        "       train_regret_loser_6[slice71],\r\n",
        "       train_regret_loser_7[slice71],\r\n",
        "       train_regret_loser_8[slice71],\r\n",
        "       train_regret_loser_9[slice71],\r\n",
        "       train_regret_loser_10[slice71],\r\n",
        "       train_regret_loser_11[slice71],\r\n",
        "       train_regret_loser_12[slice71],\r\n",
        "       train_regret_loser_13[slice71],\r\n",
        "       train_regret_loser_14[slice71],\r\n",
        "       train_regret_loser_15[slice71],\r\n",
        "       train_regret_loser_16[slice71],\r\n",
        "       train_regret_loser_17[slice71],\r\n",
        "       train_regret_loser_18[slice71],\r\n",
        "       train_regret_loser_19[slice71],\r\n",
        "       train_regret_loser_20[slice71]]\r\n",
        "\r\n",
        "winner71 = [train_regret_winner_1[slice71],\r\n",
        "       train_regret_winner_2[slice71],\r\n",
        "       train_regret_winner_3[slice71],\r\n",
        "       train_regret_winner_4[slice71],\r\n",
        "       train_regret_winner_5[slice71],\r\n",
        "       train_regret_winner_6[slice71],\r\n",
        "       train_regret_winner_7[slice71],\r\n",
        "       train_regret_winner_8[slice71],\r\n",
        "       train_regret_winner_9[slice71],\r\n",
        "       train_regret_winner_10[slice71],\r\n",
        "       train_regret_winner_11[slice71],\r\n",
        "       train_regret_winner_12[slice71],\r\n",
        "       train_regret_winner_13[slice71],\r\n",
        "       train_regret_winner_14[slice71],\r\n",
        "       train_regret_winner_15[slice71],\r\n",
        "       train_regret_winner_16[slice71],\r\n",
        "       train_regret_winner_17[slice71],\r\n",
        "       train_regret_winner_18[slice71],\r\n",
        "       train_regret_winner_19[slice71],\r\n",
        "       train_regret_winner_20[slice71]]\r\n",
        "\r\n",
        "loser71_results = pd.DataFrame(loser71).sort_values(by=[0], ascending=False)\r\n",
        "winner71_results = pd.DataFrame(winner71).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser71 = np.asarray(loser71_results[4:5][0])[0]\r\n",
        "median_loser71 = np.asarray(loser71_results[9:10][0])[0]\r\n",
        "upper_loser71 = np.asarray(loser71_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner71 = np.asarray(winner71_results[4:5][0])[0]\r\n",
        "median_winner71 = np.asarray(winner71_results[9:10][0])[0]\r\n",
        "upper_winner71 = np.asarray(winner71_results[14:15][0])[0]"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6Mg0x9vuxL4"
      },
      "source": [
        "# Iteration81 :\r\n",
        "\r\n",
        "slice81 = 80\r\n",
        "\r\n",
        "loser81 = [train_regret_loser_1[slice81],\r\n",
        "       train_regret_loser_2[slice81],\r\n",
        "       train_regret_loser_3[slice81],\r\n",
        "       train_regret_loser_4[slice81],\r\n",
        "       train_regret_loser_5[slice81],\r\n",
        "       train_regret_loser_6[slice81],\r\n",
        "       train_regret_loser_7[slice81],\r\n",
        "       train_regret_loser_8[slice81],\r\n",
        "       train_regret_loser_9[slice81],\r\n",
        "       train_regret_loser_10[slice81],\r\n",
        "       train_regret_loser_11[slice81],\r\n",
        "       train_regret_loser_12[slice81],\r\n",
        "       train_regret_loser_13[slice81],\r\n",
        "       train_regret_loser_14[slice81],\r\n",
        "       train_regret_loser_15[slice81],\r\n",
        "       train_regret_loser_16[slice81],\r\n",
        "       train_regret_loser_17[slice81],\r\n",
        "       train_regret_loser_18[slice81],\r\n",
        "       train_regret_loser_19[slice81],\r\n",
        "       train_regret_loser_20[slice81]]\r\n",
        "\r\n",
        "winner81 = [train_regret_winner_1[slice81],\r\n",
        "       train_regret_winner_2[slice81],\r\n",
        "       train_regret_winner_3[slice81],\r\n",
        "       train_regret_winner_4[slice81],\r\n",
        "       train_regret_winner_5[slice81],\r\n",
        "       train_regret_winner_6[slice81],\r\n",
        "       train_regret_winner_7[slice81],\r\n",
        "       train_regret_winner_8[slice81],\r\n",
        "       train_regret_winner_9[slice81],\r\n",
        "       train_regret_winner_10[slice81],\r\n",
        "       train_regret_winner_11[slice81],\r\n",
        "       train_regret_winner_12[slice81],\r\n",
        "       train_regret_winner_13[slice81],\r\n",
        "       train_regret_winner_14[slice81],\r\n",
        "       train_regret_winner_15[slice81],\r\n",
        "       train_regret_winner_16[slice81],\r\n",
        "       train_regret_winner_17[slice81],\r\n",
        "       train_regret_winner_18[slice81],\r\n",
        "       train_regret_winner_19[slice81],\r\n",
        "       train_regret_winner_20[slice81]]\r\n",
        "\r\n",
        "loser81_results = pd.DataFrame(loser81).sort_values(by=[0], ascending=False)\r\n",
        "winner81_results = pd.DataFrame(winner81).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser81 = np.asarray(loser81_results[4:5][0])[0]\r\n",
        "median_loser81 = np.asarray(loser81_results[9:10][0])[0]\r\n",
        "upper_loser81 = np.asarray(loser81_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner81 = np.asarray(winner81_results[4:5][0])[0]\r\n",
        "median_winner81 = np.asarray(winner81_results[9:10][0])[0]\r\n",
        "upper_winner81 = np.asarray(winner81_results[14:15][0])[0]"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Gu9K8XlteV4"
      },
      "source": [
        "# Iteration91 :\r\n",
        "\r\n",
        "slice91 = 90\r\n",
        "\r\n",
        "loser91 = [train_regret_loser_1[slice91],\r\n",
        "       train_regret_loser_2[slice91],\r\n",
        "       train_regret_loser_3[slice91],\r\n",
        "       train_regret_loser_4[slice91],\r\n",
        "       train_regret_loser_5[slice91],\r\n",
        "       train_regret_loser_6[slice91],\r\n",
        "       train_regret_loser_7[slice91],\r\n",
        "       train_regret_loser_8[slice91],\r\n",
        "       train_regret_loser_9[slice91],\r\n",
        "       train_regret_loser_10[slice91],\r\n",
        "       train_regret_loser_11[slice91],\r\n",
        "       train_regret_loser_12[slice91],\r\n",
        "       train_regret_loser_13[slice91],\r\n",
        "       train_regret_loser_14[slice91],\r\n",
        "       train_regret_loser_15[slice91],\r\n",
        "       train_regret_loser_16[slice91],\r\n",
        "       train_regret_loser_17[slice91],\r\n",
        "       train_regret_loser_18[slice91],\r\n",
        "       train_regret_loser_19[slice91],\r\n",
        "       train_regret_loser_20[slice91]]\r\n",
        "\r\n",
        "winner91 = [train_regret_winner_1[slice91],\r\n",
        "       train_regret_winner_2[slice91],\r\n",
        "       train_regret_winner_3[slice91],\r\n",
        "       train_regret_winner_4[slice91],\r\n",
        "       train_regret_winner_5[slice91],\r\n",
        "       train_regret_winner_6[slice91],\r\n",
        "       train_regret_winner_7[slice91],\r\n",
        "       train_regret_winner_8[slice91],\r\n",
        "       train_regret_winner_9[slice91],\r\n",
        "       train_regret_winner_10[slice91],\r\n",
        "       train_regret_winner_11[slice91],\r\n",
        "       train_regret_winner_12[slice91],\r\n",
        "       train_regret_winner_13[slice91],\r\n",
        "       train_regret_winner_14[slice91],\r\n",
        "       train_regret_winner_15[slice91],\r\n",
        "       train_regret_winner_16[slice91],\r\n",
        "       train_regret_winner_17[slice91],\r\n",
        "       train_regret_winner_18[slice91],\r\n",
        "       train_regret_winner_19[slice91],\r\n",
        "       train_regret_winner_20[slice91]]\r\n",
        "\r\n",
        "loser91_results = pd.DataFrame(loser91).sort_values(by=[0], ascending=False)\r\n",
        "winner91_results = pd.DataFrame(winner91).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser91 = np.asarray(loser91_results[4:5][0])[0]\r\n",
        "median_loser91 = np.asarray(loser91_results[9:10][0])[0]\r\n",
        "upper_loser91 = np.asarray(loser91_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner91 = np.asarray(winner91_results[4:5][0])[0]\r\n",
        "median_winner91 = np.asarray(winner91_results[9:10][0])[0]\r\n",
        "upper_winner91 = np.asarray(winner91_results[14:15][0])[0]"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVVeft6steXv"
      },
      "source": [
        "# Iteration101 :\r\n",
        "\r\n",
        "slice101 = 100\r\n",
        "\r\n",
        "loser101 = [train_regret_loser_1[slice101],\r\n",
        "       train_regret_loser_2[slice101],\r\n",
        "       train_regret_loser_3[slice101],\r\n",
        "       train_regret_loser_4[slice101],\r\n",
        "       train_regret_loser_5[slice101],\r\n",
        "       train_regret_loser_6[slice101],\r\n",
        "       train_regret_loser_7[slice101],\r\n",
        "       train_regret_loser_8[slice101],\r\n",
        "       train_regret_loser_9[slice101],\r\n",
        "       train_regret_loser_10[slice101],\r\n",
        "       train_regret_loser_11[slice101],\r\n",
        "       train_regret_loser_12[slice101],\r\n",
        "       train_regret_loser_13[slice101],\r\n",
        "       train_regret_loser_14[slice101],\r\n",
        "       train_regret_loser_15[slice101],\r\n",
        "       train_regret_loser_16[slice101],\r\n",
        "       train_regret_loser_17[slice101],\r\n",
        "       train_regret_loser_18[slice101],\r\n",
        "       train_regret_loser_19[slice101],\r\n",
        "       train_regret_loser_20[slice101]]\r\n",
        "\r\n",
        "winner101 = [train_regret_winner_1[slice101],\r\n",
        "       train_regret_winner_2[slice101],\r\n",
        "       train_regret_winner_3[slice101],\r\n",
        "       train_regret_winner_4[slice101],\r\n",
        "       train_regret_winner_5[slice101],\r\n",
        "       train_regret_winner_6[slice101],\r\n",
        "       train_regret_winner_7[slice101],\r\n",
        "       train_regret_winner_8[slice101],\r\n",
        "       train_regret_winner_9[slice101],\r\n",
        "       train_regret_winner_10[slice101],\r\n",
        "       train_regret_winner_11[slice101],\r\n",
        "       train_regret_winner_12[slice101],\r\n",
        "       train_regret_winner_13[slice101],\r\n",
        "       train_regret_winner_14[slice101],\r\n",
        "       train_regret_winner_15[slice101],\r\n",
        "       train_regret_winner_16[slice101],\r\n",
        "       train_regret_winner_17[slice101],\r\n",
        "       train_regret_winner_18[slice101],\r\n",
        "       train_regret_winner_19[slice101],\r\n",
        "       train_regret_winner_20[slice101]]\r\n",
        "\r\n",
        "loser101_results = pd.DataFrame(loser101).sort_values(by=[0], ascending=False)\r\n",
        "winner101_results = pd.DataFrame(winner101).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser101 = np.asarray(loser101_results[4:5][0])[0]\r\n",
        "median_loser101 = np.asarray(loser101_results[9:10][0])[0]\r\n",
        "upper_loser101 = np.asarray(loser101_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner101 = np.asarray(winner101_results[4:5][0])[0]\r\n",
        "median_winner101 = np.asarray(winner101_results[9:10][0])[0]\r\n",
        "upper_winner101 = np.asarray(winner101_results[14:15][0])[0]"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "maqbtTXXtebT"
      },
      "source": [
        "# Iteration2 :\r\n",
        "\r\n",
        "slice2 = 1\r\n",
        "\r\n",
        "loser2 = [train_regret_loser_1[slice2],\r\n",
        "       train_regret_loser_2[slice2],\r\n",
        "       train_regret_loser_3[slice2],\r\n",
        "       train_regret_loser_4[slice2],\r\n",
        "       train_regret_loser_5[slice2],\r\n",
        "       train_regret_loser_6[slice2],\r\n",
        "       train_regret_loser_7[slice2],\r\n",
        "       train_regret_loser_8[slice2],\r\n",
        "       train_regret_loser_9[slice2],\r\n",
        "       train_regret_loser_10[slice2],\r\n",
        "       train_regret_loser_11[slice2],\r\n",
        "       train_regret_loser_12[slice2],\r\n",
        "       train_regret_loser_13[slice2],\r\n",
        "       train_regret_loser_14[slice2],\r\n",
        "       train_regret_loser_15[slice2],\r\n",
        "       train_regret_loser_16[slice2],\r\n",
        "       train_regret_loser_17[slice2],\r\n",
        "       train_regret_loser_18[slice2],\r\n",
        "       train_regret_loser_19[slice2],\r\n",
        "       train_regret_loser_20[slice2]]\r\n",
        "\r\n",
        "winner2 = [train_regret_winner_1[slice2],\r\n",
        "       train_regret_winner_2[slice2],\r\n",
        "       train_regret_winner_3[slice2],\r\n",
        "       train_regret_winner_4[slice2],\r\n",
        "       train_regret_winner_5[slice2],\r\n",
        "       train_regret_winner_6[slice2],\r\n",
        "       train_regret_winner_7[slice2],\r\n",
        "       train_regret_winner_8[slice2],\r\n",
        "       train_regret_winner_9[slice2],\r\n",
        "       train_regret_winner_10[slice2],\r\n",
        "       train_regret_winner_11[slice2],\r\n",
        "       train_regret_winner_12[slice2],\r\n",
        "       train_regret_winner_13[slice2],\r\n",
        "       train_regret_winner_14[slice2],\r\n",
        "       train_regret_winner_15[slice2],\r\n",
        "       train_regret_winner_16[slice2],\r\n",
        "       train_regret_winner_17[slice2],\r\n",
        "       train_regret_winner_18[slice2],\r\n",
        "       train_regret_winner_19[slice2],\r\n",
        "       train_regret_winner_20[slice2]]\r\n",
        "\r\n",
        "loser2_results = pd.DataFrame(loser2).sort_values(by=[0], ascending=False)\r\n",
        "winner2_results = pd.DataFrame(winner2).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser2 = np.asarray(loser2_results[4:5][0])[0]\r\n",
        "median_loser2 = np.asarray(loser2_results[9:10][0])[0]\r\n",
        "upper_loser2 = np.asarray(loser2_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner2 = np.asarray(winner2_results[4:5][0])[0]\r\n",
        "median_winner2 = np.asarray(winner2_results[9:10][0])[0]\r\n",
        "upper_winner2 = np.asarray(winner2_results[14:15][0])[0]"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWIBh0dQw-ZM"
      },
      "source": [
        "# Iteration12 :\r\n",
        "\r\n",
        "slice12 = 11\r\n",
        "\r\n",
        "loser12 = [train_regret_loser_1[slice12],\r\n",
        "       train_regret_loser_2[slice12],\r\n",
        "       train_regret_loser_3[slice12],\r\n",
        "       train_regret_loser_4[slice12],\r\n",
        "       train_regret_loser_5[slice12],\r\n",
        "       train_regret_loser_6[slice12],\r\n",
        "       train_regret_loser_7[slice12],\r\n",
        "       train_regret_loser_8[slice12],\r\n",
        "       train_regret_loser_9[slice12],\r\n",
        "       train_regret_loser_10[slice12],\r\n",
        "       train_regret_loser_11[slice12],\r\n",
        "       train_regret_loser_12[slice12],\r\n",
        "       train_regret_loser_13[slice12],\r\n",
        "       train_regret_loser_14[slice12],\r\n",
        "       train_regret_loser_15[slice12],\r\n",
        "       train_regret_loser_16[slice12],\r\n",
        "       train_regret_loser_17[slice12],\r\n",
        "       train_regret_loser_18[slice12],\r\n",
        "       train_regret_loser_19[slice12],\r\n",
        "       train_regret_loser_20[slice12]]\r\n",
        "\r\n",
        "winner12 = [train_regret_winner_1[slice12],\r\n",
        "       train_regret_winner_2[slice12],\r\n",
        "       train_regret_winner_3[slice12],\r\n",
        "       train_regret_winner_4[slice12],\r\n",
        "       train_regret_winner_5[slice12],\r\n",
        "       train_regret_winner_6[slice12],\r\n",
        "       train_regret_winner_7[slice12],\r\n",
        "       train_regret_winner_8[slice12],\r\n",
        "       train_regret_winner_9[slice12],\r\n",
        "       train_regret_winner_10[slice12],\r\n",
        "       train_regret_winner_11[slice12],\r\n",
        "       train_regret_winner_12[slice12],\r\n",
        "       train_regret_winner_13[slice12],\r\n",
        "       train_regret_winner_14[slice12],\r\n",
        "       train_regret_winner_15[slice12],\r\n",
        "       train_regret_winner_16[slice12],\r\n",
        "       train_regret_winner_17[slice12],\r\n",
        "       train_regret_winner_18[slice12],\r\n",
        "       train_regret_winner_19[slice12],\r\n",
        "       train_regret_winner_20[slice12]]\r\n",
        "\r\n",
        "loser12_results = pd.DataFrame(loser12).sort_values(by=[0], ascending=False)\r\n",
        "winner12_results = pd.DataFrame(winner12).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser12 = np.asarray(loser12_results[4:5][0])[0]\r\n",
        "median_loser12 = np.asarray(loser12_results[9:10][0])[0]\r\n",
        "upper_loser12 = np.asarray(loser12_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner12 = np.asarray(winner12_results[4:5][0])[0]\r\n",
        "median_winner12 = np.asarray(winner12_results[9:10][0])[0]\r\n",
        "upper_winner12 = np.asarray(winner12_results[14:15][0])[0]"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjIhsei3w-cV"
      },
      "source": [
        "# Iteration22 :\r\n",
        "\r\n",
        "slice22 = 21\r\n",
        "\r\n",
        "loser22 = [train_regret_loser_1[slice22],\r\n",
        "       train_regret_loser_2[slice22],\r\n",
        "       train_regret_loser_3[slice22],\r\n",
        "       train_regret_loser_4[slice22],\r\n",
        "       train_regret_loser_5[slice22],\r\n",
        "       train_regret_loser_6[slice22],\r\n",
        "       train_regret_loser_7[slice22],\r\n",
        "       train_regret_loser_8[slice22],\r\n",
        "       train_regret_loser_9[slice22],\r\n",
        "       train_regret_loser_10[slice22],\r\n",
        "       train_regret_loser_11[slice22],\r\n",
        "       train_regret_loser_12[slice22],\r\n",
        "       train_regret_loser_13[slice22],\r\n",
        "       train_regret_loser_14[slice22],\r\n",
        "       train_regret_loser_15[slice22],\r\n",
        "       train_regret_loser_16[slice22],\r\n",
        "       train_regret_loser_17[slice22],\r\n",
        "       train_regret_loser_18[slice22],\r\n",
        "       train_regret_loser_19[slice22],\r\n",
        "       train_regret_loser_20[slice22]]\r\n",
        "\r\n",
        "winner22 = [train_regret_winner_1[slice22],\r\n",
        "       train_regret_winner_2[slice22],\r\n",
        "       train_regret_winner_3[slice22],\r\n",
        "       train_regret_winner_4[slice22],\r\n",
        "       train_regret_winner_5[slice22],\r\n",
        "       train_regret_winner_6[slice22],\r\n",
        "       train_regret_winner_7[slice22],\r\n",
        "       train_regret_winner_8[slice22],\r\n",
        "       train_regret_winner_9[slice22],\r\n",
        "       train_regret_winner_10[slice22],\r\n",
        "       train_regret_winner_11[slice22],\r\n",
        "       train_regret_winner_12[slice22],\r\n",
        "       train_regret_winner_13[slice22],\r\n",
        "       train_regret_winner_14[slice22],\r\n",
        "       train_regret_winner_15[slice22],\r\n",
        "       train_regret_winner_16[slice22],\r\n",
        "       train_regret_winner_17[slice22],\r\n",
        "       train_regret_winner_18[slice22],\r\n",
        "       train_regret_winner_19[slice22],\r\n",
        "       train_regret_winner_20[slice22]]\r\n",
        "\r\n",
        "loser22_results = pd.DataFrame(loser22).sort_values(by=[0], ascending=False)\r\n",
        "winner22_results = pd.DataFrame(winner22).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser22 = np.asarray(loser22_results[4:5][0])[0]\r\n",
        "median_loser22 = np.asarray(loser22_results[9:10][0])[0]\r\n",
        "upper_loser22 = np.asarray(loser22_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner22 = np.asarray(winner22_results[4:5][0])[0]\r\n",
        "median_winner22 = np.asarray(winner22_results[9:10][0])[0]\r\n",
        "upper_winner22 = np.asarray(winner22_results[14:15][0])[0]"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ur9p9GkHw-fq"
      },
      "source": [
        "# Iteration32 :\r\n",
        "\r\n",
        "slice32 = 31\r\n",
        "\r\n",
        "loser32 = [train_regret_loser_1[slice32],\r\n",
        "       train_regret_loser_2[slice32],\r\n",
        "       train_regret_loser_3[slice32],\r\n",
        "       train_regret_loser_4[slice32],\r\n",
        "       train_regret_loser_5[slice32],\r\n",
        "       train_regret_loser_6[slice32],\r\n",
        "       train_regret_loser_7[slice32],\r\n",
        "       train_regret_loser_8[slice32],\r\n",
        "       train_regret_loser_9[slice32],\r\n",
        "       train_regret_loser_10[slice32],\r\n",
        "       train_regret_loser_11[slice32],\r\n",
        "       train_regret_loser_12[slice32],\r\n",
        "       train_regret_loser_13[slice32],\r\n",
        "       train_regret_loser_14[slice32],\r\n",
        "       train_regret_loser_15[slice32],\r\n",
        "       train_regret_loser_16[slice32],\r\n",
        "       train_regret_loser_17[slice32],\r\n",
        "       train_regret_loser_18[slice32],\r\n",
        "       train_regret_loser_19[slice32],\r\n",
        "       train_regret_loser_20[slice32]]\r\n",
        "\r\n",
        "winner32 = [train_regret_winner_1[slice32],\r\n",
        "       train_regret_winner_2[slice32],\r\n",
        "       train_regret_winner_3[slice32],\r\n",
        "       train_regret_winner_4[slice32],\r\n",
        "       train_regret_winner_5[slice32],\r\n",
        "       train_regret_winner_6[slice32],\r\n",
        "       train_regret_winner_7[slice32],\r\n",
        "       train_regret_winner_8[slice32],\r\n",
        "       train_regret_winner_9[slice32],\r\n",
        "       train_regret_winner_10[slice32],\r\n",
        "       train_regret_winner_11[slice32],\r\n",
        "       train_regret_winner_12[slice32],\r\n",
        "       train_regret_winner_13[slice32],\r\n",
        "       train_regret_winner_14[slice32],\r\n",
        "       train_regret_winner_15[slice32],\r\n",
        "       train_regret_winner_16[slice32],\r\n",
        "       train_regret_winner_17[slice32],\r\n",
        "       train_regret_winner_18[slice32],\r\n",
        "       train_regret_winner_19[slice32],\r\n",
        "       train_regret_winner_20[slice32]]\r\n",
        "\r\n",
        "loser32_results = pd.DataFrame(loser32).sort_values(by=[0], ascending=False)\r\n",
        "winner32_results = pd.DataFrame(winner32).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser32 = np.asarray(loser32_results[4:5][0])[0]\r\n",
        "median_loser32 = np.asarray(loser32_results[9:10][0])[0]\r\n",
        "upper_loser32 = np.asarray(loser32_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner32 = np.asarray(winner32_results[4:5][0])[0]\r\n",
        "median_winner32 = np.asarray(winner32_results[9:10][0])[0]\r\n",
        "upper_winner32 = np.asarray(winner32_results[14:15][0])[0]"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jd_rzHB8w-hy"
      },
      "source": [
        "# Iteration42 :\r\n",
        "\r\n",
        "slice42 = 41\r\n",
        "\r\n",
        "loser42 = [train_regret_loser_1[slice42],\r\n",
        "       train_regret_loser_2[slice42],\r\n",
        "       train_regret_loser_3[slice42],\r\n",
        "       train_regret_loser_4[slice42],\r\n",
        "       train_regret_loser_5[slice42],\r\n",
        "       train_regret_loser_6[slice42],\r\n",
        "       train_regret_loser_7[slice42],\r\n",
        "       train_regret_loser_8[slice42],\r\n",
        "       train_regret_loser_9[slice42],\r\n",
        "       train_regret_loser_10[slice42],\r\n",
        "       train_regret_loser_11[slice42],\r\n",
        "       train_regret_loser_12[slice42],\r\n",
        "       train_regret_loser_13[slice42],\r\n",
        "       train_regret_loser_14[slice42],\r\n",
        "       train_regret_loser_15[slice42],\r\n",
        "       train_regret_loser_16[slice42],\r\n",
        "       train_regret_loser_17[slice42],\r\n",
        "       train_regret_loser_18[slice42],\r\n",
        "       train_regret_loser_19[slice42],\r\n",
        "       train_regret_loser_20[slice42]]\r\n",
        "\r\n",
        "winner42 = [train_regret_winner_1[slice42],\r\n",
        "       train_regret_winner_2[slice42],\r\n",
        "       train_regret_winner_3[slice42],\r\n",
        "       train_regret_winner_4[slice42],\r\n",
        "       train_regret_winner_5[slice42],\r\n",
        "       train_regret_winner_6[slice42],\r\n",
        "       train_regret_winner_7[slice42],\r\n",
        "       train_regret_winner_8[slice42],\r\n",
        "       train_regret_winner_9[slice42],\r\n",
        "       train_regret_winner_10[slice42],\r\n",
        "       train_regret_winner_11[slice42],\r\n",
        "       train_regret_winner_12[slice42],\r\n",
        "       train_regret_winner_13[slice42],\r\n",
        "       train_regret_winner_14[slice42],\r\n",
        "       train_regret_winner_15[slice42],\r\n",
        "       train_regret_winner_16[slice42],\r\n",
        "       train_regret_winner_17[slice42],\r\n",
        "       train_regret_winner_18[slice42],\r\n",
        "       train_regret_winner_19[slice42],\r\n",
        "       train_regret_winner_20[slice42]]\r\n",
        "\r\n",
        "loser42_results = pd.DataFrame(loser42).sort_values(by=[0], ascending=False)\r\n",
        "winner42_results = pd.DataFrame(winner42).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser42 = np.asarray(loser42_results[4:5][0])[0]\r\n",
        "median_loser42 = np.asarray(loser42_results[9:10][0])[0]\r\n",
        "upper_loser42 = np.asarray(loser42_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner42 = np.asarray(winner42_results[4:5][0])[0]\r\n",
        "median_winner42 = np.asarray(winner42_results[9:10][0])[0]\r\n",
        "upper_winner42 = np.asarray(winner42_results[14:15][0])[0]"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKUUG6vAw-kz"
      },
      "source": [
        "# Iteration52 :\r\n",
        "\r\n",
        "slice52 = 51\r\n",
        "\r\n",
        "loser52 = [train_regret_loser_1[slice52],\r\n",
        "       train_regret_loser_2[slice52],\r\n",
        "       train_regret_loser_3[slice52],\r\n",
        "       train_regret_loser_4[slice52],\r\n",
        "       train_regret_loser_5[slice52],\r\n",
        "       train_regret_loser_6[slice52],\r\n",
        "       train_regret_loser_7[slice52],\r\n",
        "       train_regret_loser_8[slice52],\r\n",
        "       train_regret_loser_9[slice52],\r\n",
        "       train_regret_loser_10[slice52],\r\n",
        "       train_regret_loser_11[slice52],\r\n",
        "       train_regret_loser_12[slice52],\r\n",
        "       train_regret_loser_13[slice52],\r\n",
        "       train_regret_loser_14[slice52],\r\n",
        "       train_regret_loser_15[slice52],\r\n",
        "       train_regret_loser_16[slice52],\r\n",
        "       train_regret_loser_17[slice52],\r\n",
        "       train_regret_loser_18[slice52],\r\n",
        "       train_regret_loser_19[slice52],\r\n",
        "       train_regret_loser_20[slice52]]\r\n",
        "\r\n",
        "winner52 = [train_regret_winner_1[slice52],\r\n",
        "       train_regret_winner_2[slice52],\r\n",
        "       train_regret_winner_3[slice52],\r\n",
        "       train_regret_winner_4[slice52],\r\n",
        "       train_regret_winner_5[slice52],\r\n",
        "       train_regret_winner_6[slice52],\r\n",
        "       train_regret_winner_7[slice52],\r\n",
        "       train_regret_winner_8[slice52],\r\n",
        "       train_regret_winner_9[slice52],\r\n",
        "       train_regret_winner_10[slice52],\r\n",
        "       train_regret_winner_11[slice52],\r\n",
        "       train_regret_winner_12[slice52],\r\n",
        "       train_regret_winner_13[slice52],\r\n",
        "       train_regret_winner_14[slice52],\r\n",
        "       train_regret_winner_15[slice52],\r\n",
        "       train_regret_winner_16[slice52],\r\n",
        "       train_regret_winner_17[slice52],\r\n",
        "       train_regret_winner_18[slice52],\r\n",
        "       train_regret_winner_19[slice52],\r\n",
        "       train_regret_winner_20[slice52]]\r\n",
        "\r\n",
        "loser52_results = pd.DataFrame(loser52).sort_values(by=[0], ascending=False)\r\n",
        "winner52_results = pd.DataFrame(winner52).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser52 = np.asarray(loser52_results[4:5][0])[0]\r\n",
        "median_loser52 = np.asarray(loser52_results[9:10][0])[0]\r\n",
        "upper_loser52 = np.asarray(loser52_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner52 = np.asarray(winner52_results[4:5][0])[0]\r\n",
        "median_winner52 = np.asarray(winner52_results[9:10][0])[0]\r\n",
        "upper_winner52 = np.asarray(winner52_results[14:15][0])[0]"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-9cJNkew-nY"
      },
      "source": [
        "# Iteration62 :\r\n",
        "\r\n",
        "slice62 = 61\r\n",
        "\r\n",
        "loser62 = [train_regret_loser_1[slice62],\r\n",
        "       train_regret_loser_2[slice62],\r\n",
        "       train_regret_loser_3[slice62],\r\n",
        "       train_regret_loser_4[slice62],\r\n",
        "       train_regret_loser_5[slice62],\r\n",
        "       train_regret_loser_6[slice62],\r\n",
        "       train_regret_loser_7[slice62],\r\n",
        "       train_regret_loser_8[slice62],\r\n",
        "       train_regret_loser_9[slice62],\r\n",
        "       train_regret_loser_10[slice62],\r\n",
        "       train_regret_loser_11[slice62],\r\n",
        "       train_regret_loser_12[slice62],\r\n",
        "       train_regret_loser_13[slice62],\r\n",
        "       train_regret_loser_14[slice62],\r\n",
        "       train_regret_loser_15[slice62],\r\n",
        "       train_regret_loser_16[slice62],\r\n",
        "       train_regret_loser_17[slice62],\r\n",
        "       train_regret_loser_18[slice62],\r\n",
        "       train_regret_loser_19[slice62],\r\n",
        "       train_regret_loser_20[slice62]]\r\n",
        "\r\n",
        "winner62 = [train_regret_winner_1[slice62],\r\n",
        "       train_regret_winner_2[slice62],\r\n",
        "       train_regret_winner_3[slice62],\r\n",
        "       train_regret_winner_4[slice62],\r\n",
        "       train_regret_winner_5[slice62],\r\n",
        "       train_regret_winner_6[slice62],\r\n",
        "       train_regret_winner_7[slice62],\r\n",
        "       train_regret_winner_8[slice62],\r\n",
        "       train_regret_winner_9[slice62],\r\n",
        "       train_regret_winner_10[slice62],\r\n",
        "       train_regret_winner_11[slice62],\r\n",
        "       train_regret_winner_12[slice62],\r\n",
        "       train_regret_winner_13[slice62],\r\n",
        "       train_regret_winner_14[slice62],\r\n",
        "       train_regret_winner_15[slice62],\r\n",
        "       train_regret_winner_16[slice62],\r\n",
        "       train_regret_winner_17[slice62],\r\n",
        "       train_regret_winner_18[slice62],\r\n",
        "       train_regret_winner_19[slice62],\r\n",
        "       train_regret_winner_20[slice62]]\r\n",
        "\r\n",
        "loser62_results = pd.DataFrame(loser62).sort_values(by=[0], ascending=False)\r\n",
        "winner62_results = pd.DataFrame(winner62).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser62 = np.asarray(loser62_results[4:5][0])[0]\r\n",
        "median_loser62 = np.asarray(loser62_results[9:10][0])[0]\r\n",
        "upper_loser62 = np.asarray(loser62_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner62 = np.asarray(winner62_results[4:5][0])[0]\r\n",
        "median_winner62 = np.asarray(winner62_results[9:10][0])[0]\r\n",
        "upper_winner62 = np.asarray(winner62_results[14:15][0])[0]"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DZ8kHHmw-qK"
      },
      "source": [
        "# Iteration72 :\r\n",
        "\r\n",
        "slice72 = 71\r\n",
        "\r\n",
        "loser72 = [train_regret_loser_1[slice72],\r\n",
        "       train_regret_loser_2[slice72],\r\n",
        "       train_regret_loser_3[slice72],\r\n",
        "       train_regret_loser_4[slice72],\r\n",
        "       train_regret_loser_5[slice72],\r\n",
        "       train_regret_loser_6[slice72],\r\n",
        "       train_regret_loser_7[slice72],\r\n",
        "       train_regret_loser_8[slice72],\r\n",
        "       train_regret_loser_9[slice72],\r\n",
        "       train_regret_loser_10[slice72],\r\n",
        "       train_regret_loser_11[slice72],\r\n",
        "       train_regret_loser_12[slice72],\r\n",
        "       train_regret_loser_13[slice72],\r\n",
        "       train_regret_loser_14[slice72],\r\n",
        "       train_regret_loser_15[slice72],\r\n",
        "       train_regret_loser_16[slice72],\r\n",
        "       train_regret_loser_17[slice72],\r\n",
        "       train_regret_loser_18[slice72],\r\n",
        "       train_regret_loser_19[slice72],\r\n",
        "       train_regret_loser_20[slice72]]\r\n",
        "\r\n",
        "winner72 = [train_regret_winner_1[slice72],\r\n",
        "       train_regret_winner_2[slice72],\r\n",
        "       train_regret_winner_3[slice72],\r\n",
        "       train_regret_winner_4[slice72],\r\n",
        "       train_regret_winner_5[slice72],\r\n",
        "       train_regret_winner_6[slice72],\r\n",
        "       train_regret_winner_7[slice72],\r\n",
        "       train_regret_winner_8[slice72],\r\n",
        "       train_regret_winner_9[slice72],\r\n",
        "       train_regret_winner_10[slice72],\r\n",
        "       train_regret_winner_11[slice72],\r\n",
        "       train_regret_winner_12[slice72],\r\n",
        "       train_regret_winner_13[slice72],\r\n",
        "       train_regret_winner_14[slice72],\r\n",
        "       train_regret_winner_15[slice72],\r\n",
        "       train_regret_winner_16[slice72],\r\n",
        "       train_regret_winner_17[slice72],\r\n",
        "       train_regret_winner_18[slice72],\r\n",
        "       train_regret_winner_19[slice72],\r\n",
        "       train_regret_winner_20[slice72]]\r\n",
        "\r\n",
        "loser72_results = pd.DataFrame(loser72).sort_values(by=[0], ascending=False)\r\n",
        "winner72_results = pd.DataFrame(winner72).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser72 = np.asarray(loser72_results[4:5][0])[0]\r\n",
        "median_loser72 = np.asarray(loser72_results[9:10][0])[0]\r\n",
        "upper_loser72 = np.asarray(loser72_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner72 = np.asarray(winner72_results[4:5][0])[0]\r\n",
        "median_winner72 = np.asarray(winner72_results[9:10][0])[0]\r\n",
        "upper_winner72 = np.asarray(winner72_results[14:15][0])[0]"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBzBvnVyw-sz"
      },
      "source": [
        "# Iteration82 :\r\n",
        "\r\n",
        "slice82 = 81\r\n",
        "\r\n",
        "loser82 = [train_regret_loser_1[slice82],\r\n",
        "       train_regret_loser_2[slice82],\r\n",
        "       train_regret_loser_3[slice82],\r\n",
        "       train_regret_loser_4[slice82],\r\n",
        "       train_regret_loser_5[slice82],\r\n",
        "       train_regret_loser_6[slice82],\r\n",
        "       train_regret_loser_7[slice82],\r\n",
        "       train_regret_loser_8[slice82],\r\n",
        "       train_regret_loser_9[slice82],\r\n",
        "       train_regret_loser_10[slice82],\r\n",
        "       train_regret_loser_11[slice82],\r\n",
        "       train_regret_loser_12[slice82],\r\n",
        "       train_regret_loser_13[slice82],\r\n",
        "       train_regret_loser_14[slice82],\r\n",
        "       train_regret_loser_15[slice82],\r\n",
        "       train_regret_loser_16[slice82],\r\n",
        "       train_regret_loser_17[slice82],\r\n",
        "       train_regret_loser_18[slice82],\r\n",
        "       train_regret_loser_19[slice82],\r\n",
        "       train_regret_loser_20[slice82]]\r\n",
        "\r\n",
        "winner82 = [train_regret_winner_1[slice82],\r\n",
        "       train_regret_winner_2[slice82],\r\n",
        "       train_regret_winner_3[slice82],\r\n",
        "       train_regret_winner_4[slice82],\r\n",
        "       train_regret_winner_5[slice82],\r\n",
        "       train_regret_winner_6[slice82],\r\n",
        "       train_regret_winner_7[slice82],\r\n",
        "       train_regret_winner_8[slice82],\r\n",
        "       train_regret_winner_9[slice82],\r\n",
        "       train_regret_winner_10[slice82],\r\n",
        "       train_regret_winner_11[slice82],\r\n",
        "       train_regret_winner_12[slice82],\r\n",
        "       train_regret_winner_13[slice82],\r\n",
        "       train_regret_winner_14[slice82],\r\n",
        "       train_regret_winner_15[slice82],\r\n",
        "       train_regret_winner_16[slice82],\r\n",
        "       train_regret_winner_17[slice82],\r\n",
        "       train_regret_winner_18[slice82],\r\n",
        "       train_regret_winner_19[slice82],\r\n",
        "       train_regret_winner_20[slice82]]\r\n",
        "\r\n",
        "loser82_results = pd.DataFrame(loser82).sort_values(by=[0], ascending=False)\r\n",
        "winner82_results = pd.DataFrame(winner82).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser82 = np.asarray(loser82_results[4:5][0])[0]\r\n",
        "median_loser82 = np.asarray(loser82_results[9:10][0])[0]\r\n",
        "upper_loser82 = np.asarray(loser82_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner82 = np.asarray(winner82_results[4:5][0])[0]\r\n",
        "median_winner82 = np.asarray(winner82_results[9:10][0])[0]\r\n",
        "upper_winner82 = np.asarray(winner82_results[14:15][0])[0]"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Qj--n6Bw-y2"
      },
      "source": [
        "# Iteration92 :\r\n",
        "\r\n",
        "slice92 = 91\r\n",
        "\r\n",
        "loser92 = [train_regret_loser_1[slice92],\r\n",
        "       train_regret_loser_2[slice92],\r\n",
        "       train_regret_loser_3[slice92],\r\n",
        "       train_regret_loser_4[slice92],\r\n",
        "       train_regret_loser_5[slice92],\r\n",
        "       train_regret_loser_6[slice92],\r\n",
        "       train_regret_loser_7[slice92],\r\n",
        "       train_regret_loser_8[slice92],\r\n",
        "       train_regret_loser_9[slice92],\r\n",
        "       train_regret_loser_10[slice92],\r\n",
        "       train_regret_loser_11[slice92],\r\n",
        "       train_regret_loser_12[slice92],\r\n",
        "       train_regret_loser_13[slice92],\r\n",
        "       train_regret_loser_14[slice92],\r\n",
        "       train_regret_loser_15[slice92],\r\n",
        "       train_regret_loser_16[slice92],\r\n",
        "       train_regret_loser_17[slice92],\r\n",
        "       train_regret_loser_18[slice92],\r\n",
        "       train_regret_loser_19[slice92],\r\n",
        "       train_regret_loser_20[slice92]]\r\n",
        "\r\n",
        "winner92 = [train_regret_winner_1[slice92],\r\n",
        "       train_regret_winner_2[slice92],\r\n",
        "       train_regret_winner_3[slice92],\r\n",
        "       train_regret_winner_4[slice92],\r\n",
        "       train_regret_winner_5[slice92],\r\n",
        "       train_regret_winner_6[slice92],\r\n",
        "       train_regret_winner_7[slice92],\r\n",
        "       train_regret_winner_8[slice92],\r\n",
        "       train_regret_winner_9[slice92],\r\n",
        "       train_regret_winner_10[slice92],\r\n",
        "       train_regret_winner_11[slice92],\r\n",
        "       train_regret_winner_12[slice92],\r\n",
        "       train_regret_winner_13[slice92],\r\n",
        "       train_regret_winner_14[slice92],\r\n",
        "       train_regret_winner_15[slice92],\r\n",
        "       train_regret_winner_16[slice92],\r\n",
        "       train_regret_winner_17[slice92],\r\n",
        "       train_regret_winner_18[slice92],\r\n",
        "       train_regret_winner_19[slice92],\r\n",
        "       train_regret_winner_20[slice92]]\r\n",
        "\r\n",
        "loser92_results = pd.DataFrame(loser92).sort_values(by=[0], ascending=False)\r\n",
        "winner92_results = pd.DataFrame(winner92).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser92 = np.asarray(loser92_results[4:5][0])[0]\r\n",
        "median_loser92 = np.asarray(loser92_results[9:10][0])[0]\r\n",
        "upper_loser92 = np.asarray(loser92_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner92 = np.asarray(winner92_results[4:5][0])[0]\r\n",
        "median_winner92 = np.asarray(winner92_results[9:10][0])[0]\r\n",
        "upper_winner92 = np.asarray(winner92_results[14:15][0])[0]"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aaJ5KqLdw-0L"
      },
      "source": [
        "# Iteration3 :\r\n",
        "\r\n",
        "slice3 = 2\r\n",
        "\r\n",
        "loser3 = [train_regret_loser_1[slice3],\r\n",
        "       train_regret_loser_2[slice3],\r\n",
        "       train_regret_loser_3[slice3],\r\n",
        "       train_regret_loser_4[slice3],\r\n",
        "       train_regret_loser_5[slice3],\r\n",
        "       train_regret_loser_6[slice3],\r\n",
        "       train_regret_loser_7[slice3],\r\n",
        "       train_regret_loser_8[slice3],\r\n",
        "       train_regret_loser_9[slice3],\r\n",
        "       train_regret_loser_10[slice3],\r\n",
        "       train_regret_loser_11[slice3],\r\n",
        "       train_regret_loser_12[slice3],\r\n",
        "       train_regret_loser_13[slice3],\r\n",
        "       train_regret_loser_14[slice3],\r\n",
        "       train_regret_loser_15[slice3],\r\n",
        "       train_regret_loser_16[slice3],\r\n",
        "       train_regret_loser_17[slice3],\r\n",
        "       train_regret_loser_18[slice3],\r\n",
        "       train_regret_loser_19[slice3],\r\n",
        "       train_regret_loser_20[slice3]]\r\n",
        "\r\n",
        "winner3 = [train_regret_winner_1[slice3],\r\n",
        "       train_regret_winner_2[slice3],\r\n",
        "       train_regret_winner_3[slice3],\r\n",
        "       train_regret_winner_4[slice3],\r\n",
        "       train_regret_winner_5[slice3],\r\n",
        "       train_regret_winner_6[slice3],\r\n",
        "       train_regret_winner_7[slice3],\r\n",
        "       train_regret_winner_8[slice3],\r\n",
        "       train_regret_winner_9[slice3],\r\n",
        "       train_regret_winner_10[slice3],\r\n",
        "       train_regret_winner_11[slice3],\r\n",
        "       train_regret_winner_12[slice3],\r\n",
        "       train_regret_winner_13[slice3],\r\n",
        "       train_regret_winner_14[slice3],\r\n",
        "       train_regret_winner_15[slice3],\r\n",
        "       train_regret_winner_16[slice3],\r\n",
        "       train_regret_winner_17[slice3],\r\n",
        "       train_regret_winner_18[slice3],\r\n",
        "       train_regret_winner_19[slice3],\r\n",
        "       train_regret_winner_20[slice3]]\r\n",
        "\r\n",
        "loser3_results = pd.DataFrame(loser3).sort_values(by=[0], ascending=False)\r\n",
        "winner3_results = pd.DataFrame(winner3).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser3 = np.asarray(loser3_results[4:5][0])[0]\r\n",
        "median_loser3 = np.asarray(loser3_results[9:10][0])[0]\r\n",
        "upper_loser3 = np.asarray(loser3_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner3 = np.asarray(winner3_results[4:5][0])[0]\r\n",
        "median_winner3 = np.asarray(winner3_results[9:10][0])[0]\r\n",
        "upper_winner3 = np.asarray(winner3_results[14:15][0])[0]"
      ],
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkaoT1dyw-16"
      },
      "source": [
        "# Iteration13 :\r\n",
        "\r\n",
        "slice13 = 12\r\n",
        "\r\n",
        "loser13 = [train_regret_loser_1[slice13],\r\n",
        "       train_regret_loser_2[slice13],\r\n",
        "       train_regret_loser_3[slice13],\r\n",
        "       train_regret_loser_4[slice13],\r\n",
        "       train_regret_loser_5[slice13],\r\n",
        "       train_regret_loser_6[slice13],\r\n",
        "       train_regret_loser_7[slice13],\r\n",
        "       train_regret_loser_8[slice13],\r\n",
        "       train_regret_loser_9[slice13],\r\n",
        "       train_regret_loser_10[slice13],\r\n",
        "       train_regret_loser_11[slice13],\r\n",
        "       train_regret_loser_12[slice13],\r\n",
        "       train_regret_loser_13[slice13],\r\n",
        "       train_regret_loser_14[slice13],\r\n",
        "       train_regret_loser_15[slice13],\r\n",
        "       train_regret_loser_16[slice13],\r\n",
        "       train_regret_loser_17[slice13],\r\n",
        "       train_regret_loser_18[slice13],\r\n",
        "       train_regret_loser_19[slice13],\r\n",
        "       train_regret_loser_20[slice13]]\r\n",
        "\r\n",
        "winner13 = [train_regret_winner_1[slice13],\r\n",
        "       train_regret_winner_2[slice13],\r\n",
        "       train_regret_winner_3[slice13],\r\n",
        "       train_regret_winner_4[slice13],\r\n",
        "       train_regret_winner_5[slice13],\r\n",
        "       train_regret_winner_6[slice13],\r\n",
        "       train_regret_winner_7[slice13],\r\n",
        "       train_regret_winner_8[slice13],\r\n",
        "       train_regret_winner_9[slice13],\r\n",
        "       train_regret_winner_10[slice13],\r\n",
        "       train_regret_winner_11[slice13],\r\n",
        "       train_regret_winner_12[slice13],\r\n",
        "       train_regret_winner_13[slice13],\r\n",
        "       train_regret_winner_14[slice13],\r\n",
        "       train_regret_winner_15[slice13],\r\n",
        "       train_regret_winner_16[slice13],\r\n",
        "       train_regret_winner_17[slice13],\r\n",
        "       train_regret_winner_18[slice13],\r\n",
        "       train_regret_winner_19[slice13],\r\n",
        "       train_regret_winner_20[slice13]]\r\n",
        "\r\n",
        "loser13_results = pd.DataFrame(loser12).sort_values(by=[0], ascending=False)\r\n",
        "winner13_results = pd.DataFrame(winner12).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser13 = np.asarray(loser13_results[4:5][0])[0]\r\n",
        "median_loser13 = np.asarray(loser13_results[9:10][0])[0]\r\n",
        "upper_loser13 = np.asarray(loser13_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner13 = np.asarray(winner13_results[4:5][0])[0]\r\n",
        "median_winner13 = np.asarray(winner13_results[9:10][0])[0]\r\n",
        "upper_winner13 = np.asarray(winner13_results[14:15][0])[0]"
      ],
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZbVMrBaw-4B"
      },
      "source": [
        "# Iteration23 :\r\n",
        "\r\n",
        "slice23 = 22\r\n",
        "\r\n",
        "loser23 = [train_regret_loser_1[slice23],\r\n",
        "       train_regret_loser_2[slice23],\r\n",
        "       train_regret_loser_3[slice23],\r\n",
        "       train_regret_loser_4[slice23],\r\n",
        "       train_regret_loser_5[slice23],\r\n",
        "       train_regret_loser_6[slice23],\r\n",
        "       train_regret_loser_7[slice23],\r\n",
        "       train_regret_loser_8[slice23],\r\n",
        "       train_regret_loser_9[slice23],\r\n",
        "       train_regret_loser_10[slice23],\r\n",
        "       train_regret_loser_11[slice23],\r\n",
        "       train_regret_loser_12[slice23],\r\n",
        "       train_regret_loser_13[slice23],\r\n",
        "       train_regret_loser_14[slice23],\r\n",
        "       train_regret_loser_15[slice23],\r\n",
        "       train_regret_loser_16[slice23],\r\n",
        "       train_regret_loser_17[slice23],\r\n",
        "       train_regret_loser_18[slice23],\r\n",
        "       train_regret_loser_19[slice23],\r\n",
        "       train_regret_loser_20[slice23]]\r\n",
        "\r\n",
        "winner23 = [train_regret_winner_1[slice23],\r\n",
        "       train_regret_winner_2[slice23],\r\n",
        "       train_regret_winner_3[slice23],\r\n",
        "       train_regret_winner_4[slice23],\r\n",
        "       train_regret_winner_5[slice23],\r\n",
        "       train_regret_winner_6[slice23],\r\n",
        "       train_regret_winner_7[slice23],\r\n",
        "       train_regret_winner_8[slice23],\r\n",
        "       train_regret_winner_9[slice23],\r\n",
        "       train_regret_winner_10[slice23],\r\n",
        "       train_regret_winner_11[slice23],\r\n",
        "       train_regret_winner_12[slice23],\r\n",
        "       train_regret_winner_13[slice23],\r\n",
        "       train_regret_winner_14[slice23],\r\n",
        "       train_regret_winner_15[slice23],\r\n",
        "       train_regret_winner_16[slice23],\r\n",
        "       train_regret_winner_17[slice23],\r\n",
        "       train_regret_winner_18[slice23],\r\n",
        "       train_regret_winner_19[slice23],\r\n",
        "       train_regret_winner_20[slice23]]\r\n",
        "\r\n",
        "loser23_results = pd.DataFrame(loser23).sort_values(by=[0], ascending=False)\r\n",
        "winner23_results = pd.DataFrame(winner23).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser23 = np.asarray(loser23_results[4:5][0])[0]\r\n",
        "median_loser23 = np.asarray(loser23_results[9:10][0])[0]\r\n",
        "upper_loser23 = np.asarray(loser23_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner23 = np.asarray(winner23_results[4:5][0])[0]\r\n",
        "median_winner23 = np.asarray(winner23_results[9:10][0])[0]\r\n",
        "upper_winner23 = np.asarray(winner23_results[14:15][0])[0]"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUa6HRCUw-7G"
      },
      "source": [
        "# Iteration33 :\r\n",
        "\r\n",
        "slice33 = 32\r\n",
        "\r\n",
        "loser33 = [train_regret_loser_1[slice33],\r\n",
        "       train_regret_loser_2[slice33],\r\n",
        "       train_regret_loser_3[slice33],\r\n",
        "       train_regret_loser_4[slice33],\r\n",
        "       train_regret_loser_5[slice33],\r\n",
        "       train_regret_loser_6[slice33],\r\n",
        "       train_regret_loser_7[slice33],\r\n",
        "       train_regret_loser_8[slice33],\r\n",
        "       train_regret_loser_9[slice33],\r\n",
        "       train_regret_loser_10[slice33],\r\n",
        "       train_regret_loser_11[slice33],\r\n",
        "       train_regret_loser_12[slice33],\r\n",
        "       train_regret_loser_13[slice33],\r\n",
        "       train_regret_loser_14[slice33],\r\n",
        "       train_regret_loser_15[slice33],\r\n",
        "       train_regret_loser_16[slice33],\r\n",
        "       train_regret_loser_17[slice33],\r\n",
        "       train_regret_loser_18[slice33],\r\n",
        "       train_regret_loser_19[slice33],\r\n",
        "       train_regret_loser_20[slice33]]\r\n",
        "\r\n",
        "winner33 = [train_regret_winner_1[slice33],\r\n",
        "       train_regret_winner_2[slice33],\r\n",
        "       train_regret_winner_3[slice33],\r\n",
        "       train_regret_winner_4[slice33],\r\n",
        "       train_regret_winner_5[slice33],\r\n",
        "       train_regret_winner_6[slice33],\r\n",
        "       train_regret_winner_7[slice33],\r\n",
        "       train_regret_winner_8[slice33],\r\n",
        "       train_regret_winner_9[slice33],\r\n",
        "       train_regret_winner_10[slice33],\r\n",
        "       train_regret_winner_11[slice33],\r\n",
        "       train_regret_winner_12[slice33],\r\n",
        "       train_regret_winner_13[slice33],\r\n",
        "       train_regret_winner_14[slice33],\r\n",
        "       train_regret_winner_15[slice33],\r\n",
        "       train_regret_winner_16[slice33],\r\n",
        "       train_regret_winner_17[slice33],\r\n",
        "       train_regret_winner_18[slice33],\r\n",
        "       train_regret_winner_19[slice33],\r\n",
        "       train_regret_winner_20[slice33]]\r\n",
        "\r\n",
        "loser33_results = pd.DataFrame(loser33).sort_values(by=[0], ascending=False)\r\n",
        "winner33_results = pd.DataFrame(winner33).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser33 = np.asarray(loser33_results[4:5][0])[0]\r\n",
        "median_loser33 = np.asarray(loser33_results[9:10][0])[0]\r\n",
        "upper_loser33 = np.asarray(loser33_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner33 = np.asarray(winner33_results[4:5][0])[0]\r\n",
        "median_winner33 = np.asarray(winner33_results[9:10][0])[0]\r\n",
        "upper_winner33 = np.asarray(winner33_results[14:15][0])[0]"
      ],
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4EVxQwsxw-9n"
      },
      "source": [
        "# Iteration43 :\r\n",
        "\r\n",
        "slice43 = 42\r\n",
        "\r\n",
        "loser43 = [train_regret_loser_1[slice43],\r\n",
        "       train_regret_loser_2[slice43],\r\n",
        "       train_regret_loser_3[slice43],\r\n",
        "       train_regret_loser_4[slice43],\r\n",
        "       train_regret_loser_5[slice43],\r\n",
        "       train_regret_loser_6[slice43],\r\n",
        "       train_regret_loser_7[slice43],\r\n",
        "       train_regret_loser_8[slice43],\r\n",
        "       train_regret_loser_9[slice43],\r\n",
        "       train_regret_loser_10[slice43],\r\n",
        "       train_regret_loser_11[slice43],\r\n",
        "       train_regret_loser_12[slice43],\r\n",
        "       train_regret_loser_13[slice43],\r\n",
        "       train_regret_loser_14[slice43],\r\n",
        "       train_regret_loser_15[slice43],\r\n",
        "       train_regret_loser_16[slice43],\r\n",
        "       train_regret_loser_17[slice43],\r\n",
        "       train_regret_loser_18[slice43],\r\n",
        "       train_regret_loser_19[slice43],\r\n",
        "       train_regret_loser_20[slice43]]\r\n",
        "\r\n",
        "winner43 = [train_regret_winner_1[slice43],\r\n",
        "       train_regret_winner_2[slice43],\r\n",
        "       train_regret_winner_3[slice43],\r\n",
        "       train_regret_winner_4[slice43],\r\n",
        "       train_regret_winner_5[slice43],\r\n",
        "       train_regret_winner_6[slice43],\r\n",
        "       train_regret_winner_7[slice43],\r\n",
        "       train_regret_winner_8[slice43],\r\n",
        "       train_regret_winner_9[slice43],\r\n",
        "       train_regret_winner_10[slice43],\r\n",
        "       train_regret_winner_11[slice43],\r\n",
        "       train_regret_winner_12[slice43],\r\n",
        "       train_regret_winner_13[slice43],\r\n",
        "       train_regret_winner_14[slice43],\r\n",
        "       train_regret_winner_15[slice43],\r\n",
        "       train_regret_winner_16[slice43],\r\n",
        "       train_regret_winner_17[slice43],\r\n",
        "       train_regret_winner_18[slice43],\r\n",
        "       train_regret_winner_19[slice43],\r\n",
        "       train_regret_winner_20[slice43]]\r\n",
        "\r\n",
        "loser43_results = pd.DataFrame(loser43).sort_values(by=[0], ascending=False)\r\n",
        "winner43_results = pd.DataFrame(winner43).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser43 = np.asarray(loser43_results[4:5][0])[0]\r\n",
        "median_loser43 = np.asarray(loser43_results[9:10][0])[0]\r\n",
        "upper_loser43 = np.asarray(loser43_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner43 = np.asarray(winner43_results[4:5][0])[0]\r\n",
        "median_winner43 = np.asarray(winner43_results[9:10][0])[0]\r\n",
        "upper_winner43 = np.asarray(winner43_results[14:15][0])[0]"
      ],
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQtRyMv-w_AZ"
      },
      "source": [
        "# Iteration53 :\r\n",
        "\r\n",
        "slice53 = 52\r\n",
        "\r\n",
        "loser53 = [train_regret_loser_1[slice53],\r\n",
        "       train_regret_loser_2[slice53],\r\n",
        "       train_regret_loser_3[slice53],\r\n",
        "       train_regret_loser_4[slice53],\r\n",
        "       train_regret_loser_5[slice53],\r\n",
        "       train_regret_loser_6[slice53],\r\n",
        "       train_regret_loser_7[slice53],\r\n",
        "       train_regret_loser_8[slice53],\r\n",
        "       train_regret_loser_9[slice53],\r\n",
        "       train_regret_loser_10[slice53],\r\n",
        "       train_regret_loser_11[slice53],\r\n",
        "       train_regret_loser_12[slice53],\r\n",
        "       train_regret_loser_13[slice53],\r\n",
        "       train_regret_loser_14[slice53],\r\n",
        "       train_regret_loser_15[slice53],\r\n",
        "       train_regret_loser_16[slice53],\r\n",
        "       train_regret_loser_17[slice53],\r\n",
        "       train_regret_loser_18[slice53],\r\n",
        "       train_regret_loser_19[slice53],\r\n",
        "       train_regret_loser_20[slice53]]\r\n",
        "\r\n",
        "winner53 = [train_regret_winner_1[slice53],\r\n",
        "       train_regret_winner_2[slice53],\r\n",
        "       train_regret_winner_3[slice53],\r\n",
        "       train_regret_winner_4[slice53],\r\n",
        "       train_regret_winner_5[slice53],\r\n",
        "       train_regret_winner_6[slice53],\r\n",
        "       train_regret_winner_7[slice53],\r\n",
        "       train_regret_winner_8[slice53],\r\n",
        "       train_regret_winner_9[slice53],\r\n",
        "       train_regret_winner_10[slice53],\r\n",
        "       train_regret_winner_11[slice53],\r\n",
        "       train_regret_winner_12[slice53],\r\n",
        "       train_regret_winner_13[slice53],\r\n",
        "       train_regret_winner_14[slice53],\r\n",
        "       train_regret_winner_15[slice53],\r\n",
        "       train_regret_winner_16[slice53],\r\n",
        "       train_regret_winner_17[slice53],\r\n",
        "       train_regret_winner_18[slice53],\r\n",
        "       train_regret_winner_19[slice53],\r\n",
        "       train_regret_winner_20[slice53]]\r\n",
        "\r\n",
        "loser53_results = pd.DataFrame(loser53).sort_values(by=[0], ascending=False)\r\n",
        "winner53_results = pd.DataFrame(winner53).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser53 = np.asarray(loser53_results[4:5][0])[0]\r\n",
        "median_loser53 = np.asarray(loser53_results[9:10][0])[0]\r\n",
        "upper_loser53 = np.asarray(loser53_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner53 = np.asarray(winner53_results[4:5][0])[0]\r\n",
        "median_winner53 = np.asarray(winner53_results[9:10][0])[0]\r\n",
        "upper_winner53 = np.asarray(winner53_results[14:15][0])[0]"
      ],
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1CIMEXXYw_DR"
      },
      "source": [
        "# Iteration63 :\r\n",
        "\r\n",
        "slice63 = 62\r\n",
        "\r\n",
        "loser63 = [train_regret_loser_1[slice63],\r\n",
        "       train_regret_loser_2[slice63],\r\n",
        "       train_regret_loser_3[slice63],\r\n",
        "       train_regret_loser_4[slice63],\r\n",
        "       train_regret_loser_5[slice63],\r\n",
        "       train_regret_loser_6[slice63],\r\n",
        "       train_regret_loser_7[slice63],\r\n",
        "       train_regret_loser_8[slice63],\r\n",
        "       train_regret_loser_9[slice63],\r\n",
        "       train_regret_loser_10[slice63],\r\n",
        "       train_regret_loser_11[slice63],\r\n",
        "       train_regret_loser_12[slice63],\r\n",
        "       train_regret_loser_13[slice63],\r\n",
        "       train_regret_loser_14[slice63],\r\n",
        "       train_regret_loser_15[slice63],\r\n",
        "       train_regret_loser_16[slice63],\r\n",
        "       train_regret_loser_17[slice63],\r\n",
        "       train_regret_loser_18[slice63],\r\n",
        "       train_regret_loser_19[slice63],\r\n",
        "       train_regret_loser_20[slice63]]\r\n",
        "\r\n",
        "winner63 = [train_regret_winner_1[slice63],\r\n",
        "       train_regret_winner_2[slice63],\r\n",
        "       train_regret_winner_3[slice63],\r\n",
        "       train_regret_winner_4[slice63],\r\n",
        "       train_regret_winner_5[slice63],\r\n",
        "       train_regret_winner_6[slice63],\r\n",
        "       train_regret_winner_7[slice63],\r\n",
        "       train_regret_winner_8[slice63],\r\n",
        "       train_regret_winner_9[slice63],\r\n",
        "       train_regret_winner_10[slice63],\r\n",
        "       train_regret_winner_11[slice63],\r\n",
        "       train_regret_winner_12[slice63],\r\n",
        "       train_regret_winner_13[slice63],\r\n",
        "       train_regret_winner_14[slice63],\r\n",
        "       train_regret_winner_15[slice63],\r\n",
        "       train_regret_winner_16[slice63],\r\n",
        "       train_regret_winner_17[slice63],\r\n",
        "       train_regret_winner_18[slice63],\r\n",
        "       train_regret_winner_19[slice63],\r\n",
        "       train_regret_winner_20[slice63]]\r\n",
        "\r\n",
        "loser63_results = pd.DataFrame(loser63).sort_values(by=[0], ascending=False)\r\n",
        "winner63_results = pd.DataFrame(winner63).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser63 = np.asarray(loser63_results[4:5][0])[0]\r\n",
        "median_loser63 = np.asarray(loser63_results[9:10][0])[0]\r\n",
        "upper_loser63 = np.asarray(loser63_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner63 = np.asarray(winner63_results[4:5][0])[0]\r\n",
        "median_winner63 = np.asarray(winner63_results[9:10][0])[0]\r\n",
        "upper_winner63 = np.asarray(winner63_results[14:15][0])[0]"
      ],
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xg4GfwU_w_GF"
      },
      "source": [
        "# Iteration73 :\r\n",
        "\r\n",
        "slice73 = 72\r\n",
        "\r\n",
        "loser73 = [train_regret_loser_1[slice73],\r\n",
        "       train_regret_loser_2[slice73],\r\n",
        "       train_regret_loser_3[slice73],\r\n",
        "       train_regret_loser_4[slice73],\r\n",
        "       train_regret_loser_5[slice73],\r\n",
        "       train_regret_loser_6[slice73],\r\n",
        "       train_regret_loser_7[slice73],\r\n",
        "       train_regret_loser_8[slice73],\r\n",
        "       train_regret_loser_9[slice73],\r\n",
        "       train_regret_loser_10[slice73],\r\n",
        "       train_regret_loser_11[slice73],\r\n",
        "       train_regret_loser_12[slice73],\r\n",
        "       train_regret_loser_13[slice73],\r\n",
        "       train_regret_loser_14[slice73],\r\n",
        "       train_regret_loser_15[slice73],\r\n",
        "       train_regret_loser_16[slice73],\r\n",
        "       train_regret_loser_17[slice73],\r\n",
        "       train_regret_loser_18[slice73],\r\n",
        "       train_regret_loser_19[slice73],\r\n",
        "       train_regret_loser_20[slice73]]\r\n",
        "\r\n",
        "winner73 = [train_regret_winner_1[slice73],\r\n",
        "       train_regret_winner_2[slice73],\r\n",
        "       train_regret_winner_3[slice73],\r\n",
        "       train_regret_winner_4[slice73],\r\n",
        "       train_regret_winner_5[slice73],\r\n",
        "       train_regret_winner_6[slice73],\r\n",
        "       train_regret_winner_7[slice73],\r\n",
        "       train_regret_winner_8[slice73],\r\n",
        "       train_regret_winner_9[slice73],\r\n",
        "       train_regret_winner_10[slice73],\r\n",
        "       train_regret_winner_11[slice73],\r\n",
        "       train_regret_winner_12[slice73],\r\n",
        "       train_regret_winner_13[slice73],\r\n",
        "       train_regret_winner_14[slice73],\r\n",
        "       train_regret_winner_15[slice73],\r\n",
        "       train_regret_winner_16[slice73],\r\n",
        "       train_regret_winner_17[slice73],\r\n",
        "       train_regret_winner_18[slice73],\r\n",
        "       train_regret_winner_19[slice73],\r\n",
        "       train_regret_winner_20[slice73]]\r\n",
        "\r\n",
        "loser73_results = pd.DataFrame(loser73).sort_values(by=[0], ascending=False)\r\n",
        "winner73_results = pd.DataFrame(winner73).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser73 = np.asarray(loser73_results[4:5][0])[0]\r\n",
        "median_loser73 = np.asarray(loser73_results[9:10][0])[0]\r\n",
        "upper_loser73 = np.asarray(loser73_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner73 = np.asarray(winner73_results[4:5][0])[0]\r\n",
        "median_winner73 = np.asarray(winner73_results[9:10][0])[0]\r\n",
        "upper_winner73 = np.asarray(winner73_results[14:15][0])[0]"
      ],
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WmpcEV4w_I9"
      },
      "source": [
        "# Iteration83 :\r\n",
        "\r\n",
        "slice83 = 82\r\n",
        "\r\n",
        "loser83 = [train_regret_loser_1[slice83],\r\n",
        "       train_regret_loser_2[slice83],\r\n",
        "       train_regret_loser_3[slice83],\r\n",
        "       train_regret_loser_4[slice83],\r\n",
        "       train_regret_loser_5[slice83],\r\n",
        "       train_regret_loser_6[slice83],\r\n",
        "       train_regret_loser_7[slice83],\r\n",
        "       train_regret_loser_8[slice83],\r\n",
        "       train_regret_loser_9[slice83],\r\n",
        "       train_regret_loser_10[slice83],\r\n",
        "       train_regret_loser_11[slice83],\r\n",
        "       train_regret_loser_12[slice83],\r\n",
        "       train_regret_loser_13[slice83],\r\n",
        "       train_regret_loser_14[slice83],\r\n",
        "       train_regret_loser_15[slice83],\r\n",
        "       train_regret_loser_16[slice83],\r\n",
        "       train_regret_loser_17[slice83],\r\n",
        "       train_regret_loser_18[slice83],\r\n",
        "       train_regret_loser_19[slice83],\r\n",
        "       train_regret_loser_20[slice83]]\r\n",
        "\r\n",
        "winner83 = [train_regret_winner_1[slice83],\r\n",
        "       train_regret_winner_2[slice83],\r\n",
        "       train_regret_winner_3[slice83],\r\n",
        "       train_regret_winner_4[slice83],\r\n",
        "       train_regret_winner_5[slice83],\r\n",
        "       train_regret_winner_6[slice83],\r\n",
        "       train_regret_winner_7[slice83],\r\n",
        "       train_regret_winner_8[slice83],\r\n",
        "       train_regret_winner_9[slice83],\r\n",
        "       train_regret_winner_10[slice83],\r\n",
        "       train_regret_winner_11[slice83],\r\n",
        "       train_regret_winner_12[slice83],\r\n",
        "       train_regret_winner_13[slice83],\r\n",
        "       train_regret_winner_14[slice83],\r\n",
        "       train_regret_winner_15[slice83],\r\n",
        "       train_regret_winner_16[slice83],\r\n",
        "       train_regret_winner_17[slice83],\r\n",
        "       train_regret_winner_18[slice83],\r\n",
        "       train_regret_winner_19[slice83],\r\n",
        "       train_regret_winner_20[slice83]]\r\n",
        "\r\n",
        "loser83_results = pd.DataFrame(loser83).sort_values(by=[0], ascending=False)\r\n",
        "winner83_results = pd.DataFrame(winner83).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser83 = np.asarray(loser83_results[4:5][0])[0]\r\n",
        "median_loser83 = np.asarray(loser83_results[9:10][0])[0]\r\n",
        "upper_loser83 = np.asarray(loser83_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner83 = np.asarray(winner83_results[4:5][0])[0]\r\n",
        "median_winner83 = np.asarray(winner83_results[9:10][0])[0]\r\n",
        "upper_winner83 = np.asarray(winner83_results[14:15][0])[0]"
      ],
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGPwoBZyw_MF"
      },
      "source": [
        "# Iteration93 :\r\n",
        "\r\n",
        "slice93 = 92\r\n",
        "\r\n",
        "loser93 = [train_regret_loser_1[slice93],\r\n",
        "       train_regret_loser_2[slice93],\r\n",
        "       train_regret_loser_3[slice93],\r\n",
        "       train_regret_loser_4[slice93],\r\n",
        "       train_regret_loser_5[slice93],\r\n",
        "       train_regret_loser_6[slice93],\r\n",
        "       train_regret_loser_7[slice93],\r\n",
        "       train_regret_loser_8[slice93],\r\n",
        "       train_regret_loser_9[slice93],\r\n",
        "       train_regret_loser_10[slice93],\r\n",
        "       train_regret_loser_11[slice93],\r\n",
        "       train_regret_loser_12[slice93],\r\n",
        "       train_regret_loser_13[slice93],\r\n",
        "       train_regret_loser_14[slice93],\r\n",
        "       train_regret_loser_15[slice93],\r\n",
        "       train_regret_loser_16[slice93],\r\n",
        "       train_regret_loser_17[slice93],\r\n",
        "       train_regret_loser_18[slice93],\r\n",
        "       train_regret_loser_19[slice93],\r\n",
        "       train_regret_loser_20[slice93]]\r\n",
        "\r\n",
        "winner93 = [train_regret_winner_1[slice93],\r\n",
        "       train_regret_winner_2[slice93],\r\n",
        "       train_regret_winner_3[slice93],\r\n",
        "       train_regret_winner_4[slice93],\r\n",
        "       train_regret_winner_5[slice93],\r\n",
        "       train_regret_winner_6[slice93],\r\n",
        "       train_regret_winner_7[slice93],\r\n",
        "       train_regret_winner_8[slice93],\r\n",
        "       train_regret_winner_9[slice93],\r\n",
        "       train_regret_winner_10[slice93],\r\n",
        "       train_regret_winner_11[slice93],\r\n",
        "       train_regret_winner_12[slice93],\r\n",
        "       train_regret_winner_13[slice93],\r\n",
        "       train_regret_winner_14[slice93],\r\n",
        "       train_regret_winner_15[slice93],\r\n",
        "       train_regret_winner_16[slice93],\r\n",
        "       train_regret_winner_17[slice93],\r\n",
        "       train_regret_winner_18[slice93],\r\n",
        "       train_regret_winner_19[slice93],\r\n",
        "       train_regret_winner_20[slice93]]\r\n",
        "\r\n",
        "loser93_results = pd.DataFrame(loser93).sort_values(by=[0], ascending=False)\r\n",
        "winner93_results = pd.DataFrame(winner93).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser93 = np.asarray(loser93_results[4:5][0])[0]\r\n",
        "median_loser93 = np.asarray(loser93_results[9:10][0])[0]\r\n",
        "upper_loser93 = np.asarray(loser93_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner93 = np.asarray(winner93_results[4:5][0])[0]\r\n",
        "median_winner93 = np.asarray(winner93_results[9:10][0])[0]\r\n",
        "upper_winner93 = np.asarray(winner93_results[14:15][0])[0]"
      ],
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_ghdGzPw_Oq"
      },
      "source": [
        "# Iteration4 :\r\n",
        "\r\n",
        "slice4 = 3\r\n",
        "\r\n",
        "loser4 = [train_regret_loser_1[slice4],\r\n",
        "       train_regret_loser_2[slice4],\r\n",
        "       train_regret_loser_3[slice4],\r\n",
        "       train_regret_loser_4[slice4],\r\n",
        "       train_regret_loser_5[slice4],\r\n",
        "       train_regret_loser_6[slice4],\r\n",
        "       train_regret_loser_7[slice4],\r\n",
        "       train_regret_loser_8[slice4],\r\n",
        "       train_regret_loser_9[slice4],\r\n",
        "       train_regret_loser_10[slice4],\r\n",
        "       train_regret_loser_11[slice4],\r\n",
        "       train_regret_loser_12[slice4],\r\n",
        "       train_regret_loser_13[slice4],\r\n",
        "       train_regret_loser_14[slice4],\r\n",
        "       train_regret_loser_15[slice4],\r\n",
        "       train_regret_loser_16[slice4],\r\n",
        "       train_regret_loser_17[slice4],\r\n",
        "       train_regret_loser_18[slice4],\r\n",
        "       train_regret_loser_19[slice4],\r\n",
        "       train_regret_loser_20[slice4]]\r\n",
        "\r\n",
        "winner4 = [train_regret_winner_1[slice4],\r\n",
        "       train_regret_winner_2[slice4],\r\n",
        "       train_regret_winner_3[slice4],\r\n",
        "       train_regret_winner_4[slice4],\r\n",
        "       train_regret_winner_5[slice4],\r\n",
        "       train_regret_winner_6[slice4],\r\n",
        "       train_regret_winner_7[slice4],\r\n",
        "       train_regret_winner_8[slice4],\r\n",
        "       train_regret_winner_9[slice4],\r\n",
        "       train_regret_winner_10[slice4],\r\n",
        "       train_regret_winner_11[slice4],\r\n",
        "       train_regret_winner_12[slice4],\r\n",
        "       train_regret_winner_13[slice4],\r\n",
        "       train_regret_winner_14[slice4],\r\n",
        "       train_regret_winner_15[slice4],\r\n",
        "       train_regret_winner_16[slice4],\r\n",
        "       train_regret_winner_17[slice4],\r\n",
        "       train_regret_winner_18[slice4],\r\n",
        "       train_regret_winner_19[slice4],\r\n",
        "       train_regret_winner_20[slice4]]\r\n",
        "\r\n",
        "loser4_results = pd.DataFrame(loser4).sort_values(by=[0], ascending=False)\r\n",
        "winner4_results = pd.DataFrame(winner4).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser4 = np.asarray(loser4_results[4:5][0])[0]\r\n",
        "median_loser4 = np.asarray(loser4_results[9:10][0])[0]\r\n",
        "upper_loser4 = np.asarray(loser4_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner4 = np.asarray(winner4_results[4:5][0])[0]\r\n",
        "median_winner4 = np.asarray(winner4_results[9:10][0])[0]\r\n",
        "upper_winner4 = np.asarray(winner4_results[14:15][0])[0]"
      ],
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNvDt4QMw_Rz"
      },
      "source": [
        "# Iteration14 :\r\n",
        "\r\n",
        "slice14 = 13\r\n",
        "\r\n",
        "loser14 = [train_regret_loser_1[slice14],\r\n",
        "       train_regret_loser_2[slice14],\r\n",
        "       train_regret_loser_3[slice14],\r\n",
        "       train_regret_loser_4[slice14],\r\n",
        "       train_regret_loser_5[slice14],\r\n",
        "       train_regret_loser_6[slice14],\r\n",
        "       train_regret_loser_7[slice14],\r\n",
        "       train_regret_loser_8[slice14],\r\n",
        "       train_regret_loser_9[slice14],\r\n",
        "       train_regret_loser_10[slice14],\r\n",
        "       train_regret_loser_11[slice14],\r\n",
        "       train_regret_loser_12[slice14],\r\n",
        "       train_regret_loser_13[slice14],\r\n",
        "       train_regret_loser_14[slice14],\r\n",
        "       train_regret_loser_15[slice14],\r\n",
        "       train_regret_loser_16[slice14],\r\n",
        "       train_regret_loser_17[slice14],\r\n",
        "       train_regret_loser_18[slice14],\r\n",
        "       train_regret_loser_19[slice14],\r\n",
        "       train_regret_loser_20[slice14]]\r\n",
        "\r\n",
        "winner14 = [train_regret_winner_1[slice14],\r\n",
        "       train_regret_winner_2[slice14],\r\n",
        "       train_regret_winner_3[slice14],\r\n",
        "       train_regret_winner_4[slice14],\r\n",
        "       train_regret_winner_5[slice14],\r\n",
        "       train_regret_winner_6[slice14],\r\n",
        "       train_regret_winner_7[slice14],\r\n",
        "       train_regret_winner_8[slice14],\r\n",
        "       train_regret_winner_9[slice14],\r\n",
        "       train_regret_winner_10[slice14],\r\n",
        "       train_regret_winner_11[slice14],\r\n",
        "       train_regret_winner_12[slice14],\r\n",
        "       train_regret_winner_13[slice14],\r\n",
        "       train_regret_winner_14[slice14],\r\n",
        "       train_regret_winner_15[slice14],\r\n",
        "       train_regret_winner_16[slice14],\r\n",
        "       train_regret_winner_17[slice14],\r\n",
        "       train_regret_winner_18[slice14],\r\n",
        "       train_regret_winner_19[slice14],\r\n",
        "       train_regret_winner_20[slice14]]\r\n",
        "\r\n",
        "loser14_results = pd.DataFrame(loser14).sort_values(by=[0], ascending=False)\r\n",
        "winner14_results = pd.DataFrame(winner14).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser14 = np.asarray(loser14_results[4:5][0])[0]\r\n",
        "median_loser14 = np.asarray(loser14_results[9:10][0])[0]\r\n",
        "upper_loser14 = np.asarray(loser14_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner14 = np.asarray(winner14_results[4:5][0])[0]\r\n",
        "median_winner14 = np.asarray(winner14_results[9:10][0])[0]\r\n",
        "upper_winner14 = np.asarray(winner14_results[14:15][0])[0]"
      ],
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zx3kMQV0w_Ua"
      },
      "source": [
        "# Iteration24 :\r\n",
        "\r\n",
        "slice24 = 23\r\n",
        "\r\n",
        "loser24 = [train_regret_loser_1[slice24],\r\n",
        "       train_regret_loser_2[slice24],\r\n",
        "       train_regret_loser_3[slice24],\r\n",
        "       train_regret_loser_4[slice24],\r\n",
        "       train_regret_loser_5[slice24],\r\n",
        "       train_regret_loser_6[slice24],\r\n",
        "       train_regret_loser_7[slice24],\r\n",
        "       train_regret_loser_8[slice24],\r\n",
        "       train_regret_loser_9[slice24],\r\n",
        "       train_regret_loser_10[slice24],\r\n",
        "       train_regret_loser_11[slice24],\r\n",
        "       train_regret_loser_12[slice24],\r\n",
        "       train_regret_loser_13[slice24],\r\n",
        "       train_regret_loser_14[slice24],\r\n",
        "       train_regret_loser_15[slice24],\r\n",
        "       train_regret_loser_16[slice24],\r\n",
        "       train_regret_loser_17[slice24],\r\n",
        "       train_regret_loser_18[slice24],\r\n",
        "       train_regret_loser_19[slice24],\r\n",
        "       train_regret_loser_20[slice24]]\r\n",
        "\r\n",
        "winner24 = [train_regret_winner_1[slice24],\r\n",
        "       train_regret_winner_2[slice24],\r\n",
        "       train_regret_winner_3[slice24],\r\n",
        "       train_regret_winner_4[slice24],\r\n",
        "       train_regret_winner_5[slice24],\r\n",
        "       train_regret_winner_6[slice24],\r\n",
        "       train_regret_winner_7[slice24],\r\n",
        "       train_regret_winner_8[slice24],\r\n",
        "       train_regret_winner_9[slice24],\r\n",
        "       train_regret_winner_10[slice24],\r\n",
        "       train_regret_winner_11[slice24],\r\n",
        "       train_regret_winner_12[slice24],\r\n",
        "       train_regret_winner_13[slice24],\r\n",
        "       train_regret_winner_14[slice24],\r\n",
        "       train_regret_winner_15[slice24],\r\n",
        "       train_regret_winner_16[slice24],\r\n",
        "       train_regret_winner_17[slice24],\r\n",
        "       train_regret_winner_18[slice24],\r\n",
        "       train_regret_winner_19[slice24],\r\n",
        "       train_regret_winner_20[slice24]]\r\n",
        "\r\n",
        "loser24_results = pd.DataFrame(loser24).sort_values(by=[0], ascending=False)\r\n",
        "winner24_results = pd.DataFrame(winner24).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser24 = np.asarray(loser24_results[4:5][0])[0]\r\n",
        "median_loser24 = np.asarray(loser24_results[9:10][0])[0]\r\n",
        "upper_loser24 = np.asarray(loser24_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner24 = np.asarray(winner24_results[4:5][0])[0]\r\n",
        "median_winner24 = np.asarray(winner24_results[9:10][0])[0]\r\n",
        "upper_winner24 = np.asarray(winner24_results[14:15][0])[0]"
      ],
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4ljkyMfw_Xj"
      },
      "source": [
        "# Iteration34 :\r\n",
        "\r\n",
        "slice34 = 33\r\n",
        "\r\n",
        "loser34 = [train_regret_loser_1[slice34],\r\n",
        "       train_regret_loser_2[slice34],\r\n",
        "       train_regret_loser_3[slice34],\r\n",
        "       train_regret_loser_4[slice34],\r\n",
        "       train_regret_loser_5[slice34],\r\n",
        "       train_regret_loser_6[slice34],\r\n",
        "       train_regret_loser_7[slice34],\r\n",
        "       train_regret_loser_8[slice34],\r\n",
        "       train_regret_loser_9[slice34],\r\n",
        "       train_regret_loser_10[slice34],\r\n",
        "       train_regret_loser_11[slice34],\r\n",
        "       train_regret_loser_12[slice34],\r\n",
        "       train_regret_loser_13[slice34],\r\n",
        "       train_regret_loser_14[slice34],\r\n",
        "       train_regret_loser_15[slice34],\r\n",
        "       train_regret_loser_16[slice34],\r\n",
        "       train_regret_loser_17[slice34],\r\n",
        "       train_regret_loser_18[slice34],\r\n",
        "       train_regret_loser_19[slice34],\r\n",
        "       train_regret_loser_20[slice34]]\r\n",
        "\r\n",
        "winner34 = [train_regret_winner_1[slice34],\r\n",
        "       train_regret_winner_2[slice34],\r\n",
        "       train_regret_winner_3[slice34],\r\n",
        "       train_regret_winner_4[slice34],\r\n",
        "       train_regret_winner_5[slice34],\r\n",
        "       train_regret_winner_6[slice34],\r\n",
        "       train_regret_winner_7[slice34],\r\n",
        "       train_regret_winner_8[slice34],\r\n",
        "       train_regret_winner_9[slice34],\r\n",
        "       train_regret_winner_10[slice34],\r\n",
        "       train_regret_winner_11[slice34],\r\n",
        "       train_regret_winner_12[slice34],\r\n",
        "       train_regret_winner_13[slice34],\r\n",
        "       train_regret_winner_14[slice34],\r\n",
        "       train_regret_winner_15[slice34],\r\n",
        "       train_regret_winner_16[slice34],\r\n",
        "       train_regret_winner_17[slice34],\r\n",
        "       train_regret_winner_18[slice34],\r\n",
        "       train_regret_winner_19[slice34],\r\n",
        "       train_regret_winner_20[slice34]]\r\n",
        "\r\n",
        "loser34_results = pd.DataFrame(loser34).sort_values(by=[0], ascending=False)\r\n",
        "winner34_results = pd.DataFrame(winner34).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser34 = np.asarray(loser34_results[4:5][0])[0]\r\n",
        "median_loser34 = np.asarray(loser34_results[9:10][0])[0]\r\n",
        "upper_loser34 = np.asarray(loser34_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner34 = np.asarray(winner34_results[4:5][0])[0]\r\n",
        "median_winner34 = np.asarray(winner34_results[9:10][0])[0]\r\n",
        "upper_winner34 = np.asarray(winner34_results[14:15][0])[0]"
      ],
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGKj-NR8w_ae"
      },
      "source": [
        "# Iteration44 :\r\n",
        "\r\n",
        "slice44 = 43\r\n",
        "\r\n",
        "loser44 = [train_regret_loser_1[slice44],\r\n",
        "       train_regret_loser_2[slice44],\r\n",
        "       train_regret_loser_3[slice44],\r\n",
        "       train_regret_loser_4[slice44],\r\n",
        "       train_regret_loser_5[slice44],\r\n",
        "       train_regret_loser_6[slice44],\r\n",
        "       train_regret_loser_7[slice44],\r\n",
        "       train_regret_loser_8[slice44],\r\n",
        "       train_regret_loser_9[slice44],\r\n",
        "       train_regret_loser_10[slice44],\r\n",
        "       train_regret_loser_11[slice44],\r\n",
        "       train_regret_loser_12[slice44],\r\n",
        "       train_regret_loser_13[slice44],\r\n",
        "       train_regret_loser_14[slice44],\r\n",
        "       train_regret_loser_15[slice44],\r\n",
        "       train_regret_loser_16[slice44],\r\n",
        "       train_regret_loser_17[slice44],\r\n",
        "       train_regret_loser_18[slice44],\r\n",
        "       train_regret_loser_19[slice44],\r\n",
        "       train_regret_loser_20[slice44]]\r\n",
        "\r\n",
        "winner44 = [train_regret_winner_1[slice44],\r\n",
        "       train_regret_winner_2[slice44],\r\n",
        "       train_regret_winner_3[slice44],\r\n",
        "       train_regret_winner_4[slice44],\r\n",
        "       train_regret_winner_5[slice44],\r\n",
        "       train_regret_winner_6[slice44],\r\n",
        "       train_regret_winner_7[slice44],\r\n",
        "       train_regret_winner_8[slice44],\r\n",
        "       train_regret_winner_9[slice44],\r\n",
        "       train_regret_winner_10[slice44],\r\n",
        "       train_regret_winner_11[slice44],\r\n",
        "       train_regret_winner_12[slice44],\r\n",
        "       train_regret_winner_13[slice44],\r\n",
        "       train_regret_winner_14[slice44],\r\n",
        "       train_regret_winner_15[slice44],\r\n",
        "       train_regret_winner_16[slice44],\r\n",
        "       train_regret_winner_17[slice44],\r\n",
        "       train_regret_winner_18[slice44],\r\n",
        "       train_regret_winner_19[slice44],\r\n",
        "       train_regret_winner_20[slice44]]\r\n",
        "\r\n",
        "loser44_results = pd.DataFrame(loser44).sort_values(by=[0], ascending=False)\r\n",
        "winner44_results = pd.DataFrame(winner44).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser44 = np.asarray(loser44_results[4:5][0])[0]\r\n",
        "median_loser44 = np.asarray(loser44_results[9:10][0])[0]\r\n",
        "upper_loser44 = np.asarray(loser44_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner44 = np.asarray(winner44_results[4:5][0])[0]\r\n",
        "median_winner44 = np.asarray(winner44_results[9:10][0])[0]\r\n",
        "upper_winner44 = np.asarray(winner44_results[14:15][0])[0]"
      ],
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJzx094Fw_c_"
      },
      "source": [
        "# Iteration54 :\r\n",
        "\r\n",
        "slice54 = 53\r\n",
        "\r\n",
        "loser54 = [train_regret_loser_1[slice54],\r\n",
        "       train_regret_loser_2[slice54],\r\n",
        "       train_regret_loser_3[slice54],\r\n",
        "       train_regret_loser_4[slice54],\r\n",
        "       train_regret_loser_5[slice54],\r\n",
        "       train_regret_loser_6[slice54],\r\n",
        "       train_regret_loser_7[slice54],\r\n",
        "       train_regret_loser_8[slice54],\r\n",
        "       train_regret_loser_9[slice54],\r\n",
        "       train_regret_loser_10[slice54],\r\n",
        "       train_regret_loser_11[slice54],\r\n",
        "       train_regret_loser_12[slice54],\r\n",
        "       train_regret_loser_13[slice54],\r\n",
        "       train_regret_loser_14[slice54],\r\n",
        "       train_regret_loser_15[slice54],\r\n",
        "       train_regret_loser_16[slice54],\r\n",
        "       train_regret_loser_17[slice54],\r\n",
        "       train_regret_loser_18[slice54],\r\n",
        "       train_regret_loser_19[slice54],\r\n",
        "       train_regret_loser_20[slice54]]\r\n",
        "\r\n",
        "winner54 = [train_regret_winner_1[slice54],\r\n",
        "       train_regret_winner_2[slice54],\r\n",
        "       train_regret_winner_3[slice54],\r\n",
        "       train_regret_winner_4[slice54],\r\n",
        "       train_regret_winner_5[slice54],\r\n",
        "       train_regret_winner_6[slice54],\r\n",
        "       train_regret_winner_7[slice54],\r\n",
        "       train_regret_winner_8[slice54],\r\n",
        "       train_regret_winner_9[slice54],\r\n",
        "       train_regret_winner_10[slice54],\r\n",
        "       train_regret_winner_11[slice54],\r\n",
        "       train_regret_winner_12[slice54],\r\n",
        "       train_regret_winner_13[slice54],\r\n",
        "       train_regret_winner_14[slice54],\r\n",
        "       train_regret_winner_15[slice54],\r\n",
        "       train_regret_winner_16[slice54],\r\n",
        "       train_regret_winner_17[slice54],\r\n",
        "       train_regret_winner_18[slice54],\r\n",
        "       train_regret_winner_19[slice54],\r\n",
        "       train_regret_winner_20[slice54]]\r\n",
        "\r\n",
        "loser54_results = pd.DataFrame(loser54).sort_values(by=[0], ascending=False)\r\n",
        "winner54_results = pd.DataFrame(winner54).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser54 = np.asarray(loser54_results[4:5][0])[0]\r\n",
        "median_loser54 = np.asarray(loser54_results[9:10][0])[0]\r\n",
        "upper_loser54 = np.asarray(loser54_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner54 = np.asarray(winner54_results[4:5][0])[0]\r\n",
        "median_winner54 = np.asarray(winner54_results[9:10][0])[0]\r\n",
        "upper_winner54 = np.asarray(winner54_results[14:15][0])[0]"
      ],
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEVYMqj_w_fr"
      },
      "source": [
        "# Iteration64 :\r\n",
        "\r\n",
        "slice64 = 63\r\n",
        "\r\n",
        "loser64 = [train_regret_loser_1[slice64],\r\n",
        "       train_regret_loser_2[slice64],\r\n",
        "       train_regret_loser_3[slice64],\r\n",
        "       train_regret_loser_4[slice64],\r\n",
        "       train_regret_loser_5[slice64],\r\n",
        "       train_regret_loser_6[slice64],\r\n",
        "       train_regret_loser_7[slice64],\r\n",
        "       train_regret_loser_8[slice64],\r\n",
        "       train_regret_loser_9[slice64],\r\n",
        "       train_regret_loser_10[slice64],\r\n",
        "       train_regret_loser_11[slice64],\r\n",
        "       train_regret_loser_12[slice64],\r\n",
        "       train_regret_loser_13[slice64],\r\n",
        "       train_regret_loser_14[slice64],\r\n",
        "       train_regret_loser_15[slice64],\r\n",
        "       train_regret_loser_16[slice64],\r\n",
        "       train_regret_loser_17[slice64],\r\n",
        "       train_regret_loser_18[slice64],\r\n",
        "       train_regret_loser_19[slice64],\r\n",
        "       train_regret_loser_20[slice64]]\r\n",
        "\r\n",
        "winner64 = [train_regret_winner_1[slice64],\r\n",
        "       train_regret_winner_2[slice64],\r\n",
        "       train_regret_winner_3[slice64],\r\n",
        "       train_regret_winner_4[slice64],\r\n",
        "       train_regret_winner_5[slice64],\r\n",
        "       train_regret_winner_6[slice64],\r\n",
        "       train_regret_winner_7[slice64],\r\n",
        "       train_regret_winner_8[slice64],\r\n",
        "       train_regret_winner_9[slice64],\r\n",
        "       train_regret_winner_10[slice64],\r\n",
        "       train_regret_winner_11[slice64],\r\n",
        "       train_regret_winner_12[slice64],\r\n",
        "       train_regret_winner_13[slice64],\r\n",
        "       train_regret_winner_14[slice64],\r\n",
        "       train_regret_winner_15[slice64],\r\n",
        "       train_regret_winner_16[slice64],\r\n",
        "       train_regret_winner_17[slice64],\r\n",
        "       train_regret_winner_18[slice64],\r\n",
        "       train_regret_winner_19[slice64],\r\n",
        "       train_regret_winner_20[slice64]]\r\n",
        "\r\n",
        "loser64_results = pd.DataFrame(loser64).sort_values(by=[0], ascending=False)\r\n",
        "winner64_results = pd.DataFrame(winner64).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser64 = np.asarray(loser64_results[4:5][0])[0]\r\n",
        "median_loser64 = np.asarray(loser64_results[9:10][0])[0]\r\n",
        "upper_loser64 = np.asarray(loser64_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner64 = np.asarray(winner64_results[4:5][0])[0]\r\n",
        "median_winner64 = np.asarray(winner64_results[9:10][0])[0]\r\n",
        "upper_winner64 = np.asarray(winner64_results[14:15][0])[0]"
      ],
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvr3S_p0w_ii"
      },
      "source": [
        "# Iteration74 :\r\n",
        "\r\n",
        "slice74 = 73\r\n",
        "\r\n",
        "loser74 = [train_regret_loser_1[slice74],\r\n",
        "       train_regret_loser_2[slice74],\r\n",
        "       train_regret_loser_3[slice74],\r\n",
        "       train_regret_loser_4[slice74],\r\n",
        "       train_regret_loser_5[slice74],\r\n",
        "       train_regret_loser_6[slice74],\r\n",
        "       train_regret_loser_7[slice74],\r\n",
        "       train_regret_loser_8[slice74],\r\n",
        "       train_regret_loser_9[slice74],\r\n",
        "       train_regret_loser_10[slice74],\r\n",
        "       train_regret_loser_11[slice74],\r\n",
        "       train_regret_loser_12[slice74],\r\n",
        "       train_regret_loser_13[slice74],\r\n",
        "       train_regret_loser_14[slice74],\r\n",
        "       train_regret_loser_15[slice74],\r\n",
        "       train_regret_loser_16[slice74],\r\n",
        "       train_regret_loser_17[slice74],\r\n",
        "       train_regret_loser_18[slice74],\r\n",
        "       train_regret_loser_19[slice74],\r\n",
        "       train_regret_loser_20[slice74]]\r\n",
        "\r\n",
        "winner74 = [train_regret_winner_1[slice74],\r\n",
        "       train_regret_winner_2[slice74],\r\n",
        "       train_regret_winner_3[slice74],\r\n",
        "       train_regret_winner_4[slice74],\r\n",
        "       train_regret_winner_5[slice74],\r\n",
        "       train_regret_winner_6[slice74],\r\n",
        "       train_regret_winner_7[slice74],\r\n",
        "       train_regret_winner_8[slice74],\r\n",
        "       train_regret_winner_9[slice74],\r\n",
        "       train_regret_winner_10[slice74],\r\n",
        "       train_regret_winner_11[slice74],\r\n",
        "       train_regret_winner_12[slice74],\r\n",
        "       train_regret_winner_13[slice74],\r\n",
        "       train_regret_winner_14[slice74],\r\n",
        "       train_regret_winner_15[slice74],\r\n",
        "       train_regret_winner_16[slice74],\r\n",
        "       train_regret_winner_17[slice74],\r\n",
        "       train_regret_winner_18[slice74],\r\n",
        "       train_regret_winner_19[slice74],\r\n",
        "       train_regret_winner_20[slice74]]\r\n",
        "\r\n",
        "loser74_results = pd.DataFrame(loser74).sort_values(by=[0], ascending=False)\r\n",
        "winner74_results = pd.DataFrame(winner74).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser74 = np.asarray(loser74_results[4:5][0])[0]\r\n",
        "median_loser74 = np.asarray(loser74_results[9:10][0])[0]\r\n",
        "upper_loser74 = np.asarray(loser74_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner74 = np.asarray(winner74_results[4:5][0])[0]\r\n",
        "median_winner74 = np.asarray(winner74_results[9:10][0])[0]\r\n",
        "upper_winner74 = np.asarray(winner74_results[14:15][0])[0]"
      ],
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEAvUVK7w_lO"
      },
      "source": [
        "# Iteration84 :\r\n",
        "\r\n",
        "slice84 = 83\r\n",
        "\r\n",
        "loser84 = [train_regret_loser_1[slice84],\r\n",
        "       train_regret_loser_2[slice84],\r\n",
        "       train_regret_loser_3[slice84],\r\n",
        "       train_regret_loser_4[slice84],\r\n",
        "       train_regret_loser_5[slice84],\r\n",
        "       train_regret_loser_6[slice84],\r\n",
        "       train_regret_loser_7[slice84],\r\n",
        "       train_regret_loser_8[slice84],\r\n",
        "       train_regret_loser_9[slice84],\r\n",
        "       train_regret_loser_10[slice84],\r\n",
        "       train_regret_loser_11[slice84],\r\n",
        "       train_regret_loser_12[slice84],\r\n",
        "       train_regret_loser_13[slice84],\r\n",
        "       train_regret_loser_14[slice84],\r\n",
        "       train_regret_loser_15[slice84],\r\n",
        "       train_regret_loser_16[slice84],\r\n",
        "       train_regret_loser_17[slice84],\r\n",
        "       train_regret_loser_18[slice84],\r\n",
        "       train_regret_loser_19[slice84],\r\n",
        "       train_regret_loser_20[slice84]]\r\n",
        "\r\n",
        "winner84 = [train_regret_winner_1[slice84],\r\n",
        "       train_regret_winner_2[slice84],\r\n",
        "       train_regret_winner_3[slice84],\r\n",
        "       train_regret_winner_4[slice84],\r\n",
        "       train_regret_winner_5[slice84],\r\n",
        "       train_regret_winner_6[slice84],\r\n",
        "       train_regret_winner_7[slice84],\r\n",
        "       train_regret_winner_8[slice84],\r\n",
        "       train_regret_winner_9[slice84],\r\n",
        "       train_regret_winner_10[slice84],\r\n",
        "       train_regret_winner_11[slice84],\r\n",
        "       train_regret_winner_12[slice84],\r\n",
        "       train_regret_winner_13[slice84],\r\n",
        "       train_regret_winner_14[slice84],\r\n",
        "       train_regret_winner_15[slice84],\r\n",
        "       train_regret_winner_16[slice84],\r\n",
        "       train_regret_winner_17[slice84],\r\n",
        "       train_regret_winner_18[slice84],\r\n",
        "       train_regret_winner_19[slice84],\r\n",
        "       train_regret_winner_20[slice84]]\r\n",
        "\r\n",
        "loser84_results = pd.DataFrame(loser84).sort_values(by=[0], ascending=False)\r\n",
        "winner84_results = pd.DataFrame(winner84).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser84 = np.asarray(loser84_results[4:5][0])[0]\r\n",
        "median_loser84 = np.asarray(loser84_results[9:10][0])[0]\r\n",
        "upper_loser84 = np.asarray(loser84_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner84 = np.asarray(winner84_results[4:5][0])[0]\r\n",
        "median_winner84 = np.asarray(winner84_results[9:10][0])[0]\r\n",
        "upper_winner84 = np.asarray(winner84_results[14:15][0])[0]"
      ],
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9TMZzhdGw_oC"
      },
      "source": [
        "# Iteration94 :\r\n",
        "\r\n",
        "slice94 = 93\r\n",
        "\r\n",
        "loser94 = [train_regret_loser_1[slice94],\r\n",
        "       train_regret_loser_2[slice94],\r\n",
        "       train_regret_loser_3[slice94],\r\n",
        "       train_regret_loser_4[slice94],\r\n",
        "       train_regret_loser_5[slice94],\r\n",
        "       train_regret_loser_6[slice94],\r\n",
        "       train_regret_loser_7[slice94],\r\n",
        "       train_regret_loser_8[slice94],\r\n",
        "       train_regret_loser_9[slice94],\r\n",
        "       train_regret_loser_10[slice94],\r\n",
        "       train_regret_loser_11[slice94],\r\n",
        "       train_regret_loser_12[slice94],\r\n",
        "       train_regret_loser_13[slice94],\r\n",
        "       train_regret_loser_14[slice94],\r\n",
        "       train_regret_loser_15[slice94],\r\n",
        "       train_regret_loser_16[slice94],\r\n",
        "       train_regret_loser_17[slice94],\r\n",
        "       train_regret_loser_18[slice94],\r\n",
        "       train_regret_loser_19[slice94],\r\n",
        "       train_regret_loser_20[slice94]]\r\n",
        "\r\n",
        "winner94 = [train_regret_winner_1[slice94],\r\n",
        "       train_regret_winner_2[slice94],\r\n",
        "       train_regret_winner_3[slice94],\r\n",
        "       train_regret_winner_4[slice94],\r\n",
        "       train_regret_winner_5[slice94],\r\n",
        "       train_regret_winner_6[slice94],\r\n",
        "       train_regret_winner_7[slice94],\r\n",
        "       train_regret_winner_8[slice94],\r\n",
        "       train_regret_winner_9[slice94],\r\n",
        "       train_regret_winner_10[slice94],\r\n",
        "       train_regret_winner_11[slice94],\r\n",
        "       train_regret_winner_12[slice94],\r\n",
        "       train_regret_winner_13[slice94],\r\n",
        "       train_regret_winner_14[slice94],\r\n",
        "       train_regret_winner_15[slice94],\r\n",
        "       train_regret_winner_16[slice94],\r\n",
        "       train_regret_winner_17[slice94],\r\n",
        "       train_regret_winner_18[slice94],\r\n",
        "       train_regret_winner_19[slice94],\r\n",
        "       train_regret_winner_20[slice94]]\r\n",
        "\r\n",
        "loser94_results = pd.DataFrame(loser94).sort_values(by=[0], ascending=False)\r\n",
        "winner94_results = pd.DataFrame(winner94).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser94 = np.asarray(loser94_results[4:5][0])[0]\r\n",
        "median_loser94 = np.asarray(loser94_results[9:10][0])[0]\r\n",
        "upper_loser94 = np.asarray(loser94_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner94 = np.asarray(winner94_results[4:5][0])[0]\r\n",
        "median_winner94 = np.asarray(winner94_results[9:10][0])[0]\r\n",
        "upper_winner94 = np.asarray(winner94_results[14:15][0])[0]"
      ],
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMxHQOgaw_qu"
      },
      "source": [
        "# Iteration5 :\r\n",
        "\r\n",
        "slice5 = 4\r\n",
        "\r\n",
        "loser5 = [train_regret_loser_1[slice5],\r\n",
        "       train_regret_loser_2[slice5],\r\n",
        "       train_regret_loser_3[slice5],\r\n",
        "       train_regret_loser_4[slice5],\r\n",
        "       train_regret_loser_5[slice5],\r\n",
        "       train_regret_loser_6[slice5],\r\n",
        "       train_regret_loser_7[slice5],\r\n",
        "       train_regret_loser_8[slice5],\r\n",
        "       train_regret_loser_9[slice5],\r\n",
        "       train_regret_loser_10[slice5],\r\n",
        "       train_regret_loser_11[slice5],\r\n",
        "       train_regret_loser_12[slice5],\r\n",
        "       train_regret_loser_13[slice5],\r\n",
        "       train_regret_loser_14[slice5],\r\n",
        "       train_regret_loser_15[slice5],\r\n",
        "       train_regret_loser_16[slice5],\r\n",
        "       train_regret_loser_17[slice5],\r\n",
        "       train_regret_loser_18[slice5],\r\n",
        "       train_regret_loser_19[slice5],\r\n",
        "       train_regret_loser_20[slice5]]\r\n",
        "\r\n",
        "winner5 = [train_regret_winner_1[slice5],\r\n",
        "       train_regret_winner_2[slice5],\r\n",
        "       train_regret_winner_3[slice5],\r\n",
        "       train_regret_winner_4[slice5],\r\n",
        "       train_regret_winner_5[slice5],\r\n",
        "       train_regret_winner_6[slice5],\r\n",
        "       train_regret_winner_7[slice5],\r\n",
        "       train_regret_winner_8[slice5],\r\n",
        "       train_regret_winner_9[slice5],\r\n",
        "       train_regret_winner_10[slice5],\r\n",
        "       train_regret_winner_11[slice5],\r\n",
        "       train_regret_winner_12[slice5],\r\n",
        "       train_regret_winner_13[slice5],\r\n",
        "       train_regret_winner_14[slice5],\r\n",
        "       train_regret_winner_15[slice5],\r\n",
        "       train_regret_winner_16[slice5],\r\n",
        "       train_regret_winner_17[slice5],\r\n",
        "       train_regret_winner_18[slice5],\r\n",
        "       train_regret_winner_19[slice5],\r\n",
        "       train_regret_winner_20[slice5]]\r\n",
        "\r\n",
        "loser5_results = pd.DataFrame(loser5).sort_values(by=[0], ascending=False)\r\n",
        "winner5_results = pd.DataFrame(winner5).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser5 = np.asarray(loser5_results[4:5][0])[0]\r\n",
        "median_loser5 = np.asarray(loser5_results[9:10][0])[0]\r\n",
        "upper_loser5 = np.asarray(loser5_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner5 = np.asarray(winner5_results[4:5][0])[0]\r\n",
        "median_winner5 = np.asarray(winner5_results[9:10][0])[0]\r\n",
        "upper_winner5 = np.asarray(winner5_results[14:15][0])[0]"
      ],
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPVFQTfNw_wF"
      },
      "source": [
        "# Iteration15 :\r\n",
        "\r\n",
        "slice15 = 14\r\n",
        "\r\n",
        "loser15 = [train_regret_loser_1[slice15],\r\n",
        "       train_regret_loser_2[slice15],\r\n",
        "       train_regret_loser_3[slice15],\r\n",
        "       train_regret_loser_4[slice15],\r\n",
        "       train_regret_loser_5[slice15],\r\n",
        "       train_regret_loser_6[slice15],\r\n",
        "       train_regret_loser_7[slice15],\r\n",
        "       train_regret_loser_8[slice15],\r\n",
        "       train_regret_loser_9[slice15],\r\n",
        "       train_regret_loser_10[slice15],\r\n",
        "       train_regret_loser_11[slice15],\r\n",
        "       train_regret_loser_12[slice15],\r\n",
        "       train_regret_loser_13[slice15],\r\n",
        "       train_regret_loser_14[slice15],\r\n",
        "       train_regret_loser_15[slice15],\r\n",
        "       train_regret_loser_16[slice15],\r\n",
        "       train_regret_loser_17[slice15],\r\n",
        "       train_regret_loser_18[slice15],\r\n",
        "       train_regret_loser_19[slice15],\r\n",
        "       train_regret_loser_20[slice15]]\r\n",
        "\r\n",
        "winner15 = [train_regret_winner_1[slice15],\r\n",
        "       train_regret_winner_2[slice15],\r\n",
        "       train_regret_winner_3[slice15],\r\n",
        "       train_regret_winner_4[slice15],\r\n",
        "       train_regret_winner_5[slice15],\r\n",
        "       train_regret_winner_6[slice15],\r\n",
        "       train_regret_winner_7[slice15],\r\n",
        "       train_regret_winner_8[slice15],\r\n",
        "       train_regret_winner_9[slice15],\r\n",
        "       train_regret_winner_10[slice15],\r\n",
        "       train_regret_winner_11[slice15],\r\n",
        "       train_regret_winner_12[slice15],\r\n",
        "       train_regret_winner_13[slice15],\r\n",
        "       train_regret_winner_14[slice15],\r\n",
        "       train_regret_winner_15[slice15],\r\n",
        "       train_regret_winner_16[slice15],\r\n",
        "       train_regret_winner_17[slice15],\r\n",
        "       train_regret_winner_18[slice15],\r\n",
        "       train_regret_winner_19[slice15],\r\n",
        "       train_regret_winner_20[slice15]]\r\n",
        "\r\n",
        "loser15_results = pd.DataFrame(loser15).sort_values(by=[0], ascending=False)\r\n",
        "winner15_results = pd.DataFrame(winner15).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser15 = np.asarray(loser15_results[4:5][0])[0]\r\n",
        "median_loser15 = np.asarray(loser15_results[9:10][0])[0]\r\n",
        "upper_loser15 = np.asarray(loser15_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner15 = np.asarray(winner15_results[4:5][0])[0]\r\n",
        "median_winner15 = np.asarray(winner15_results[9:10][0])[0]\r\n",
        "upper_winner15 = np.asarray(winner15_results[14:15][0])[0]"
      ],
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADBmBA3ew_zK"
      },
      "source": [
        "# Iteration25 :\r\n",
        "\r\n",
        "slice25 = 24\r\n",
        "\r\n",
        "loser25 = [train_regret_loser_1[slice25],\r\n",
        "       train_regret_loser_2[slice25],\r\n",
        "       train_regret_loser_3[slice25],\r\n",
        "       train_regret_loser_4[slice25],\r\n",
        "       train_regret_loser_5[slice25],\r\n",
        "       train_regret_loser_6[slice25],\r\n",
        "       train_regret_loser_7[slice25],\r\n",
        "       train_regret_loser_8[slice25],\r\n",
        "       train_regret_loser_9[slice25],\r\n",
        "       train_regret_loser_10[slice25],\r\n",
        "       train_regret_loser_11[slice25],\r\n",
        "       train_regret_loser_12[slice25],\r\n",
        "       train_regret_loser_13[slice25],\r\n",
        "       train_regret_loser_14[slice25],\r\n",
        "       train_regret_loser_15[slice25],\r\n",
        "       train_regret_loser_16[slice25],\r\n",
        "       train_regret_loser_17[slice25],\r\n",
        "       train_regret_loser_18[slice25],\r\n",
        "       train_regret_loser_19[slice25],\r\n",
        "       train_regret_loser_20[slice25]]\r\n",
        "\r\n",
        "winner25 = [train_regret_winner_1[slice25],\r\n",
        "       train_regret_winner_2[slice25],\r\n",
        "       train_regret_winner_3[slice25],\r\n",
        "       train_regret_winner_4[slice25],\r\n",
        "       train_regret_winner_5[slice25],\r\n",
        "       train_regret_winner_6[slice25],\r\n",
        "       train_regret_winner_7[slice25],\r\n",
        "       train_regret_winner_8[slice25],\r\n",
        "       train_regret_winner_9[slice25],\r\n",
        "       train_regret_winner_10[slice25],\r\n",
        "       train_regret_winner_11[slice25],\r\n",
        "       train_regret_winner_12[slice25],\r\n",
        "       train_regret_winner_13[slice25],\r\n",
        "       train_regret_winner_14[slice25],\r\n",
        "       train_regret_winner_15[slice25],\r\n",
        "       train_regret_winner_16[slice25],\r\n",
        "       train_regret_winner_17[slice25],\r\n",
        "       train_regret_winner_18[slice25],\r\n",
        "       train_regret_winner_19[slice25],\r\n",
        "       train_regret_winner_20[slice25]]\r\n",
        "\r\n",
        "loser25_results = pd.DataFrame(loser25).sort_values(by=[0], ascending=False)\r\n",
        "winner25_results = pd.DataFrame(winner25).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser25 = np.asarray(loser25_results[4:5][0])[0]\r\n",
        "median_loser25 = np.asarray(loser25_results[9:10][0])[0]\r\n",
        "upper_loser25 = np.asarray(loser25_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner25 = np.asarray(winner25_results[4:5][0])[0]\r\n",
        "median_winner25 = np.asarray(winner25_results[9:10][0])[0]\r\n",
        "upper_winner25 = np.asarray(winner25_results[14:15][0])[0]"
      ],
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUk9UtSTw_2N"
      },
      "source": [
        "# Iteration35 :\r\n",
        "\r\n",
        "slice35 = 34\r\n",
        "\r\n",
        "loser35 = [train_regret_loser_1[slice35],\r\n",
        "       train_regret_loser_2[slice35],\r\n",
        "       train_regret_loser_3[slice35],\r\n",
        "       train_regret_loser_4[slice35],\r\n",
        "       train_regret_loser_5[slice35],\r\n",
        "       train_regret_loser_6[slice35],\r\n",
        "       train_regret_loser_7[slice35],\r\n",
        "       train_regret_loser_8[slice35],\r\n",
        "       train_regret_loser_9[slice35],\r\n",
        "       train_regret_loser_10[slice35],\r\n",
        "       train_regret_loser_11[slice35],\r\n",
        "       train_regret_loser_12[slice35],\r\n",
        "       train_regret_loser_13[slice35],\r\n",
        "       train_regret_loser_14[slice35],\r\n",
        "       train_regret_loser_15[slice35],\r\n",
        "       train_regret_loser_16[slice35],\r\n",
        "       train_regret_loser_17[slice35],\r\n",
        "       train_regret_loser_18[slice35],\r\n",
        "       train_regret_loser_19[slice35],\r\n",
        "       train_regret_loser_20[slice35]]\r\n",
        "\r\n",
        "winner35 = [train_regret_winner_1[slice35],\r\n",
        "       train_regret_winner_2[slice35],\r\n",
        "       train_regret_winner_3[slice35],\r\n",
        "       train_regret_winner_4[slice35],\r\n",
        "       train_regret_winner_5[slice35],\r\n",
        "       train_regret_winner_6[slice35],\r\n",
        "       train_regret_winner_7[slice35],\r\n",
        "       train_regret_winner_8[slice35],\r\n",
        "       train_regret_winner_9[slice35],\r\n",
        "       train_regret_winner_10[slice35],\r\n",
        "       train_regret_winner_11[slice35],\r\n",
        "       train_regret_winner_12[slice35],\r\n",
        "       train_regret_winner_13[slice35],\r\n",
        "       train_regret_winner_14[slice35],\r\n",
        "       train_regret_winner_15[slice35],\r\n",
        "       train_regret_winner_16[slice35],\r\n",
        "       train_regret_winner_17[slice35],\r\n",
        "       train_regret_winner_18[slice35],\r\n",
        "       train_regret_winner_19[slice35],\r\n",
        "       train_regret_winner_20[slice35]]\r\n",
        "\r\n",
        "loser35_results = pd.DataFrame(loser35).sort_values(by=[0], ascending=False)\r\n",
        "winner35_results = pd.DataFrame(winner35).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser35 = np.asarray(loser35_results[4:5][0])[0]\r\n",
        "median_loser35 = np.asarray(loser35_results[9:10][0])[0]\r\n",
        "upper_loser35 = np.asarray(loser35_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner35 = np.asarray(winner35_results[4:5][0])[0]\r\n",
        "median_winner35 = np.asarray(winner35_results[9:10][0])[0]\r\n",
        "upper_winner35 = np.asarray(winner35_results[14:15][0])[0]"
      ],
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xX3VgqD_w_4z"
      },
      "source": [
        "# Iteration45 :\r\n",
        "\r\n",
        "slice45 = 44\r\n",
        "\r\n",
        "loser45 = [train_regret_loser_1[slice45],\r\n",
        "       train_regret_loser_2[slice45],\r\n",
        "       train_regret_loser_3[slice45],\r\n",
        "       train_regret_loser_4[slice45],\r\n",
        "       train_regret_loser_5[slice45],\r\n",
        "       train_regret_loser_6[slice45],\r\n",
        "       train_regret_loser_7[slice45],\r\n",
        "       train_regret_loser_8[slice45],\r\n",
        "       train_regret_loser_9[slice45],\r\n",
        "       train_regret_loser_10[slice45],\r\n",
        "       train_regret_loser_11[slice45],\r\n",
        "       train_regret_loser_12[slice45],\r\n",
        "       train_regret_loser_13[slice45],\r\n",
        "       train_regret_loser_14[slice45],\r\n",
        "       train_regret_loser_15[slice45],\r\n",
        "       train_regret_loser_16[slice45],\r\n",
        "       train_regret_loser_17[slice45],\r\n",
        "       train_regret_loser_18[slice45],\r\n",
        "       train_regret_loser_19[slice45],\r\n",
        "       train_regret_loser_20[slice45]]\r\n",
        "\r\n",
        "winner45 = [train_regret_winner_1[slice45],\r\n",
        "       train_regret_winner_2[slice45],\r\n",
        "       train_regret_winner_3[slice45],\r\n",
        "       train_regret_winner_4[slice45],\r\n",
        "       train_regret_winner_5[slice45],\r\n",
        "       train_regret_winner_6[slice45],\r\n",
        "       train_regret_winner_7[slice45],\r\n",
        "       train_regret_winner_8[slice45],\r\n",
        "       train_regret_winner_9[slice45],\r\n",
        "       train_regret_winner_10[slice45],\r\n",
        "       train_regret_winner_11[slice45],\r\n",
        "       train_regret_winner_12[slice45],\r\n",
        "       train_regret_winner_13[slice45],\r\n",
        "       train_regret_winner_14[slice45],\r\n",
        "       train_regret_winner_15[slice45],\r\n",
        "       train_regret_winner_16[slice45],\r\n",
        "       train_regret_winner_17[slice45],\r\n",
        "       train_regret_winner_18[slice45],\r\n",
        "       train_regret_winner_19[slice45],\r\n",
        "       train_regret_winner_20[slice45]]\r\n",
        "\r\n",
        "loser45_results = pd.DataFrame(loser45).sort_values(by=[0], ascending=False)\r\n",
        "winner45_results = pd.DataFrame(winner45).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser45 = np.asarray(loser45_results[4:5][0])[0]\r\n",
        "median_loser45 = np.asarray(loser45_results[9:10][0])[0]\r\n",
        "upper_loser45 = np.asarray(loser45_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner45 = np.asarray(winner45_results[4:5][0])[0]\r\n",
        "median_winner45 = np.asarray(winner45_results[9:10][0])[0]\r\n",
        "upper_winner45 = np.asarray(winner45_results[14:15][0])[0]"
      ],
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o35W-AOzw_8F"
      },
      "source": [
        "# Iteration55 :\r\n",
        "\r\n",
        "slice55 = 54\r\n",
        "\r\n",
        "loser55 = [train_regret_loser_1[slice55],\r\n",
        "       train_regret_loser_2[slice55],\r\n",
        "       train_regret_loser_3[slice55],\r\n",
        "       train_regret_loser_4[slice55],\r\n",
        "       train_regret_loser_5[slice55],\r\n",
        "       train_regret_loser_6[slice55],\r\n",
        "       train_regret_loser_7[slice55],\r\n",
        "       train_regret_loser_8[slice55],\r\n",
        "       train_regret_loser_9[slice55],\r\n",
        "       train_regret_loser_10[slice55],\r\n",
        "       train_regret_loser_11[slice55],\r\n",
        "       train_regret_loser_12[slice55],\r\n",
        "       train_regret_loser_13[slice55],\r\n",
        "       train_regret_loser_14[slice55],\r\n",
        "       train_regret_loser_15[slice55],\r\n",
        "       train_regret_loser_16[slice55],\r\n",
        "       train_regret_loser_17[slice55],\r\n",
        "       train_regret_loser_18[slice55],\r\n",
        "       train_regret_loser_19[slice55],\r\n",
        "       train_regret_loser_20[slice55]]\r\n",
        "\r\n",
        "winner55 = [train_regret_winner_1[slice55],\r\n",
        "       train_regret_winner_2[slice55],\r\n",
        "       train_regret_winner_3[slice55],\r\n",
        "       train_regret_winner_4[slice55],\r\n",
        "       train_regret_winner_5[slice55],\r\n",
        "       train_regret_winner_6[slice55],\r\n",
        "       train_regret_winner_7[slice55],\r\n",
        "       train_regret_winner_8[slice55],\r\n",
        "       train_regret_winner_9[slice55],\r\n",
        "       train_regret_winner_10[slice55],\r\n",
        "       train_regret_winner_11[slice55],\r\n",
        "       train_regret_winner_12[slice55],\r\n",
        "       train_regret_winner_13[slice55],\r\n",
        "       train_regret_winner_14[slice55],\r\n",
        "       train_regret_winner_15[slice55],\r\n",
        "       train_regret_winner_16[slice55],\r\n",
        "       train_regret_winner_17[slice55],\r\n",
        "       train_regret_winner_18[slice55],\r\n",
        "       train_regret_winner_19[slice55],\r\n",
        "       train_regret_winner_20[slice55]]\r\n",
        "\r\n",
        "loser55_results = pd.DataFrame(loser55).sort_values(by=[0], ascending=False)\r\n",
        "winner55_results = pd.DataFrame(winner55).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser55 = np.asarray(loser55_results[4:5][0])[0]\r\n",
        "median_loser55 = np.asarray(loser55_results[9:10][0])[0]\r\n",
        "upper_loser55 = np.asarray(loser55_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner55 = np.asarray(winner55_results[4:5][0])[0]\r\n",
        "median_winner55 = np.asarray(winner55_results[9:10][0])[0]\r\n",
        "upper_winner55 = np.asarray(winner55_results[14:15][0])[0]"
      ],
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUAjKXWdw__J"
      },
      "source": [
        "# Iteration65 :\r\n",
        "\r\n",
        "slice65 = 64\r\n",
        "\r\n",
        "loser65 = [train_regret_loser_1[slice65],\r\n",
        "       train_regret_loser_2[slice65],\r\n",
        "       train_regret_loser_3[slice65],\r\n",
        "       train_regret_loser_4[slice65],\r\n",
        "       train_regret_loser_5[slice65],\r\n",
        "       train_regret_loser_6[slice65],\r\n",
        "       train_regret_loser_7[slice65],\r\n",
        "       train_regret_loser_8[slice65],\r\n",
        "       train_regret_loser_9[slice65],\r\n",
        "       train_regret_loser_10[slice65],\r\n",
        "       train_regret_loser_11[slice65],\r\n",
        "       train_regret_loser_12[slice65],\r\n",
        "       train_regret_loser_13[slice65],\r\n",
        "       train_regret_loser_14[slice65],\r\n",
        "       train_regret_loser_15[slice65],\r\n",
        "       train_regret_loser_16[slice65],\r\n",
        "       train_regret_loser_17[slice65],\r\n",
        "       train_regret_loser_18[slice65],\r\n",
        "       train_regret_loser_19[slice65],\r\n",
        "       train_regret_loser_20[slice65]]\r\n",
        "\r\n",
        "winner65 = [train_regret_winner_1[slice65],\r\n",
        "       train_regret_winner_2[slice65],\r\n",
        "       train_regret_winner_3[slice65],\r\n",
        "       train_regret_winner_4[slice65],\r\n",
        "       train_regret_winner_5[slice65],\r\n",
        "       train_regret_winner_6[slice65],\r\n",
        "       train_regret_winner_7[slice65],\r\n",
        "       train_regret_winner_8[slice65],\r\n",
        "       train_regret_winner_9[slice65],\r\n",
        "       train_regret_winner_10[slice65],\r\n",
        "       train_regret_winner_11[slice65],\r\n",
        "       train_regret_winner_12[slice65],\r\n",
        "       train_regret_winner_13[slice65],\r\n",
        "       train_regret_winner_14[slice65],\r\n",
        "       train_regret_winner_15[slice65],\r\n",
        "       train_regret_winner_16[slice65],\r\n",
        "       train_regret_winner_17[slice65],\r\n",
        "       train_regret_winner_18[slice65],\r\n",
        "       train_regret_winner_19[slice65],\r\n",
        "       train_regret_winner_20[slice65]]\r\n",
        "\r\n",
        "loser65_results = pd.DataFrame(loser65).sort_values(by=[0], ascending=False)\r\n",
        "winner65_results = pd.DataFrame(winner65).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser65 = np.asarray(loser65_results[4:5][0])[0]\r\n",
        "median_loser65 = np.asarray(loser65_results[9:10][0])[0]\r\n",
        "upper_loser65 = np.asarray(loser65_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner65 = np.asarray(winner65_results[4:5][0])[0]\r\n",
        "median_winner65 = np.asarray(winner65_results[9:10][0])[0]\r\n",
        "upper_winner65 = np.asarray(winner65_results[14:15][0])[0]"
      ],
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D00FcmZwxAB9"
      },
      "source": [
        "# Iteration75 :\r\n",
        "\r\n",
        "slice75 = 74\r\n",
        "\r\n",
        "loser75 = [train_regret_loser_1[slice75],\r\n",
        "       train_regret_loser_2[slice75],\r\n",
        "       train_regret_loser_3[slice75],\r\n",
        "       train_regret_loser_4[slice75],\r\n",
        "       train_regret_loser_5[slice75],\r\n",
        "       train_regret_loser_6[slice75],\r\n",
        "       train_regret_loser_7[slice75],\r\n",
        "       train_regret_loser_8[slice75],\r\n",
        "       train_regret_loser_9[slice75],\r\n",
        "       train_regret_loser_10[slice75],\r\n",
        "       train_regret_loser_11[slice75],\r\n",
        "       train_regret_loser_12[slice75],\r\n",
        "       train_regret_loser_13[slice75],\r\n",
        "       train_regret_loser_14[slice75],\r\n",
        "       train_regret_loser_15[slice75],\r\n",
        "       train_regret_loser_16[slice75],\r\n",
        "       train_regret_loser_17[slice75],\r\n",
        "       train_regret_loser_18[slice75],\r\n",
        "       train_regret_loser_19[slice75],\r\n",
        "       train_regret_loser_20[slice75]]\r\n",
        "\r\n",
        "winner75 = [train_regret_winner_1[slice75],\r\n",
        "       train_regret_winner_2[slice75],\r\n",
        "       train_regret_winner_3[slice75],\r\n",
        "       train_regret_winner_4[slice75],\r\n",
        "       train_regret_winner_5[slice75],\r\n",
        "       train_regret_winner_6[slice75],\r\n",
        "       train_regret_winner_7[slice75],\r\n",
        "       train_regret_winner_8[slice75],\r\n",
        "       train_regret_winner_9[slice75],\r\n",
        "       train_regret_winner_10[slice75],\r\n",
        "       train_regret_winner_11[slice75],\r\n",
        "       train_regret_winner_12[slice75],\r\n",
        "       train_regret_winner_13[slice75],\r\n",
        "       train_regret_winner_14[slice75],\r\n",
        "       train_regret_winner_15[slice75],\r\n",
        "       train_regret_winner_16[slice75],\r\n",
        "       train_regret_winner_17[slice75],\r\n",
        "       train_regret_winner_18[slice75],\r\n",
        "       train_regret_winner_19[slice75],\r\n",
        "       train_regret_winner_20[slice75]]\r\n",
        "\r\n",
        "loser75_results = pd.DataFrame(loser75).sort_values(by=[0], ascending=False)\r\n",
        "winner75_results = pd.DataFrame(winner75).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser75 = np.asarray(loser75_results[4:5][0])[0]\r\n",
        "median_loser75 = np.asarray(loser75_results[9:10][0])[0]\r\n",
        "upper_loser75 = np.asarray(loser75_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner75 = np.asarray(winner75_results[4:5][0])[0]\r\n",
        "median_winner75 = np.asarray(winner75_results[9:10][0])[0]\r\n",
        "upper_winner75 = np.asarray(winner75_results[14:15][0])[0]"
      ],
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvnzmprxxAFB"
      },
      "source": [
        "# Iteration85 :\r\n",
        "\r\n",
        "slice85 = 84\r\n",
        "\r\n",
        "loser85 = [train_regret_loser_1[slice85],\r\n",
        "       train_regret_loser_2[slice85],\r\n",
        "       train_regret_loser_3[slice85],\r\n",
        "       train_regret_loser_4[slice85],\r\n",
        "       train_regret_loser_5[slice85],\r\n",
        "       train_regret_loser_6[slice85],\r\n",
        "       train_regret_loser_7[slice85],\r\n",
        "       train_regret_loser_8[slice85],\r\n",
        "       train_regret_loser_9[slice85],\r\n",
        "       train_regret_loser_10[slice85],\r\n",
        "       train_regret_loser_11[slice85],\r\n",
        "       train_regret_loser_12[slice85],\r\n",
        "       train_regret_loser_13[slice85],\r\n",
        "       train_regret_loser_14[slice85],\r\n",
        "       train_regret_loser_15[slice85],\r\n",
        "       train_regret_loser_16[slice85],\r\n",
        "       train_regret_loser_17[slice85],\r\n",
        "       train_regret_loser_18[slice85],\r\n",
        "       train_regret_loser_19[slice85],\r\n",
        "       train_regret_loser_20[slice85]]\r\n",
        "\r\n",
        "winner85 = [train_regret_winner_1[slice85],\r\n",
        "       train_regret_winner_2[slice85],\r\n",
        "       train_regret_winner_3[slice85],\r\n",
        "       train_regret_winner_4[slice85],\r\n",
        "       train_regret_winner_5[slice85],\r\n",
        "       train_regret_winner_6[slice85],\r\n",
        "       train_regret_winner_7[slice85],\r\n",
        "       train_regret_winner_8[slice85],\r\n",
        "       train_regret_winner_9[slice85],\r\n",
        "       train_regret_winner_10[slice85],\r\n",
        "       train_regret_winner_11[slice85],\r\n",
        "       train_regret_winner_12[slice85],\r\n",
        "       train_regret_winner_13[slice85],\r\n",
        "       train_regret_winner_14[slice85],\r\n",
        "       train_regret_winner_15[slice85],\r\n",
        "       train_regret_winner_16[slice85],\r\n",
        "       train_regret_winner_17[slice85],\r\n",
        "       train_regret_winner_18[slice85],\r\n",
        "       train_regret_winner_19[slice85],\r\n",
        "       train_regret_winner_20[slice85]]\r\n",
        "\r\n",
        "loser85_results = pd.DataFrame(loser85).sort_values(by=[0], ascending=False)\r\n",
        "winner85_results = pd.DataFrame(winner85).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser85 = np.asarray(loser85_results[4:5][0])[0]\r\n",
        "median_loser85 = np.asarray(loser85_results[9:10][0])[0]\r\n",
        "upper_loser85 = np.asarray(loser85_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner85 = np.asarray(winner85_results[4:5][0])[0]\r\n",
        "median_winner85 = np.asarray(winner85_results[9:10][0])[0]\r\n",
        "upper_winner85 = np.asarray(winner85_results[14:15][0])[0]"
      ],
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztvJhrN6xAH1"
      },
      "source": [
        "# Iteration95 :\r\n",
        "\r\n",
        "slice95 = 94\r\n",
        "\r\n",
        "loser95 = [train_regret_loser_1[slice95],\r\n",
        "       train_regret_loser_2[slice95],\r\n",
        "       train_regret_loser_3[slice95],\r\n",
        "       train_regret_loser_4[slice95],\r\n",
        "       train_regret_loser_5[slice95],\r\n",
        "       train_regret_loser_6[slice95],\r\n",
        "       train_regret_loser_7[slice95],\r\n",
        "       train_regret_loser_8[slice95],\r\n",
        "       train_regret_loser_9[slice95],\r\n",
        "       train_regret_loser_10[slice95],\r\n",
        "       train_regret_loser_11[slice95],\r\n",
        "       train_regret_loser_12[slice95],\r\n",
        "       train_regret_loser_13[slice95],\r\n",
        "       train_regret_loser_14[slice95],\r\n",
        "       train_regret_loser_15[slice95],\r\n",
        "       train_regret_loser_16[slice95],\r\n",
        "       train_regret_loser_17[slice95],\r\n",
        "       train_regret_loser_18[slice95],\r\n",
        "       train_regret_loser_19[slice95],\r\n",
        "       train_regret_loser_20[slice95]]\r\n",
        "\r\n",
        "winner95 = [train_regret_winner_1[slice95],\r\n",
        "       train_regret_winner_2[slice95],\r\n",
        "       train_regret_winner_3[slice95],\r\n",
        "       train_regret_winner_4[slice95],\r\n",
        "       train_regret_winner_5[slice95],\r\n",
        "       train_regret_winner_6[slice95],\r\n",
        "       train_regret_winner_7[slice95],\r\n",
        "       train_regret_winner_8[slice95],\r\n",
        "       train_regret_winner_9[slice95],\r\n",
        "       train_regret_winner_10[slice95],\r\n",
        "       train_regret_winner_11[slice95],\r\n",
        "       train_regret_winner_12[slice95],\r\n",
        "       train_regret_winner_13[slice95],\r\n",
        "       train_regret_winner_14[slice95],\r\n",
        "       train_regret_winner_15[slice95],\r\n",
        "       train_regret_winner_16[slice95],\r\n",
        "       train_regret_winner_17[slice95],\r\n",
        "       train_regret_winner_18[slice95],\r\n",
        "       train_regret_winner_19[slice95],\r\n",
        "       train_regret_winner_20[slice95]]\r\n",
        "\r\n",
        "loser95_results = pd.DataFrame(loser95).sort_values(by=[0], ascending=False)\r\n",
        "winner95_results = pd.DataFrame(winner95).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser95 = np.asarray(loser95_results[4:5][0])[0]\r\n",
        "median_loser95 = np.asarray(loser95_results[9:10][0])[0]\r\n",
        "upper_loser95 = np.asarray(loser95_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner95 = np.asarray(winner95_results[4:5][0])[0]\r\n",
        "median_winner95 = np.asarray(winner95_results[9:10][0])[0]\r\n",
        "upper_winner95 = np.asarray(winner95_results[14:15][0])[0]"
      ],
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SK-4nr-OxALA"
      },
      "source": [
        "# Iteration6 :\r\n",
        "\r\n",
        "slice6 = 5\r\n",
        "\r\n",
        "loser6 = [train_regret_loser_1[slice6],\r\n",
        "       train_regret_loser_2[slice6],\r\n",
        "       train_regret_loser_3[slice6],\r\n",
        "       train_regret_loser_4[slice6],\r\n",
        "       train_regret_loser_5[slice6],\r\n",
        "       train_regret_loser_6[slice6],\r\n",
        "       train_regret_loser_7[slice6],\r\n",
        "       train_regret_loser_8[slice6],\r\n",
        "       train_regret_loser_9[slice6],\r\n",
        "       train_regret_loser_10[slice6],\r\n",
        "       train_regret_loser_11[slice6],\r\n",
        "       train_regret_loser_12[slice6],\r\n",
        "       train_regret_loser_13[slice6],\r\n",
        "       train_regret_loser_14[slice6],\r\n",
        "       train_regret_loser_15[slice6],\r\n",
        "       train_regret_loser_16[slice6],\r\n",
        "       train_regret_loser_17[slice6],\r\n",
        "       train_regret_loser_18[slice6],\r\n",
        "       train_regret_loser_19[slice6],\r\n",
        "       train_regret_loser_20[slice6]]\r\n",
        "\r\n",
        "winner6 = [train_regret_winner_1[slice6],\r\n",
        "       train_regret_winner_2[slice6],\r\n",
        "       train_regret_winner_3[slice6],\r\n",
        "       train_regret_winner_4[slice6],\r\n",
        "       train_regret_winner_5[slice6],\r\n",
        "       train_regret_winner_6[slice6],\r\n",
        "       train_regret_winner_7[slice6],\r\n",
        "       train_regret_winner_8[slice6],\r\n",
        "       train_regret_winner_9[slice6],\r\n",
        "       train_regret_winner_10[slice6],\r\n",
        "       train_regret_winner_11[slice6],\r\n",
        "       train_regret_winner_12[slice6],\r\n",
        "       train_regret_winner_13[slice6],\r\n",
        "       train_regret_winner_14[slice6],\r\n",
        "       train_regret_winner_15[slice6],\r\n",
        "       train_regret_winner_16[slice6],\r\n",
        "       train_regret_winner_17[slice6],\r\n",
        "       train_regret_winner_18[slice6],\r\n",
        "       train_regret_winner_19[slice6],\r\n",
        "       train_regret_winner_20[slice6]]\r\n",
        "\r\n",
        "loser6_results = pd.DataFrame(loser6).sort_values(by=[0], ascending=False)\r\n",
        "winner6_results = pd.DataFrame(winner6).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser6 = np.asarray(loser6_results[4:5][0])[0]\r\n",
        "median_loser6 = np.asarray(loser6_results[9:10][0])[0]\r\n",
        "upper_loser6 = np.asarray(loser6_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner6 = np.asarray(winner6_results[4:5][0])[0]\r\n",
        "median_winner6 = np.asarray(winner6_results[9:10][0])[0]\r\n",
        "upper_winner6 = np.asarray(winner6_results[14:15][0])[0]"
      ],
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSwfNX5ExANt"
      },
      "source": [
        "# Iteration16 :\r\n",
        "\r\n",
        "slice16 = 15\r\n",
        "\r\n",
        "loser16 = [train_regret_loser_1[slice16],\r\n",
        "       train_regret_loser_2[slice16],\r\n",
        "       train_regret_loser_3[slice16],\r\n",
        "       train_regret_loser_4[slice16],\r\n",
        "       train_regret_loser_5[slice16],\r\n",
        "       train_regret_loser_6[slice16],\r\n",
        "       train_regret_loser_7[slice16],\r\n",
        "       train_regret_loser_8[slice16],\r\n",
        "       train_regret_loser_9[slice16],\r\n",
        "       train_regret_loser_10[slice16],\r\n",
        "       train_regret_loser_11[slice16],\r\n",
        "       train_regret_loser_12[slice16],\r\n",
        "       train_regret_loser_13[slice16],\r\n",
        "       train_regret_loser_14[slice16],\r\n",
        "       train_regret_loser_15[slice16],\r\n",
        "       train_regret_loser_16[slice16],\r\n",
        "       train_regret_loser_17[slice16],\r\n",
        "       train_regret_loser_18[slice16],\r\n",
        "       train_regret_loser_19[slice16],\r\n",
        "       train_regret_loser_20[slice16]]\r\n",
        "\r\n",
        "winner16 = [train_regret_winner_1[slice16],\r\n",
        "       train_regret_winner_2[slice16],\r\n",
        "       train_regret_winner_3[slice16],\r\n",
        "       train_regret_winner_4[slice16],\r\n",
        "       train_regret_winner_5[slice16],\r\n",
        "       train_regret_winner_6[slice16],\r\n",
        "       train_regret_winner_7[slice16],\r\n",
        "       train_regret_winner_8[slice16],\r\n",
        "       train_regret_winner_9[slice16],\r\n",
        "       train_regret_winner_10[slice16],\r\n",
        "       train_regret_winner_11[slice16],\r\n",
        "       train_regret_winner_12[slice16],\r\n",
        "       train_regret_winner_13[slice16],\r\n",
        "       train_regret_winner_14[slice16],\r\n",
        "       train_regret_winner_15[slice16],\r\n",
        "       train_regret_winner_16[slice16],\r\n",
        "       train_regret_winner_17[slice16],\r\n",
        "       train_regret_winner_18[slice16],\r\n",
        "       train_regret_winner_19[slice16],\r\n",
        "       train_regret_winner_20[slice16]]\r\n",
        "\r\n",
        "loser16_results = pd.DataFrame(loser16).sort_values(by=[0], ascending=False)\r\n",
        "winner16_results = pd.DataFrame(winner16).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser16 = np.asarray(loser16_results[4:5][0])[0]\r\n",
        "median_loser16 = np.asarray(loser16_results[9:10][0])[0]\r\n",
        "upper_loser16 = np.asarray(loser16_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner16 = np.asarray(winner16_results[4:5][0])[0]\r\n",
        "median_winner16 = np.asarray(winner16_results[9:10][0])[0]\r\n",
        "upper_winner16 = np.asarray(winner16_results[14:15][0])[0]"
      ],
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOB3hOLIxAQy"
      },
      "source": [
        "# Iteration26 :\r\n",
        "\r\n",
        "slice26 = 25\r\n",
        "\r\n",
        "loser26 = [train_regret_loser_1[slice26],\r\n",
        "       train_regret_loser_2[slice26],\r\n",
        "       train_regret_loser_3[slice26],\r\n",
        "       train_regret_loser_4[slice26],\r\n",
        "       train_regret_loser_5[slice26],\r\n",
        "       train_regret_loser_6[slice26],\r\n",
        "       train_regret_loser_7[slice26],\r\n",
        "       train_regret_loser_8[slice26],\r\n",
        "       train_regret_loser_9[slice26],\r\n",
        "       train_regret_loser_10[slice26],\r\n",
        "       train_regret_loser_11[slice26],\r\n",
        "       train_regret_loser_12[slice26],\r\n",
        "       train_regret_loser_13[slice26],\r\n",
        "       train_regret_loser_14[slice26],\r\n",
        "       train_regret_loser_15[slice26],\r\n",
        "       train_regret_loser_16[slice26],\r\n",
        "       train_regret_loser_17[slice26],\r\n",
        "       train_regret_loser_18[slice26],\r\n",
        "       train_regret_loser_19[slice26],\r\n",
        "       train_regret_loser_20[slice26]]\r\n",
        "\r\n",
        "winner26 = [train_regret_winner_1[slice26],\r\n",
        "       train_regret_winner_2[slice26],\r\n",
        "       train_regret_winner_3[slice26],\r\n",
        "       train_regret_winner_4[slice26],\r\n",
        "       train_regret_winner_5[slice26],\r\n",
        "       train_regret_winner_6[slice26],\r\n",
        "       train_regret_winner_7[slice26],\r\n",
        "       train_regret_winner_8[slice26],\r\n",
        "       train_regret_winner_9[slice26],\r\n",
        "       train_regret_winner_10[slice26],\r\n",
        "       train_regret_winner_11[slice26],\r\n",
        "       train_regret_winner_12[slice26],\r\n",
        "       train_regret_winner_13[slice26],\r\n",
        "       train_regret_winner_14[slice26],\r\n",
        "       train_regret_winner_15[slice26],\r\n",
        "       train_regret_winner_16[slice26],\r\n",
        "       train_regret_winner_17[slice26],\r\n",
        "       train_regret_winner_18[slice26],\r\n",
        "       train_regret_winner_19[slice26],\r\n",
        "       train_regret_winner_20[slice26]]\r\n",
        "\r\n",
        "loser26_results = pd.DataFrame(loser26).sort_values(by=[0], ascending=False)\r\n",
        "winner26_results = pd.DataFrame(winner26).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser26 = np.asarray(loser26_results[4:5][0])[0]\r\n",
        "median_loser26 = np.asarray(loser26_results[9:10][0])[0]\r\n",
        "upper_loser26 = np.asarray(loser26_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner26 = np.asarray(winner26_results[4:5][0])[0]\r\n",
        "median_winner26 = np.asarray(winner26_results[9:10][0])[0]\r\n",
        "upper_winner26 = np.asarray(winner26_results[14:15][0])[0]"
      ],
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-0hQMtOxATh"
      },
      "source": [
        "# Iteration36 :\r\n",
        "\r\n",
        "slice36 = 35\r\n",
        "\r\n",
        "loser36 = [train_regret_loser_1[slice36],\r\n",
        "       train_regret_loser_2[slice36],\r\n",
        "       train_regret_loser_3[slice36],\r\n",
        "       train_regret_loser_4[slice36],\r\n",
        "       train_regret_loser_5[slice36],\r\n",
        "       train_regret_loser_6[slice36],\r\n",
        "       train_regret_loser_7[slice36],\r\n",
        "       train_regret_loser_8[slice36],\r\n",
        "       train_regret_loser_9[slice36],\r\n",
        "       train_regret_loser_10[slice36],\r\n",
        "       train_regret_loser_11[slice36],\r\n",
        "       train_regret_loser_12[slice36],\r\n",
        "       train_regret_loser_13[slice36],\r\n",
        "       train_regret_loser_14[slice36],\r\n",
        "       train_regret_loser_15[slice36],\r\n",
        "       train_regret_loser_16[slice36],\r\n",
        "       train_regret_loser_17[slice36],\r\n",
        "       train_regret_loser_18[slice36],\r\n",
        "       train_regret_loser_19[slice36],\r\n",
        "       train_regret_loser_20[slice36]]\r\n",
        "\r\n",
        "winner36 = [train_regret_winner_1[slice36],\r\n",
        "       train_regret_winner_2[slice36],\r\n",
        "       train_regret_winner_3[slice36],\r\n",
        "       train_regret_winner_4[slice36],\r\n",
        "       train_regret_winner_5[slice36],\r\n",
        "       train_regret_winner_6[slice36],\r\n",
        "       train_regret_winner_7[slice36],\r\n",
        "       train_regret_winner_8[slice36],\r\n",
        "       train_regret_winner_9[slice36],\r\n",
        "       train_regret_winner_10[slice36],\r\n",
        "       train_regret_winner_11[slice36],\r\n",
        "       train_regret_winner_12[slice36],\r\n",
        "       train_regret_winner_13[slice36],\r\n",
        "       train_regret_winner_14[slice36],\r\n",
        "       train_regret_winner_15[slice36],\r\n",
        "       train_regret_winner_16[slice36],\r\n",
        "       train_regret_winner_17[slice36],\r\n",
        "       train_regret_winner_18[slice36],\r\n",
        "       train_regret_winner_19[slice36],\r\n",
        "       train_regret_winner_20[slice36]]\r\n",
        "\r\n",
        "loser36_results = pd.DataFrame(loser36).sort_values(by=[0], ascending=False)\r\n",
        "winner36_results = pd.DataFrame(winner36).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser36 = np.asarray(loser36_results[4:5][0])[0]\r\n",
        "median_loser36 = np.asarray(loser36_results[9:10][0])[0]\r\n",
        "upper_loser36 = np.asarray(loser36_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner36 = np.asarray(winner36_results[4:5][0])[0]\r\n",
        "median_winner36 = np.asarray(winner36_results[9:10][0])[0]\r\n",
        "upper_winner36 = np.asarray(winner36_results[14:15][0])[0]"
      ],
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSqBLqTgxAWs"
      },
      "source": [
        "# Iteration46 :\r\n",
        "\r\n",
        "slice46 = 45\r\n",
        "\r\n",
        "loser46 = [train_regret_loser_1[slice46],\r\n",
        "       train_regret_loser_2[slice46],\r\n",
        "       train_regret_loser_3[slice46],\r\n",
        "       train_regret_loser_4[slice46],\r\n",
        "       train_regret_loser_5[slice46],\r\n",
        "       train_regret_loser_6[slice46],\r\n",
        "       train_regret_loser_7[slice46],\r\n",
        "       train_regret_loser_8[slice46],\r\n",
        "       train_regret_loser_9[slice46],\r\n",
        "       train_regret_loser_10[slice46],\r\n",
        "       train_regret_loser_11[slice46],\r\n",
        "       train_regret_loser_12[slice46],\r\n",
        "       train_regret_loser_13[slice46],\r\n",
        "       train_regret_loser_14[slice46],\r\n",
        "       train_regret_loser_15[slice46],\r\n",
        "       train_regret_loser_16[slice46],\r\n",
        "       train_regret_loser_17[slice46],\r\n",
        "       train_regret_loser_18[slice46],\r\n",
        "       train_regret_loser_19[slice46],\r\n",
        "       train_regret_loser_20[slice46]]\r\n",
        "\r\n",
        "winner46 = [train_regret_winner_1[slice46],\r\n",
        "       train_regret_winner_2[slice46],\r\n",
        "       train_regret_winner_3[slice46],\r\n",
        "       train_regret_winner_4[slice46],\r\n",
        "       train_regret_winner_5[slice46],\r\n",
        "       train_regret_winner_6[slice46],\r\n",
        "       train_regret_winner_7[slice46],\r\n",
        "       train_regret_winner_8[slice46],\r\n",
        "       train_regret_winner_9[slice46],\r\n",
        "       train_regret_winner_10[slice46],\r\n",
        "       train_regret_winner_11[slice46],\r\n",
        "       train_regret_winner_12[slice46],\r\n",
        "       train_regret_winner_13[slice46],\r\n",
        "       train_regret_winner_14[slice46],\r\n",
        "       train_regret_winner_15[slice46],\r\n",
        "       train_regret_winner_16[slice46],\r\n",
        "       train_regret_winner_17[slice46],\r\n",
        "       train_regret_winner_18[slice46],\r\n",
        "       train_regret_winner_19[slice46],\r\n",
        "       train_regret_winner_20[slice46]]\r\n",
        "\r\n",
        "loser46_results = pd.DataFrame(loser46).sort_values(by=[0], ascending=False)\r\n",
        "winner46_results = pd.DataFrame(winner46).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser46 = np.asarray(loser46_results[4:5][0])[0]\r\n",
        "median_loser46 = np.asarray(loser46_results[9:10][0])[0]\r\n",
        "upper_loser46 = np.asarray(loser46_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner46 = np.asarray(winner46_results[4:5][0])[0]\r\n",
        "median_winner46 = np.asarray(winner46_results[9:10][0])[0]\r\n",
        "upper_winner46 = np.asarray(winner46_results[14:15][0])[0]"
      ],
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFwO80w6xAZZ"
      },
      "source": [
        "# Iteration56 :\r\n",
        "\r\n",
        "slice56 = 55\r\n",
        "\r\n",
        "loser56 = [train_regret_loser_1[slice56],\r\n",
        "       train_regret_loser_2[slice56],\r\n",
        "       train_regret_loser_3[slice56],\r\n",
        "       train_regret_loser_4[slice56],\r\n",
        "       train_regret_loser_5[slice56],\r\n",
        "       train_regret_loser_6[slice56],\r\n",
        "       train_regret_loser_7[slice56],\r\n",
        "       train_regret_loser_8[slice56],\r\n",
        "       train_regret_loser_9[slice56],\r\n",
        "       train_regret_loser_10[slice56],\r\n",
        "       train_regret_loser_11[slice56],\r\n",
        "       train_regret_loser_12[slice56],\r\n",
        "       train_regret_loser_13[slice56],\r\n",
        "       train_regret_loser_14[slice56],\r\n",
        "       train_regret_loser_15[slice56],\r\n",
        "       train_regret_loser_16[slice56],\r\n",
        "       train_regret_loser_17[slice56],\r\n",
        "       train_regret_loser_18[slice56],\r\n",
        "       train_regret_loser_19[slice56],\r\n",
        "       train_regret_loser_20[slice56]]\r\n",
        "\r\n",
        "winner56 = [train_regret_winner_1[slice56],\r\n",
        "       train_regret_winner_2[slice56],\r\n",
        "       train_regret_winner_3[slice56],\r\n",
        "       train_regret_winner_4[slice56],\r\n",
        "       train_regret_winner_5[slice56],\r\n",
        "       train_regret_winner_6[slice56],\r\n",
        "       train_regret_winner_7[slice56],\r\n",
        "       train_regret_winner_8[slice56],\r\n",
        "       train_regret_winner_9[slice56],\r\n",
        "       train_regret_winner_10[slice56],\r\n",
        "       train_regret_winner_11[slice56],\r\n",
        "       train_regret_winner_12[slice56],\r\n",
        "       train_regret_winner_13[slice56],\r\n",
        "       train_regret_winner_14[slice56],\r\n",
        "       train_regret_winner_15[slice56],\r\n",
        "       train_regret_winner_16[slice56],\r\n",
        "       train_regret_winner_17[slice56],\r\n",
        "       train_regret_winner_18[slice56],\r\n",
        "       train_regret_winner_19[slice56],\r\n",
        "       train_regret_winner_20[slice56]]\r\n",
        "\r\n",
        "loser56_results = pd.DataFrame(loser56).sort_values(by=[0], ascending=False)\r\n",
        "winner56_results = pd.DataFrame(winner56).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser56 = np.asarray(loser56_results[4:5][0])[0]\r\n",
        "median_loser56 = np.asarray(loser56_results[9:10][0])[0]\r\n",
        "upper_loser56 = np.asarray(loser56_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner56 = np.asarray(winner56_results[4:5][0])[0]\r\n",
        "median_winner56 = np.asarray(winner56_results[9:10][0])[0]\r\n",
        "upper_winner56 = np.asarray(winner56_results[14:15][0])[0]"
      ],
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfVZBDP8xAcC"
      },
      "source": [
        "# Iteration66 :\r\n",
        "\r\n",
        "slice66 = 65\r\n",
        "\r\n",
        "loser66 = [train_regret_loser_1[slice66],\r\n",
        "       train_regret_loser_2[slice66],\r\n",
        "       train_regret_loser_3[slice66],\r\n",
        "       train_regret_loser_4[slice66],\r\n",
        "       train_regret_loser_5[slice66],\r\n",
        "       train_regret_loser_6[slice66],\r\n",
        "       train_regret_loser_7[slice66],\r\n",
        "       train_regret_loser_8[slice66],\r\n",
        "       train_regret_loser_9[slice66],\r\n",
        "       train_regret_loser_10[slice66],\r\n",
        "       train_regret_loser_11[slice66],\r\n",
        "       train_regret_loser_12[slice66],\r\n",
        "       train_regret_loser_13[slice66],\r\n",
        "       train_regret_loser_14[slice66],\r\n",
        "       train_regret_loser_15[slice66],\r\n",
        "       train_regret_loser_16[slice66],\r\n",
        "       train_regret_loser_17[slice66],\r\n",
        "       train_regret_loser_18[slice66],\r\n",
        "       train_regret_loser_19[slice66],\r\n",
        "       train_regret_loser_20[slice66]]\r\n",
        "\r\n",
        "winner66 = [train_regret_winner_1[slice66],\r\n",
        "       train_regret_winner_2[slice66],\r\n",
        "       train_regret_winner_3[slice66],\r\n",
        "       train_regret_winner_4[slice66],\r\n",
        "       train_regret_winner_5[slice66],\r\n",
        "       train_regret_winner_6[slice66],\r\n",
        "       train_regret_winner_7[slice66],\r\n",
        "       train_regret_winner_8[slice66],\r\n",
        "       train_regret_winner_9[slice66],\r\n",
        "       train_regret_winner_10[slice66],\r\n",
        "       train_regret_winner_11[slice66],\r\n",
        "       train_regret_winner_12[slice66],\r\n",
        "       train_regret_winner_13[slice66],\r\n",
        "       train_regret_winner_14[slice66],\r\n",
        "       train_regret_winner_15[slice66],\r\n",
        "       train_regret_winner_16[slice66],\r\n",
        "       train_regret_winner_17[slice66],\r\n",
        "       train_regret_winner_18[slice66],\r\n",
        "       train_regret_winner_19[slice66],\r\n",
        "       train_regret_winner_20[slice66]]\r\n",
        "\r\n",
        "loser66_results = pd.DataFrame(loser66).sort_values(by=[0], ascending=False)\r\n",
        "winner66_results = pd.DataFrame(winner66).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser66 = np.asarray(loser66_results[4:5][0])[0]\r\n",
        "median_loser66 = np.asarray(loser66_results[9:10][0])[0]\r\n",
        "upper_loser66 = np.asarray(loser66_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner66 = np.asarray(winner66_results[4:5][0])[0]\r\n",
        "median_winner66 = np.asarray(winner66_results[9:10][0])[0]\r\n",
        "upper_winner66 = np.asarray(winner66_results[14:15][0])[0]"
      ],
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KnSYoG7bxAfL"
      },
      "source": [
        "# Iteration76 :\r\n",
        "\r\n",
        "slice76 = 75\r\n",
        "\r\n",
        "loser76 = [train_regret_loser_1[slice76],\r\n",
        "       train_regret_loser_2[slice76],\r\n",
        "       train_regret_loser_3[slice76],\r\n",
        "       train_regret_loser_4[slice76],\r\n",
        "       train_regret_loser_5[slice76],\r\n",
        "       train_regret_loser_6[slice76],\r\n",
        "       train_regret_loser_7[slice76],\r\n",
        "       train_regret_loser_8[slice76],\r\n",
        "       train_regret_loser_9[slice76],\r\n",
        "       train_regret_loser_10[slice76],\r\n",
        "       train_regret_loser_11[slice76],\r\n",
        "       train_regret_loser_12[slice76],\r\n",
        "       train_regret_loser_13[slice76],\r\n",
        "       train_regret_loser_14[slice76],\r\n",
        "       train_regret_loser_15[slice76],\r\n",
        "       train_regret_loser_16[slice76],\r\n",
        "       train_regret_loser_17[slice76],\r\n",
        "       train_regret_loser_18[slice76],\r\n",
        "       train_regret_loser_19[slice76],\r\n",
        "       train_regret_loser_20[slice76]]\r\n",
        "\r\n",
        "winner76 = [train_regret_winner_1[slice76],\r\n",
        "       train_regret_winner_2[slice76],\r\n",
        "       train_regret_winner_3[slice76],\r\n",
        "       train_regret_winner_4[slice76],\r\n",
        "       train_regret_winner_5[slice76],\r\n",
        "       train_regret_winner_6[slice76],\r\n",
        "       train_regret_winner_7[slice76],\r\n",
        "       train_regret_winner_8[slice76],\r\n",
        "       train_regret_winner_9[slice76],\r\n",
        "       train_regret_winner_10[slice76],\r\n",
        "       train_regret_winner_11[slice76],\r\n",
        "       train_regret_winner_12[slice76],\r\n",
        "       train_regret_winner_13[slice76],\r\n",
        "       train_regret_winner_14[slice76],\r\n",
        "       train_regret_winner_15[slice76],\r\n",
        "       train_regret_winner_16[slice76],\r\n",
        "       train_regret_winner_17[slice76],\r\n",
        "       train_regret_winner_18[slice76],\r\n",
        "       train_regret_winner_19[slice76],\r\n",
        "       train_regret_winner_20[slice76]]\r\n",
        "\r\n",
        "loser76_results = pd.DataFrame(loser76).sort_values(by=[0], ascending=False)\r\n",
        "winner76_results = pd.DataFrame(winner76).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser76 = np.asarray(loser76_results[4:5][0])[0]\r\n",
        "median_loser76 = np.asarray(loser76_results[9:10][0])[0]\r\n",
        "upper_loser76 = np.asarray(loser76_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner76 = np.asarray(winner76_results[4:5][0])[0]\r\n",
        "median_winner76 = np.asarray(winner76_results[9:10][0])[0]\r\n",
        "upper_winner76 = np.asarray(winner76_results[14:15][0])[0]"
      ],
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ze_Oy9mxAiO"
      },
      "source": [
        "# Iteration86 :\r\n",
        "\r\n",
        "slice86 = 85\r\n",
        "\r\n",
        "loser86 = [train_regret_loser_1[slice86],\r\n",
        "       train_regret_loser_2[slice86],\r\n",
        "       train_regret_loser_3[slice86],\r\n",
        "       train_regret_loser_4[slice86],\r\n",
        "       train_regret_loser_5[slice86],\r\n",
        "       train_regret_loser_6[slice86],\r\n",
        "       train_regret_loser_7[slice86],\r\n",
        "       train_regret_loser_8[slice86],\r\n",
        "       train_regret_loser_9[slice86],\r\n",
        "       train_regret_loser_10[slice86],\r\n",
        "       train_regret_loser_11[slice86],\r\n",
        "       train_regret_loser_12[slice86],\r\n",
        "       train_regret_loser_13[slice86],\r\n",
        "       train_regret_loser_14[slice86],\r\n",
        "       train_regret_loser_15[slice86],\r\n",
        "       train_regret_loser_16[slice86],\r\n",
        "       train_regret_loser_17[slice86],\r\n",
        "       train_regret_loser_18[slice86],\r\n",
        "       train_regret_loser_19[slice86],\r\n",
        "       train_regret_loser_20[slice86]]\r\n",
        "\r\n",
        "winner86 = [train_regret_winner_1[slice86],\r\n",
        "       train_regret_winner_2[slice86],\r\n",
        "       train_regret_winner_3[slice86],\r\n",
        "       train_regret_winner_4[slice86],\r\n",
        "       train_regret_winner_5[slice86],\r\n",
        "       train_regret_winner_6[slice86],\r\n",
        "       train_regret_winner_7[slice86],\r\n",
        "       train_regret_winner_8[slice86],\r\n",
        "       train_regret_winner_9[slice86],\r\n",
        "       train_regret_winner_10[slice86],\r\n",
        "       train_regret_winner_11[slice86],\r\n",
        "       train_regret_winner_12[slice86],\r\n",
        "       train_regret_winner_13[slice86],\r\n",
        "       train_regret_winner_14[slice86],\r\n",
        "       train_regret_winner_15[slice86],\r\n",
        "       train_regret_winner_16[slice86],\r\n",
        "       train_regret_winner_17[slice86],\r\n",
        "       train_regret_winner_18[slice86],\r\n",
        "       train_regret_winner_19[slice86],\r\n",
        "       train_regret_winner_20[slice86]]\r\n",
        "\r\n",
        "loser86_results = pd.DataFrame(loser86).sort_values(by=[0], ascending=False)\r\n",
        "winner86_results = pd.DataFrame(winner86).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser86 = np.asarray(loser86_results[4:5][0])[0]\r\n",
        "median_loser86 = np.asarray(loser86_results[9:10][0])[0]\r\n",
        "upper_loser86 = np.asarray(loser86_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner86 = np.asarray(winner86_results[4:5][0])[0]\r\n",
        "median_winner86 = np.asarray(winner86_results[9:10][0])[0]\r\n",
        "upper_winner86 = np.asarray(winner86_results[14:15][0])[0]"
      ],
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glsE9_RPxAlS"
      },
      "source": [
        "# Iteration96 :\r\n",
        "\r\n",
        "slice96 = 95\r\n",
        "\r\n",
        "loser96 = [train_regret_loser_1[slice96],\r\n",
        "       train_regret_loser_2[slice96],\r\n",
        "       train_regret_loser_3[slice96],\r\n",
        "       train_regret_loser_4[slice96],\r\n",
        "       train_regret_loser_5[slice96],\r\n",
        "       train_regret_loser_6[slice96],\r\n",
        "       train_regret_loser_7[slice96],\r\n",
        "       train_regret_loser_8[slice96],\r\n",
        "       train_regret_loser_9[slice96],\r\n",
        "       train_regret_loser_10[slice96],\r\n",
        "       train_regret_loser_11[slice96],\r\n",
        "       train_regret_loser_12[slice96],\r\n",
        "       train_regret_loser_13[slice96],\r\n",
        "       train_regret_loser_14[slice96],\r\n",
        "       train_regret_loser_15[slice96],\r\n",
        "       train_regret_loser_16[slice96],\r\n",
        "       train_regret_loser_17[slice96],\r\n",
        "       train_regret_loser_18[slice96],\r\n",
        "       train_regret_loser_19[slice96],\r\n",
        "       train_regret_loser_20[slice96]]\r\n",
        "\r\n",
        "winner96 = [train_regret_winner_1[slice96],\r\n",
        "       train_regret_winner_2[slice96],\r\n",
        "       train_regret_winner_3[slice96],\r\n",
        "       train_regret_winner_4[slice96],\r\n",
        "       train_regret_winner_5[slice96],\r\n",
        "       train_regret_winner_6[slice96],\r\n",
        "       train_regret_winner_7[slice96],\r\n",
        "       train_regret_winner_8[slice96],\r\n",
        "       train_regret_winner_9[slice96],\r\n",
        "       train_regret_winner_10[slice96],\r\n",
        "       train_regret_winner_11[slice96],\r\n",
        "       train_regret_winner_12[slice96],\r\n",
        "       train_regret_winner_13[slice96],\r\n",
        "       train_regret_winner_14[slice96],\r\n",
        "       train_regret_winner_15[slice96],\r\n",
        "       train_regret_winner_16[slice96],\r\n",
        "       train_regret_winner_17[slice96],\r\n",
        "       train_regret_winner_18[slice96],\r\n",
        "       train_regret_winner_19[slice96],\r\n",
        "       train_regret_winner_20[slice96]]\r\n",
        "\r\n",
        "loser96_results = pd.DataFrame(loser96).sort_values(by=[0], ascending=False)\r\n",
        "winner96_results = pd.DataFrame(winner96).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser96 = np.asarray(loser96_results[4:5][0])[0]\r\n",
        "median_loser96 = np.asarray(loser96_results[9:10][0])[0]\r\n",
        "upper_loser96 = np.asarray(loser96_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner96 = np.asarray(winner96_results[4:5][0])[0]\r\n",
        "median_winner96 = np.asarray(winner96_results[9:10][0])[0]\r\n",
        "upper_winner96 = np.asarray(winner96_results[14:15][0])[0]"
      ],
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IhsEfl1RxAoA"
      },
      "source": [
        "# Iteration7 :\r\n",
        "\r\n",
        "slice7 = 6\r\n",
        "\r\n",
        "loser7 = [train_regret_loser_1[slice7],\r\n",
        "       train_regret_loser_2[slice7],\r\n",
        "       train_regret_loser_3[slice7],\r\n",
        "       train_regret_loser_4[slice7],\r\n",
        "       train_regret_loser_5[slice7],\r\n",
        "       train_regret_loser_6[slice7],\r\n",
        "       train_regret_loser_7[slice7],\r\n",
        "       train_regret_loser_8[slice7],\r\n",
        "       train_regret_loser_9[slice7],\r\n",
        "       train_regret_loser_10[slice7],\r\n",
        "       train_regret_loser_11[slice7],\r\n",
        "       train_regret_loser_12[slice7],\r\n",
        "       train_regret_loser_13[slice7],\r\n",
        "       train_regret_loser_14[slice7],\r\n",
        "       train_regret_loser_15[slice7],\r\n",
        "       train_regret_loser_16[slice7],\r\n",
        "       train_regret_loser_17[slice7],\r\n",
        "       train_regret_loser_18[slice7],\r\n",
        "       train_regret_loser_19[slice7],\r\n",
        "       train_regret_loser_20[slice7]]\r\n",
        "\r\n",
        "winner7 = [train_regret_winner_1[slice7],\r\n",
        "       train_regret_winner_2[slice7],\r\n",
        "       train_regret_winner_3[slice7],\r\n",
        "       train_regret_winner_4[slice7],\r\n",
        "       train_regret_winner_5[slice7],\r\n",
        "       train_regret_winner_6[slice7],\r\n",
        "       train_regret_winner_7[slice7],\r\n",
        "       train_regret_winner_8[slice7],\r\n",
        "       train_regret_winner_9[slice7],\r\n",
        "       train_regret_winner_10[slice7],\r\n",
        "       train_regret_winner_11[slice7],\r\n",
        "       train_regret_winner_12[slice7],\r\n",
        "       train_regret_winner_13[slice7],\r\n",
        "       train_regret_winner_14[slice7],\r\n",
        "       train_regret_winner_15[slice7],\r\n",
        "       train_regret_winner_16[slice7],\r\n",
        "       train_regret_winner_17[slice7],\r\n",
        "       train_regret_winner_18[slice7],\r\n",
        "       train_regret_winner_19[slice7],\r\n",
        "       train_regret_winner_20[slice7]]\r\n",
        "\r\n",
        "loser7_results = pd.DataFrame(loser7).sort_values(by=[0], ascending=False)\r\n",
        "winner7_results = pd.DataFrame(winner7).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser7 = np.asarray(loser7_results[4:5][0])[0]\r\n",
        "median_loser7 = np.asarray(loser7_results[9:10][0])[0]\r\n",
        "upper_loser7 = np.asarray(loser7_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner7 = np.asarray(winner7_results[4:5][0])[0]\r\n",
        "median_winner7 = np.asarray(winner7_results[9:10][0])[0]\r\n",
        "upper_winner7 = np.asarray(winner7_results[14:15][0])[0]"
      ],
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUCMheLKxArL"
      },
      "source": [
        "# Iteration17 :\r\n",
        "\r\n",
        "slice17 = 16\r\n",
        "\r\n",
        "loser17 = [train_regret_loser_1[slice17],\r\n",
        "       train_regret_loser_2[slice17],\r\n",
        "       train_regret_loser_3[slice17],\r\n",
        "       train_regret_loser_4[slice17],\r\n",
        "       train_regret_loser_5[slice17],\r\n",
        "       train_regret_loser_6[slice17],\r\n",
        "       train_regret_loser_7[slice17],\r\n",
        "       train_regret_loser_8[slice17],\r\n",
        "       train_regret_loser_9[slice17],\r\n",
        "       train_regret_loser_10[slice17],\r\n",
        "       train_regret_loser_11[slice17],\r\n",
        "       train_regret_loser_12[slice17],\r\n",
        "       train_regret_loser_13[slice17],\r\n",
        "       train_regret_loser_14[slice17],\r\n",
        "       train_regret_loser_15[slice17],\r\n",
        "       train_regret_loser_16[slice17],\r\n",
        "       train_regret_loser_17[slice17],\r\n",
        "       train_regret_loser_18[slice17],\r\n",
        "       train_regret_loser_19[slice17],\r\n",
        "       train_regret_loser_20[slice17]]\r\n",
        "\r\n",
        "winner17 = [train_regret_winner_1[slice17],\r\n",
        "       train_regret_winner_2[slice17],\r\n",
        "       train_regret_winner_3[slice17],\r\n",
        "       train_regret_winner_4[slice17],\r\n",
        "       train_regret_winner_5[slice17],\r\n",
        "       train_regret_winner_6[slice17],\r\n",
        "       train_regret_winner_7[slice17],\r\n",
        "       train_regret_winner_8[slice17],\r\n",
        "       train_regret_winner_9[slice17],\r\n",
        "       train_regret_winner_10[slice17],\r\n",
        "       train_regret_winner_11[slice17],\r\n",
        "       train_regret_winner_12[slice17],\r\n",
        "       train_regret_winner_13[slice17],\r\n",
        "       train_regret_winner_14[slice17],\r\n",
        "       train_regret_winner_15[slice17],\r\n",
        "       train_regret_winner_16[slice17],\r\n",
        "       train_regret_winner_17[slice17],\r\n",
        "       train_regret_winner_18[slice17],\r\n",
        "       train_regret_winner_19[slice17],\r\n",
        "       train_regret_winner_20[slice17]]\r\n",
        "\r\n",
        "loser17_results = pd.DataFrame(loser17).sort_values(by=[0], ascending=False)\r\n",
        "winner17_results = pd.DataFrame(winner17).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser17 = np.asarray(loser17_results[4:5][0])[0]\r\n",
        "median_loser17 = np.asarray(loser17_results[9:10][0])[0]\r\n",
        "upper_loser17 = np.asarray(loser17_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner17 = np.asarray(winner17_results[4:5][0])[0]\r\n",
        "median_winner17 = np.asarray(winner17_results[9:10][0])[0]\r\n",
        "upper_winner17 = np.asarray(winner17_results[14:15][0])[0]"
      ],
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYnBmq7_xAuB"
      },
      "source": [
        "# Iteration27 :\r\n",
        "\r\n",
        "slice27 = 26\r\n",
        "\r\n",
        "loser27 = [train_regret_loser_1[slice27],\r\n",
        "       train_regret_loser_2[slice27],\r\n",
        "       train_regret_loser_3[slice27],\r\n",
        "       train_regret_loser_4[slice27],\r\n",
        "       train_regret_loser_5[slice27],\r\n",
        "       train_regret_loser_6[slice27],\r\n",
        "       train_regret_loser_7[slice27],\r\n",
        "       train_regret_loser_8[slice27],\r\n",
        "       train_regret_loser_9[slice27],\r\n",
        "       train_regret_loser_10[slice27],\r\n",
        "       train_regret_loser_11[slice27],\r\n",
        "       train_regret_loser_12[slice27],\r\n",
        "       train_regret_loser_13[slice27],\r\n",
        "       train_regret_loser_14[slice27],\r\n",
        "       train_regret_loser_15[slice27],\r\n",
        "       train_regret_loser_16[slice27],\r\n",
        "       train_regret_loser_17[slice27],\r\n",
        "       train_regret_loser_18[slice27],\r\n",
        "       train_regret_loser_19[slice27],\r\n",
        "       train_regret_loser_20[slice27]]\r\n",
        "\r\n",
        "winner27 = [train_regret_winner_1[slice27],\r\n",
        "       train_regret_winner_2[slice27],\r\n",
        "       train_regret_winner_3[slice27],\r\n",
        "       train_regret_winner_4[slice27],\r\n",
        "       train_regret_winner_5[slice27],\r\n",
        "       train_regret_winner_6[slice27],\r\n",
        "       train_regret_winner_7[slice27],\r\n",
        "       train_regret_winner_8[slice27],\r\n",
        "       train_regret_winner_9[slice27],\r\n",
        "       train_regret_winner_10[slice27],\r\n",
        "       train_regret_winner_11[slice27],\r\n",
        "       train_regret_winner_12[slice27],\r\n",
        "       train_regret_winner_13[slice27],\r\n",
        "       train_regret_winner_14[slice27],\r\n",
        "       train_regret_winner_15[slice27],\r\n",
        "       train_regret_winner_16[slice27],\r\n",
        "       train_regret_winner_17[slice27],\r\n",
        "       train_regret_winner_18[slice27],\r\n",
        "       train_regret_winner_19[slice27],\r\n",
        "       train_regret_winner_20[slice27]]\r\n",
        "\r\n",
        "loser27_results = pd.DataFrame(loser27).sort_values(by=[0], ascending=False)\r\n",
        "winner27_results = pd.DataFrame(winner27).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser27 = np.asarray(loser27_results[4:5][0])[0]\r\n",
        "median_loser27 = np.asarray(loser27_results[9:10][0])[0]\r\n",
        "upper_loser27 = np.asarray(loser27_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner27 = np.asarray(winner27_results[4:5][0])[0]\r\n",
        "median_winner27 = np.asarray(winner27_results[9:10][0])[0]\r\n",
        "upper_winner27 = np.asarray(winner27_results[14:15][0])[0]"
      ],
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ChA8KOUxAw6"
      },
      "source": [
        "# Iteration37 :\r\n",
        "\r\n",
        "slice37 = 36\r\n",
        "\r\n",
        "loser37 = [train_regret_loser_1[slice37],\r\n",
        "       train_regret_loser_2[slice37],\r\n",
        "       train_regret_loser_3[slice37],\r\n",
        "       train_regret_loser_4[slice37],\r\n",
        "       train_regret_loser_5[slice37],\r\n",
        "       train_regret_loser_6[slice37],\r\n",
        "       train_regret_loser_7[slice37],\r\n",
        "       train_regret_loser_8[slice37],\r\n",
        "       train_regret_loser_9[slice37],\r\n",
        "       train_regret_loser_10[slice37],\r\n",
        "       train_regret_loser_11[slice37],\r\n",
        "       train_regret_loser_12[slice37],\r\n",
        "       train_regret_loser_13[slice37],\r\n",
        "       train_regret_loser_14[slice37],\r\n",
        "       train_regret_loser_15[slice37],\r\n",
        "       train_regret_loser_16[slice37],\r\n",
        "       train_regret_loser_17[slice37],\r\n",
        "       train_regret_loser_18[slice37],\r\n",
        "       train_regret_loser_19[slice37],\r\n",
        "       train_regret_loser_20[slice37]]\r\n",
        "\r\n",
        "winner37 = [train_regret_winner_1[slice37],\r\n",
        "       train_regret_winner_2[slice37],\r\n",
        "       train_regret_winner_3[slice37],\r\n",
        "       train_regret_winner_4[slice37],\r\n",
        "       train_regret_winner_5[slice37],\r\n",
        "       train_regret_winner_6[slice37],\r\n",
        "       train_regret_winner_7[slice37],\r\n",
        "       train_regret_winner_8[slice37],\r\n",
        "       train_regret_winner_9[slice37],\r\n",
        "       train_regret_winner_10[slice37],\r\n",
        "       train_regret_winner_11[slice37],\r\n",
        "       train_regret_winner_12[slice37],\r\n",
        "       train_regret_winner_13[slice37],\r\n",
        "       train_regret_winner_14[slice37],\r\n",
        "       train_regret_winner_15[slice37],\r\n",
        "       train_regret_winner_16[slice37],\r\n",
        "       train_regret_winner_17[slice37],\r\n",
        "       train_regret_winner_18[slice37],\r\n",
        "       train_regret_winner_19[slice37],\r\n",
        "       train_regret_winner_20[slice37]]\r\n",
        "\r\n",
        "loser37_results = pd.DataFrame(loser37).sort_values(by=[0], ascending=False)\r\n",
        "winner37_results = pd.DataFrame(winner37).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser37 = np.asarray(loser37_results[4:5][0])[0]\r\n",
        "median_loser37 = np.asarray(loser37_results[9:10][0])[0]\r\n",
        "upper_loser37 = np.asarray(loser37_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner37 = np.asarray(winner37_results[4:5][0])[0]\r\n",
        "median_winner37 = np.asarray(winner37_results[9:10][0])[0]\r\n",
        "upper_winner37 = np.asarray(winner37_results[14:15][0])[0]"
      ],
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVsY1buDxAzx"
      },
      "source": [
        "# Iteration47 :\r\n",
        "\r\n",
        "slice47 = 46\r\n",
        "\r\n",
        "loser47 = [train_regret_loser_1[slice47],\r\n",
        "       train_regret_loser_2[slice47],\r\n",
        "       train_regret_loser_3[slice47],\r\n",
        "       train_regret_loser_4[slice47],\r\n",
        "       train_regret_loser_5[slice47],\r\n",
        "       train_regret_loser_6[slice47],\r\n",
        "       train_regret_loser_7[slice47],\r\n",
        "       train_regret_loser_8[slice47],\r\n",
        "       train_regret_loser_9[slice47],\r\n",
        "       train_regret_loser_10[slice47],\r\n",
        "       train_regret_loser_11[slice47],\r\n",
        "       train_regret_loser_12[slice47],\r\n",
        "       train_regret_loser_13[slice47],\r\n",
        "       train_regret_loser_14[slice47],\r\n",
        "       train_regret_loser_15[slice47],\r\n",
        "       train_regret_loser_16[slice47],\r\n",
        "       train_regret_loser_17[slice47],\r\n",
        "       train_regret_loser_18[slice47],\r\n",
        "       train_regret_loser_19[slice47],\r\n",
        "       train_regret_loser_20[slice47]]\r\n",
        "\r\n",
        "winner47 = [train_regret_winner_1[slice47],\r\n",
        "       train_regret_winner_2[slice47],\r\n",
        "       train_regret_winner_3[slice47],\r\n",
        "       train_regret_winner_4[slice47],\r\n",
        "       train_regret_winner_5[slice47],\r\n",
        "       train_regret_winner_6[slice47],\r\n",
        "       train_regret_winner_7[slice47],\r\n",
        "       train_regret_winner_8[slice47],\r\n",
        "       train_regret_winner_9[slice47],\r\n",
        "       train_regret_winner_10[slice47],\r\n",
        "       train_regret_winner_11[slice47],\r\n",
        "       train_regret_winner_12[slice47],\r\n",
        "       train_regret_winner_13[slice47],\r\n",
        "       train_regret_winner_14[slice47],\r\n",
        "       train_regret_winner_15[slice47],\r\n",
        "       train_regret_winner_16[slice47],\r\n",
        "       train_regret_winner_17[slice47],\r\n",
        "       train_regret_winner_18[slice47],\r\n",
        "       train_regret_winner_19[slice47],\r\n",
        "       train_regret_winner_20[slice47]]\r\n",
        "\r\n",
        "loser47_results = pd.DataFrame(loser47).sort_values(by=[0], ascending=False)\r\n",
        "winner47_results = pd.DataFrame(winner47).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser47 = np.asarray(loser47_results[4:5][0])[0]\r\n",
        "median_loser47 = np.asarray(loser47_results[9:10][0])[0]\r\n",
        "upper_loser47 = np.asarray(loser47_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner47 = np.asarray(winner47_results[4:5][0])[0]\r\n",
        "median_winner47 = np.asarray(winner47_results[9:10][0])[0]\r\n",
        "upper_winner47 = np.asarray(winner47_results[14:15][0])[0]"
      ],
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fIy_EyTxA20"
      },
      "source": [
        "# Iteration57 :\r\n",
        "\r\n",
        "slice57 = 56\r\n",
        "\r\n",
        "loser57 = [train_regret_loser_1[slice57],\r\n",
        "       train_regret_loser_2[slice57],\r\n",
        "       train_regret_loser_3[slice57],\r\n",
        "       train_regret_loser_4[slice57],\r\n",
        "       train_regret_loser_5[slice57],\r\n",
        "       train_regret_loser_6[slice57],\r\n",
        "       train_regret_loser_7[slice57],\r\n",
        "       train_regret_loser_8[slice57],\r\n",
        "       train_regret_loser_9[slice57],\r\n",
        "       train_regret_loser_10[slice57],\r\n",
        "       train_regret_loser_11[slice57],\r\n",
        "       train_regret_loser_12[slice57],\r\n",
        "       train_regret_loser_13[slice57],\r\n",
        "       train_regret_loser_14[slice57],\r\n",
        "       train_regret_loser_15[slice57],\r\n",
        "       train_regret_loser_16[slice57],\r\n",
        "       train_regret_loser_17[slice57],\r\n",
        "       train_regret_loser_18[slice57],\r\n",
        "       train_regret_loser_19[slice57],\r\n",
        "       train_regret_loser_20[slice57]]\r\n",
        "\r\n",
        "winner57 = [train_regret_winner_1[slice57],\r\n",
        "       train_regret_winner_2[slice57],\r\n",
        "       train_regret_winner_3[slice57],\r\n",
        "       train_regret_winner_4[slice57],\r\n",
        "       train_regret_winner_5[slice57],\r\n",
        "       train_regret_winner_6[slice57],\r\n",
        "       train_regret_winner_7[slice57],\r\n",
        "       train_regret_winner_8[slice57],\r\n",
        "       train_regret_winner_9[slice57],\r\n",
        "       train_regret_winner_10[slice57],\r\n",
        "       train_regret_winner_11[slice57],\r\n",
        "       train_regret_winner_12[slice57],\r\n",
        "       train_regret_winner_13[slice57],\r\n",
        "       train_regret_winner_14[slice57],\r\n",
        "       train_regret_winner_15[slice57],\r\n",
        "       train_regret_winner_16[slice57],\r\n",
        "       train_regret_winner_17[slice57],\r\n",
        "       train_regret_winner_18[slice57],\r\n",
        "       train_regret_winner_19[slice57],\r\n",
        "       train_regret_winner_20[slice57]]\r\n",
        "\r\n",
        "loser57_results = pd.DataFrame(loser57).sort_values(by=[0], ascending=False)\r\n",
        "winner57_results = pd.DataFrame(winner57).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser57 = np.asarray(loser57_results[4:5][0])[0]\r\n",
        "median_loser57 = np.asarray(loser57_results[9:10][0])[0]\r\n",
        "upper_loser57 = np.asarray(loser57_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner57 = np.asarray(winner57_results[4:5][0])[0]\r\n",
        "median_winner57 = np.asarray(winner57_results[9:10][0])[0]\r\n",
        "upper_winner57 = np.asarray(winner57_results[14:15][0])[0]"
      ],
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umwdBl8XxA5r"
      },
      "source": [
        "# Iteration67 :\r\n",
        "\r\n",
        "slice67 = 66\r\n",
        "\r\n",
        "loser67 = [train_regret_loser_1[slice67],\r\n",
        "       train_regret_loser_2[slice67],\r\n",
        "       train_regret_loser_3[slice67],\r\n",
        "       train_regret_loser_4[slice67],\r\n",
        "       train_regret_loser_5[slice67],\r\n",
        "       train_regret_loser_6[slice67],\r\n",
        "       train_regret_loser_7[slice67],\r\n",
        "       train_regret_loser_8[slice67],\r\n",
        "       train_regret_loser_9[slice67],\r\n",
        "       train_regret_loser_10[slice67],\r\n",
        "       train_regret_loser_11[slice67],\r\n",
        "       train_regret_loser_12[slice67],\r\n",
        "       train_regret_loser_13[slice67],\r\n",
        "       train_regret_loser_14[slice67],\r\n",
        "       train_regret_loser_15[slice67],\r\n",
        "       train_regret_loser_16[slice67],\r\n",
        "       train_regret_loser_17[slice67],\r\n",
        "       train_regret_loser_18[slice67],\r\n",
        "       train_regret_loser_19[slice67],\r\n",
        "       train_regret_loser_20[slice67]]\r\n",
        "\r\n",
        "winner67 = [train_regret_winner_1[slice67],\r\n",
        "       train_regret_winner_2[slice67],\r\n",
        "       train_regret_winner_3[slice67],\r\n",
        "       train_regret_winner_4[slice67],\r\n",
        "       train_regret_winner_5[slice67],\r\n",
        "       train_regret_winner_6[slice67],\r\n",
        "       train_regret_winner_7[slice67],\r\n",
        "       train_regret_winner_8[slice67],\r\n",
        "       train_regret_winner_9[slice67],\r\n",
        "       train_regret_winner_10[slice67],\r\n",
        "       train_regret_winner_11[slice67],\r\n",
        "       train_regret_winner_12[slice67],\r\n",
        "       train_regret_winner_13[slice67],\r\n",
        "       train_regret_winner_14[slice67],\r\n",
        "       train_regret_winner_15[slice67],\r\n",
        "       train_regret_winner_16[slice67],\r\n",
        "       train_regret_winner_17[slice67],\r\n",
        "       train_regret_winner_18[slice67],\r\n",
        "       train_regret_winner_19[slice67],\r\n",
        "       train_regret_winner_20[slice67]]\r\n",
        "\r\n",
        "loser67_results = pd.DataFrame(loser67).sort_values(by=[0], ascending=False)\r\n",
        "winner67_results = pd.DataFrame(winner67).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser67 = np.asarray(loser67_results[4:5][0])[0]\r\n",
        "median_loser67 = np.asarray(loser67_results[9:10][0])[0]\r\n",
        "upper_loser67 = np.asarray(loser67_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner67 = np.asarray(winner67_results[4:5][0])[0]\r\n",
        "median_winner67 = np.asarray(winner67_results[9:10][0])[0]\r\n",
        "upper_winner67 = np.asarray(winner67_results[14:15][0])[0]"
      ],
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lF3KFX-ZxA8q"
      },
      "source": [
        "# Iteration77 :\r\n",
        "\r\n",
        "slice77 = 76\r\n",
        "\r\n",
        "loser77 = [train_regret_loser_1[slice77],\r\n",
        "       train_regret_loser_2[slice77],\r\n",
        "       train_regret_loser_3[slice77],\r\n",
        "       train_regret_loser_4[slice77],\r\n",
        "       train_regret_loser_5[slice77],\r\n",
        "       train_regret_loser_6[slice77],\r\n",
        "       train_regret_loser_7[slice77],\r\n",
        "       train_regret_loser_8[slice77],\r\n",
        "       train_regret_loser_9[slice77],\r\n",
        "       train_regret_loser_10[slice77],\r\n",
        "       train_regret_loser_11[slice77],\r\n",
        "       train_regret_loser_12[slice77],\r\n",
        "       train_regret_loser_13[slice77],\r\n",
        "       train_regret_loser_14[slice77],\r\n",
        "       train_regret_loser_15[slice77],\r\n",
        "       train_regret_loser_16[slice77],\r\n",
        "       train_regret_loser_17[slice77],\r\n",
        "       train_regret_loser_18[slice77],\r\n",
        "       train_regret_loser_19[slice77],\r\n",
        "       train_regret_loser_20[slice77]]\r\n",
        "\r\n",
        "winner77 = [train_regret_winner_1[slice77],\r\n",
        "       train_regret_winner_2[slice77],\r\n",
        "       train_regret_winner_3[slice77],\r\n",
        "       train_regret_winner_4[slice77],\r\n",
        "       train_regret_winner_5[slice77],\r\n",
        "       train_regret_winner_6[slice77],\r\n",
        "       train_regret_winner_7[slice77],\r\n",
        "       train_regret_winner_8[slice77],\r\n",
        "       train_regret_winner_9[slice77],\r\n",
        "       train_regret_winner_10[slice77],\r\n",
        "       train_regret_winner_11[slice77],\r\n",
        "       train_regret_winner_12[slice77],\r\n",
        "       train_regret_winner_13[slice77],\r\n",
        "       train_regret_winner_14[slice77],\r\n",
        "       train_regret_winner_15[slice77],\r\n",
        "       train_regret_winner_16[slice77],\r\n",
        "       train_regret_winner_17[slice77],\r\n",
        "       train_regret_winner_18[slice77],\r\n",
        "       train_regret_winner_19[slice77],\r\n",
        "       train_regret_winner_20[slice77]]\r\n",
        "\r\n",
        "loser77_results = pd.DataFrame(loser77).sort_values(by=[0], ascending=False)\r\n",
        "winner77_results = pd.DataFrame(winner77).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser77 = np.asarray(loser77_results[4:5][0])[0]\r\n",
        "median_loser77 = np.asarray(loser77_results[9:10][0])[0]\r\n",
        "upper_loser77 = np.asarray(loser77_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner77 = np.asarray(winner77_results[4:5][0])[0]\r\n",
        "median_winner77 = np.asarray(winner77_results[9:10][0])[0]\r\n",
        "upper_winner77 = np.asarray(winner77_results[14:15][0])[0]"
      ],
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7CVYft39xA_t"
      },
      "source": [
        "# Iteration87 :\r\n",
        "\r\n",
        "slice87 = 86\r\n",
        "\r\n",
        "loser87 = [train_regret_loser_1[slice87],\r\n",
        "       train_regret_loser_2[slice87],\r\n",
        "       train_regret_loser_3[slice87],\r\n",
        "       train_regret_loser_4[slice87],\r\n",
        "       train_regret_loser_5[slice87],\r\n",
        "       train_regret_loser_6[slice87],\r\n",
        "       train_regret_loser_7[slice87],\r\n",
        "       train_regret_loser_8[slice87],\r\n",
        "       train_regret_loser_9[slice87],\r\n",
        "       train_regret_loser_10[slice87],\r\n",
        "       train_regret_loser_11[slice87],\r\n",
        "       train_regret_loser_12[slice87],\r\n",
        "       train_regret_loser_13[slice87],\r\n",
        "       train_regret_loser_14[slice87],\r\n",
        "       train_regret_loser_15[slice87],\r\n",
        "       train_regret_loser_16[slice87],\r\n",
        "       train_regret_loser_17[slice87],\r\n",
        "       train_regret_loser_18[slice87],\r\n",
        "       train_regret_loser_19[slice87],\r\n",
        "       train_regret_loser_20[slice87]]\r\n",
        "\r\n",
        "winner87 = [train_regret_winner_1[slice87],\r\n",
        "       train_regret_winner_2[slice87],\r\n",
        "       train_regret_winner_3[slice87],\r\n",
        "       train_regret_winner_4[slice87],\r\n",
        "       train_regret_winner_5[slice87],\r\n",
        "       train_regret_winner_6[slice87],\r\n",
        "       train_regret_winner_7[slice87],\r\n",
        "       train_regret_winner_8[slice87],\r\n",
        "       train_regret_winner_9[slice87],\r\n",
        "       train_regret_winner_10[slice87],\r\n",
        "       train_regret_winner_11[slice87],\r\n",
        "       train_regret_winner_12[slice87],\r\n",
        "       train_regret_winner_13[slice87],\r\n",
        "       train_regret_winner_14[slice87],\r\n",
        "       train_regret_winner_15[slice87],\r\n",
        "       train_regret_winner_16[slice87],\r\n",
        "       train_regret_winner_17[slice87],\r\n",
        "       train_regret_winner_18[slice87],\r\n",
        "       train_regret_winner_19[slice87],\r\n",
        "       train_regret_winner_20[slice87]]\r\n",
        "\r\n",
        "loser87_results = pd.DataFrame(loser87).sort_values(by=[0], ascending=False)\r\n",
        "winner87_results = pd.DataFrame(winner87).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser87 = np.asarray(loser87_results[4:5][0])[0]\r\n",
        "median_loser87 = np.asarray(loser87_results[9:10][0])[0]\r\n",
        "upper_loser87 = np.asarray(loser87_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner87 = np.asarray(winner87_results[4:5][0])[0]\r\n",
        "median_winner87 = np.asarray(winner87_results[9:10][0])[0]\r\n",
        "upper_winner87 = np.asarray(winner87_results[14:15][0])[0]"
      ],
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0gPkLC0yxBCs"
      },
      "source": [
        "# Iteration97 :\r\n",
        "\r\n",
        "slice97 = 96\r\n",
        "\r\n",
        "loser97 = [train_regret_loser_1[slice97],\r\n",
        "       train_regret_loser_2[slice97],\r\n",
        "       train_regret_loser_3[slice97],\r\n",
        "       train_regret_loser_4[slice97],\r\n",
        "       train_regret_loser_5[slice97],\r\n",
        "       train_regret_loser_6[slice97],\r\n",
        "       train_regret_loser_7[slice97],\r\n",
        "       train_regret_loser_8[slice97],\r\n",
        "       train_regret_loser_9[slice97],\r\n",
        "       train_regret_loser_10[slice97],\r\n",
        "       train_regret_loser_11[slice97],\r\n",
        "       train_regret_loser_12[slice97],\r\n",
        "       train_regret_loser_13[slice97],\r\n",
        "       train_regret_loser_14[slice97],\r\n",
        "       train_regret_loser_15[slice97],\r\n",
        "       train_regret_loser_16[slice97],\r\n",
        "       train_regret_loser_17[slice97],\r\n",
        "       train_regret_loser_18[slice97],\r\n",
        "       train_regret_loser_19[slice97],\r\n",
        "       train_regret_loser_20[slice97]]\r\n",
        "\r\n",
        "winner97 = [train_regret_winner_1[slice97],\r\n",
        "       train_regret_winner_2[slice97],\r\n",
        "       train_regret_winner_3[slice97],\r\n",
        "       train_regret_winner_4[slice97],\r\n",
        "       train_regret_winner_5[slice97],\r\n",
        "       train_regret_winner_6[slice97],\r\n",
        "       train_regret_winner_7[slice97],\r\n",
        "       train_regret_winner_8[slice97],\r\n",
        "       train_regret_winner_9[slice97],\r\n",
        "       train_regret_winner_10[slice97],\r\n",
        "       train_regret_winner_11[slice97],\r\n",
        "       train_regret_winner_12[slice97],\r\n",
        "       train_regret_winner_13[slice97],\r\n",
        "       train_regret_winner_14[slice97],\r\n",
        "       train_regret_winner_15[slice97],\r\n",
        "       train_regret_winner_16[slice97],\r\n",
        "       train_regret_winner_17[slice97],\r\n",
        "       train_regret_winner_18[slice97],\r\n",
        "       train_regret_winner_19[slice97],\r\n",
        "       train_regret_winner_20[slice97]]\r\n",
        "\r\n",
        "loser97_results = pd.DataFrame(loser97).sort_values(by=[0], ascending=False)\r\n",
        "winner97_results = pd.DataFrame(winner97).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser97 = np.asarray(loser97_results[4:5][0])[0]\r\n",
        "median_loser97 = np.asarray(loser97_results[9:10][0])[0]\r\n",
        "upper_loser97 = np.asarray(loser97_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner97 = np.asarray(winner97_results[4:5][0])[0]\r\n",
        "median_winner97 = np.asarray(winner97_results[9:10][0])[0]\r\n",
        "upper_winner97 = np.asarray(winner97_results[14:15][0])[0]"
      ],
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BbgYEpYUxBFr"
      },
      "source": [
        "# Iteration8 :\r\n",
        "\r\n",
        "slice8 = 7\r\n",
        "\r\n",
        "loser8 = [train_regret_loser_1[slice8],\r\n",
        "       train_regret_loser_2[slice8],\r\n",
        "       train_regret_loser_3[slice8],\r\n",
        "       train_regret_loser_4[slice8],\r\n",
        "       train_regret_loser_5[slice8],\r\n",
        "       train_regret_loser_6[slice8],\r\n",
        "       train_regret_loser_7[slice8],\r\n",
        "       train_regret_loser_8[slice8],\r\n",
        "       train_regret_loser_9[slice8],\r\n",
        "       train_regret_loser_10[slice8],\r\n",
        "       train_regret_loser_11[slice8],\r\n",
        "       train_regret_loser_12[slice8],\r\n",
        "       train_regret_loser_13[slice8],\r\n",
        "       train_regret_loser_14[slice8],\r\n",
        "       train_regret_loser_15[slice8],\r\n",
        "       train_regret_loser_16[slice8],\r\n",
        "       train_regret_loser_17[slice8],\r\n",
        "       train_regret_loser_18[slice8],\r\n",
        "       train_regret_loser_19[slice8],\r\n",
        "       train_regret_loser_20[slice8]]\r\n",
        "\r\n",
        "winner8 = [train_regret_winner_1[slice8],\r\n",
        "       train_regret_winner_2[slice8],\r\n",
        "       train_regret_winner_3[slice8],\r\n",
        "       train_regret_winner_4[slice8],\r\n",
        "       train_regret_winner_5[slice8],\r\n",
        "       train_regret_winner_6[slice8],\r\n",
        "       train_regret_winner_7[slice8],\r\n",
        "       train_regret_winner_8[slice8],\r\n",
        "       train_regret_winner_9[slice8],\r\n",
        "       train_regret_winner_10[slice8],\r\n",
        "       train_regret_winner_11[slice8],\r\n",
        "       train_regret_winner_12[slice8],\r\n",
        "       train_regret_winner_13[slice8],\r\n",
        "       train_regret_winner_14[slice8],\r\n",
        "       train_regret_winner_15[slice8],\r\n",
        "       train_regret_winner_16[slice8],\r\n",
        "       train_regret_winner_17[slice8],\r\n",
        "       train_regret_winner_18[slice8],\r\n",
        "       train_regret_winner_19[slice8],\r\n",
        "       train_regret_winner_20[slice8]]\r\n",
        "\r\n",
        "loser8_results = pd.DataFrame(loser8).sort_values(by=[0], ascending=False)\r\n",
        "winner8_results = pd.DataFrame(winner8).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser8 = np.asarray(loser8_results[4:5][0])[0]\r\n",
        "median_loser8 = np.asarray(loser8_results[9:10][0])[0]\r\n",
        "upper_loser8 = np.asarray(loser8_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner8 = np.asarray(winner8_results[4:5][0])[0]\r\n",
        "median_winner8 = np.asarray(winner8_results[9:10][0])[0]\r\n",
        "upper_winner8 = np.asarray(winner8_results[14:15][0])[0]"
      ],
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "im9dHUqvxBH9"
      },
      "source": [
        "# Iteration18 :\r\n",
        "\r\n",
        "slice18 = 17\r\n",
        "\r\n",
        "loser18 = [train_regret_loser_1[slice18],\r\n",
        "       train_regret_loser_2[slice18],\r\n",
        "       train_regret_loser_3[slice18],\r\n",
        "       train_regret_loser_4[slice18],\r\n",
        "       train_regret_loser_5[slice18],\r\n",
        "       train_regret_loser_6[slice18],\r\n",
        "       train_regret_loser_7[slice18],\r\n",
        "       train_regret_loser_8[slice18],\r\n",
        "       train_regret_loser_9[slice18],\r\n",
        "       train_regret_loser_10[slice18],\r\n",
        "       train_regret_loser_11[slice18],\r\n",
        "       train_regret_loser_12[slice18],\r\n",
        "       train_regret_loser_13[slice18],\r\n",
        "       train_regret_loser_14[slice18],\r\n",
        "       train_regret_loser_15[slice18],\r\n",
        "       train_regret_loser_16[slice18],\r\n",
        "       train_regret_loser_17[slice18],\r\n",
        "       train_regret_loser_18[slice18],\r\n",
        "       train_regret_loser_19[slice18],\r\n",
        "       train_regret_loser_20[slice18]]\r\n",
        "\r\n",
        "winner18 = [train_regret_winner_1[slice18],\r\n",
        "       train_regret_winner_2[slice18],\r\n",
        "       train_regret_winner_3[slice18],\r\n",
        "       train_regret_winner_4[slice18],\r\n",
        "       train_regret_winner_5[slice18],\r\n",
        "       train_regret_winner_6[slice18],\r\n",
        "       train_regret_winner_7[slice18],\r\n",
        "       train_regret_winner_8[slice18],\r\n",
        "       train_regret_winner_9[slice18],\r\n",
        "       train_regret_winner_10[slice18],\r\n",
        "       train_regret_winner_11[slice18],\r\n",
        "       train_regret_winner_12[slice18],\r\n",
        "       train_regret_winner_13[slice18],\r\n",
        "       train_regret_winner_14[slice18],\r\n",
        "       train_regret_winner_15[slice18],\r\n",
        "       train_regret_winner_16[slice18],\r\n",
        "       train_regret_winner_17[slice18],\r\n",
        "       train_regret_winner_18[slice18],\r\n",
        "       train_regret_winner_19[slice18],\r\n",
        "       train_regret_winner_20[slice18]]\r\n",
        "\r\n",
        "loser18_results = pd.DataFrame(loser18).sort_values(by=[0], ascending=False)\r\n",
        "winner18_results = pd.DataFrame(winner18).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser18 = np.asarray(loser18_results[4:5][0])[0]\r\n",
        "median_loser18 = np.asarray(loser18_results[9:10][0])[0]\r\n",
        "upper_loser18 = np.asarray(loser18_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner18 = np.asarray(winner18_results[4:5][0])[0]\r\n",
        "median_winner18 = np.asarray(winner18_results[9:10][0])[0]\r\n",
        "upper_winner18 = np.asarray(winner18_results[14:15][0])[0]"
      ],
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IvqFjlBDxBK4"
      },
      "source": [
        "# Iteration28 :\r\n",
        "\r\n",
        "slice28 = 27\r\n",
        "\r\n",
        "loser28 = [train_regret_loser_1[slice28],\r\n",
        "       train_regret_loser_2[slice28],\r\n",
        "       train_regret_loser_3[slice28],\r\n",
        "       train_regret_loser_4[slice28],\r\n",
        "       train_regret_loser_5[slice28],\r\n",
        "       train_regret_loser_6[slice28],\r\n",
        "       train_regret_loser_7[slice28],\r\n",
        "       train_regret_loser_8[slice28],\r\n",
        "       train_regret_loser_9[slice28],\r\n",
        "       train_regret_loser_10[slice28],\r\n",
        "       train_regret_loser_11[slice28],\r\n",
        "       train_regret_loser_12[slice28],\r\n",
        "       train_regret_loser_13[slice28],\r\n",
        "       train_regret_loser_14[slice28],\r\n",
        "       train_regret_loser_15[slice28],\r\n",
        "       train_regret_loser_16[slice28],\r\n",
        "       train_regret_loser_17[slice28],\r\n",
        "       train_regret_loser_18[slice28],\r\n",
        "       train_regret_loser_19[slice28],\r\n",
        "       train_regret_loser_20[slice28]]\r\n",
        "\r\n",
        "winner28 = [train_regret_winner_1[slice28],\r\n",
        "       train_regret_winner_2[slice28],\r\n",
        "       train_regret_winner_3[slice28],\r\n",
        "       train_regret_winner_4[slice28],\r\n",
        "       train_regret_winner_5[slice28],\r\n",
        "       train_regret_winner_6[slice28],\r\n",
        "       train_regret_winner_7[slice28],\r\n",
        "       train_regret_winner_8[slice28],\r\n",
        "       train_regret_winner_9[slice28],\r\n",
        "       train_regret_winner_10[slice28],\r\n",
        "       train_regret_winner_11[slice28],\r\n",
        "       train_regret_winner_12[slice28],\r\n",
        "       train_regret_winner_13[slice28],\r\n",
        "       train_regret_winner_14[slice28],\r\n",
        "       train_regret_winner_15[slice28],\r\n",
        "       train_regret_winner_16[slice28],\r\n",
        "       train_regret_winner_17[slice28],\r\n",
        "       train_regret_winner_18[slice28],\r\n",
        "       train_regret_winner_19[slice28],\r\n",
        "       train_regret_winner_20[slice28]]\r\n",
        "\r\n",
        "loser28_results = pd.DataFrame(loser28).sort_values(by=[0], ascending=False)\r\n",
        "winner28_results = pd.DataFrame(winner28).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser28 = np.asarray(loser28_results[4:5][0])[0]\r\n",
        "median_loser28 = np.asarray(loser28_results[9:10][0])[0]\r\n",
        "upper_loser28 = np.asarray(loser28_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner28 = np.asarray(winner28_results[4:5][0])[0]\r\n",
        "median_winner28 = np.asarray(winner28_results[9:10][0])[0]\r\n",
        "upper_winner28 = np.asarray(winner28_results[14:15][0])[0]"
      ],
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "It5daJCEted3"
      },
      "source": [
        "# Iteration38 :\r\n",
        "\r\n",
        "slice38 = 37\r\n",
        "\r\n",
        "loser38 = [train_regret_loser_1[slice38],\r\n",
        "       train_regret_loser_2[slice38],\r\n",
        "       train_regret_loser_3[slice38],\r\n",
        "       train_regret_loser_4[slice38],\r\n",
        "       train_regret_loser_5[slice38],\r\n",
        "       train_regret_loser_6[slice38],\r\n",
        "       train_regret_loser_7[slice38],\r\n",
        "       train_regret_loser_8[slice38],\r\n",
        "       train_regret_loser_9[slice38],\r\n",
        "       train_regret_loser_10[slice38],\r\n",
        "       train_regret_loser_11[slice38],\r\n",
        "       train_regret_loser_12[slice38],\r\n",
        "       train_regret_loser_13[slice38],\r\n",
        "       train_regret_loser_14[slice38],\r\n",
        "       train_regret_loser_15[slice38],\r\n",
        "       train_regret_loser_16[slice38],\r\n",
        "       train_regret_loser_17[slice38],\r\n",
        "       train_regret_loser_18[slice38],\r\n",
        "       train_regret_loser_19[slice38],\r\n",
        "       train_regret_loser_20[slice38]]\r\n",
        "\r\n",
        "winner38 = [train_regret_winner_1[slice38],\r\n",
        "       train_regret_winner_2[slice38],\r\n",
        "       train_regret_winner_3[slice38],\r\n",
        "       train_regret_winner_4[slice38],\r\n",
        "       train_regret_winner_5[slice38],\r\n",
        "       train_regret_winner_6[slice38],\r\n",
        "       train_regret_winner_7[slice38],\r\n",
        "       train_regret_winner_8[slice38],\r\n",
        "       train_regret_winner_9[slice38],\r\n",
        "       train_regret_winner_10[slice38],\r\n",
        "       train_regret_winner_11[slice38],\r\n",
        "       train_regret_winner_12[slice38],\r\n",
        "       train_regret_winner_13[slice38],\r\n",
        "       train_regret_winner_14[slice38],\r\n",
        "       train_regret_winner_15[slice38],\r\n",
        "       train_regret_winner_16[slice38],\r\n",
        "       train_regret_winner_17[slice38],\r\n",
        "       train_regret_winner_18[slice38],\r\n",
        "       train_regret_winner_19[slice38],\r\n",
        "       train_regret_winner_20[slice38]]\r\n",
        "\r\n",
        "loser38_results = pd.DataFrame(loser38).sort_values(by=[0], ascending=False)\r\n",
        "winner38_results = pd.DataFrame(winner38).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser38 = np.asarray(loser38_results[4:5][0])[0]\r\n",
        "median_loser38 = np.asarray(loser38_results[9:10][0])[0]\r\n",
        "upper_loser38 = np.asarray(loser38_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner38 = np.asarray(winner38_results[4:5][0])[0]\r\n",
        "median_winner38 = np.asarray(winner38_results[9:10][0])[0]\r\n",
        "upper_winner38 = np.asarray(winner38_results[14:15][0])[0]"
      ],
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GekXOf_gtegw"
      },
      "source": [
        "# Iteration48 :\r\n",
        "\r\n",
        "slice48 = 47\r\n",
        "\r\n",
        "loser48 = [train_regret_loser_1[slice48],\r\n",
        "       train_regret_loser_2[slice48],\r\n",
        "       train_regret_loser_3[slice48],\r\n",
        "       train_regret_loser_4[slice48],\r\n",
        "       train_regret_loser_5[slice48],\r\n",
        "       train_regret_loser_6[slice48],\r\n",
        "       train_regret_loser_7[slice48],\r\n",
        "       train_regret_loser_8[slice48],\r\n",
        "       train_regret_loser_9[slice48],\r\n",
        "       train_regret_loser_10[slice48],\r\n",
        "       train_regret_loser_11[slice48],\r\n",
        "       train_regret_loser_12[slice48],\r\n",
        "       train_regret_loser_13[slice48],\r\n",
        "       train_regret_loser_14[slice48],\r\n",
        "       train_regret_loser_15[slice48],\r\n",
        "       train_regret_loser_16[slice48],\r\n",
        "       train_regret_loser_17[slice48],\r\n",
        "       train_regret_loser_18[slice48],\r\n",
        "       train_regret_loser_19[slice48],\r\n",
        "       train_regret_loser_20[slice48]]\r\n",
        "\r\n",
        "winner48 = [train_regret_winner_1[slice48],\r\n",
        "       train_regret_winner_2[slice48],\r\n",
        "       train_regret_winner_3[slice48],\r\n",
        "       train_regret_winner_4[slice48],\r\n",
        "       train_regret_winner_5[slice48],\r\n",
        "       train_regret_winner_6[slice48],\r\n",
        "       train_regret_winner_7[slice48],\r\n",
        "       train_regret_winner_8[slice48],\r\n",
        "       train_regret_winner_9[slice48],\r\n",
        "       train_regret_winner_10[slice48],\r\n",
        "       train_regret_winner_11[slice48],\r\n",
        "       train_regret_winner_12[slice48],\r\n",
        "       train_regret_winner_13[slice48],\r\n",
        "       train_regret_winner_14[slice48],\r\n",
        "       train_regret_winner_15[slice48],\r\n",
        "       train_regret_winner_16[slice48],\r\n",
        "       train_regret_winner_17[slice48],\r\n",
        "       train_regret_winner_18[slice48],\r\n",
        "       train_regret_winner_19[slice48],\r\n",
        "       train_regret_winner_20[slice48]]\r\n",
        "\r\n",
        "loser48_results = pd.DataFrame(loser48).sort_values(by=[0], ascending=False)\r\n",
        "winner48_results = pd.DataFrame(winner48).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser48 = np.asarray(loser48_results[4:5][0])[0]\r\n",
        "median_loser48 = np.asarray(loser48_results[9:10][0])[0]\r\n",
        "upper_loser48 = np.asarray(loser48_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner48 = np.asarray(winner48_results[4:5][0])[0]\r\n",
        "median_winner48 = np.asarray(winner48_results[9:10][0])[0]\r\n",
        "upper_winner48 = np.asarray(winner48_results[14:15][0])[0]"
      ],
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNpV6knOtejv"
      },
      "source": [
        "# Iteration58 :\r\n",
        "\r\n",
        "slice58 = 57\r\n",
        "\r\n",
        "loser58 = [train_regret_loser_1[slice58],\r\n",
        "       train_regret_loser_2[slice58],\r\n",
        "       train_regret_loser_3[slice58],\r\n",
        "       train_regret_loser_4[slice58],\r\n",
        "       train_regret_loser_5[slice58],\r\n",
        "       train_regret_loser_6[slice58],\r\n",
        "       train_regret_loser_7[slice58],\r\n",
        "       train_regret_loser_8[slice58],\r\n",
        "       train_regret_loser_9[slice58],\r\n",
        "       train_regret_loser_10[slice58],\r\n",
        "       train_regret_loser_11[slice58],\r\n",
        "       train_regret_loser_12[slice58],\r\n",
        "       train_regret_loser_13[slice58],\r\n",
        "       train_regret_loser_14[slice58],\r\n",
        "       train_regret_loser_15[slice58],\r\n",
        "       train_regret_loser_16[slice58],\r\n",
        "       train_regret_loser_17[slice58],\r\n",
        "       train_regret_loser_18[slice58],\r\n",
        "       train_regret_loser_19[slice58],\r\n",
        "       train_regret_loser_20[slice58]]\r\n",
        "\r\n",
        "winner58 = [train_regret_winner_1[slice58],\r\n",
        "       train_regret_winner_2[slice58],\r\n",
        "       train_regret_winner_3[slice58],\r\n",
        "       train_regret_winner_4[slice58],\r\n",
        "       train_regret_winner_5[slice58],\r\n",
        "       train_regret_winner_6[slice58],\r\n",
        "       train_regret_winner_7[slice58],\r\n",
        "       train_regret_winner_8[slice58],\r\n",
        "       train_regret_winner_9[slice58],\r\n",
        "       train_regret_winner_10[slice58],\r\n",
        "       train_regret_winner_11[slice58],\r\n",
        "       train_regret_winner_12[slice58],\r\n",
        "       train_regret_winner_13[slice58],\r\n",
        "       train_regret_winner_14[slice58],\r\n",
        "       train_regret_winner_15[slice58],\r\n",
        "       train_regret_winner_16[slice58],\r\n",
        "       train_regret_winner_17[slice58],\r\n",
        "       train_regret_winner_18[slice58],\r\n",
        "       train_regret_winner_19[slice58],\r\n",
        "       train_regret_winner_20[slice58]]\r\n",
        "\r\n",
        "loser58_results = pd.DataFrame(loser58).sort_values(by=[0], ascending=False)\r\n",
        "winner58_results = pd.DataFrame(winner58).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser58 = np.asarray(loser58_results[4:5][0])[0]\r\n",
        "median_loser58 = np.asarray(loser58_results[9:10][0])[0]\r\n",
        "upper_loser58 = np.asarray(loser58_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner58 = np.asarray(winner58_results[4:5][0])[0]\r\n",
        "median_winner58 = np.asarray(winner58_results[9:10][0])[0]\r\n",
        "upper_winner58 = np.asarray(winner58_results[14:15][0])[0]"
      ],
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VFsqaBqtemn"
      },
      "source": [
        "# Iteration68 :\r\n",
        "\r\n",
        "slice68 = 67\r\n",
        "\r\n",
        "loser68 = [train_regret_loser_1[slice68],\r\n",
        "       train_regret_loser_2[slice68],\r\n",
        "       train_regret_loser_3[slice68],\r\n",
        "       train_regret_loser_4[slice68],\r\n",
        "       train_regret_loser_5[slice68],\r\n",
        "       train_regret_loser_6[slice68],\r\n",
        "       train_regret_loser_7[slice68],\r\n",
        "       train_regret_loser_8[slice68],\r\n",
        "       train_regret_loser_9[slice68],\r\n",
        "       train_regret_loser_10[slice68],\r\n",
        "       train_regret_loser_11[slice68],\r\n",
        "       train_regret_loser_12[slice68],\r\n",
        "       train_regret_loser_13[slice68],\r\n",
        "       train_regret_loser_14[slice68],\r\n",
        "       train_regret_loser_15[slice68],\r\n",
        "       train_regret_loser_16[slice68],\r\n",
        "       train_regret_loser_17[slice68],\r\n",
        "       train_regret_loser_18[slice68],\r\n",
        "       train_regret_loser_19[slice68],\r\n",
        "       train_regret_loser_20[slice68]]\r\n",
        "\r\n",
        "winner68 = [train_regret_winner_1[slice68],\r\n",
        "       train_regret_winner_2[slice68],\r\n",
        "       train_regret_winner_3[slice68],\r\n",
        "       train_regret_winner_4[slice68],\r\n",
        "       train_regret_winner_5[slice68],\r\n",
        "       train_regret_winner_6[slice68],\r\n",
        "       train_regret_winner_7[slice68],\r\n",
        "       train_regret_winner_8[slice68],\r\n",
        "       train_regret_winner_9[slice68],\r\n",
        "       train_regret_winner_10[slice68],\r\n",
        "       train_regret_winner_11[slice68],\r\n",
        "       train_regret_winner_12[slice68],\r\n",
        "       train_regret_winner_13[slice68],\r\n",
        "       train_regret_winner_14[slice68],\r\n",
        "       train_regret_winner_15[slice68],\r\n",
        "       train_regret_winner_16[slice68],\r\n",
        "       train_regret_winner_17[slice68],\r\n",
        "       train_regret_winner_18[slice68],\r\n",
        "       train_regret_winner_19[slice68],\r\n",
        "       train_regret_winner_20[slice68]]\r\n",
        "\r\n",
        "loser68_results = pd.DataFrame(loser68).sort_values(by=[0], ascending=False)\r\n",
        "winner68_results = pd.DataFrame(winner68).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser68 = np.asarray(loser68_results[4:5][0])[0]\r\n",
        "median_loser68 = np.asarray(loser68_results[9:10][0])[0]\r\n",
        "upper_loser68 = np.asarray(loser68_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner68 = np.asarray(winner68_results[4:5][0])[0]\r\n",
        "median_winner68 = np.asarray(winner68_results[9:10][0])[0]\r\n",
        "upper_winner68 = np.asarray(winner68_results[14:15][0])[0]"
      ],
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XlFH0Wxtepo"
      },
      "source": [
        "# Iteration78 :\r\n",
        "\r\n",
        "slice78 = 77\r\n",
        "\r\n",
        "loser78 = [train_regret_loser_1[slice78],\r\n",
        "       train_regret_loser_2[slice78],\r\n",
        "       train_regret_loser_3[slice78],\r\n",
        "       train_regret_loser_4[slice78],\r\n",
        "       train_regret_loser_5[slice78],\r\n",
        "       train_regret_loser_6[slice78],\r\n",
        "       train_regret_loser_7[slice78],\r\n",
        "       train_regret_loser_8[slice78],\r\n",
        "       train_regret_loser_9[slice78],\r\n",
        "       train_regret_loser_10[slice78],\r\n",
        "       train_regret_loser_11[slice78],\r\n",
        "       train_regret_loser_12[slice78],\r\n",
        "       train_regret_loser_13[slice78],\r\n",
        "       train_regret_loser_14[slice78],\r\n",
        "       train_regret_loser_15[slice78],\r\n",
        "       train_regret_loser_16[slice78],\r\n",
        "       train_regret_loser_17[slice78],\r\n",
        "       train_regret_loser_18[slice78],\r\n",
        "       train_regret_loser_19[slice78],\r\n",
        "       train_regret_loser_20[slice78]]\r\n",
        "\r\n",
        "winner78 = [train_regret_winner_1[slice78],\r\n",
        "       train_regret_winner_2[slice78],\r\n",
        "       train_regret_winner_3[slice78],\r\n",
        "       train_regret_winner_4[slice78],\r\n",
        "       train_regret_winner_5[slice78],\r\n",
        "       train_regret_winner_6[slice78],\r\n",
        "       train_regret_winner_7[slice78],\r\n",
        "       train_regret_winner_8[slice78],\r\n",
        "       train_regret_winner_9[slice78],\r\n",
        "       train_regret_winner_10[slice78],\r\n",
        "       train_regret_winner_11[slice78],\r\n",
        "       train_regret_winner_12[slice78],\r\n",
        "       train_regret_winner_13[slice78],\r\n",
        "       train_regret_winner_14[slice78],\r\n",
        "       train_regret_winner_15[slice78],\r\n",
        "       train_regret_winner_16[slice78],\r\n",
        "       train_regret_winner_17[slice78],\r\n",
        "       train_regret_winner_18[slice78],\r\n",
        "       train_regret_winner_19[slice78],\r\n",
        "       train_regret_winner_20[slice78]]\r\n",
        "\r\n",
        "loser78_results = pd.DataFrame(loser78).sort_values(by=[0], ascending=False)\r\n",
        "winner78_results = pd.DataFrame(winner78).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser78 = np.asarray(loser78_results[4:5][0])[0]\r\n",
        "median_loser78 = np.asarray(loser78_results[9:10][0])[0]\r\n",
        "upper_loser78 = np.asarray(loser78_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner78 = np.asarray(winner78_results[4:5][0])[0]\r\n",
        "median_winner78 = np.asarray(winner78_results[9:10][0])[0]\r\n",
        "upper_winner78 = np.asarray(winner78_results[14:15][0])[0]"
      ],
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsL0PJVetesC"
      },
      "source": [
        "# Iteration88 :\r\n",
        "\r\n",
        "slice88 = 87\r\n",
        "\r\n",
        "loser88 = [train_regret_loser_1[slice88],\r\n",
        "       train_regret_loser_2[slice88],\r\n",
        "       train_regret_loser_3[slice88],\r\n",
        "       train_regret_loser_4[slice88],\r\n",
        "       train_regret_loser_5[slice88],\r\n",
        "       train_regret_loser_6[slice88],\r\n",
        "       train_regret_loser_7[slice88],\r\n",
        "       train_regret_loser_8[slice88],\r\n",
        "       train_regret_loser_9[slice88],\r\n",
        "       train_regret_loser_10[slice88],\r\n",
        "       train_regret_loser_11[slice88],\r\n",
        "       train_regret_loser_12[slice88],\r\n",
        "       train_regret_loser_13[slice88],\r\n",
        "       train_regret_loser_14[slice88],\r\n",
        "       train_regret_loser_15[slice88],\r\n",
        "       train_regret_loser_16[slice88],\r\n",
        "       train_regret_loser_17[slice88],\r\n",
        "       train_regret_loser_18[slice88],\r\n",
        "       train_regret_loser_19[slice88],\r\n",
        "       train_regret_loser_20[slice88]]\r\n",
        "\r\n",
        "winner88 = [train_regret_winner_1[slice88],\r\n",
        "       train_regret_winner_2[slice88],\r\n",
        "       train_regret_winner_3[slice88],\r\n",
        "       train_regret_winner_4[slice88],\r\n",
        "       train_regret_winner_5[slice88],\r\n",
        "       train_regret_winner_6[slice88],\r\n",
        "       train_regret_winner_7[slice88],\r\n",
        "       train_regret_winner_8[slice88],\r\n",
        "       train_regret_winner_9[slice88],\r\n",
        "       train_regret_winner_10[slice88],\r\n",
        "       train_regret_winner_11[slice88],\r\n",
        "       train_regret_winner_12[slice88],\r\n",
        "       train_regret_winner_13[slice88],\r\n",
        "       train_regret_winner_14[slice88],\r\n",
        "       train_regret_winner_15[slice88],\r\n",
        "       train_regret_winner_16[slice88],\r\n",
        "       train_regret_winner_17[slice88],\r\n",
        "       train_regret_winner_18[slice88],\r\n",
        "       train_regret_winner_19[slice88],\r\n",
        "       train_regret_winner_20[slice88]]\r\n",
        "\r\n",
        "loser88_results = pd.DataFrame(loser88).sort_values(by=[0], ascending=False)\r\n",
        "winner88_results = pd.DataFrame(winner88).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser88 = np.asarray(loser88_results[4:5][0])[0]\r\n",
        "median_loser88 = np.asarray(loser88_results[9:10][0])[0]\r\n",
        "upper_loser88 = np.asarray(loser88_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner88 = np.asarray(winner88_results[4:5][0])[0]\r\n",
        "median_winner88 = np.asarray(winner88_results[9:10][0])[0]\r\n",
        "upper_winner88 = np.asarray(winner88_results[14:15][0])[0]"
      ],
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYWEWGku1o_9"
      },
      "source": [
        "# Iteration98 :\r\n",
        "\r\n",
        "slice98 = 97\r\n",
        "\r\n",
        "loser98 = [train_regret_loser_1[slice98],\r\n",
        "       train_regret_loser_2[slice98],\r\n",
        "       train_regret_loser_3[slice98],\r\n",
        "       train_regret_loser_4[slice98],\r\n",
        "       train_regret_loser_5[slice98],\r\n",
        "       train_regret_loser_6[slice98],\r\n",
        "       train_regret_loser_7[slice98],\r\n",
        "       train_regret_loser_8[slice98],\r\n",
        "       train_regret_loser_9[slice98],\r\n",
        "       train_regret_loser_10[slice98],\r\n",
        "       train_regret_loser_11[slice98],\r\n",
        "       train_regret_loser_12[slice98],\r\n",
        "       train_regret_loser_13[slice98],\r\n",
        "       train_regret_loser_14[slice98],\r\n",
        "       train_regret_loser_15[slice98],\r\n",
        "       train_regret_loser_16[slice98],\r\n",
        "       train_regret_loser_17[slice98],\r\n",
        "       train_regret_loser_18[slice98],\r\n",
        "       train_regret_loser_19[slice98],\r\n",
        "       train_regret_loser_20[slice98]]\r\n",
        "\r\n",
        "winner98 = [train_regret_winner_1[slice98],\r\n",
        "       train_regret_winner_2[slice98],\r\n",
        "       train_regret_winner_3[slice98],\r\n",
        "       train_regret_winner_4[slice98],\r\n",
        "       train_regret_winner_5[slice98],\r\n",
        "       train_regret_winner_6[slice98],\r\n",
        "       train_regret_winner_7[slice98],\r\n",
        "       train_regret_winner_8[slice98],\r\n",
        "       train_regret_winner_9[slice98],\r\n",
        "       train_regret_winner_10[slice98],\r\n",
        "       train_regret_winner_11[slice98],\r\n",
        "       train_regret_winner_12[slice98],\r\n",
        "       train_regret_winner_13[slice98],\r\n",
        "       train_regret_winner_14[slice98],\r\n",
        "       train_regret_winner_15[slice98],\r\n",
        "       train_regret_winner_16[slice98],\r\n",
        "       train_regret_winner_17[slice98],\r\n",
        "       train_regret_winner_18[slice98],\r\n",
        "       train_regret_winner_19[slice98],\r\n",
        "       train_regret_winner_20[slice98]]\r\n",
        "\r\n",
        "loser98_results = pd.DataFrame(loser98).sort_values(by=[0], ascending=False)\r\n",
        "winner98_results = pd.DataFrame(winner98).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser98 = np.asarray(loser98_results[4:5][0])[0]\r\n",
        "median_loser98 = np.asarray(loser98_results[9:10][0])[0]\r\n",
        "upper_loser98 = np.asarray(loser98_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner98 = np.asarray(winner98_results[4:5][0])[0]\r\n",
        "median_winner98 = np.asarray(winner98_results[9:10][0])[0]\r\n",
        "upper_winner98 = np.asarray(winner98_results[14:15][0])[0]"
      ],
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "guLGgcX91pDK"
      },
      "source": [
        "# Iteration9 :\r\n",
        "\r\n",
        "slice9 = 8\r\n",
        "\r\n",
        "loser9 = [train_regret_loser_1[slice9],\r\n",
        "       train_regret_loser_2[slice9],\r\n",
        "       train_regret_loser_3[slice9],\r\n",
        "       train_regret_loser_4[slice9],\r\n",
        "       train_regret_loser_5[slice9],\r\n",
        "       train_regret_loser_6[slice9],\r\n",
        "       train_regret_loser_7[slice9],\r\n",
        "       train_regret_loser_8[slice9],\r\n",
        "       train_regret_loser_9[slice9],\r\n",
        "       train_regret_loser_10[slice9],\r\n",
        "       train_regret_loser_11[slice9],\r\n",
        "       train_regret_loser_12[slice9],\r\n",
        "       train_regret_loser_13[slice9],\r\n",
        "       train_regret_loser_14[slice9],\r\n",
        "       train_regret_loser_15[slice9],\r\n",
        "       train_regret_loser_16[slice9],\r\n",
        "       train_regret_loser_17[slice9],\r\n",
        "       train_regret_loser_18[slice9],\r\n",
        "       train_regret_loser_19[slice9],\r\n",
        "       train_regret_loser_20[slice9]]\r\n",
        "\r\n",
        "winner9 = [train_regret_winner_1[slice9],\r\n",
        "       train_regret_winner_2[slice9],\r\n",
        "       train_regret_winner_3[slice9],\r\n",
        "       train_regret_winner_4[slice9],\r\n",
        "       train_regret_winner_5[slice9],\r\n",
        "       train_regret_winner_6[slice9],\r\n",
        "       train_regret_winner_7[slice9],\r\n",
        "       train_regret_winner_8[slice9],\r\n",
        "       train_regret_winner_9[slice9],\r\n",
        "       train_regret_winner_10[slice9],\r\n",
        "       train_regret_winner_11[slice9],\r\n",
        "       train_regret_winner_12[slice9],\r\n",
        "       train_regret_winner_13[slice9],\r\n",
        "       train_regret_winner_14[slice9],\r\n",
        "       train_regret_winner_15[slice9],\r\n",
        "       train_regret_winner_16[slice9],\r\n",
        "       train_regret_winner_17[slice9],\r\n",
        "       train_regret_winner_18[slice9],\r\n",
        "       train_regret_winner_19[slice9],\r\n",
        "       train_regret_winner_20[slice9]]\r\n",
        "\r\n",
        "loser9_results = pd.DataFrame(loser9).sort_values(by=[0], ascending=False)\r\n",
        "winner9_results = pd.DataFrame(winner9).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser9 = np.asarray(loser9_results[4:5][0])[0]\r\n",
        "median_loser9 = np.asarray(loser9_results[9:10][0])[0]\r\n",
        "upper_loser9 = np.asarray(loser9_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner9 = np.asarray(winner9_results[4:5][0])[0]\r\n",
        "median_winner9 = np.asarray(winner9_results[9:10][0])[0]\r\n",
        "upper_winner9 = np.asarray(winner9_results[14:15][0])[0]"
      ],
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3_9-eoT1pGd"
      },
      "source": [
        "# Iteration19 :\r\n",
        "\r\n",
        "slice19 = 18\r\n",
        "\r\n",
        "loser19 = [train_regret_loser_1[slice19],\r\n",
        "       train_regret_loser_2[slice19],\r\n",
        "       train_regret_loser_3[slice19],\r\n",
        "       train_regret_loser_4[slice19],\r\n",
        "       train_regret_loser_5[slice19],\r\n",
        "       train_regret_loser_6[slice19],\r\n",
        "       train_regret_loser_7[slice19],\r\n",
        "       train_regret_loser_8[slice19],\r\n",
        "       train_regret_loser_9[slice19],\r\n",
        "       train_regret_loser_10[slice19],\r\n",
        "       train_regret_loser_11[slice19],\r\n",
        "       train_regret_loser_12[slice19],\r\n",
        "       train_regret_loser_13[slice19],\r\n",
        "       train_regret_loser_14[slice19],\r\n",
        "       train_regret_loser_15[slice19],\r\n",
        "       train_regret_loser_16[slice19],\r\n",
        "       train_regret_loser_17[slice19],\r\n",
        "       train_regret_loser_18[slice19],\r\n",
        "       train_regret_loser_19[slice19],\r\n",
        "       train_regret_loser_20[slice19]]\r\n",
        "\r\n",
        "winner19 = [train_regret_winner_1[slice19],\r\n",
        "       train_regret_winner_2[slice19],\r\n",
        "       train_regret_winner_3[slice19],\r\n",
        "       train_regret_winner_4[slice19],\r\n",
        "       train_regret_winner_5[slice19],\r\n",
        "       train_regret_winner_6[slice19],\r\n",
        "       train_regret_winner_7[slice19],\r\n",
        "       train_regret_winner_8[slice19],\r\n",
        "       train_regret_winner_9[slice19],\r\n",
        "       train_regret_winner_10[slice19],\r\n",
        "       train_regret_winner_11[slice19],\r\n",
        "       train_regret_winner_12[slice19],\r\n",
        "       train_regret_winner_13[slice19],\r\n",
        "       train_regret_winner_14[slice19],\r\n",
        "       train_regret_winner_15[slice19],\r\n",
        "       train_regret_winner_16[slice19],\r\n",
        "       train_regret_winner_17[slice19],\r\n",
        "       train_regret_winner_18[slice19],\r\n",
        "       train_regret_winner_19[slice19],\r\n",
        "       train_regret_winner_20[slice19]]\r\n",
        "\r\n",
        "loser19_results = pd.DataFrame(loser19).sort_values(by=[0], ascending=False)\r\n",
        "winner19_results = pd.DataFrame(winner19).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser19 = np.asarray(loser19_results[4:5][0])[0]\r\n",
        "median_loser19 = np.asarray(loser19_results[9:10][0])[0]\r\n",
        "upper_loser19 = np.asarray(loser19_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner19 = np.asarray(winner19_results[4:5][0])[0]\r\n",
        "median_winner19 = np.asarray(winner19_results[9:10][0])[0]\r\n",
        "upper_winner19 = np.asarray(winner19_results[14:15][0])[0]"
      ],
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ae5x13M1pRp"
      },
      "source": [
        "# Iteration29 :\r\n",
        "\r\n",
        "slice29 = 28\r\n",
        "\r\n",
        "loser29 = [train_regret_loser_1[slice29],\r\n",
        "       train_regret_loser_2[slice29],\r\n",
        "       train_regret_loser_3[slice29],\r\n",
        "       train_regret_loser_4[slice29],\r\n",
        "       train_regret_loser_5[slice29],\r\n",
        "       train_regret_loser_6[slice29],\r\n",
        "       train_regret_loser_7[slice29],\r\n",
        "       train_regret_loser_8[slice29],\r\n",
        "       train_regret_loser_9[slice29],\r\n",
        "       train_regret_loser_10[slice29],\r\n",
        "       train_regret_loser_11[slice29],\r\n",
        "       train_regret_loser_12[slice29],\r\n",
        "       train_regret_loser_13[slice29],\r\n",
        "       train_regret_loser_14[slice29],\r\n",
        "       train_regret_loser_15[slice29],\r\n",
        "       train_regret_loser_16[slice29],\r\n",
        "       train_regret_loser_17[slice29],\r\n",
        "       train_regret_loser_18[slice29],\r\n",
        "       train_regret_loser_19[slice29],\r\n",
        "       train_regret_loser_20[slice29]]\r\n",
        "\r\n",
        "winner29 = [train_regret_winner_1[slice29],\r\n",
        "       train_regret_winner_2[slice29],\r\n",
        "       train_regret_winner_3[slice29],\r\n",
        "       train_regret_winner_4[slice29],\r\n",
        "       train_regret_winner_5[slice29],\r\n",
        "       train_regret_winner_6[slice29],\r\n",
        "       train_regret_winner_7[slice29],\r\n",
        "       train_regret_winner_8[slice29],\r\n",
        "       train_regret_winner_9[slice29],\r\n",
        "       train_regret_winner_10[slice29],\r\n",
        "       train_regret_winner_11[slice29],\r\n",
        "       train_regret_winner_12[slice29],\r\n",
        "       train_regret_winner_13[slice29],\r\n",
        "       train_regret_winner_14[slice29],\r\n",
        "       train_regret_winner_15[slice29],\r\n",
        "       train_regret_winner_16[slice29],\r\n",
        "       train_regret_winner_17[slice29],\r\n",
        "       train_regret_winner_18[slice29],\r\n",
        "       train_regret_winner_19[slice29],\r\n",
        "       train_regret_winner_20[slice29]]\r\n",
        "\r\n",
        "loser29_results = pd.DataFrame(loser29).sort_values(by=[0], ascending=False)\r\n",
        "winner29_results = pd.DataFrame(winner29).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser29 = np.asarray(loser29_results[4:5][0])[0]\r\n",
        "median_loser29 = np.asarray(loser29_results[9:10][0])[0]\r\n",
        "upper_loser29 = np.asarray(loser29_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner29 = np.asarray(winner29_results[4:5][0])[0]\r\n",
        "median_winner29 = np.asarray(winner29_results[9:10][0])[0]\r\n",
        "upper_winner29 = np.asarray(winner29_results[14:15][0])[0]"
      ],
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XR9aPvBy1pUc"
      },
      "source": [
        "# Iteration39 :\r\n",
        "\r\n",
        "slice39 = 38\r\n",
        "\r\n",
        "loser39 = [train_regret_loser_1[slice39],\r\n",
        "       train_regret_loser_2[slice39],\r\n",
        "       train_regret_loser_3[slice39],\r\n",
        "       train_regret_loser_4[slice39],\r\n",
        "       train_regret_loser_5[slice39],\r\n",
        "       train_regret_loser_6[slice39],\r\n",
        "       train_regret_loser_7[slice39],\r\n",
        "       train_regret_loser_8[slice39],\r\n",
        "       train_regret_loser_9[slice39],\r\n",
        "       train_regret_loser_10[slice39],\r\n",
        "       train_regret_loser_11[slice39],\r\n",
        "       train_regret_loser_12[slice39],\r\n",
        "       train_regret_loser_13[slice39],\r\n",
        "       train_regret_loser_14[slice39],\r\n",
        "       train_regret_loser_15[slice39],\r\n",
        "       train_regret_loser_16[slice39],\r\n",
        "       train_regret_loser_17[slice39],\r\n",
        "       train_regret_loser_18[slice39],\r\n",
        "       train_regret_loser_19[slice39],\r\n",
        "       train_regret_loser_20[slice39]]\r\n",
        "\r\n",
        "winner39 = [train_regret_winner_1[slice39],\r\n",
        "       train_regret_winner_2[slice39],\r\n",
        "       train_regret_winner_3[slice39],\r\n",
        "       train_regret_winner_4[slice39],\r\n",
        "       train_regret_winner_5[slice39],\r\n",
        "       train_regret_winner_6[slice39],\r\n",
        "       train_regret_winner_7[slice39],\r\n",
        "       train_regret_winner_8[slice39],\r\n",
        "       train_regret_winner_9[slice39],\r\n",
        "       train_regret_winner_10[slice39],\r\n",
        "       train_regret_winner_11[slice39],\r\n",
        "       train_regret_winner_12[slice39],\r\n",
        "       train_regret_winner_13[slice39],\r\n",
        "       train_regret_winner_14[slice39],\r\n",
        "       train_regret_winner_15[slice39],\r\n",
        "       train_regret_winner_16[slice39],\r\n",
        "       train_regret_winner_17[slice39],\r\n",
        "       train_regret_winner_18[slice39],\r\n",
        "       train_regret_winner_19[slice39],\r\n",
        "       train_regret_winner_20[slice39]]\r\n",
        "\r\n",
        "loser39_results = pd.DataFrame(loser39).sort_values(by=[0], ascending=False)\r\n",
        "winner39_results = pd.DataFrame(winner39).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser39 = np.asarray(loser39_results[4:5][0])[0]\r\n",
        "median_loser39 = np.asarray(loser39_results[9:10][0])[0]\r\n",
        "upper_loser39 = np.asarray(loser39_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner39 = np.asarray(winner39_results[4:5][0])[0]\r\n",
        "median_winner39 = np.asarray(winner39_results[9:10][0])[0]\r\n",
        "upper_winner39 = np.asarray(winner39_results[14:15][0])[0]"
      ],
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vb7vh2K1pXp"
      },
      "source": [
        "# Iteration49 :\r\n",
        "\r\n",
        "slice49 = 48\r\n",
        "\r\n",
        "loser49 = [train_regret_loser_1[slice49],\r\n",
        "       train_regret_loser_2[slice49],\r\n",
        "       train_regret_loser_3[slice49],\r\n",
        "       train_regret_loser_4[slice49],\r\n",
        "       train_regret_loser_5[slice49],\r\n",
        "       train_regret_loser_6[slice49],\r\n",
        "       train_regret_loser_7[slice49],\r\n",
        "       train_regret_loser_8[slice49],\r\n",
        "       train_regret_loser_9[slice49],\r\n",
        "       train_regret_loser_10[slice49],\r\n",
        "       train_regret_loser_11[slice49],\r\n",
        "       train_regret_loser_12[slice49],\r\n",
        "       train_regret_loser_13[slice49],\r\n",
        "       train_regret_loser_14[slice49],\r\n",
        "       train_regret_loser_15[slice49],\r\n",
        "       train_regret_loser_16[slice49],\r\n",
        "       train_regret_loser_17[slice49],\r\n",
        "       train_regret_loser_18[slice49],\r\n",
        "       train_regret_loser_19[slice49],\r\n",
        "       train_regret_loser_20[slice49]]\r\n",
        "\r\n",
        "winner49 = [train_regret_winner_1[slice49],\r\n",
        "       train_regret_winner_2[slice49],\r\n",
        "       train_regret_winner_3[slice49],\r\n",
        "       train_regret_winner_4[slice49],\r\n",
        "       train_regret_winner_5[slice49],\r\n",
        "       train_regret_winner_6[slice49],\r\n",
        "       train_regret_winner_7[slice49],\r\n",
        "       train_regret_winner_8[slice49],\r\n",
        "       train_regret_winner_9[slice49],\r\n",
        "       train_regret_winner_10[slice49],\r\n",
        "       train_regret_winner_11[slice49],\r\n",
        "       train_regret_winner_12[slice49],\r\n",
        "       train_regret_winner_13[slice49],\r\n",
        "       train_regret_winner_14[slice49],\r\n",
        "       train_regret_winner_15[slice49],\r\n",
        "       train_regret_winner_16[slice49],\r\n",
        "       train_regret_winner_17[slice49],\r\n",
        "       train_regret_winner_18[slice49],\r\n",
        "       train_regret_winner_19[slice49],\r\n",
        "       train_regret_winner_20[slice49]]\r\n",
        "\r\n",
        "loser49_results = pd.DataFrame(loser49).sort_values(by=[0], ascending=False)\r\n",
        "winner49_results = pd.DataFrame(winner49).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser49 = np.asarray(loser49_results[4:5][0])[0]\r\n",
        "median_loser49 = np.asarray(loser49_results[9:10][0])[0]\r\n",
        "upper_loser49 = np.asarray(loser49_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner49 = np.asarray(winner49_results[4:5][0])[0]\r\n",
        "median_winner49 = np.asarray(winner49_results[9:10][0])[0]\r\n",
        "upper_winner49 = np.asarray(winner49_results[14:15][0])[0]"
      ],
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3wN7DWwt1paM"
      },
      "source": [
        "# Iteration59 :\r\n",
        "\r\n",
        "slice59 = 58\r\n",
        "\r\n",
        "loser59 = [train_regret_loser_1[slice59],\r\n",
        "       train_regret_loser_2[slice59],\r\n",
        "       train_regret_loser_3[slice59],\r\n",
        "       train_regret_loser_4[slice59],\r\n",
        "       train_regret_loser_5[slice59],\r\n",
        "       train_regret_loser_6[slice59],\r\n",
        "       train_regret_loser_7[slice59],\r\n",
        "       train_regret_loser_8[slice59],\r\n",
        "       train_regret_loser_9[slice59],\r\n",
        "       train_regret_loser_10[slice59],\r\n",
        "       train_regret_loser_11[slice59],\r\n",
        "       train_regret_loser_12[slice59],\r\n",
        "       train_regret_loser_13[slice59],\r\n",
        "       train_regret_loser_14[slice59],\r\n",
        "       train_regret_loser_15[slice59],\r\n",
        "       train_regret_loser_16[slice59],\r\n",
        "       train_regret_loser_17[slice59],\r\n",
        "       train_regret_loser_18[slice59],\r\n",
        "       train_regret_loser_19[slice59],\r\n",
        "       train_regret_loser_20[slice59]]\r\n",
        "\r\n",
        "winner59 = [train_regret_winner_1[slice59],\r\n",
        "       train_regret_winner_2[slice59],\r\n",
        "       train_regret_winner_3[slice59],\r\n",
        "       train_regret_winner_4[slice59],\r\n",
        "       train_regret_winner_5[slice59],\r\n",
        "       train_regret_winner_6[slice59],\r\n",
        "       train_regret_winner_7[slice59],\r\n",
        "       train_regret_winner_8[slice59],\r\n",
        "       train_regret_winner_9[slice59],\r\n",
        "       train_regret_winner_10[slice59],\r\n",
        "       train_regret_winner_11[slice59],\r\n",
        "       train_regret_winner_12[slice59],\r\n",
        "       train_regret_winner_13[slice59],\r\n",
        "       train_regret_winner_14[slice59],\r\n",
        "       train_regret_winner_15[slice59],\r\n",
        "       train_regret_winner_16[slice59],\r\n",
        "       train_regret_winner_17[slice59],\r\n",
        "       train_regret_winner_18[slice59],\r\n",
        "       train_regret_winner_19[slice59],\r\n",
        "       train_regret_winner_20[slice59]]\r\n",
        "\r\n",
        "loser59_results = pd.DataFrame(loser59).sort_values(by=[0], ascending=False)\r\n",
        "winner59_results = pd.DataFrame(winner59).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser59 = np.asarray(loser59_results[4:5][0])[0]\r\n",
        "median_loser59 = np.asarray(loser59_results[9:10][0])[0]\r\n",
        "upper_loser59 = np.asarray(loser59_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner59 = np.asarray(winner59_results[4:5][0])[0]\r\n",
        "median_winner59 = np.asarray(winner59_results[9:10][0])[0]\r\n",
        "upper_winner59 = np.asarray(winner59_results[14:15][0])[0]"
      ],
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqtgekml1pde"
      },
      "source": [
        "# Iteration69 :\r\n",
        "\r\n",
        "slice69 = 68\r\n",
        "\r\n",
        "loser69 = [train_regret_loser_1[slice69],\r\n",
        "       train_regret_loser_2[slice69],\r\n",
        "       train_regret_loser_3[slice69],\r\n",
        "       train_regret_loser_4[slice69],\r\n",
        "       train_regret_loser_5[slice69],\r\n",
        "       train_regret_loser_6[slice69],\r\n",
        "       train_regret_loser_7[slice69],\r\n",
        "       train_regret_loser_8[slice69],\r\n",
        "       train_regret_loser_9[slice69],\r\n",
        "       train_regret_loser_10[slice69],\r\n",
        "       train_regret_loser_11[slice69],\r\n",
        "       train_regret_loser_12[slice69],\r\n",
        "       train_regret_loser_13[slice69],\r\n",
        "       train_regret_loser_14[slice69],\r\n",
        "       train_regret_loser_15[slice69],\r\n",
        "       train_regret_loser_16[slice69],\r\n",
        "       train_regret_loser_17[slice69],\r\n",
        "       train_regret_loser_18[slice69],\r\n",
        "       train_regret_loser_19[slice69],\r\n",
        "       train_regret_loser_20[slice69]]\r\n",
        "\r\n",
        "winner69 = [train_regret_winner_1[slice69],\r\n",
        "       train_regret_winner_2[slice69],\r\n",
        "       train_regret_winner_3[slice69],\r\n",
        "       train_regret_winner_4[slice69],\r\n",
        "       train_regret_winner_5[slice69],\r\n",
        "       train_regret_winner_6[slice69],\r\n",
        "       train_regret_winner_7[slice69],\r\n",
        "       train_regret_winner_8[slice69],\r\n",
        "       train_regret_winner_9[slice69],\r\n",
        "       train_regret_winner_10[slice69],\r\n",
        "       train_regret_winner_11[slice69],\r\n",
        "       train_regret_winner_12[slice69],\r\n",
        "       train_regret_winner_13[slice69],\r\n",
        "       train_regret_winner_14[slice69],\r\n",
        "       train_regret_winner_15[slice69],\r\n",
        "       train_regret_winner_16[slice69],\r\n",
        "       train_regret_winner_17[slice69],\r\n",
        "       train_regret_winner_18[slice69],\r\n",
        "       train_regret_winner_19[slice69],\r\n",
        "       train_regret_winner_20[slice69]]\r\n",
        "\r\n",
        "loser69_results = pd.DataFrame(loser69).sort_values(by=[0], ascending=False)\r\n",
        "winner69_results = pd.DataFrame(winner69).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser69 = np.asarray(loser69_results[4:5][0])[0]\r\n",
        "median_loser69 = np.asarray(loser69_results[9:10][0])[0]\r\n",
        "upper_loser69 = np.asarray(loser69_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner69 = np.asarray(winner69_results[4:5][0])[0]\r\n",
        "median_winner69 = np.asarray(winner69_results[9:10][0])[0]\r\n",
        "upper_winner69 = np.asarray(winner69_results[14:15][0])[0]"
      ],
      "execution_count": 165,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPFTa1171pgF"
      },
      "source": [
        "# Iteration79 :\r\n",
        "\r\n",
        "slice79 = 78\r\n",
        "\r\n",
        "loser79 = [train_regret_loser_1[slice79],\r\n",
        "       train_regret_loser_2[slice79],\r\n",
        "       train_regret_loser_3[slice79],\r\n",
        "       train_regret_loser_4[slice79],\r\n",
        "       train_regret_loser_5[slice79],\r\n",
        "       train_regret_loser_6[slice79],\r\n",
        "       train_regret_loser_7[slice79],\r\n",
        "       train_regret_loser_8[slice79],\r\n",
        "       train_regret_loser_9[slice79],\r\n",
        "       train_regret_loser_10[slice79],\r\n",
        "       train_regret_loser_11[slice79],\r\n",
        "       train_regret_loser_12[slice79],\r\n",
        "       train_regret_loser_13[slice79],\r\n",
        "       train_regret_loser_14[slice79],\r\n",
        "       train_regret_loser_15[slice79],\r\n",
        "       train_regret_loser_16[slice79],\r\n",
        "       train_regret_loser_17[slice79],\r\n",
        "       train_regret_loser_18[slice79],\r\n",
        "       train_regret_loser_19[slice79],\r\n",
        "       train_regret_loser_20[slice79]]\r\n",
        "\r\n",
        "winner79 = [train_regret_winner_1[slice79],\r\n",
        "       train_regret_winner_2[slice79],\r\n",
        "       train_regret_winner_3[slice79],\r\n",
        "       train_regret_winner_4[slice79],\r\n",
        "       train_regret_winner_5[slice79],\r\n",
        "       train_regret_winner_6[slice79],\r\n",
        "       train_regret_winner_7[slice79],\r\n",
        "       train_regret_winner_8[slice79],\r\n",
        "       train_regret_winner_9[slice79],\r\n",
        "       train_regret_winner_10[slice79],\r\n",
        "       train_regret_winner_11[slice79],\r\n",
        "       train_regret_winner_12[slice79],\r\n",
        "       train_regret_winner_13[slice79],\r\n",
        "       train_regret_winner_14[slice79],\r\n",
        "       train_regret_winner_15[slice79],\r\n",
        "       train_regret_winner_16[slice79],\r\n",
        "       train_regret_winner_17[slice79],\r\n",
        "       train_regret_winner_18[slice79],\r\n",
        "       train_regret_winner_19[slice79],\r\n",
        "       train_regret_winner_20[slice79]]\r\n",
        "\r\n",
        "loser79_results = pd.DataFrame(loser79).sort_values(by=[0], ascending=False)\r\n",
        "winner79_results = pd.DataFrame(winner79).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser79 = np.asarray(loser79_results[4:5][0])[0]\r\n",
        "median_loser79 = np.asarray(loser79_results[9:10][0])[0]\r\n",
        "upper_loser79 = np.asarray(loser79_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner79 = np.asarray(winner79_results[4:5][0])[0]\r\n",
        "median_winner79 = np.asarray(winner79_results[9:10][0])[0]\r\n",
        "upper_winner79 = np.asarray(winner79_results[14:15][0])[0]"
      ],
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTnRF4lx1pio"
      },
      "source": [
        "# Iteration89 :\r\n",
        "\r\n",
        "slice89 = 88\r\n",
        "\r\n",
        "loser89 = [train_regret_loser_1[slice89],\r\n",
        "       train_regret_loser_2[slice89],\r\n",
        "       train_regret_loser_3[slice89],\r\n",
        "       train_regret_loser_4[slice89],\r\n",
        "       train_regret_loser_5[slice89],\r\n",
        "       train_regret_loser_6[slice89],\r\n",
        "       train_regret_loser_7[slice89],\r\n",
        "       train_regret_loser_8[slice89],\r\n",
        "       train_regret_loser_9[slice89],\r\n",
        "       train_regret_loser_10[slice89],\r\n",
        "       train_regret_loser_11[slice89],\r\n",
        "       train_regret_loser_12[slice89],\r\n",
        "       train_regret_loser_13[slice89],\r\n",
        "       train_regret_loser_14[slice89],\r\n",
        "       train_regret_loser_15[slice89],\r\n",
        "       train_regret_loser_16[slice89],\r\n",
        "       train_regret_loser_17[slice89],\r\n",
        "       train_regret_loser_18[slice89],\r\n",
        "       train_regret_loser_19[slice89],\r\n",
        "       train_regret_loser_20[slice89]]\r\n",
        "\r\n",
        "winner89 = [train_regret_winner_1[slice89],\r\n",
        "       train_regret_winner_2[slice89],\r\n",
        "       train_regret_winner_3[slice89],\r\n",
        "       train_regret_winner_4[slice89],\r\n",
        "       train_regret_winner_5[slice89],\r\n",
        "       train_regret_winner_6[slice89],\r\n",
        "       train_regret_winner_7[slice89],\r\n",
        "       train_regret_winner_8[slice89],\r\n",
        "       train_regret_winner_9[slice89],\r\n",
        "       train_regret_winner_10[slice89],\r\n",
        "       train_regret_winner_11[slice89],\r\n",
        "       train_regret_winner_12[slice89],\r\n",
        "       train_regret_winner_13[slice89],\r\n",
        "       train_regret_winner_14[slice89],\r\n",
        "       train_regret_winner_15[slice89],\r\n",
        "       train_regret_winner_16[slice89],\r\n",
        "       train_regret_winner_17[slice89],\r\n",
        "       train_regret_winner_18[slice89],\r\n",
        "       train_regret_winner_19[slice89],\r\n",
        "       train_regret_winner_20[slice89]]\r\n",
        "\r\n",
        "loser89_results = pd.DataFrame(loser89).sort_values(by=[0], ascending=False)\r\n",
        "winner89_results = pd.DataFrame(winner89).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser89 = np.asarray(loser89_results[4:5][0])[0]\r\n",
        "median_loser89 = np.asarray(loser89_results[9:10][0])[0]\r\n",
        "upper_loser89 = np.asarray(loser89_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner89 = np.asarray(winner89_results[4:5][0])[0]\r\n",
        "median_winner89 = np.asarray(winner89_results[9:10][0])[0]\r\n",
        "upper_winner89 = np.asarray(winner89_results[14:15][0])[0]"
      ],
      "execution_count": 167,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hiLAF38i1plO"
      },
      "source": [
        "# Iteration99 :\r\n",
        "\r\n",
        "slice99 = 98\r\n",
        "\r\n",
        "loser99 = [train_regret_loser_1[slice99],\r\n",
        "       train_regret_loser_2[slice99],\r\n",
        "       train_regret_loser_3[slice99],\r\n",
        "       train_regret_loser_4[slice99],\r\n",
        "       train_regret_loser_5[slice99],\r\n",
        "       train_regret_loser_6[slice99],\r\n",
        "       train_regret_loser_7[slice99],\r\n",
        "       train_regret_loser_8[slice99],\r\n",
        "       train_regret_loser_9[slice99],\r\n",
        "       train_regret_loser_10[slice99],\r\n",
        "       train_regret_loser_11[slice99],\r\n",
        "       train_regret_loser_12[slice99],\r\n",
        "       train_regret_loser_13[slice99],\r\n",
        "       train_regret_loser_14[slice99],\r\n",
        "       train_regret_loser_15[slice99],\r\n",
        "       train_regret_loser_16[slice99],\r\n",
        "       train_regret_loser_17[slice99],\r\n",
        "       train_regret_loser_18[slice99],\r\n",
        "       train_regret_loser_19[slice99],\r\n",
        "       train_regret_loser_20[slice99]]\r\n",
        "\r\n",
        "winner99 = [train_regret_winner_1[slice99],\r\n",
        "       train_regret_winner_2[slice99],\r\n",
        "       train_regret_winner_3[slice99],\r\n",
        "       train_regret_winner_4[slice99],\r\n",
        "       train_regret_winner_5[slice99],\r\n",
        "       train_regret_winner_6[slice99],\r\n",
        "       train_regret_winner_7[slice99],\r\n",
        "       train_regret_winner_8[slice99],\r\n",
        "       train_regret_winner_9[slice99],\r\n",
        "       train_regret_winner_10[slice99],\r\n",
        "       train_regret_winner_11[slice99],\r\n",
        "       train_regret_winner_12[slice99],\r\n",
        "       train_regret_winner_13[slice99],\r\n",
        "       train_regret_winner_14[slice99],\r\n",
        "       train_regret_winner_15[slice99],\r\n",
        "       train_regret_winner_16[slice99],\r\n",
        "       train_regret_winner_17[slice99],\r\n",
        "       train_regret_winner_18[slice99],\r\n",
        "       train_regret_winner_19[slice99],\r\n",
        "       train_regret_winner_20[slice99]]\r\n",
        "\r\n",
        "loser99_results = pd.DataFrame(loser99).sort_values(by=[0], ascending=False)\r\n",
        "winner99_results = pd.DataFrame(winner99).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser99 = np.asarray(loser99_results[4:5][0])[0]\r\n",
        "median_loser99 = np.asarray(loser99_results[9:10][0])[0]\r\n",
        "upper_loser99 = np.asarray(loser99_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner99 = np.asarray(winner99_results[4:5][0])[0]\r\n",
        "median_winner99 = np.asarray(winner99_results[9:10][0])[0]\r\n",
        "upper_winner99 = np.asarray(winner99_results[14:15][0])[0]"
      ],
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBd_N0F21pn0"
      },
      "source": [
        "# Iteration10 :\r\n",
        "\r\n",
        "slice10 = 9\r\n",
        "\r\n",
        "loser10 = [train_regret_loser_1[slice10],\r\n",
        "       train_regret_loser_2[slice10],\r\n",
        "       train_regret_loser_3[slice10],\r\n",
        "       train_regret_loser_4[slice10],\r\n",
        "       train_regret_loser_5[slice10],\r\n",
        "       train_regret_loser_6[slice10],\r\n",
        "       train_regret_loser_7[slice10],\r\n",
        "       train_regret_loser_8[slice10],\r\n",
        "       train_regret_loser_9[slice10],\r\n",
        "       train_regret_loser_10[slice10],\r\n",
        "       train_regret_loser_11[slice10],\r\n",
        "       train_regret_loser_12[slice10],\r\n",
        "       train_regret_loser_13[slice10],\r\n",
        "       train_regret_loser_14[slice10],\r\n",
        "       train_regret_loser_15[slice10],\r\n",
        "       train_regret_loser_16[slice10],\r\n",
        "       train_regret_loser_17[slice10],\r\n",
        "       train_regret_loser_18[slice10],\r\n",
        "       train_regret_loser_19[slice10],\r\n",
        "       train_regret_loser_20[slice10]]\r\n",
        "\r\n",
        "winner10 = [train_regret_winner_1[slice10],\r\n",
        "       train_regret_winner_2[slice10],\r\n",
        "       train_regret_winner_3[slice10],\r\n",
        "       train_regret_winner_4[slice10],\r\n",
        "       train_regret_winner_5[slice10],\r\n",
        "       train_regret_winner_6[slice10],\r\n",
        "       train_regret_winner_7[slice10],\r\n",
        "       train_regret_winner_8[slice10],\r\n",
        "       train_regret_winner_9[slice10],\r\n",
        "       train_regret_winner_10[slice10],\r\n",
        "       train_regret_winner_11[slice10],\r\n",
        "       train_regret_winner_12[slice10],\r\n",
        "       train_regret_winner_13[slice10],\r\n",
        "       train_regret_winner_14[slice10],\r\n",
        "       train_regret_winner_15[slice10],\r\n",
        "       train_regret_winner_16[slice10],\r\n",
        "       train_regret_winner_17[slice10],\r\n",
        "       train_regret_winner_18[slice10],\r\n",
        "       train_regret_winner_19[slice10],\r\n",
        "       train_regret_winner_20[slice10]]\r\n",
        "\r\n",
        "loser10_results = pd.DataFrame(loser10).sort_values(by=[0], ascending=False)\r\n",
        "winner10_results = pd.DataFrame(winner10).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser10 = np.asarray(loser10_results[4:5][0])[0]\r\n",
        "median_loser10 = np.asarray(loser10_results[9:10][0])[0]\r\n",
        "upper_loser10 = np.asarray(loser10_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner10 = np.asarray(winner10_results[4:5][0])[0]\r\n",
        "median_winner10 = np.asarray(winner10_results[9:10][0])[0]\r\n",
        "upper_winner10 = np.asarray(winner10_results[14:15][0])[0]"
      ],
      "execution_count": 169,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXjdqc4s1pqb"
      },
      "source": [
        "# Iteration20 :\r\n",
        "\r\n",
        "slice20 = 19\r\n",
        "\r\n",
        "loser20 = [train_regret_loser_1[slice20],\r\n",
        "       train_regret_loser_2[slice20],\r\n",
        "       train_regret_loser_3[slice20],\r\n",
        "       train_regret_loser_4[slice20],\r\n",
        "       train_regret_loser_5[slice20],\r\n",
        "       train_regret_loser_6[slice20],\r\n",
        "       train_regret_loser_7[slice20],\r\n",
        "       train_regret_loser_8[slice20],\r\n",
        "       train_regret_loser_9[slice20],\r\n",
        "       train_regret_loser_10[slice20],\r\n",
        "       train_regret_loser_11[slice20],\r\n",
        "       train_regret_loser_12[slice20],\r\n",
        "       train_regret_loser_13[slice20],\r\n",
        "       train_regret_loser_14[slice20],\r\n",
        "       train_regret_loser_15[slice20],\r\n",
        "       train_regret_loser_16[slice20],\r\n",
        "       train_regret_loser_17[slice20],\r\n",
        "       train_regret_loser_18[slice20],\r\n",
        "       train_regret_loser_19[slice20],\r\n",
        "       train_regret_loser_20[slice20]]\r\n",
        "\r\n",
        "winner20 = [train_regret_winner_1[slice20],\r\n",
        "       train_regret_winner_2[slice20],\r\n",
        "       train_regret_winner_3[slice20],\r\n",
        "       train_regret_winner_4[slice20],\r\n",
        "       train_regret_winner_5[slice20],\r\n",
        "       train_regret_winner_6[slice20],\r\n",
        "       train_regret_winner_7[slice20],\r\n",
        "       train_regret_winner_8[slice20],\r\n",
        "       train_regret_winner_9[slice20],\r\n",
        "       train_regret_winner_10[slice20],\r\n",
        "       train_regret_winner_11[slice20],\r\n",
        "       train_regret_winner_12[slice20],\r\n",
        "       train_regret_winner_13[slice20],\r\n",
        "       train_regret_winner_14[slice20],\r\n",
        "       train_regret_winner_15[slice20],\r\n",
        "       train_regret_winner_16[slice20],\r\n",
        "       train_regret_winner_17[slice20],\r\n",
        "       train_regret_winner_18[slice20],\r\n",
        "       train_regret_winner_19[slice20],\r\n",
        "       train_regret_winner_20[slice20]]\r\n",
        "\r\n",
        "loser20_results = pd.DataFrame(loser20).sort_values(by=[0], ascending=False)\r\n",
        "winner20_results = pd.DataFrame(winner20).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser20 = np.asarray(loser20_results[4:5][0])[0]\r\n",
        "median_loser20 = np.asarray(loser20_results[9:10][0])[0]\r\n",
        "upper_loser20 = np.asarray(loser20_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner20 = np.asarray(winner20_results[4:5][0])[0]\r\n",
        "median_winner20 = np.asarray(winner20_results[9:10][0])[0]\r\n",
        "upper_winner20 = np.asarray(winner20_results[14:15][0])[0]"
      ],
      "execution_count": 170,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQBVS6dx1ptB"
      },
      "source": [
        "# Iteration30 :\r\n",
        "\r\n",
        "slice30 = 29\r\n",
        "\r\n",
        "loser30 = [train_regret_loser_1[slice30],\r\n",
        "       train_regret_loser_2[slice30],\r\n",
        "       train_regret_loser_3[slice30],\r\n",
        "       train_regret_loser_4[slice30],\r\n",
        "       train_regret_loser_5[slice30],\r\n",
        "       train_regret_loser_6[slice30],\r\n",
        "       train_regret_loser_7[slice30],\r\n",
        "       train_regret_loser_8[slice30],\r\n",
        "       train_regret_loser_9[slice30],\r\n",
        "       train_regret_loser_10[slice30],\r\n",
        "       train_regret_loser_11[slice30],\r\n",
        "       train_regret_loser_12[slice30],\r\n",
        "       train_regret_loser_13[slice30],\r\n",
        "       train_regret_loser_14[slice30],\r\n",
        "       train_regret_loser_15[slice30],\r\n",
        "       train_regret_loser_16[slice30],\r\n",
        "       train_regret_loser_17[slice30],\r\n",
        "       train_regret_loser_18[slice30],\r\n",
        "       train_regret_loser_19[slice30],\r\n",
        "       train_regret_loser_20[slice30]]\r\n",
        "\r\n",
        "winner30 = [train_regret_winner_1[slice30],\r\n",
        "       train_regret_winner_2[slice30],\r\n",
        "       train_regret_winner_3[slice30],\r\n",
        "       train_regret_winner_4[slice30],\r\n",
        "       train_regret_winner_5[slice30],\r\n",
        "       train_regret_winner_6[slice30],\r\n",
        "       train_regret_winner_7[slice30],\r\n",
        "       train_regret_winner_8[slice30],\r\n",
        "       train_regret_winner_9[slice30],\r\n",
        "       train_regret_winner_10[slice30],\r\n",
        "       train_regret_winner_11[slice30],\r\n",
        "       train_regret_winner_12[slice30],\r\n",
        "       train_regret_winner_13[slice30],\r\n",
        "       train_regret_winner_14[slice30],\r\n",
        "       train_regret_winner_15[slice30],\r\n",
        "       train_regret_winner_16[slice30],\r\n",
        "       train_regret_winner_17[slice30],\r\n",
        "       train_regret_winner_18[slice30],\r\n",
        "       train_regret_winner_19[slice30],\r\n",
        "       train_regret_winner_20[slice30]]\r\n",
        "\r\n",
        "loser30_results = pd.DataFrame(loser30).sort_values(by=[0], ascending=False)\r\n",
        "winner30_results = pd.DataFrame(winner30).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser30 = np.asarray(loser30_results[4:5][0])[0]\r\n",
        "median_loser30 = np.asarray(loser30_results[9:10][0])[0]\r\n",
        "upper_loser30 = np.asarray(loser30_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner30 = np.asarray(winner30_results[4:5][0])[0]\r\n",
        "median_winner30 = np.asarray(winner30_results[9:10][0])[0]\r\n",
        "upper_winner30 = np.asarray(winner30_results[14:15][0])[0]"
      ],
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-iWfLkj1pvn"
      },
      "source": [
        "# Iteration40 :\r\n",
        "\r\n",
        "slice40 = 39\r\n",
        "\r\n",
        "loser40 = [train_regret_loser_1[slice40],\r\n",
        "       train_regret_loser_2[slice40],\r\n",
        "       train_regret_loser_3[slice40],\r\n",
        "       train_regret_loser_4[slice40],\r\n",
        "       train_regret_loser_5[slice40],\r\n",
        "       train_regret_loser_6[slice40],\r\n",
        "       train_regret_loser_7[slice40],\r\n",
        "       train_regret_loser_8[slice40],\r\n",
        "       train_regret_loser_9[slice40],\r\n",
        "       train_regret_loser_10[slice40],\r\n",
        "       train_regret_loser_11[slice40],\r\n",
        "       train_regret_loser_12[slice40],\r\n",
        "       train_regret_loser_13[slice40],\r\n",
        "       train_regret_loser_14[slice40],\r\n",
        "       train_regret_loser_15[slice40],\r\n",
        "       train_regret_loser_16[slice40],\r\n",
        "       train_regret_loser_17[slice40],\r\n",
        "       train_regret_loser_18[slice40],\r\n",
        "       train_regret_loser_19[slice40],\r\n",
        "       train_regret_loser_20[slice40]]\r\n",
        "\r\n",
        "winner40 = [train_regret_winner_1[slice40],\r\n",
        "       train_regret_winner_2[slice40],\r\n",
        "       train_regret_winner_3[slice40],\r\n",
        "       train_regret_winner_4[slice40],\r\n",
        "       train_regret_winner_5[slice40],\r\n",
        "       train_regret_winner_6[slice40],\r\n",
        "       train_regret_winner_7[slice40],\r\n",
        "       train_regret_winner_8[slice40],\r\n",
        "       train_regret_winner_9[slice40],\r\n",
        "       train_regret_winner_10[slice40],\r\n",
        "       train_regret_winner_11[slice40],\r\n",
        "       train_regret_winner_12[slice40],\r\n",
        "       train_regret_winner_13[slice40],\r\n",
        "       train_regret_winner_14[slice40],\r\n",
        "       train_regret_winner_15[slice40],\r\n",
        "       train_regret_winner_16[slice40],\r\n",
        "       train_regret_winner_17[slice40],\r\n",
        "       train_regret_winner_18[slice40],\r\n",
        "       train_regret_winner_19[slice40],\r\n",
        "       train_regret_winner_20[slice40]]\r\n",
        "\r\n",
        "loser40_results = pd.DataFrame(loser40).sort_values(by=[0], ascending=False)\r\n",
        "winner40_results = pd.DataFrame(winner40).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser40 = np.asarray(loser40_results[4:5][0])[0]\r\n",
        "median_loser40 = np.asarray(loser40_results[9:10][0])[0]\r\n",
        "upper_loser40 = np.asarray(loser40_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner40 = np.asarray(winner40_results[4:5][0])[0]\r\n",
        "median_winner40 = np.asarray(winner40_results[9:10][0])[0]\r\n",
        "upper_winner40 = np.asarray(winner40_results[14:15][0])[0]"
      ],
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-G5i-Cjw2U7K"
      },
      "source": [
        "# Iteration50 :\r\n",
        "\r\n",
        "slice50 = 49\r\n",
        "\r\n",
        "loser50 = [train_regret_loser_1[slice50],\r\n",
        "       train_regret_loser_2[slice50],\r\n",
        "       train_regret_loser_3[slice50],\r\n",
        "       train_regret_loser_4[slice50],\r\n",
        "       train_regret_loser_5[slice50],\r\n",
        "       train_regret_loser_6[slice50],\r\n",
        "       train_regret_loser_7[slice50],\r\n",
        "       train_regret_loser_8[slice50],\r\n",
        "       train_regret_loser_9[slice50],\r\n",
        "       train_regret_loser_10[slice50],\r\n",
        "       train_regret_loser_11[slice50],\r\n",
        "       train_regret_loser_12[slice50],\r\n",
        "       train_regret_loser_13[slice50],\r\n",
        "       train_regret_loser_14[slice50],\r\n",
        "       train_regret_loser_15[slice50],\r\n",
        "       train_regret_loser_16[slice50],\r\n",
        "       train_regret_loser_17[slice50],\r\n",
        "       train_regret_loser_18[slice50],\r\n",
        "       train_regret_loser_19[slice50],\r\n",
        "       train_regret_loser_20[slice50]]\r\n",
        "\r\n",
        "winner50 = [train_regret_winner_1[slice50],\r\n",
        "       train_regret_winner_2[slice50],\r\n",
        "       train_regret_winner_3[slice50],\r\n",
        "       train_regret_winner_4[slice50],\r\n",
        "       train_regret_winner_5[slice50],\r\n",
        "       train_regret_winner_6[slice50],\r\n",
        "       train_regret_winner_7[slice50],\r\n",
        "       train_regret_winner_8[slice50],\r\n",
        "       train_regret_winner_9[slice50],\r\n",
        "       train_regret_winner_10[slice50],\r\n",
        "       train_regret_winner_11[slice50],\r\n",
        "       train_regret_winner_12[slice50],\r\n",
        "       train_regret_winner_13[slice50],\r\n",
        "       train_regret_winner_14[slice50],\r\n",
        "       train_regret_winner_15[slice50],\r\n",
        "       train_regret_winner_16[slice50],\r\n",
        "       train_regret_winner_17[slice50],\r\n",
        "       train_regret_winner_18[slice50],\r\n",
        "       train_regret_winner_19[slice50],\r\n",
        "       train_regret_winner_20[slice50]]\r\n",
        "\r\n",
        "loser50_results = pd.DataFrame(loser50).sort_values(by=[0], ascending=False)\r\n",
        "winner50_results = pd.DataFrame(winner50).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser50 = np.asarray(loser50_results[4:5][0])[0]\r\n",
        "median_loser50 = np.asarray(loser50_results[9:10][0])[0]\r\n",
        "upper_loser50 = np.asarray(loser50_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner50 = np.asarray(winner50_results[4:5][0])[0]\r\n",
        "median_winner50 = np.asarray(winner50_results[9:10][0])[0]\r\n",
        "upper_winner50 = np.asarray(winner50_results[14:15][0])[0]"
      ],
      "execution_count": 173,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RY6qxZQA2U-r"
      },
      "source": [
        "# Iteration60 :\r\n",
        "\r\n",
        "slice60 = 59\r\n",
        "\r\n",
        "loser60 = [train_regret_loser_1[slice60],\r\n",
        "       train_regret_loser_2[slice60],\r\n",
        "       train_regret_loser_3[slice60],\r\n",
        "       train_regret_loser_4[slice60],\r\n",
        "       train_regret_loser_5[slice60],\r\n",
        "       train_regret_loser_6[slice60],\r\n",
        "       train_regret_loser_7[slice60],\r\n",
        "       train_regret_loser_8[slice60],\r\n",
        "       train_regret_loser_9[slice60],\r\n",
        "       train_regret_loser_10[slice60],\r\n",
        "       train_regret_loser_11[slice60],\r\n",
        "       train_regret_loser_12[slice60],\r\n",
        "       train_regret_loser_13[slice60],\r\n",
        "       train_regret_loser_14[slice60],\r\n",
        "       train_regret_loser_15[slice60],\r\n",
        "       train_regret_loser_16[slice60],\r\n",
        "       train_regret_loser_17[slice60],\r\n",
        "       train_regret_loser_18[slice60],\r\n",
        "       train_regret_loser_19[slice60],\r\n",
        "       train_regret_loser_20[slice60]]\r\n",
        "\r\n",
        "winner60 = [train_regret_winner_1[slice60],\r\n",
        "       train_regret_winner_2[slice60],\r\n",
        "       train_regret_winner_3[slice60],\r\n",
        "       train_regret_winner_4[slice60],\r\n",
        "       train_regret_winner_5[slice60],\r\n",
        "       train_regret_winner_6[slice60],\r\n",
        "       train_regret_winner_7[slice60],\r\n",
        "       train_regret_winner_8[slice60],\r\n",
        "       train_regret_winner_9[slice60],\r\n",
        "       train_regret_winner_10[slice60],\r\n",
        "       train_regret_winner_11[slice60],\r\n",
        "       train_regret_winner_12[slice60],\r\n",
        "       train_regret_winner_13[slice60],\r\n",
        "       train_regret_winner_14[slice60],\r\n",
        "       train_regret_winner_15[slice60],\r\n",
        "       train_regret_winner_16[slice60],\r\n",
        "       train_regret_winner_17[slice60],\r\n",
        "       train_regret_winner_18[slice60],\r\n",
        "       train_regret_winner_19[slice60],\r\n",
        "       train_regret_winner_20[slice60]]\r\n",
        "\r\n",
        "loser60_results = pd.DataFrame(loser60).sort_values(by=[0], ascending=False)\r\n",
        "winner60_results = pd.DataFrame(winner60).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser60 = np.asarray(loser60_results[4:5][0])[0]\r\n",
        "median_loser60 = np.asarray(loser60_results[9:10][0])[0]\r\n",
        "upper_loser60 = np.asarray(loser60_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner60 = np.asarray(winner60_results[4:5][0])[0]\r\n",
        "median_winner60 = np.asarray(winner60_results[9:10][0])[0]\r\n",
        "upper_winner60 = np.asarray(winner60_results[14:15][0])[0]"
      ],
      "execution_count": 174,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBqgVxVH2VCU"
      },
      "source": [
        "# Iteration70 :\r\n",
        "\r\n",
        "slice70 = 69\r\n",
        "\r\n",
        "loser70 = [train_regret_loser_1[slice70],\r\n",
        "       train_regret_loser_2[slice70],\r\n",
        "       train_regret_loser_3[slice70],\r\n",
        "       train_regret_loser_4[slice70],\r\n",
        "       train_regret_loser_5[slice70],\r\n",
        "       train_regret_loser_6[slice70],\r\n",
        "       train_regret_loser_7[slice70],\r\n",
        "       train_regret_loser_8[slice70],\r\n",
        "       train_regret_loser_9[slice70],\r\n",
        "       train_regret_loser_10[slice70],\r\n",
        "       train_regret_loser_11[slice70],\r\n",
        "       train_regret_loser_12[slice70],\r\n",
        "       train_regret_loser_13[slice70],\r\n",
        "       train_regret_loser_14[slice70],\r\n",
        "       train_regret_loser_15[slice70],\r\n",
        "       train_regret_loser_16[slice70],\r\n",
        "       train_regret_loser_17[slice70],\r\n",
        "       train_regret_loser_18[slice70],\r\n",
        "       train_regret_loser_19[slice70],\r\n",
        "       train_regret_loser_20[slice70]]\r\n",
        "\r\n",
        "winner70 = [train_regret_winner_1[slice70],\r\n",
        "       train_regret_winner_2[slice70],\r\n",
        "       train_regret_winner_3[slice70],\r\n",
        "       train_regret_winner_4[slice70],\r\n",
        "       train_regret_winner_5[slice70],\r\n",
        "       train_regret_winner_6[slice70],\r\n",
        "       train_regret_winner_7[slice70],\r\n",
        "       train_regret_winner_8[slice70],\r\n",
        "       train_regret_winner_9[slice70],\r\n",
        "       train_regret_winner_10[slice70],\r\n",
        "       train_regret_winner_11[slice70],\r\n",
        "       train_regret_winner_12[slice70],\r\n",
        "       train_regret_winner_13[slice70],\r\n",
        "       train_regret_winner_14[slice70],\r\n",
        "       train_regret_winner_15[slice70],\r\n",
        "       train_regret_winner_16[slice70],\r\n",
        "       train_regret_winner_17[slice70],\r\n",
        "       train_regret_winner_18[slice70],\r\n",
        "       train_regret_winner_19[slice70],\r\n",
        "       train_regret_winner_20[slice70]]\r\n",
        "\r\n",
        "loser70_results = pd.DataFrame(loser70).sort_values(by=[0], ascending=False)\r\n",
        "winner70_results = pd.DataFrame(winner70).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser70 = np.asarray(loser70_results[4:5][0])[0]\r\n",
        "median_loser70 = np.asarray(loser70_results[9:10][0])[0]\r\n",
        "upper_loser70 = np.asarray(loser70_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner70 = np.asarray(winner70_results[4:5][0])[0]\r\n",
        "median_winner70 = np.asarray(winner70_results[9:10][0])[0]\r\n",
        "upper_winner70 = np.asarray(winner70_results[14:15][0])[0]"
      ],
      "execution_count": 175,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YF7mlDZL1pyN"
      },
      "source": [
        "# Iteration80 :\r\n",
        "\r\n",
        "slice80 = 79\r\n",
        "\r\n",
        "loser80 = [train_regret_loser_1[slice80],\r\n",
        "       train_regret_loser_2[slice80],\r\n",
        "       train_regret_loser_3[slice80],\r\n",
        "       train_regret_loser_4[slice80],\r\n",
        "       train_regret_loser_5[slice80],\r\n",
        "       train_regret_loser_6[slice80],\r\n",
        "       train_regret_loser_7[slice80],\r\n",
        "       train_regret_loser_8[slice80],\r\n",
        "       train_regret_loser_9[slice80],\r\n",
        "       train_regret_loser_10[slice80],\r\n",
        "       train_regret_loser_11[slice80],\r\n",
        "       train_regret_loser_12[slice80],\r\n",
        "       train_regret_loser_13[slice80],\r\n",
        "       train_regret_loser_14[slice80],\r\n",
        "       train_regret_loser_15[slice80],\r\n",
        "       train_regret_loser_16[slice80],\r\n",
        "       train_regret_loser_17[slice80],\r\n",
        "       train_regret_loser_18[slice80],\r\n",
        "       train_regret_loser_19[slice80],\r\n",
        "       train_regret_loser_20[slice80]]\r\n",
        "\r\n",
        "winner80 = [train_regret_winner_1[slice80],\r\n",
        "       train_regret_winner_2[slice80],\r\n",
        "       train_regret_winner_3[slice80],\r\n",
        "       train_regret_winner_4[slice80],\r\n",
        "       train_regret_winner_5[slice80],\r\n",
        "       train_regret_winner_6[slice80],\r\n",
        "       train_regret_winner_7[slice80],\r\n",
        "       train_regret_winner_8[slice80],\r\n",
        "       train_regret_winner_9[slice80],\r\n",
        "       train_regret_winner_10[slice80],\r\n",
        "       train_regret_winner_11[slice80],\r\n",
        "       train_regret_winner_12[slice80],\r\n",
        "       train_regret_winner_13[slice80],\r\n",
        "       train_regret_winner_14[slice80],\r\n",
        "       train_regret_winner_15[slice80],\r\n",
        "       train_regret_winner_16[slice80],\r\n",
        "       train_regret_winner_17[slice80],\r\n",
        "       train_regret_winner_18[slice80],\r\n",
        "       train_regret_winner_19[slice80],\r\n",
        "       train_regret_winner_20[slice80]]\r\n",
        "\r\n",
        "loser80_results = pd.DataFrame(loser80).sort_values(by=[0], ascending=False)\r\n",
        "winner80_results = pd.DataFrame(winner80).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser80 = np.asarray(loser80_results[4:5][0])[0]\r\n",
        "median_loser80 = np.asarray(loser80_results[9:10][0])[0]\r\n",
        "upper_loser80 = np.asarray(loser80_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner80 = np.asarray(winner80_results[4:5][0])[0]\r\n",
        "median_winner80 = np.asarray(winner80_results[9:10][0])[0]\r\n",
        "upper_winner80 = np.asarray(winner80_results[14:15][0])[0]"
      ],
      "execution_count": 176,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1Rw8IkD1p0z"
      },
      "source": [
        "# Iteration90 :\r\n",
        "\r\n",
        "slice90 = 89\r\n",
        "\r\n",
        "loser90 = [train_regret_loser_1[slice90],\r\n",
        "       train_regret_loser_2[slice90],\r\n",
        "       train_regret_loser_3[slice90],\r\n",
        "       train_regret_loser_4[slice90],\r\n",
        "       train_regret_loser_5[slice90],\r\n",
        "       train_regret_loser_6[slice90],\r\n",
        "       train_regret_loser_7[slice90],\r\n",
        "       train_regret_loser_8[slice90],\r\n",
        "       train_regret_loser_9[slice90],\r\n",
        "       train_regret_loser_10[slice90],\r\n",
        "       train_regret_loser_11[slice90],\r\n",
        "       train_regret_loser_12[slice90],\r\n",
        "       train_regret_loser_13[slice90],\r\n",
        "       train_regret_loser_14[slice90],\r\n",
        "       train_regret_loser_15[slice90],\r\n",
        "       train_regret_loser_16[slice90],\r\n",
        "       train_regret_loser_17[slice90],\r\n",
        "       train_regret_loser_18[slice90],\r\n",
        "       train_regret_loser_19[slice90],\r\n",
        "       train_regret_loser_20[slice90]]\r\n",
        "\r\n",
        "winner90 = [train_regret_winner_1[slice90],\r\n",
        "       train_regret_winner_2[slice90],\r\n",
        "       train_regret_winner_3[slice90],\r\n",
        "       train_regret_winner_4[slice90],\r\n",
        "       train_regret_winner_5[slice90],\r\n",
        "       train_regret_winner_6[slice90],\r\n",
        "       train_regret_winner_7[slice90],\r\n",
        "       train_regret_winner_8[slice90],\r\n",
        "       train_regret_winner_9[slice90],\r\n",
        "       train_regret_winner_10[slice90],\r\n",
        "       train_regret_winner_11[slice90],\r\n",
        "       train_regret_winner_12[slice90],\r\n",
        "       train_regret_winner_13[slice90],\r\n",
        "       train_regret_winner_14[slice90],\r\n",
        "       train_regret_winner_15[slice90],\r\n",
        "       train_regret_winner_16[slice90],\r\n",
        "       train_regret_winner_17[slice90],\r\n",
        "       train_regret_winner_18[slice90],\r\n",
        "       train_regret_winner_19[slice90],\r\n",
        "       train_regret_winner_20[slice90]]\r\n",
        "\r\n",
        "loser90_results = pd.DataFrame(loser90).sort_values(by=[0], ascending=False)\r\n",
        "winner90_results = pd.DataFrame(winner90).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser90 = np.asarray(loser90_results[4:5][0])[0]\r\n",
        "median_loser90 = np.asarray(loser90_results[9:10][0])[0]\r\n",
        "upper_loser90 = np.asarray(loser90_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner90 = np.asarray(winner90_results[4:5][0])[0]\r\n",
        "median_winner90 = np.asarray(winner90_results[9:10][0])[0]\r\n",
        "upper_winner90 = np.asarray(winner90_results[14:15][0])[0]"
      ],
      "execution_count": 177,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iiljCrq21p3a"
      },
      "source": [
        "# Iteration100 :\r\n",
        "\r\n",
        "slice100 = 99\r\n",
        "\r\n",
        "loser100 = [train_regret_loser_1[slice100],\r\n",
        "       train_regret_loser_2[slice100],\r\n",
        "       train_regret_loser_3[slice100],\r\n",
        "       train_regret_loser_4[slice100],\r\n",
        "       train_regret_loser_5[slice100],\r\n",
        "       train_regret_loser_6[slice100],\r\n",
        "       train_regret_loser_7[slice100],\r\n",
        "       train_regret_loser_8[slice100],\r\n",
        "       train_regret_loser_9[slice100],\r\n",
        "       train_regret_loser_10[slice100],\r\n",
        "       train_regret_loser_11[slice100],\r\n",
        "       train_regret_loser_12[slice100],\r\n",
        "       train_regret_loser_13[slice100],\r\n",
        "       train_regret_loser_14[slice100],\r\n",
        "       train_regret_loser_15[slice100],\r\n",
        "       train_regret_loser_16[slice100],\r\n",
        "       train_regret_loser_17[slice100],\r\n",
        "       train_regret_loser_18[slice100],\r\n",
        "       train_regret_loser_19[slice100],\r\n",
        "       train_regret_loser_20[slice100]]\r\n",
        "\r\n",
        "winner100 = [train_regret_winner_1[slice100],\r\n",
        "       train_regret_winner_2[slice100],\r\n",
        "       train_regret_winner_3[slice100],\r\n",
        "       train_regret_winner_4[slice100],\r\n",
        "       train_regret_winner_5[slice100],\r\n",
        "       train_regret_winner_6[slice100],\r\n",
        "       train_regret_winner_7[slice100],\r\n",
        "       train_regret_winner_8[slice100],\r\n",
        "       train_regret_winner_9[slice100],\r\n",
        "       train_regret_winner_10[slice100],\r\n",
        "       train_regret_winner_11[slice100],\r\n",
        "       train_regret_winner_12[slice100],\r\n",
        "       train_regret_winner_13[slice100],\r\n",
        "       train_regret_winner_14[slice100],\r\n",
        "       train_regret_winner_15[slice100],\r\n",
        "       train_regret_winner_16[slice100],\r\n",
        "       train_regret_winner_17[slice100],\r\n",
        "       train_regret_winner_18[slice100],\r\n",
        "       train_regret_winner_19[slice100],\r\n",
        "       train_regret_winner_20[slice100]]\r\n",
        "\r\n",
        "loser100_results = pd.DataFrame(loser100).sort_values(by=[0], ascending=False)\r\n",
        "winner100_results = pd.DataFrame(winner100).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser100 = np.asarray(loser100_results[4:5][0])[0]\r\n",
        "median_loser100 = np.asarray(loser100_results[9:10][0])[0]\r\n",
        "upper_loser100 = np.asarray(loser100_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner100 = np.asarray(winner100_results[4:5][0])[0]\r\n",
        "median_winner100 = np.asarray(winner100_results[9:10][0])[0]\r\n",
        "upper_winner100 = np.asarray(winner100_results[14:15][0])[0]"
      ],
      "execution_count": 178,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OItJgqUT2oNb"
      },
      "source": [
        "### Summarize arrays: 'Loser'\r\n",
        "\r\n",
        "lower_loser = [lower_loser1,\r\n",
        "            lower_loser2,\r\n",
        "            lower_loser3,\r\n",
        "            lower_loser4,\r\n",
        "            lower_loser5,\r\n",
        "            lower_loser6,\r\n",
        "            lower_loser7,\r\n",
        "            lower_loser8,\r\n",
        "            lower_loser9,\r\n",
        "            lower_loser10,\r\n",
        "            lower_loser11,\r\n",
        "            lower_loser12,\r\n",
        "            lower_loser13,\r\n",
        "            lower_loser14,\r\n",
        "            lower_loser15,\r\n",
        "            lower_loser16,\r\n",
        "            lower_loser17,\r\n",
        "            lower_loser18,\r\n",
        "            lower_loser19,\r\n",
        "            lower_loser20,\r\n",
        "            lower_loser21,\r\n",
        "            lower_loser22,\r\n",
        "            lower_loser23,\r\n",
        "            lower_loser24,\r\n",
        "            lower_loser25,\r\n",
        "            lower_loser26,\r\n",
        "            lower_loser27,\r\n",
        "            lower_loser28,\r\n",
        "            lower_loser29,\r\n",
        "            lower_loser30,\r\n",
        "            lower_loser31,\r\n",
        "            lower_loser32,\r\n",
        "            lower_loser33,\r\n",
        "            lower_loser34,\r\n",
        "            lower_loser35,\r\n",
        "            lower_loser36,\r\n",
        "            lower_loser37,\r\n",
        "            lower_loser38,\r\n",
        "            lower_loser39,\r\n",
        "            lower_loser40,\r\n",
        "            lower_loser41,\r\n",
        "            lower_loser42,\r\n",
        "            lower_loser43,\r\n",
        "            lower_loser44,\r\n",
        "            lower_loser45,\r\n",
        "            lower_loser46,\r\n",
        "            lower_loser47,\r\n",
        "            lower_loser48,\r\n",
        "            lower_loser49,\r\n",
        "            lower_loser50,\r\n",
        "            lower_loser51,\r\n",
        "            lower_loser52,\r\n",
        "            lower_loser53,\r\n",
        "            lower_loser54,\r\n",
        "            lower_loser55,\r\n",
        "            lower_loser56,\r\n",
        "            lower_loser57,\r\n",
        "            lower_loser58,\r\n",
        "            lower_loser59,\r\n",
        "            lower_loser60,\r\n",
        "            lower_loser61,\r\n",
        "            lower_loser62,\r\n",
        "            lower_loser63,\r\n",
        "            lower_loser64,\r\n",
        "            lower_loser65,\r\n",
        "            lower_loser66,\r\n",
        "            lower_loser67,\r\n",
        "            lower_loser68,\r\n",
        "            lower_loser69,\r\n",
        "            lower_loser70,\r\n",
        "            lower_loser71,\r\n",
        "            lower_loser72,\r\n",
        "            lower_loser73,\r\n",
        "            lower_loser74,\r\n",
        "            lower_loser75,\r\n",
        "            lower_loser76,\r\n",
        "            lower_loser77,\r\n",
        "            lower_loser78,\r\n",
        "            lower_loser79,\r\n",
        "            lower_loser80,\r\n",
        "            lower_loser81,\r\n",
        "            lower_loser82,\r\n",
        "            lower_loser83,\r\n",
        "            lower_loser84,\r\n",
        "            lower_loser85,\r\n",
        "            lower_loser86,\r\n",
        "            lower_loser87,\r\n",
        "            lower_loser88,\r\n",
        "            lower_loser89,\r\n",
        "            lower_loser90,\r\n",
        "            lower_loser91,\r\n",
        "            lower_loser92,\r\n",
        "            lower_loser93,\r\n",
        "            lower_loser94,\r\n",
        "            lower_loser95,\r\n",
        "            lower_loser96,\r\n",
        "            lower_loser97,\r\n",
        "            lower_loser98,\r\n",
        "            lower_loser99,\r\n",
        "            lower_loser100,\r\n",
        "            lower_loser101]\r\n",
        "\r\n",
        "median_loser = [median_loser1,\r\n",
        "            median_loser2,\r\n",
        "            median_loser3,\r\n",
        "            median_loser4,\r\n",
        "            median_loser5,\r\n",
        "            median_loser6,\r\n",
        "            median_loser7,\r\n",
        "            median_loser8,\r\n",
        "            median_loser9,\r\n",
        "            median_loser10,\r\n",
        "            median_loser11,\r\n",
        "            median_loser12,\r\n",
        "            median_loser13,\r\n",
        "            median_loser14,\r\n",
        "            median_loser15,\r\n",
        "            median_loser16,\r\n",
        "            median_loser17,\r\n",
        "            median_loser18,\r\n",
        "            median_loser19,\r\n",
        "            median_loser20,\r\n",
        "            median_loser21,\r\n",
        "            median_loser22,\r\n",
        "            median_loser23,\r\n",
        "            median_loser24,\r\n",
        "            median_loser25,\r\n",
        "            median_loser26,\r\n",
        "            median_loser27,\r\n",
        "            median_loser28,\r\n",
        "            median_loser29,\r\n",
        "            median_loser30,\r\n",
        "            median_loser31,\r\n",
        "            median_loser32,\r\n",
        "            median_loser33,\r\n",
        "            median_loser34,\r\n",
        "            median_loser35,\r\n",
        "            median_loser36,\r\n",
        "            median_loser37,\r\n",
        "            median_loser38,\r\n",
        "            median_loser39,\r\n",
        "            median_loser40,\r\n",
        "            median_loser41,\r\n",
        "            median_loser42,\r\n",
        "            median_loser43,\r\n",
        "            median_loser44,\r\n",
        "            median_loser45,\r\n",
        "            median_loser46,\r\n",
        "            median_loser47,\r\n",
        "            median_loser48,\r\n",
        "            median_loser49,\r\n",
        "            median_loser50,\r\n",
        "            median_loser51,\r\n",
        "            median_loser52,\r\n",
        "            median_loser53,\r\n",
        "            median_loser54,\r\n",
        "            median_loser55,\r\n",
        "            median_loser56,\r\n",
        "            median_loser57,\r\n",
        "            median_loser58,\r\n",
        "            median_loser59,\r\n",
        "            median_loser60,\r\n",
        "            median_loser61,\r\n",
        "            median_loser62,\r\n",
        "            median_loser63,\r\n",
        "            median_loser64,\r\n",
        "            median_loser65,\r\n",
        "            median_loser66,\r\n",
        "            median_loser67,\r\n",
        "            median_loser68,\r\n",
        "            median_loser69,\r\n",
        "            median_loser70,\r\n",
        "            median_loser71,\r\n",
        "            median_loser72,\r\n",
        "            median_loser73,\r\n",
        "            median_loser74,\r\n",
        "            median_loser75,\r\n",
        "            median_loser76,\r\n",
        "            median_loser77,\r\n",
        "            median_loser78,\r\n",
        "            median_loser79,\r\n",
        "            median_loser80,\r\n",
        "            median_loser81,\r\n",
        "            median_loser82,\r\n",
        "            median_loser83,\r\n",
        "            median_loser84,\r\n",
        "            median_loser85,\r\n",
        "            median_loser86,\r\n",
        "            median_loser87,\r\n",
        "            median_loser88,\r\n",
        "            median_loser89,\r\n",
        "            median_loser90,\r\n",
        "            median_loser91,\r\n",
        "            median_loser92,\r\n",
        "            median_loser93,\r\n",
        "            median_loser94,\r\n",
        "            median_loser95,\r\n",
        "            median_loser96,\r\n",
        "            median_loser97,\r\n",
        "            median_loser98,\r\n",
        "            median_loser99,\r\n",
        "            median_loser100,\r\n",
        "            median_loser101]\r\n",
        "\r\n",
        "upper_loser = [upper_loser1,\r\n",
        "            upper_loser2,\r\n",
        "            upper_loser3,\r\n",
        "            upper_loser4,\r\n",
        "            upper_loser5,\r\n",
        "            upper_loser6,\r\n",
        "            upper_loser7,\r\n",
        "            upper_loser8,\r\n",
        "            upper_loser9,\r\n",
        "            upper_loser10,\r\n",
        "            upper_loser11,\r\n",
        "            upper_loser12,\r\n",
        "            upper_loser13,\r\n",
        "            upper_loser14,\r\n",
        "            upper_loser15,\r\n",
        "            upper_loser16,\r\n",
        "            upper_loser17,\r\n",
        "            upper_loser18,\r\n",
        "            upper_loser19,\r\n",
        "            upper_loser20,\r\n",
        "            upper_loser21,\r\n",
        "            upper_loser22,\r\n",
        "            upper_loser23,\r\n",
        "            upper_loser24,\r\n",
        "            upper_loser25,\r\n",
        "            upper_loser26,\r\n",
        "            upper_loser27,\r\n",
        "            upper_loser28,\r\n",
        "            upper_loser29,\r\n",
        "            upper_loser30,\r\n",
        "            upper_loser31,\r\n",
        "            upper_loser32,\r\n",
        "            upper_loser33,\r\n",
        "            upper_loser34,\r\n",
        "            upper_loser35,\r\n",
        "            upper_loser36,\r\n",
        "            upper_loser37,\r\n",
        "            upper_loser38,\r\n",
        "            upper_loser39,\r\n",
        "            upper_loser40,\r\n",
        "            upper_loser41,\r\n",
        "            upper_loser42,\r\n",
        "            upper_loser43,\r\n",
        "            upper_loser44,\r\n",
        "            upper_loser45,\r\n",
        "            upper_loser46,\r\n",
        "            upper_loser47,\r\n",
        "            upper_loser48,\r\n",
        "            upper_loser49,\r\n",
        "            upper_loser50,\r\n",
        "            upper_loser51,\r\n",
        "            upper_loser52,\r\n",
        "            upper_loser53,\r\n",
        "            upper_loser54,\r\n",
        "            upper_loser55,\r\n",
        "            upper_loser56,\r\n",
        "            upper_loser57,\r\n",
        "            upper_loser58,\r\n",
        "            upper_loser59,\r\n",
        "            upper_loser60,\r\n",
        "            upper_loser61,\r\n",
        "            upper_loser62,\r\n",
        "            upper_loser63,\r\n",
        "            upper_loser64,\r\n",
        "            upper_loser65,\r\n",
        "            upper_loser66,\r\n",
        "            upper_loser67,\r\n",
        "            upper_loser68,\r\n",
        "            upper_loser69,\r\n",
        "            upper_loser70,\r\n",
        "            upper_loser71,\r\n",
        "            upper_loser72,\r\n",
        "            upper_loser73,\r\n",
        "            upper_loser74,\r\n",
        "            upper_loser75,\r\n",
        "            upper_loser76,\r\n",
        "            upper_loser77,\r\n",
        "            upper_loser78,\r\n",
        "            upper_loser79,\r\n",
        "            upper_loser80,\r\n",
        "            upper_loser81,\r\n",
        "            upper_loser82,\r\n",
        "            upper_loser83,\r\n",
        "            upper_loser84,\r\n",
        "            upper_loser85,\r\n",
        "            upper_loser86,\r\n",
        "            upper_loser87,\r\n",
        "            upper_loser88,\r\n",
        "            upper_loser89,\r\n",
        "            upper_loser90,\r\n",
        "            upper_loser91,\r\n",
        "            upper_loser92,\r\n",
        "            upper_loser93,\r\n",
        "            upper_loser94,\r\n",
        "            upper_loser95,\r\n",
        "            upper_loser96,\r\n",
        "            upper_loser97,\r\n",
        "            upper_loser98,\r\n",
        "            upper_loser99,\r\n",
        "            upper_loser100,\r\n",
        "            upper_loser101]"
      ],
      "execution_count": 179,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u19FndLT2oU3"
      },
      "source": [
        "### Summarize arrays: 'Winner'\r\n",
        "\r\n",
        "lower_winner = [lower_winner1,\r\n",
        "            lower_winner2,\r\n",
        "            lower_winner3,\r\n",
        "            lower_winner4,\r\n",
        "            lower_winner5,\r\n",
        "            lower_winner6,\r\n",
        "            lower_winner7,\r\n",
        "            lower_winner8,\r\n",
        "            lower_winner9,\r\n",
        "            lower_winner10,\r\n",
        "            lower_winner11,\r\n",
        "            lower_winner12,\r\n",
        "            lower_winner13,\r\n",
        "            lower_winner14,\r\n",
        "            lower_winner15,\r\n",
        "            lower_winner16,\r\n",
        "            lower_winner17,\r\n",
        "            lower_winner18,\r\n",
        "            lower_winner19,\r\n",
        "            lower_winner20,\r\n",
        "            lower_winner21,\r\n",
        "            lower_winner22,\r\n",
        "            lower_winner23,\r\n",
        "            lower_winner24,\r\n",
        "            lower_winner25,\r\n",
        "            lower_winner26,\r\n",
        "            lower_winner27,\r\n",
        "            lower_winner28,\r\n",
        "            lower_winner29,\r\n",
        "            lower_winner30,\r\n",
        "            lower_winner31,\r\n",
        "            lower_winner32,\r\n",
        "            lower_winner33,\r\n",
        "            lower_winner34,\r\n",
        "            lower_winner35,\r\n",
        "            lower_winner36,\r\n",
        "            lower_winner37,\r\n",
        "            lower_winner38,\r\n",
        "            lower_winner39,\r\n",
        "            lower_winner40,\r\n",
        "            lower_winner41,\r\n",
        "            lower_winner42,\r\n",
        "            lower_winner43,\r\n",
        "            lower_winner44,\r\n",
        "            lower_winner45,\r\n",
        "            lower_winner46,\r\n",
        "            lower_winner47,\r\n",
        "            lower_winner48,\r\n",
        "            lower_winner49,\r\n",
        "            lower_winner50,\r\n",
        "            lower_winner51,\r\n",
        "            lower_winner52,\r\n",
        "            lower_winner53,\r\n",
        "            lower_winner54,\r\n",
        "            lower_winner55,\r\n",
        "            lower_winner56,\r\n",
        "            lower_winner57,\r\n",
        "            lower_winner58,\r\n",
        "            lower_winner59,\r\n",
        "            lower_winner60,\r\n",
        "            lower_winner61,\r\n",
        "            lower_winner62,\r\n",
        "            lower_winner63,\r\n",
        "            lower_winner64,\r\n",
        "            lower_winner65,\r\n",
        "            lower_winner66,\r\n",
        "            lower_winner67,\r\n",
        "            lower_winner68,\r\n",
        "            lower_winner69,\r\n",
        "            lower_winner70,\r\n",
        "            lower_winner71,\r\n",
        "            lower_winner72,\r\n",
        "            lower_winner73,\r\n",
        "            lower_winner74,\r\n",
        "            lower_winner75,\r\n",
        "            lower_winner76,\r\n",
        "            lower_winner77,\r\n",
        "            lower_winner78,\r\n",
        "            lower_winner79,\r\n",
        "            lower_winner80,\r\n",
        "            lower_winner81,\r\n",
        "            lower_winner82,\r\n",
        "            lower_winner83,\r\n",
        "            lower_winner84,\r\n",
        "            lower_winner85,\r\n",
        "            lower_winner86,\r\n",
        "            lower_winner87,\r\n",
        "            lower_winner88,\r\n",
        "            lower_winner89,\r\n",
        "            lower_winner90,\r\n",
        "            lower_winner91,\r\n",
        "            lower_winner92,\r\n",
        "            lower_winner93,\r\n",
        "            lower_winner94,\r\n",
        "            lower_winner95,\r\n",
        "            lower_winner96,\r\n",
        "            lower_winner97,\r\n",
        "            lower_winner98,\r\n",
        "            lower_winner99,\r\n",
        "            lower_winner100,\r\n",
        "            lower_winner101]\r\n",
        "\r\n",
        "median_winner = [median_winner1,\r\n",
        "            median_winner2,\r\n",
        "            median_winner3,\r\n",
        "            median_winner4,\r\n",
        "            median_winner5,\r\n",
        "            median_winner6,\r\n",
        "            median_winner7,\r\n",
        "            median_winner8,\r\n",
        "            median_winner9,\r\n",
        "            median_winner10,\r\n",
        "            median_winner11,\r\n",
        "            median_winner12,\r\n",
        "            median_winner13,\r\n",
        "            median_winner14,\r\n",
        "            median_winner15,\r\n",
        "            median_winner16,\r\n",
        "            median_winner17,\r\n",
        "            median_winner18,\r\n",
        "            median_winner19,\r\n",
        "            median_winner20,\r\n",
        "            median_winner21,\r\n",
        "            median_winner22,\r\n",
        "            median_winner23,\r\n",
        "            median_winner24,\r\n",
        "            median_winner25,\r\n",
        "            median_winner26,\r\n",
        "            median_winner27,\r\n",
        "            median_winner28,\r\n",
        "            median_winner29,\r\n",
        "            median_winner30,\r\n",
        "            median_winner31,\r\n",
        "            median_winner32,\r\n",
        "            median_winner33,\r\n",
        "            median_winner34,\r\n",
        "            median_winner35,\r\n",
        "            median_winner36,\r\n",
        "            median_winner37,\r\n",
        "            median_winner38,\r\n",
        "            median_winner39,\r\n",
        "            median_winner40,\r\n",
        "            median_winner41,\r\n",
        "            median_winner42,\r\n",
        "            median_winner43,\r\n",
        "            median_winner44,\r\n",
        "            median_winner45,\r\n",
        "            median_winner46,\r\n",
        "            median_winner47,\r\n",
        "            median_winner48,\r\n",
        "            median_winner49,\r\n",
        "            median_winner50,\r\n",
        "            median_winner51,\r\n",
        "            median_winner52,\r\n",
        "            median_winner53,\r\n",
        "            median_winner54,\r\n",
        "            median_winner55,\r\n",
        "            median_winner56,\r\n",
        "            median_winner57,\r\n",
        "            median_winner58,\r\n",
        "            median_winner59,\r\n",
        "            median_winner60,\r\n",
        "            median_winner61,\r\n",
        "            median_winner62,\r\n",
        "            median_winner63,\r\n",
        "            median_winner64,\r\n",
        "            median_winner65,\r\n",
        "            median_winner66,\r\n",
        "            median_winner67,\r\n",
        "            median_winner68,\r\n",
        "            median_winner69,\r\n",
        "            median_winner70,\r\n",
        "            median_winner71,\r\n",
        "            median_winner72,\r\n",
        "            median_winner73,\r\n",
        "            median_winner74,\r\n",
        "            median_winner75,\r\n",
        "            median_winner76,\r\n",
        "            median_winner77,\r\n",
        "            median_winner78,\r\n",
        "            median_winner79,\r\n",
        "            median_winner80,\r\n",
        "            median_winner81,\r\n",
        "            median_winner82,\r\n",
        "            median_winner83,\r\n",
        "            median_winner84,\r\n",
        "            median_winner85,\r\n",
        "            median_winner86,\r\n",
        "            median_winner87,\r\n",
        "            median_winner88,\r\n",
        "            median_winner89,\r\n",
        "            median_winner90,\r\n",
        "            median_winner91,\r\n",
        "            median_winner92,\r\n",
        "            median_winner93,\r\n",
        "            median_winner94,\r\n",
        "            median_winner95,\r\n",
        "            median_winner96,\r\n",
        "            median_winner97,\r\n",
        "            median_winner98,\r\n",
        "            median_winner99,\r\n",
        "            median_winner100,\r\n",
        "            median_winner101]\r\n",
        "\r\n",
        "upper_winner = [upper_winner1,\r\n",
        "            upper_winner2,\r\n",
        "            upper_winner3,\r\n",
        "            upper_winner4,\r\n",
        "            upper_winner5,\r\n",
        "            upper_winner6,\r\n",
        "            upper_winner7,\r\n",
        "            upper_winner8,\r\n",
        "            upper_winner9,\r\n",
        "            upper_winner10,\r\n",
        "            upper_winner11,\r\n",
        "            upper_winner12,\r\n",
        "            upper_winner13,\r\n",
        "            upper_winner14,\r\n",
        "            upper_winner15,\r\n",
        "            upper_winner16,\r\n",
        "            upper_winner17,\r\n",
        "            upper_winner18,\r\n",
        "            upper_winner19,\r\n",
        "            upper_winner20,\r\n",
        "            upper_winner21,\r\n",
        "            upper_winner22,\r\n",
        "            upper_winner23,\r\n",
        "            upper_winner24,\r\n",
        "            upper_winner25,\r\n",
        "            upper_winner26,\r\n",
        "            upper_winner27,\r\n",
        "            upper_winner28,\r\n",
        "            upper_winner29,\r\n",
        "            upper_winner30,\r\n",
        "            upper_winner31,\r\n",
        "            upper_winner32,\r\n",
        "            upper_winner33,\r\n",
        "            upper_winner34,\r\n",
        "            upper_winner35,\r\n",
        "            upper_winner36,\r\n",
        "            upper_winner37,\r\n",
        "            upper_winner38,\r\n",
        "            upper_winner39,\r\n",
        "            upper_winner40,\r\n",
        "            upper_winner41,\r\n",
        "            upper_winner42,\r\n",
        "            upper_winner43,\r\n",
        "            upper_winner44,\r\n",
        "            upper_winner45,\r\n",
        "            upper_winner46,\r\n",
        "            upper_winner47,\r\n",
        "            upper_winner48,\r\n",
        "            upper_winner49,\r\n",
        "            upper_winner50,\r\n",
        "            upper_winner51,\r\n",
        "            upper_winner52,\r\n",
        "            upper_winner53,\r\n",
        "            upper_winner54,\r\n",
        "            upper_winner55,\r\n",
        "            upper_winner56,\r\n",
        "            upper_winner57,\r\n",
        "            upper_winner58,\r\n",
        "            upper_winner59,\r\n",
        "            upper_winner60,\r\n",
        "            upper_winner61,\r\n",
        "            upper_winner62,\r\n",
        "            upper_winner63,\r\n",
        "            upper_winner64,\r\n",
        "            upper_winner65,\r\n",
        "            upper_winner66,\r\n",
        "            upper_winner67,\r\n",
        "            upper_winner68,\r\n",
        "            upper_winner69,\r\n",
        "            upper_winner70,\r\n",
        "            upper_winner71,\r\n",
        "            upper_winner72,\r\n",
        "            upper_winner73,\r\n",
        "            upper_winner74,\r\n",
        "            upper_winner75,\r\n",
        "            upper_winner76,\r\n",
        "            upper_winner77,\r\n",
        "            upper_winner78,\r\n",
        "            upper_winner79,\r\n",
        "            upper_winner80,\r\n",
        "            upper_winner81,\r\n",
        "            upper_winner82,\r\n",
        "            upper_winner83,\r\n",
        "            upper_winner84,\r\n",
        "            upper_winner85,\r\n",
        "            upper_winner86,\r\n",
        "            upper_winner87,\r\n",
        "            upper_winner88,\r\n",
        "            upper_winner89,\r\n",
        "            upper_winner90,\r\n",
        "            upper_winner91,\r\n",
        "            upper_winner92,\r\n",
        "            upper_winner93,\r\n",
        "            upper_winner94,\r\n",
        "            upper_winner95,\r\n",
        "            upper_winner96,\r\n",
        "            upper_winner97,\r\n",
        "            upper_winner98,\r\n",
        "            upper_winner99,\r\n",
        "            upper_winner100,\r\n",
        "            upper_winner101]"
      ],
      "execution_count": 180,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84XIzmWD2oba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "outputId": "61aebd6e-aaf4-46ce-b8ae-2de4e0ce3208"
      },
      "source": [
        "### Visualize!\r\n",
        "\r\n",
        "title = obj_func\r\n",
        "\r\n",
        "plt.figure()\r\n",
        "\r\n",
        "plt.plot(median_loser, color = 'Green')\r\n",
        "plt.plot(median_winner, color = 'Purple')\r\n",
        "\r\n",
        "xstar = np.arange(0, max_iter+1, step=1)\r\n",
        "plt.fill_between(xstar, lower_loser, upper_loser, facecolor = 'Green', alpha=0.4, label='GP EI Regret IQR: dEI')\r\n",
        "plt.fill_between(xstar, lower_winner, upper_winner, facecolor = 'Purple', alpha=0.4, label='STP CBM Regret IQR: dCBM')\r\n",
        "\r\n",
        "plt.title(title, weight = 'bold', family = 'Arial')\r\n",
        "plt.xlabel('(Post-initialization) iteration $\\it{k}$', weight = 'bold', family = 'Arial') \r\n",
        "plt.ylabel('log(Regret)', weight = 'bold', family = 'Arial') \r\n",
        "plt.legend(loc=1) # add plot legend\r\n",
        "\r\n",
        "plt.show()"
      ],
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "findfont: Font family ['Arial'] not found. Falling back to DejaVu Sans.\n",
            "findfont: Font family ['Arial'] not found. Falling back to DejaVu Sans.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5dkR2Id2oiu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be1eb603-c3b9-4057-cbbb-a69a903095e6"
      },
      "source": [
        "time_lose, time_win"
      ],
      "execution_count": 182,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3589.180408716202, 16631.375532865524)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 182
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNLBF57p2opi"
      },
      "source": [
        ""
      ],
      "execution_count": 182,
      "outputs": []
    }
  ]
}