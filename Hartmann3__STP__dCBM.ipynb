{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Hartmann3__STP__dCBM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RaQo_XsusPbm"
      },
      "source": [
        "Hartmann3 synthetic function:\r\n",
        "\r\n",
        "GP EI: (exact GP EI gradients) vs. STP CBM: (exact STP CBM gradients)\r\n",
        "\r\n",
        "https://www.sfu.ca/~ssurjano/hart3.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bhgucjJgsYfm",
        "outputId": "c537a0b2-3491-4fdb-eff1-51f55f0a31c8"
      },
      "source": [
        "!pip install pyGPGO"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyGPGO in /usr/local/lib/python3.6/dist-packages (0.4.0.dev1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from pyGPGO) (1.4.1)\n",
            "Requirement already satisfied: pyMC3 in /usr/local/lib/python3.6/dist-packages (from pyGPGO) (3.7)\n",
            "Requirement already satisfied: theano in /usr/local/lib/python3.6/dist-packages (from pyGPGO) (1.0.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pyGPGO) (1.19.5)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from pyGPGO) (1.0.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from pyGPGO) (0.22.2.post1)\n",
            "Requirement already satisfied: patsy>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from pyMC3->pyGPGO) (0.5.1)\n",
            "Requirement already satisfied: pandas>=0.18.0 in /usr/local/lib/python3.6/dist-packages (from pyMC3->pyGPGO) (1.1.5)\n",
            "Requirement already satisfied: h5py>=2.7.0 in /usr/local/lib/python3.6/dist-packages (from pyMC3->pyGPGO) (2.10.0)\n",
            "Requirement already satisfied: tqdm>=4.8.4 in /usr/local/lib/python3.6/dist-packages (from pyMC3->pyGPGO) (4.41.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from theano->pyGPGO) (1.15.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.18.0->pyMC3->pyGPGO) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.18.0->pyMC3->pyGPGO) (2018.9)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E99qMZverfrP"
      },
      "source": [
        "### Import modules:\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "import scipy as sp\r\n",
        "import pandas as pd\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import warnings\r\n",
        "import time\r\n",
        "\r\n",
        "from pyGPGO.logger import EventLogger\r\n",
        "from pyGPGO.GPGO import GPGO\r\n",
        "from pyGPGO.surrogates.GaussianProcess import GaussianProcess\r\n",
        "from pyGPGO.surrogates.tStudentProcess import tStudentProcess\r\n",
        "from pyGPGO.acquisition import Acquisition\r\n",
        "from pyGPGO.covfunc import squaredExponential\r\n",
        "\r\n",
        "from collections import OrderedDict\r\n",
        "from joblib import Parallel, delayed\r\n",
        "from numpy.linalg import slogdet, inv, cholesky, solve\r\n",
        "from scipy.optimize import minimize\r\n",
        "from scipy.spatial.distance import cdist\r\n",
        "from scipy.special import gamma\r\n",
        "from scipy.stats import norm, t\r\n",
        "from matplotlib.pyplot import rc\r\n",
        "from mpl_toolkits.axes_grid1.inset_locator import inset_axes, mark_inset\r\n",
        "\r\n",
        "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xiJSpz2P9qXK"
      },
      "source": [
        "n_start_AcqFunc = 250 #multi-start iterations to avoid local optima in AcqFunc optimization"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzPUbCs3s9nu"
      },
      "source": [
        "### Inputs:\r\n",
        "\r\n",
        "obj_func = 'Hartmann3'\r\n",
        "n_test = n_start_AcqFunc # test points\r\n",
        "df = 3 # nu\r\n",
        "\r\n",
        "util_loser = 'dEI_GP'\r\n",
        "util_winner = 'dCBM_STP'\r\n",
        "\r\n",
        "v=1 \r\n",
        "delta=.1\r\n",
        "\r\n",
        "n_init = 5 # random initialisations"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-AMqjmpbtAb1"
      },
      "source": [
        "### Objective function:\r\n",
        "\r\n",
        "if obj_func == 'Hartmann3': # 3-D\r\n",
        "            \r\n",
        "    # True y bounds:\r\n",
        "    y_lb = -3.86278\r\n",
        "    operator = -1 # targets global minimum \r\n",
        "    y_global_orig = y_lb * operator # targets global minimum\r\n",
        "            \r\n",
        "    # Constraints:\r\n",
        "    lb = 0\r\n",
        "    ub = 1\r\n",
        "    \r\n",
        "    # Input array dimension(s):\r\n",
        "    dim = 3\r\n",
        "\r\n",
        "    # 3-D inputs' parameter bounds:\r\n",
        "    param = {'x1_training': ('cont', [lb, ub]),\r\n",
        "             'x2_training': ('cont', [lb, ub]),\r\n",
        "             'x3_training': ('cont', [lb, ub])}\r\n",
        "    \r\n",
        "    max_iter = (10 * dim)*0 + 100  # iterations of Bayesian optimisation\r\n",
        "    \r\n",
        "    # Test data:\r\n",
        "    x1_test = np.linspace(lb, ub, n_test) \r\n",
        "    x2_test = np.linspace(lb, ub, n_test)\r\n",
        "    x3_test = np.linspace(lb, ub, n_test)\r\n",
        "    Xstar_d = np.column_stack((x1_test, x2_test, x3_test))\r\n",
        "    \r\n",
        "    def f_syn_polarity(x1_training, x2_training, x3_training):\r\n",
        "       \r\n",
        "        value = np.array([x1_training, x2_training, x3_training])\r\n",
        "      \r\n",
        "        a = np.array([[3.0, 10, 30],\r\n",
        "                      [0.1, 10, 35],\r\n",
        "                      [3.0, 10, 30],\r\n",
        "                      [0.1, 10, 35]])\r\n",
        "        \r\n",
        "        alpha = np.array([1.0, 1.2, 3.0, 3.2])\r\n",
        "      \r\n",
        "        p = np.array([[.3689, .1170, .2673],\r\n",
        "                      [.4699, .4387, .7470],\r\n",
        "                      [.1091, .8732, .5547],\r\n",
        "                      [.3810, .5743, .8828]])\r\n",
        "  \r\n",
        "        s = 0\r\n",
        "        for i in [0,1,2,3]:\r\n",
        "            sm = a[i,0]*(value[0]-p[i,0])**2\r\n",
        "            sm += a[i,1]*(value[1]-p[i,1])**2\r\n",
        "            sm += a[i,2]*(value[2]-p[i,2])**2\r\n",
        "            s += alpha[i]*np.exp(-sm)\r\n",
        "        result = -s\r\n",
        "        \r\n",
        "        return operator * result"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5GlOrB5CyJkY"
      },
      "source": [
        "Beta_CBM = v * (2 * np.log((max_iter**(dim/2 + 2) * (np.pi**2))/(3 * delta)))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyQX-59DtC7-"
      },
      "source": [
        "### Cumulative Regret Calculator:\r\n",
        "\r\n",
        "def min_max_array(x):\r\n",
        "    new_list = []\r\n",
        "    for i, num in enumerate(x):\r\n",
        "            new_list.append(np.min(x[0:i+1]))\r\n",
        "    return new_list"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dE_I_YSntHxl"
      },
      "source": [
        "### Set-seeds:\r\n",
        "\r\n",
        "run_num_1 = 111\r\n",
        "run_num_2 = 113\r\n",
        "run_num_3 = 3333\r\n",
        "run_num_4 = 444\r\n",
        "run_num_5 = 5555\r\n",
        "run_num_6 = 6\r\n",
        "run_num_7 = 777\r\n",
        "run_num_8 = 887\r\n",
        "run_num_9 = 99\r\n",
        "run_num_10 = 1000\r\n",
        "run_num_11 = 1113\r\n",
        "run_num_12 = 1234\r\n",
        "run_num_13 = 2345\r\n",
        "run_num_14 = 88\r\n",
        "run_num_15 = 1556\r\n",
        "run_num_16 = 1666\r\n",
        "run_num_17 = 717\r\n",
        "run_num_18 = 8\r\n",
        "run_num_19 = 1998\r\n",
        "run_num_20 = 2000"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oe29v5N5tKjF"
      },
      "source": [
        "### Derivatives - Squared-exponential covariance function:\r\n",
        "\r\n",
        "def l2norm_(X, Xstar):\r\n",
        "    \r\n",
        "    return cdist(X, Xstar)\r\n",
        "\r\n",
        "def kronDelta(X, Xstar):\r\n",
        "\r\n",
        "    return cdist(X, Xstar) < np.finfo(np.float32).eps\r\n",
        "\r\n",
        "class squaredExponentialDeriv(squaredExponential):\r\n",
        "    \r\n",
        "    def K(self, X, Xstar):\r\n",
        "        \r\n",
        "        r = (l2norm_(X, Xstar)/self.l)\r\n",
        "        K = self.sigmaf * np.exp(-1/2*r **2) + self.sigman * kronDelta(X, Xstar)\r\n",
        "        return K\r\n",
        "    \r\n",
        "    def dK(self, X, Xstar):\r\n",
        "        \r\n",
        "        r = (l2norm_(X, Xstar)/self.l)\r\n",
        "        dK = self.sigmaf/self.l**2 * np.exp(-1/2 * r **2) * l2norm_(X, Xstar)\r\n",
        "        return dK\r\n",
        "    \r\n",
        "        \r\n",
        "    def d2K(self, X, Xstar):\r\n",
        "        \r\n",
        "        r = (l2norm_(X, Xstar)/self.l)\r\n",
        "        d2K = self.sigmaf/self.l**2 * np.exp(-1/2 * r **2) * (r**2-1)\r\n",
        "        return d2K\r\n",
        "    \r\n",
        "cov_func = squaredExponential()\r\n",
        "d_cov_func = squaredExponentialDeriv()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEBX5l7TtNSN"
      },
      "source": [
        "class Acquisition_new(Acquisition):    \r\n",
        "    def __init__(self, mode, eps=1e-08, **params):\r\n",
        "        \r\n",
        "        self.params = params\r\n",
        "        self.eps = eps\r\n",
        "\r\n",
        "        mode_dict = {\r\n",
        "            'dEI_GP': self.dEI_GP,\r\n",
        "            'dCBM_STP': self.dCBM_STP\r\n",
        "        }\r\n",
        "\r\n",
        "        self.f = mode_dict[mode]\r\n",
        "    \r\n",
        "    def dEI_GP(self, tau, mean, std, ds, dm, dvdv, d2v, d2m):\r\n",
        "        z = -1 * (tau - mean - self.eps) / (std + self.eps)\r\n",
        "        \r\n",
        "        dsdx = ds / 2 * (std + self.eps)\r\n",
        "        d2sdx = -dsdx**2 / ((std + self.eps)) - dvdv / (std + self.eps) - d2v / (std + self.eps)\r\n",
        "        dmdx = (dm - z * dsdx) / (std + self.eps)\r\n",
        "        d2mdx = (d2m - (z * d2sdx + 2 * dmdx * dsdx)) / (std + self.eps)\r\n",
        "        \r\n",
        "        f = (std + self.eps) * (z * norm.cdf(z) + norm.pdf(z)[0])\r\n",
        "        df = (f / (std + self.eps) * dsdx + (std + self.eps) * norm.cdf(z) * dmdx)\r\n",
        "        d2f = (f / (std + self.eps) * d2sdx + dsdx * dmdx * norm.cdf(z) \\\r\n",
        "            + d2mdx * (std + self.eps) * norm.cdf(z) + dsdx * norm.cdf(z) * dmdx \\\r\n",
        "            + norm.pdf(z)[0] * (std + self.eps) * dmdx)\r\n",
        "            \r\n",
        "        return f, df, d2f\r\n",
        "\r\n",
        "    def dCBM_STP(self, tau, mean, std, ds, dm, dvdv, d2v, d2m, nu=3.0):\r\n",
        "    \r\n",
        "        gamma = -1 * (y_global_orig - mean - self.eps) / (std + self.eps)\r\n",
        "        dsdx = ds / 2 * (std + self.eps)\r\n",
        "        dmdx = (dm - gamma * dsdx) / (std + self.eps)\r\n",
        "\r\n",
        "        f = (std + self.eps) * (gamma + np.sqrt(Beta_CBM))\r\n",
        "        df = dsdx * (gamma + np.sqrt(Beta_CBM)) + (std + self.eps) * (dmdx + np.sqrt(Beta_CBM))\r\n",
        "        return f, df\r\n",
        "    \r\n",
        "    def _eval(self, tau, mean, std):\r\n",
        "    \r\n",
        "        return self.f(tau, mean, std, **self.params)\r\n",
        "    \r\n",
        "    def d_eval(self, tau, mean, std, ds, dm, dvdv, d2v, d2m):\r\n",
        "    \r\n",
        "        return self.f(tau, mean, std, ds, dm, dvdv, d2v, d2m, **self.params)\r\n",
        "\r\n",
        "    def d_eval_stp(self, tau, mean, std, ds, dm, dvdv, d2v, d2m, nu=3.0):\r\n",
        "    \r\n",
        "        return self.f(tau, mean, std, ds, dm, dvdv, d2v, d2m, nu=3.0, **self.params)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SdY8rHwtRJ_"
      },
      "source": [
        "### Surrogate derivatives: \r\n",
        "\r\n",
        "from scipy.linalg import cholesky, solve\r\n",
        "\r\n",
        "class dGaussianProcess(GaussianProcess):\r\n",
        "    l = 1\r\n",
        "    sigmaf = 1e-4\r\n",
        "    sigman = 1e-6\r\n",
        "\r\n",
        "    def AcqGrad(self, Xstar, return_std=False):\r\n",
        "        r_X = l2norm_(self.X, self.X)/self.l\r\n",
        "        K = self.sigmaf * np.exp(-1/2*r_X **2) + self.sigman * kronDelta(self.X, self.X)\r\n",
        "        L = cholesky(K).T\r\n",
        "        alpha = solve(L.T, solve(L, self.y))\r\n",
        "        Xstar = np.atleast_2d(Xstar)\r\n",
        "        Kstar = squaredExponentialDeriv.K(self, self.X, Xstar).T\r\n",
        "        dKstar = squaredExponentialDeriv.dK(self, self.X, Xstar).T\r\n",
        "        d2Kstar = squaredExponentialDeriv.d2K(self, self.X, Xstar).T\r\n",
        "        v = solve(self.L, Kstar.T)\r\n",
        "        dv = solve(self.L, dKstar.T)\r\n",
        "        d2v = solve(self.L, d2Kstar.T)\r\n",
        "        \r\n",
        "        ds = -2 * np.dot(dv.T, v)\r\n",
        "        dvdv = np.dot(dv.T, dv)\r\n",
        "        d2s = -2 * (dvdv + d2v)\r\n",
        "        \r\n",
        "        dm = np.dot(dKstar, alpha)\r\n",
        "        d2m = np.dot(d2Kstar, alpha)\r\n",
        "        return ds, dm, dvdv, d2v, d2m\r\n",
        "\r\n",
        "class dtStudentProcess(tStudentProcess):\r\n",
        "    l = 1\r\n",
        "    sigmaf = 1e-4\r\n",
        "    sigman = 1e-6\r\n",
        "    \r\n",
        "    def AcqGrad(self, Xstar, return_std=False):\r\n",
        "        r_X = l2norm_(self.X, self.X)/self.l\r\n",
        "        K = self.sigmaf * np.exp(-1/2*r_X **2) + self.sigman * kronDelta(self.X, self.X)\r\n",
        "        L = cholesky(self.K11).T\r\n",
        "        alpha = solve(L.T, solve(L, self.y))\r\n",
        "        Xstar = np.atleast_2d(Xstar)\r\n",
        "        Kstar = squaredExponentialDeriv.K(self, self.X, Xstar).T\r\n",
        "        dKstar = squaredExponentialDeriv.dK(self, self.X, Xstar).T\r\n",
        "        d2Kstar = squaredExponentialDeriv.d2K(self, self.X, Xstar).T\r\n",
        "        v = solve(L, Kstar.T)\r\n",
        "        dv = solve(L, dKstar.T)\r\n",
        "        d2v = solve(L, d2Kstar.T)\r\n",
        "        \r\n",
        "        ds = -2 * np.dot(dv.T, v)\r\n",
        "        dvdv = np.dot(dv.T, dv)\r\n",
        "        d2s = -2 * (dvdv + d2v)\r\n",
        "        \r\n",
        "        dm = np.dot(dKstar, alpha)\r\n",
        "        d2m = np.dot(d2Kstar, alpha)\r\n",
        "        return ds, dm, dvdv, d2v, d2m"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNv4Z0EqtUi2"
      },
      "source": [
        "class dGPGO(GPGO):  \r\n",
        "    n_start = n_start_AcqFunc\r\n",
        "    eps = 1e-08\r\n",
        "        \r\n",
        "    def func(self, xnew):\r\n",
        "        new_mean, new_var = self.GP.predict(xnew, return_std=True)\r\n",
        "        new_std = np.sqrt(new_var + 1e-6)\r\n",
        "        ds, dm, dvdv, d2v, d2m = self.GP.AcqGrad(xnew, return_std=True)\r\n",
        "        f  = np.empty((self.n_start,))\r\n",
        "        df = np.empty((self.n_start,))\r\n",
        "        f  = -self.A.d_eval(self.tau, new_mean, new_std, ds=ds, dm=dm, dvdv=dvdv, d2v=d2v, d2m=d2m)[0]\r\n",
        "        df = -self.A.d_eval(self.tau, new_mean, new_std, ds=ds, dm=dm, dvdv=dvdv, d2v=d2v, d2m=d2m)[1]\r\n",
        "        df_array = np.full((len(xnew),),df)\r\n",
        "        return f, df_array\r\n",
        "        \r\n",
        "    def d_optimizeAcq(self, method='L-BFGS-B', n_start=n_start):\r\n",
        "        start_points_dict = [self._sampleParam() for i in range(n_start)]\r\n",
        "        start_points_arr = np.array([list(s.values())\r\n",
        "                                     for s in start_points_dict])\r\n",
        "        x_best = np.empty((n_start, len(self.parameter_key)))\r\n",
        "        f_best = np.empty((n_start,))\r\n",
        "        opt = Parallel(n_jobs=self.n_jobs)(delayed(minimize)(self.func,\r\n",
        "                                                                 x0=start_point,\r\n",
        "                                                                 method=method,\r\n",
        "                                                                 jac = True,\r\n",
        "                                                                 bounds=self.parameter_range) for start_point in\r\n",
        "                                               start_points_arr)\r\n",
        "        x_best = np.array([res.x for res in opt])\r\n",
        "        f_best = np.array([np.atleast_1d(res.fun)[0] for res in opt])\r\n",
        "\r\n",
        "        self.x_best = x_best\r\n",
        "        self.f_best = f_best\r\n",
        "        self.best = x_best[np.argmin(f_best)]\r\n",
        "    \r\n",
        "    def run(self, max_iter=10, init_evals=3, resume=False):\r\n",
        "        \r\n",
        "        if not resume:\r\n",
        "            self.init_evals = init_evals\r\n",
        "            self._firstRun(self.init_evals)\r\n",
        "            self.logger._printInit(self)\r\n",
        "        for iteration in range(max_iter):\r\n",
        "            self.d_optimizeAcq()\r\n",
        "            self.updateGP()\r\n",
        "            self.logger._printCurrent(self)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7vea4uj-GOi"
      },
      "source": [
        "## dGPGO_stp - BayesOpt derivatives' class: Student's-t\r\n",
        "\r\n",
        "class dGPGO_stp(GPGO):  \r\n",
        "    n_start = 100\r\n",
        "        \r\n",
        "    def func_stp(self, xnew):\r\n",
        "        new_mean, new_var = self.GP.predict(xnew, return_std=True)\r\n",
        "        new_std = np.sqrt(new_var + 1e-6)\r\n",
        "        ds, dm, dvdv, d2v, d2m = self.GP.AcqGrad(xnew, return_std=True)\r\n",
        "        f  = np.empty((self.n_start,))\r\n",
        "        df = np.empty((self.n_start,))\r\n",
        "        f  = -self.A.d_eval_stp(self.tau, new_mean, new_std, ds=ds, dm=dm, dvdv=dvdv, d2v=d2v, d2m=d2m, nu=df)[0]\r\n",
        "        df = -self.A.d_eval_stp(self.tau, new_mean, new_std, ds=ds, dm=dm, dvdv=dvdv, d2v=d2v, d2m=d2m, nu=df)[1]\r\n",
        "        df_array = np.full((len(xnew),),df)\r\n",
        "        return f, df_array\r\n",
        "        \r\n",
        "    def d_optimizeAcq_stp(self, method='L-BFGS-B', n_start=n_start):\r\n",
        "        start_points_dict = [self._sampleParam() for i in range(n_start)]\r\n",
        "        start_points_arr = np.array([list(s.values())\r\n",
        "                                     for s in start_points_dict])\r\n",
        "        x_best = np.empty((n_start, len(self.parameter_key)))\r\n",
        "        f_best = np.empty((n_start,))\r\n",
        "        opt = Parallel(n_jobs=self.n_jobs)(delayed(minimize)(self.func_stp,\r\n",
        "                                                                 x0=start_point,\r\n",
        "                                                                 method=method,\r\n",
        "                                                                 jac = True,\r\n",
        "                                                                 bounds=self.parameter_range) for start_point in\r\n",
        "                                               start_points_arr)\r\n",
        "        x_best = np.array([res.x for res in opt])\r\n",
        "        f_best = np.array([np.atleast_1d(res.fun)[0] for res in opt])\r\n",
        "\r\n",
        "        self.x_best = x_best\r\n",
        "        self.f_best = f_best\r\n",
        "        self.best = x_best[np.argmin(f_best)]\r\n",
        "    \r\n",
        "    def run(self, max_iter=10, init_evals=3, resume=False):\r\n",
        "        \r\n",
        "        if not resume:\r\n",
        "            self.init_evals = init_evals\r\n",
        "            self._firstRun(self.init_evals)\r\n",
        "            self.logger._printInit(self)\r\n",
        "        for iteration in range(max_iter):\r\n",
        "            self.d_optimizeAcq_stp()\r\n",
        "            self.updateGP()\r\n",
        "            self.logger._printCurrent(self)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "expMSaSUtcB-"
      },
      "source": [
        "### d2GPGO - BayesOpt class: \r\n",
        "\r\n",
        "class d2GPGO(GPGO):  \r\n",
        "    n_start = 100\r\n",
        "    p = np.full((n_start,1),1)\r\n",
        "    \r\n",
        "    def func(self, xnew):\r\n",
        "        new_mean, new_var = self.GP.predict(xnew, return_std=True)\r\n",
        "        new_std = np.sqrt(new_var + 1e-6)\r\n",
        "        ds, dm, dvdv, d2s, d2m = self.GP.AcqGrad(xnew, return_std=True)\r\n",
        "        f  = np.empty((self.n_start,))\r\n",
        "        df = np.empty((self.n_start,))\r\n",
        "        f  = -self.A.d_eval(self.tau, new_mean, new_std, ds=ds, dm=dm, dvdv=dvdv, d2s=d2s, d2m=d2m)[0]\r\n",
        "        df = -self.A.d_eval(self.tau, new_mean, new_std, ds=ds, dm=dm, dvdv=dvdv, d2s=d2s, d2m=d2m)[1]\r\n",
        "        df_array = np.full((len(xnew),),df)\r\n",
        "        return f, df_array\r\n",
        "    \r\n",
        "    def hessp_nonzero(self, xnew, p):\r\n",
        "        new_mean, new_var = self.GP.predict(xnew, return_std=True)\r\n",
        "        new_std = np.sqrt(new_var + 1e-6)\r\n",
        "        ds, dm, dvdv, d2s, d2m = self.GP.AcqGrad(xnew, return_std=True)\r\n",
        "        df2 = np.empty((self.n_start,))\r\n",
        "        df2 = -self.A.d_eval(self.tau, new_mean, new_std, ds=ds, dm=dm, dvdv=dvdv, d2s=d2s, d2m=d2m)[2]\r\n",
        "        H2 = np.empty((self.n_start,))\r\n",
        "        df2 = np.asarray(df2)\r\n",
        "        p = np.asarray(p)\r\n",
        "        H2 = np.multiply(df2,p)\r\n",
        "        return H2\r\n",
        "    \r\n",
        "    def d_optimizeAcq(self, method='Newton-CG', n_start=n_start):\r\n",
        "        start_points_dict = [self._sampleParam() for i in range(n_start)]\r\n",
        "        start_points_arr = np.array([list(s.values())\r\n",
        "                                     for s in start_points_dict])\r\n",
        "        x_best = np.empty((n_start, len(self.parameter_key)))\r\n",
        "        f_best = np.empty((n_start,))\r\n",
        "        opt = Parallel(n_jobs=self.n_jobs)(delayed(minimize)(self.func,\r\n",
        "                                                                 x0=start_point,\r\n",
        "                                                                 method=method,\r\n",
        "                                                                 jac = True,\r\n",
        "                                                                 hessp = self.hessp_nonzero,\r\n",
        "                                                                 bounds=self.parameter_range) for start_point in\r\n",
        "                                               start_points_arr)\r\n",
        "        \r\n",
        "        x_best = np.array([res.x for res in opt])\r\n",
        "        f_best = np.array([np.atleast_1d(res.fun)[0] for res in opt])\r\n",
        "\r\n",
        "        self.best = x_best[np.argmin(f_best)]     \r\n",
        "    \r\n",
        "    def run(self, max_iter=10, init_evals=3, resume=False):\r\n",
        "        \r\n",
        "        if not resume:\r\n",
        "            self.init_evals = init_evals\r\n",
        "            self._firstRun(self.init_evals)\r\n",
        "            self.logger._printInit(self)\r\n",
        "        for iteration in range(max_iter):\r\n",
        "            self.d_optimizeAcq()\r\n",
        "            self.updateGP()\r\n",
        "            self.logger._printCurrent(self)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "umKPgDuEtcxH",
        "outputId": "d7278816-bf95-4e04-f0e9-607a21986aeb"
      },
      "source": [
        "start_lose = time.time()\r\n",
        "start_lose"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1613491905.8138368"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mfEuNIqvtdSv",
        "outputId": "c16ad042-0bb2-4e29-80df-c842988a7d5a"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 1\r\n",
        "\r\n",
        "np.random.seed(run_num_1)\r\n",
        "surrogate_loser_1 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "loser_1 = dGPGO(surrogate_loser_1, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_1.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.61217018 0.16906975 0.43605902]. \t  0.37345684462559386 \t 2.3951473341797507\n",
            "init   \t [0.76926247 0.2953253  0.14916296]. \t  0.296162062221701 \t 2.3951473341797507\n",
            "init   \t [0.02247832 0.42022449 0.23868214]. \t  0.2904113287153621 \t 2.3951473341797507\n",
            "init   \t [0.33765619 0.99071246 0.23772645]. \t  0.11013785080555143 \t 2.3951473341797507\n",
            "init   \t [0.08119266 0.66960024 0.62124292]. \t  2.3951473341797507 \t 2.3951473341797507\n",
            "1      \t [0.00485804 0.68286261 0.96018946]. \t  \u001b[92m2.420367869177603\u001b[0m \t 2.420367869177603\n",
            "2      \t [0.02878539 0.86819916 0.77091931]. \t  1.7656871014364126 \t 2.420367869177603\n",
            "3      \t [0.07314746 0.48191287 0.83700332]. \t  \u001b[92m3.6368599908062187\u001b[0m \t 3.6368599908062187\n",
            "4      \t [0.01690823 0.20303658 0.75373379]. \t  1.1282788852081818 \t 3.6368599908062187\n",
            "5      \t [0.8642259  0.77437694 0.99435991]. \t  1.4015971324221925 \t 3.6368599908062187\n",
            "6      \t [0.45635757 0.34106775 0.99893775]. \t  1.2764378468913773 \t 3.6368599908062187\n",
            "7      \t [0.51811283 0.98946856 0.87150225]. \t  0.6790214648802388 \t 3.6368599908062187\n",
            "8      \t [0.99661639 0.78557754 0.7341533 ]. \t  1.357762610045871 \t 3.6368599908062187\n",
            "9      \t [0.23891206 0.52279384 0.744595  ]. \t  2.9890323710648232 \t 3.6368599908062187\n",
            "10     \t [0.81269136 0.02513207 0.05336369]. \t  0.12895269805080278 \t 3.6368599908062187\n",
            "11     \t [0.04939583 0.93739076 0.03475733]. \t  0.0010293495994891455 \t 3.6368599908062187\n",
            "12     \t [0.98025994 0.94493063 0.93523017]. \t  0.7397702819810867 \t 3.6368599908062187\n",
            "13     \t [0.98287372 0.41064655 0.75548703]. \t  2.5063594077603826 \t 3.6368599908062187\n",
            "14     \t [0.00341871 0.50652167 0.82130032]. \t  \u001b[92m3.6535048897742355\u001b[0m \t 3.6535048897742355\n",
            "15     \t [0.0224553  0.43458959 0.97377221]. \t  2.141981244272377 \t 3.6535048897742355\n",
            "16     \t [0.89463363 0.84820422 0.04541421]. \t  0.000670221629578516 \t 3.6535048897742355\n",
            "17     \t [0.9759748  0.04775781 0.19456793]. \t  0.2692230786952312 \t 3.6535048897742355\n",
            "18     \t [0.88619742 0.72803771 0.45496813]. \t  0.3281958486317813 \t 3.6535048897742355\n",
            "19     \t [0.05937133 0.48562134 0.68935549]. \t  2.2034066459332107 \t 3.6535048897742355\n",
            "20     \t [4.73482421e-15 1.01648540e-14 1.28361034e-14]. \t  0.06797411659014863 \t 3.6535048897742355\n",
            "21     \t [0.81031067 0.04242296 0.86517943]. \t  0.33494165490779715 \t 3.6535048897742355\n",
            "22     \t [0.45397534 0.74837454 0.05695549]. \t  0.00588119212802071 \t 3.6535048897742355\n",
            "23     \t [0.32540257 0.55083361 0.88747698]. \t  \u001b[92m3.7415841024865695\u001b[0m \t 3.7415841024865695\n",
            "24     \t [0.04042811 0.41229048 0.03240058]. \t  0.057886452972554375 \t 3.7415841024865695\n",
            "25     \t [0.39551633 0.63464245 0.85084623]. \t  3.6329062955054745 \t 3.7415841024865695\n",
            "26     \t [0.15112422 0.57169865 0.8695877 ]. \t  \u001b[92m3.8131284252695528\u001b[0m \t 3.8131284252695528\n",
            "27     \t [0.15032874 0.60928825 0.85721694]. \t  3.7489817639287413 \t 3.8131284252695528\n",
            "28     \t [0.74361257 0.16587207 0.9127195 ]. \t  0.7937697608354773 \t 3.8131284252695528\n",
            "29     \t [0.33554799 0.1960815  0.29285402]. \t  0.9219313064285296 \t 3.8131284252695528\n",
            "30     \t [0.23522587 0.33616893 0.39189588]. \t  0.45360567040791877 \t 3.8131284252695528\n",
            "31     \t [0.34104117 0.1003848  0.35929984]. \t  0.7758886749514444 \t 3.8131284252695528\n",
            "32     \t [0.74081242 0.2689585  0.55926302]. \t  0.3561168332330422 \t 3.8131284252695528\n",
            "33     \t [0.28570408 0.59115037 0.91467622]. \t  3.456253786611963 \t 3.8131284252695528\n",
            "34     \t [0.71467268 0.1858588  0.43815269]. \t  0.30645954895088945 \t 3.8131284252695528\n",
            "35     \t [0.35345103 0.44407979 0.5372887 ]. \t  0.7310458138475027 \t 3.8131284252695528\n",
            "36     \t [0.13983935 0.60982176 0.8363651 ]. \t  3.7212576581323322 \t 3.8131284252695528\n",
            "37     \t [0.13551095 0.6147752  0.86099234]. \t  3.7217456557246953 \t 3.8131284252695528\n",
            "38     \t [0.10902695 0.23131956 0.48553758]. \t  0.2882119729872925 \t 3.8131284252695528\n",
            "39     \t [0.02291614 0.84016591 0.0677154 ]. \t  0.0034917025704461944 \t 3.8131284252695528\n",
            "40     \t [0.05766926 0.62078636 0.81547445]. \t  3.5677144595940287 \t 3.8131284252695528\n",
            "41     \t [0.21425196 0.55005322 0.57341181]. \t  1.4974418409613515 \t 3.8131284252695528\n",
            "42     \t [0.15975379 0.0859061  0.84242869]. \t  0.5263639474321362 \t 3.8131284252695528\n",
            "43     \t [0.67406653 0.50991186 0.8188701 ]. \t  3.624086501118737 \t 3.8131284252695528\n",
            "44     \t [0.56610359 0.55232818 0.85012893]. \t  \u001b[92m3.825256006651254\u001b[0m \t 3.825256006651254\n",
            "45     \t [0.80344365 0.55216636 0.8689101 ]. \t  3.740217065167692 \t 3.825256006651254\n",
            "46     \t [0.63902865 0.60026654 0.85043859]. \t  3.722030318046956 \t 3.825256006651254\n",
            "47     \t [0.17251606 0.20286351 0.62992262]. \t  0.5516254039236159 \t 3.825256006651254\n",
            "48     \t [0.17397627 0.42220867 0.13521323]. \t  0.2102484680473599 \t 3.825256006651254\n",
            "49     \t [0.96427781 0.39668576 0.90907467]. \t  2.6618830342778494 \t 3.825256006651254\n",
            "50     \t [0.68936383 0.66598774 0.82319554]. \t  3.2366176409919944 \t 3.825256006651254\n",
            "51     \t [0.4098947  0.03980913 0.32341492]. \t  0.8538210043890206 \t 3.825256006651254\n",
            "52     \t [0.57307037 0.54365438 0.89673594]. \t  3.6427611518696232 \t 3.825256006651254\n",
            "53     \t [0.97344354 0.58114841 0.77316942]. \t  2.992698046283317 \t 3.825256006651254\n",
            "54     \t [0.5634305  0.72109247 0.84012619]. \t  2.9223035948746707 \t 3.825256006651254\n",
            "55     \t [0.94325517 0.68353852 0.09056374]. \t  0.006284268308091021 \t 3.825256006651254\n",
            "56     \t [0.52111688 0.40014383 0.78101946]. \t  2.817593833921735 \t 3.825256006651254\n",
            "57     \t [0.09605805 0.15641423 0.76030068]. \t  0.8630190336972412 \t 3.825256006651254\n",
            "58     \t [0.50107907 0.74984541 0.40285632]. \t  0.8316034757573986 \t 3.825256006651254\n",
            "59     \t [0.6360143  0.17643848 0.6802233 ]. \t  0.6809071664843318 \t 3.825256006651254\n",
            "60     \t [0.80083048 0.87131534 0.70079956]. \t  0.954051341659558 \t 3.825256006651254\n",
            "61     \t [0.82945898 0.45413776 0.05193529]. \t  0.04229423308324152 \t 3.825256006651254\n",
            "62     \t [0.00645321 0.41674021 0.89383033]. \t  3.012380711862429 \t 3.825256006651254\n",
            "63     \t [0.03893406 0.41065296 0.22052005]. \t  0.29746538350005025 \t 3.825256006651254\n",
            "64     \t [0.41043802 0.7724817  0.31190719]. \t  0.36545761370116336 \t 3.825256006651254\n",
            "65     \t [0.10812468 0.72529452 0.25465347]. \t  0.18203581392260854 \t 3.825256006651254\n",
            "66     \t [0.24858926 0.45365363 0.28750452]. \t  0.3624328268339379 \t 3.825256006651254\n",
            "67     \t [0.36281072 0.18520399 0.67829577]. \t  0.7170562744784025 \t 3.825256006651254\n",
            "68     \t [0.15911236 0.11059501 0.65089183]. \t  0.3669042997391726 \t 3.825256006651254\n",
            "69     \t [0.86350529 0.8473428  0.26259772]. \t  0.04416400359341738 \t 3.825256006651254\n",
            "70     \t [0.91551351 0.96186868 0.09233347]. \t  0.0007755993199785646 \t 3.825256006651254\n",
            "71     \t [0.54915703 0.56985442 0.26726008]. \t  0.173083197347087 \t 3.825256006651254\n",
            "72     \t [0.25627324 0.96587806 0.97601747]. \t  0.5330995829102597 \t 3.825256006651254\n",
            "73     \t [0.94115139 0.34761178 0.07356107]. \t  0.07136458178756505 \t 3.825256006651254\n",
            "74     \t [0.48760414 0.88948427 0.93965469]. \t  1.1227699240858733 \t 3.825256006651254\n",
            "75     \t [0.11304749 0.9094962  0.33213995]. \t  0.671613628713174 \t 3.825256006651254\n",
            "76     \t [0.27651602 0.0953211  0.81854326]. \t  0.5872636327758214 \t 3.825256006651254\n",
            "77     \t [0.85890336 0.09590625 0.90600383]. \t  0.4619459676508832 \t 3.825256006651254\n",
            "78     \t [0.23733868 0.82048291 0.73915572]. \t  2.124101861946425 \t 3.825256006651254\n",
            "79     \t [0.14510227 0.95514932 0.67105549]. \t  2.084284345766824 \t 3.825256006651254\n",
            "80     \t [0.62299085 0.06122236 0.99608914]. \t  0.17879623427674915 \t 3.825256006651254\n",
            "81     \t [0.29088308 0.7915848  0.41403377]. \t  1.4173589192745526 \t 3.825256006651254\n",
            "82     \t [0.04076394 0.57291442 0.54014621]. \t  1.474778439507063 \t 3.825256006651254\n",
            "83     \t [0.75370983 0.59648656 0.59667461]. \t  0.9825631878460681 \t 3.825256006651254\n",
            "84     \t [0.59983715 0.47183885 0.69463084]. \t  2.06949302073836 \t 3.825256006651254\n",
            "85     \t [0.33242802 0.83723309 0.23185022]. \t  0.11719684568063422 \t 3.825256006651254\n",
            "86     \t [0.07444371 0.91348219 0.27435046]. \t  0.27969467234132755 \t 3.825256006651254\n",
            "87     \t [0.12414229 0.18733278 0.12349369]. \t  0.4276920407976786 \t 3.825256006651254\n",
            "88     \t [0.04087888 0.24199679 0.91266509]. \t  1.323603308572735 \t 3.825256006651254\n",
            "89     \t [0.82437346 0.62993189 0.86780455]. \t  3.530389788348192 \t 3.825256006651254\n",
            "90     \t [0.51211929 0.25967277 0.9643693 ]. \t  1.1073875208142783 \t 3.825256006651254\n",
            "91     \t [0.37851489 0.68753796 0.86748997]. \t  3.2710525685540577 \t 3.825256006651254\n",
            "92     \t [0.91092111 0.42303455 0.93094604]. \t  2.642199576880523 \t 3.825256006651254\n",
            "93     \t [0.84348377 0.94747191 0.42779077]. \t  0.35056261602863853 \t 3.825256006651254\n",
            "94     \t [0.91441912 0.00287177 0.42191247]. \t  0.18004712031761186 \t 3.825256006651254\n",
            "95     \t [0.73026186 0.35943098 0.6904105 ]. \t  1.5865167617832956 \t 3.825256006651254\n",
            "96     \t [0.49324108 0.4044849  0.28058902]. \t  0.43862141669775573 \t 3.825256006651254\n",
            "97     \t [0.7886927  0.92922132 0.01214836]. \t  0.00022036803397238445 \t 3.825256006651254\n",
            "98     \t [0.80472456 0.53074051 0.03159763]. \t  0.01934647763876607 \t 3.825256006651254\n",
            "99     \t [0.39965895 0.49225993 0.45506877]. \t  0.5537815478508364 \t 3.825256006651254\n",
            "100    \t [0.32115229 0.55739422 0.943681  ]. \t  3.080532019803937 \t 3.825256006651254\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1j6UBhkdtdVe",
        "outputId": "6891dd9a-d1cb-4a59-b1ab-2c17df6d75e3"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 2\r\n",
        "\r\n",
        "np.random.seed(run_num_2)\r\n",
        "surrogate_loser_2 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "loser_2 = dGPGO(surrogate_loser_2, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_2.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.85198549 0.0739036  0.89493176]. \t  0.3999083566189884 \t 2.6229838112516717\n",
            "init   \t [0.43649355 0.12767773 0.57585787]. \t  0.24461577848211966 \t 2.6229838112516717\n",
            "init   \t [0.84047092 0.43512055 0.69591056]. \t  1.8897715258798413 \t 2.6229838112516717\n",
            "init   \t [0.6846381  0.70064837 0.77969426]. \t  2.6229838112516717 \t 2.6229838112516717\n",
            "init   \t [0.64274937 0.96102617 0.10846489]. \t  0.003309399496320042 \t 2.6229838112516717\n",
            "1      \t [0.84869923 0.93241717 0.93297728]. \t  0.833544515077231 \t 2.6229838112516717\n",
            "2      \t [0.02085923 0.80205168 0.9123246 ]. \t  2.0046127391285764 \t 2.6229838112516717\n",
            "3      \t [0.43460979 0.69789505 0.78145073]. \t  \u001b[92m2.8479518965640622\u001b[0m \t 2.8479518965640622\n",
            "4      \t [0.40901198 0.61566029 0.92724937]. \t  \u001b[92m3.234958703146847\u001b[0m \t 3.234958703146847\n",
            "5      \t [0.53391398 0.48806221 0.96132956]. \t  2.6256349202144156 \t 3.234958703146847\n",
            "6      \t [0.45067926 0.76261712 0.98454777]. \t  1.627202746319736 \t 3.234958703146847\n",
            "7      \t [0.06197566 0.44308783 0.973175  ]. \t  2.202961061743121 \t 3.234958703146847\n",
            "8      \t [0.99887885 0.8814706  0.48834169]. \t  0.26549097985455744 \t 3.234958703146847\n",
            "9      \t [0.19880172 0.54879555 0.85046577]. \t  \u001b[92m3.854455563667816\u001b[0m \t 3.854455563667816\n",
            "10     \t [0.09144843 0.59811154 0.76482532]. \t  3.2199307487595137 \t 3.854455563667816\n",
            "11     \t [0.32567482 0.45811003 0.79755265]. \t  3.337630738095901 \t 3.854455563667816\n",
            "12     \t [0.98111823 0.59311218 0.88315099]. \t  3.5627915294440626 \t 3.854455563667816\n",
            "13     \t [0.90888605 0.96545518 0.00619077]. \t  8.893896213612553e-05 \t 3.854455563667816\n",
            "14     \t [0.52728504 0.03560865 0.00673939]. \t  0.11323883329280729 \t 3.854455563667816\n",
            "15     \t [0.80834889 0.13656897 0.75847908]. \t  0.7445052723203085 \t 3.854455563667816\n",
            "16     \t [0.19243375 0.6201231  0.88889506]. \t  3.596119614056206 \t 3.854455563667816\n",
            "17     \t [0.70664598 0.00359851 0.80572512]. \t  0.2584865256522639 \t 3.854455563667816\n",
            "18     \t [0.685457   0.1875226  0.86702466]. \t  1.0885576658432579 \t 3.854455563667816\n",
            "19     \t [0.25029555 0.03218495 0.41040645]. \t  0.48831139871669466 \t 3.854455563667816\n",
            "20     \t [0.0129074  0.09277758 0.0120601 ]. \t  0.09628222495811294 \t 3.854455563667816\n",
            "21     \t [0.22895586 0.32638851 0.44386193]. \t  0.38289739220068636 \t 3.854455563667816\n",
            "22     \t [0.46913283 0.82262434 0.20066583]. \t  0.05199322975955362 \t 3.854455563667816\n",
            "23     \t [0.34408423 0.77738329 0.00861005]. \t  0.002013207710795406 \t 3.854455563667816\n",
            "24     \t [0.2526129  0.56276918 0.62926111]. \t  1.8798504053753444 \t 3.854455563667816\n",
            "25     \t [0.80792377 0.52815806 0.01975468]. \t  0.01649489836218101 \t 3.854455563667816\n",
            "26     \t [0.31206494 0.14585568 0.16799338]. \t  0.7307676260265302 \t 3.854455563667816\n",
            "27     \t [0.09918691 0.51251405 0.82103803]. \t  3.6962924579227248 \t 3.854455563667816\n",
            "28     \t [0.26889312 0.46964109 0.1967037 ]. \t  0.25266311998872126 \t 3.854455563667816\n",
            "29     \t [0.27452343 0.49067769 0.19135809]. \t  0.21490083226807472 \t 3.854455563667816\n",
            "30     \t [0.41783706 0.46348636 0.08625525]. \t  0.11238386633298683 \t 3.854455563667816\n",
            "31     \t [0.2030483  0.60445376 0.6118741 ]. \t  2.008738081126147 \t 3.854455563667816\n",
            "32     \t [0.44338236 0.20986102 0.51086629]. \t  0.28470762022492946 \t 3.854455563667816\n",
            "33     \t [0.85132587 0.65806184 0.68313386]. \t  1.5772789254037718 \t 3.854455563667816\n",
            "34     \t [0.11315979 0.99591054 0.19748993]. \t  0.056450542512549926 \t 3.854455563667816\n",
            "35     \t [0.38871972 0.08593287 0.46946098]. \t  0.31822054482285544 \t 3.854455563667816\n",
            "36     \t [2.55343109e-01 6.77527185e-01 3.57837085e-04]. \t  0.005091131363578613 \t 3.854455563667816\n",
            "37     \t [0.40142707 0.84353954 0.4657078 ]. \t  1.8342725319658606 \t 3.854455563667816\n",
            "38     \t [0.46338869 0.97746975 0.7029069 ]. \t  1.2196926196732307 \t 3.854455563667816\n",
            "39     \t [0.9369779  0.06889953 0.08134261]. \t  0.13150996341975008 \t 3.854455563667816\n",
            "40     \t [0.2256699  0.07992419 0.41748843]. \t  0.4819630201952553 \t 3.854455563667816\n",
            "41     \t [0.95541622 0.17322212 0.6920158 ]. \t  0.6974544610532448 \t 3.854455563667816\n",
            "42     \t [0.44579868 0.17694172 0.74144773]. \t  0.9388601851126399 \t 3.854455563667816\n",
            "43     \t [0.52246356 0.82266054 0.13911684]. \t  0.013759219160063875 \t 3.854455563667816\n",
            "44     \t [0.72982604 0.28184031 0.86459949]. \t  1.9044776108865855 \t 3.854455563667816\n",
            "45     \t [0.63818509 0.39198514 0.45536791]. \t  0.2892748188116343 \t 3.854455563667816\n",
            "46     \t [0.73136775 0.30934836 0.92232944]. \t  1.8276447749873623 \t 3.854455563667816\n",
            "47     \t [0.77047443 0.64214483 0.12903927]. \t  0.024102114086029674 \t 3.854455563667816\n",
            "48     \t [0.4634485  0.65534339 0.23827586]. \t  0.11593556414242427 \t 3.854455563667816\n",
            "49     \t [0.99628529 0.70527452 0.94125411]. \t  2.4583799953099077 \t 3.854455563667816\n",
            "50     \t [0.98012153 0.29487901 0.54063701]. \t  0.273707971859134 \t 3.854455563667816\n",
            "51     \t [0.20844235 0.33272232 0.11427776]. \t  0.2883861055346616 \t 3.854455563667816\n",
            "52     \t [0.83243348 0.27176422 0.0498824 ]. \t  0.10004439130921053 \t 3.854455563667816\n",
            "53     \t [0.31228377 0.9497917  0.78925978]. \t  1.1372129515685026 \t 3.854455563667816\n",
            "54     \t [0.81204389 0.10745564 0.99319514]. \t  0.2793664207698889 \t 3.854455563667816\n",
            "55     \t [0.30559737 0.74140756 0.20001864]. \t  0.0690608605636907 \t 3.854455563667816\n",
            "56     \t [0.30355771 0.60105121 0.66701023]. \t  2.232680275582264 \t 3.854455563667816\n",
            "57     \t [0.32655384 0.45283861 0.70914601]. \t  2.3157027494336946 \t 3.854455563667816\n",
            "58     \t [0.98846866 0.48338566 0.96643613]. \t  2.4354802136178684 \t 3.854455563667816\n",
            "59     \t [0.04941912 0.54241983 0.36607595]. \t  0.4385741473197778 \t 3.854455563667816\n",
            "60     \t [0.17888481 0.27261652 0.57983518]. \t  0.5075266432155109 \t 3.854455563667816\n",
            "61     \t [0.60451084 0.31746966 0.12296527]. \t  0.30343594735798185 \t 3.854455563667816\n",
            "62     \t [0.69919355 0.08866689 0.64966157]. \t  0.30694751763473066 \t 3.854455563667816\n",
            "63     \t [0.89315899 0.15618225 0.85935459]. \t  0.8735852961978772 \t 3.854455563667816\n",
            "64     \t [0.1521253  0.35901651 0.22965336]. \t  0.4723852558929425 \t 3.854455563667816\n",
            "65     \t [0.23231678 0.10972766 0.93641936]. \t  0.44882914638217647 \t 3.854455563667816\n",
            "66     \t [0.84188226 0.70723005 0.54045426]. \t  0.6264810020583895 \t 3.854455563667816\n",
            "67     \t [0.01124263 0.9495282  0.89764946]. \t  0.8860680765708808 \t 3.854455563667816\n",
            "68     \t [0.64184498 0.26595085 0.80861442]. \t  1.7950464127622898 \t 3.854455563667816\n",
            "69     \t [0.0229726  0.00499224 0.67044876]. \t  0.17729951913373557 \t 3.854455563667816\n",
            "70     \t [0.51570772 0.65260972 0.10114952]. \t  0.025592110981376887 \t 3.854455563667816\n",
            "71     \t [0.60347441 0.897619   0.45446129]. \t  1.06949486043582 \t 3.854455563667816\n",
            "72     \t [0.95758577 0.64651273 0.9322479 ]. \t  2.9288989872413875 \t 3.854455563667816\n",
            "73     \t [0.78857677 0.49823768 0.51203495]. \t  0.38734467751295526 \t 3.854455563667816\n",
            "74     \t [0.43165387 0.02353938 0.45456709]. \t  0.3284207744430256 \t 3.854455563667816\n",
            "75     \t [0.93712698 0.40753674 0.39799078]. \t  0.1357588290930862 \t 3.854455563667816\n",
            "76     \t [0.75852101 0.56650411 0.29588659]. \t  0.12717945507032985 \t 3.854455563667816\n",
            "77     \t [0.65699665 0.67754593 0.95225407]. \t  2.573110674137345 \t 3.854455563667816\n",
            "78     \t [0.21411235 0.92073944 0.18720244]. \t  0.050558991438194395 \t 3.854455563667816\n",
            "79     \t [0.81186963 0.95466875 0.19513996]. \t  0.013622693124886443 \t 3.854455563667816\n",
            "80     \t [0.13321787 0.16889317 0.15110243]. \t  0.549728293542864 \t 3.854455563667816\n",
            "81     \t [0.50055678 0.40552636 0.35889602]. \t  0.39460953216206407 \t 3.854455563667816\n",
            "82     \t [0.68937348 0.74734445 0.93696005]. \t  2.2621603595408324 \t 3.854455563667816\n",
            "83     \t [0.24422687 0.25258948 0.42558462]. \t  0.43458421843040185 \t 3.854455563667816\n",
            "84     \t [0.33290521 0.92785349 0.70086949]. \t  1.7092276465727636 \t 3.854455563667816\n",
            "85     \t [0.23066553 0.98634866 0.51216754]. \t  2.405127214497184 \t 3.854455563667816\n",
            "86     \t [0.77062047 0.44235163 0.41154312]. \t  0.2070073279439252 \t 3.854455563667816\n",
            "87     \t [0.89972384 0.5416144  0.91950053]. \t  3.316834149142124 \t 3.854455563667816\n",
            "88     \t [0.35886225 0.27348523 0.98584931]. \t  1.0168568786796437 \t 3.854455563667816\n",
            "89     \t [0.24008624 0.71862237 0.60323517]. \t  2.5243228209177246 \t 3.854455563667816\n",
            "90     \t [0.70227469 0.71073892 0.38639049]. \t  0.3630229515433469 \t 3.854455563667816\n",
            "91     \t [0.04630934 0.53729892 0.00134421]. \t  0.015083670432843367 \t 3.854455563667816\n",
            "92     \t [0.68416238 0.83798606 0.92513195]. \t  1.5835707271787056 \t 3.854455563667816\n",
            "93     \t [0.80857863 0.30322871 0.42341   ]. \t  0.23276557630011466 \t 3.854455563667816\n",
            "94     \t [0.87240962 0.53505616 0.5612704 ]. \t  0.5766218070407124 \t 3.854455563667816\n",
            "95     \t [0.37075559 0.35467445 0.40266831]. \t  0.4292705700245347 \t 3.854455563667816\n",
            "96     \t [0.07623101 0.7603222  0.69765839]. \t  2.4874462002268505 \t 3.854455563667816\n",
            "97     \t [0.52298903 0.07651244 0.54207474]. \t  0.17715826053064726 \t 3.854455563667816\n",
            "98     \t [0.18967587 0.49028662 0.51106987]. \t  0.8680665472248712 \t 3.854455563667816\n",
            "99     \t [0.77481162 0.28016538 0.68979909]. \t  1.2006144140525907 \t 3.854455563667816\n",
            "100    \t [0.34253365 0.56155319 0.19047772]. \t  0.13390384213700912 \t 3.854455563667816\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eWnL1rwztdYG",
        "outputId": "783c1b59-ee4b-415b-b66f-af9713cb2871"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 3\r\n",
        "\r\n",
        "np.random.seed(run_num_3)\r\n",
        "surrogate_loser_3 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "loser_3 = dGPGO(surrogate_loser_3, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_3.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.75157561 0.10925128 0.48612128]. \t  0.19395982560567754 \t 0.5647137279144399\n",
            "init   \t [0.49983118 0.65711    0.23588471]. \t  0.10633623603471726 \t 0.5647137279144399\n",
            "init   \t [0.61279489 0.1196524  0.7122023 ]. \t  0.5647137279144399 \t 0.5647137279144399\n",
            "init   \t [0.37256054 0.52476827 0.26328708]. \t  0.24645583811739444 \t 0.5647137279144399\n",
            "init   \t [0.62163122 0.44909976 0.21906361]. \t  0.2633149120115605 \t 0.5647137279144399\n",
            "1      \t [0.03146046 0.20692188 0.93064958]. \t  \u001b[92m0.9684227062893344\u001b[0m \t 0.9684227062893344\n",
            "2      \t [0.22074887 0.05835947 0.90763313]. \t  0.3319318657983171 \t 0.9684227062893344\n",
            "3      \t [0.80398301 0.59732101 0.97755957]. \t  \u001b[92m2.4285425171285553\u001b[0m \t 2.4285425171285553\n",
            "4      \t [0.5136305  0.6806365  0.98913253]. \t  2.0107525433877536 \t 2.4285425171285553\n",
            "5      \t [0.96047188 0.84635755 0.98454941]. \t  1.059610295269561 \t 2.4285425171285553\n",
            "6      \t [0.73881103 0.2852016  0.92866174]. \t  1.569439758670843 \t 2.4285425171285553\n",
            "7      \t [0.71874866 0.52230578 0.95817827]. \t  \u001b[92m2.7596541737687836\u001b[0m \t 2.7596541737687836\n",
            "8      \t [0.58383333 0.51359399 0.72217957]. \t  2.534851203896329 \t 2.7596541737687836\n",
            "9      \t [0.62672376 0.44920091 0.86855919]. \t  \u001b[92m3.425198724406297\u001b[0m \t 3.425198724406297\n",
            "10     \t [0.28917408 0.83214975 0.03756089]. \t  0.002087773705261575 \t 3.425198724406297\n",
            "11     \t [0.36636017 0.43337848 0.97670367]. \t  2.1176494503066454 \t 3.425198724406297\n",
            "12     \t [0.97330276 0.150207   0.65663233]. \t  0.4725873790641767 \t 3.425198724406297\n",
            "13     \t [0.92504032 0.05264358 0.0329761 ]. \t  0.073056480820564 \t 3.425198724406297\n",
            "14     \t [0.21283702 0.20885296 0.05657621]. \t  0.2254897218865865 \t 3.425198724406297\n",
            "15     \t [0.41892761 0.57906132 0.0018922 ]. \t  0.014281195322512614 \t 3.425198724406297\n",
            "16     \t [0.53667646 0.97181549 0.0177095 ]. \t  0.00037035759028641906 \t 3.425198724406297\n",
            "17     \t [0.76260918 0.64093003 0.56663145]. \t  0.8307494998265441 \t 3.425198724406297\n",
            "18     \t [0.71655162 0.49206118 0.79554346]. \t  3.3736815291998044 \t 3.425198724406297\n",
            "19     \t [0.45411088 0.59508763 0.96650425]. \t  2.671951945047823 \t 3.425198724406297\n",
            "20     \t [0. 0. 0.]. \t  0.06797411659013229 \t 3.425198724406297\n",
            "21     \t [0.03199185 0.82652436 0.94456978]. \t  1.5610198405723108 \t 3.425198724406297\n",
            "22     \t [0.93396482 0.49592841 0.83999157]. \t  \u001b[92m3.5853558880949756\u001b[0m \t 3.5853558880949756\n",
            "23     \t [0.57568572 0.03836061 0.34427041]. \t  0.6934393647900274 \t 3.5853558880949756\n",
            "24     \t [0.15772068 0.87127358 0.74372625]. \t  1.8724552511399 \t 3.5853558880949756\n",
            "25     \t [0.5282638  0.02165958 0.06560001]. \t  0.24968102362985506 \t 3.5853558880949756\n",
            "26     \t [0.05593608 0.00805584 0.51535353]. \t  0.1354721290304977 \t 3.5853558880949756\n",
            "27     \t [9.75901769e-01 5.69393143e-01 3.89228802e-04]. \t  0.005058269843666252 \t 3.5853558880949756\n",
            "28     \t [0.73479632 0.49575709 0.51112471]. \t  0.4256175209454758 \t 3.5853558880949756\n",
            "29     \t [0.43584131 0.50778002 0.05015357]. \t  0.05234924350693408 \t 3.5853558880949756\n",
            "30     \t [0.94500742 0.63548476 0.84862187]. \t  3.4369051224779454 \t 3.5853558880949756\n",
            "31     \t [0.01698625 0.6620232  0.11602937]. \t  0.023625415620621718 \t 3.5853558880949756\n",
            "32     \t [0.94527857 0.55625325 0.85473546]. \t  \u001b[92m3.6953364254560173\u001b[0m \t 3.6953364254560173\n",
            "33     \t [0.96868717 0.497618   0.8638675 ]. \t  3.5839898526153933 \t 3.6953364254560173\n",
            "34     \t [0.49793118 0.42066997 0.38185502]. \t  0.36716822279655575 \t 3.6953364254560173\n",
            "35     \t [0.46624611 0.63255562 0.52234503]. \t  1.2945035042624384 \t 3.6953364254560173\n",
            "36     \t [0.75738113 0.92448556 0.82374733]. \t  1.0051096970615077 \t 3.6953364254560173\n",
            "37     \t [0.13401842 0.59394694 0.07117342]. \t  0.02871568488814516 \t 3.6953364254560173\n",
            "38     \t [0.5921114  0.60036898 0.98206343]. \t  2.377598838026082 \t 3.6953364254560173\n",
            "39     \t [0.79172764 0.90784428 0.70914857]. \t  0.8431319054674943 \t 3.6953364254560173\n",
            "40     \t [0.1529027  0.26265257 0.99129722]. \t  0.9064177756037938 \t 3.6953364254560173\n",
            "41     \t [0.62518867 0.7121498  0.30259927]. \t  0.17816033134214168 \t 3.6953364254560173\n",
            "42     \t [0.24439409 0.39110835 0.57603432]. \t  0.8043123257896625 \t 3.6953364254560173\n",
            "43     \t [0.23812336 0.07427496 0.89625034]. \t  0.40560748042148276 \t 3.6953364254560173\n",
            "44     \t [0.04418732 0.53134902 0.61276949]. \t  1.6534094783377127 \t 3.6953364254560173\n",
            "45     \t [0.89190225 0.97916204 0.0016274 ]. \t  7.543539199836043e-05 \t 3.6953364254560173\n",
            "46     \t [0.8607458  0.29914426 0.76705565]. \t  1.8822648550157148 \t 3.6953364254560173\n",
            "47     \t [0.48787013 0.40950719 0.58412121]. \t  0.8188835841989224 \t 3.6953364254560173\n",
            "48     \t [0.6043061  0.76298401 0.62408145]. \t  1.5630631055365873 \t 3.6953364254560173\n",
            "49     \t [0.40684603 0.86167236 0.56044735]. \t  2.3905743122833742 \t 3.6953364254560173\n",
            "50     \t [0.62111368 0.52704732 0.84922528]. \t  \u001b[92m3.7896923749467715\u001b[0m \t 3.7896923749467715\n",
            "51     \t [0.98181869 0.54691521 0.83925238]. \t  3.647965901753936 \t 3.7896923749467715\n",
            "52     \t [0.49073662 0.44830916 0.62947485]. \t  1.3035237788368443 \t 3.7896923749467715\n",
            "53     \t [0.61633848 0.39414783 0.60448986]. \t  0.8720175831755752 \t 3.7896923749467715\n",
            "54     \t [0.23416175 0.13039445 0.7953655 ]. \t  0.7678407086210846 \t 3.7896923749467715\n",
            "55     \t [0.92134716 0.78477873 0.66050099]. \t  0.9011550630172612 \t 3.7896923749467715\n",
            "56     \t [0.57293523 0.53361841 0.30639247]. \t  0.22796920303046153 \t 3.7896923749467715\n",
            "57     \t [0.21923538 0.73686176 0.76555192]. \t  2.631803742382601 \t 3.7896923749467715\n",
            "58     \t [0.28290393 0.40273006 0.21517779]. \t  0.40796869278351705 \t 3.7896923749467715\n",
            "59     \t [0.7297733  0.40900862 0.17685063]. \t  0.2271653108899221 \t 3.7896923749467715\n",
            "60     \t [0.2230724  0.51649058 0.0899105 ]. \t  0.07523571607849378 \t 3.7896923749467715\n",
            "61     \t [0.47269005 0.57372622 0.30365754]. \t  0.24086025892892132 \t 3.7896923749467715\n",
            "62     \t [0.19179956 0.04726392 0.22522958]. \t  0.8223003040779117 \t 3.7896923749467715\n",
            "63     \t [0.02665964 0.65810741 0.86461449]. \t  3.463614870381834 \t 3.7896923749467715\n",
            "64     \t [0.21059232 0.42055126 0.12793231]. \t  0.20770560044238096 \t 3.7896923749467715\n",
            "65     \t [0.00214817 0.62386678 0.88042698]. \t  3.5886426140877425 \t 3.7896923749467715\n",
            "66     \t [0.98951797 0.57973148 0.87454844]. \t  3.6229192452423566 \t 3.7896923749467715\n",
            "67     \t [0.46904421 0.31390676 0.81104597]. \t  2.257544468526626 \t 3.7896923749467715\n",
            "68     \t [0.84478817 0.17962491 0.48977645]. \t  0.17730670216691743 \t 3.7896923749467715\n",
            "69     \t [0.65164826 0.42992198 0.58646353]. \t  0.786818072813842 \t 3.7896923749467715\n",
            "70     \t [0.61884566 0.31746878 0.49688504]. \t  0.29554206320383436 \t 3.7896923749467715\n",
            "71     \t [0.61376377 0.70204201 0.35125962]. \t  0.3258708768234984 \t 3.7896923749467715\n",
            "72     \t [0.70209444 0.16663276 0.15167109]. \t  0.46828875679206056 \t 3.7896923749467715\n",
            "73     \t [0.26262437 0.44250019 0.03060716]. \t  0.06252148954452384 \t 3.7896923749467715\n",
            "74     \t [0.16253149 0.66009353 0.2770664 ]. \t  0.2333070716240491 \t 3.7896923749467715\n",
            "75     \t [0.54743938 0.95930507 0.37960054]. \t  0.6252381735775987 \t 3.7896923749467715\n",
            "76     \t [0.16019912 0.90675208 0.41598391]. \t  1.6567154722530448 \t 3.7896923749467715\n",
            "77     \t [0.14125313 0.56127449 0.28467157]. \t  0.24526769934086157 \t 3.7896923749467715\n",
            "78     \t [0.68087076 0.47864738 0.02789553]. \t  0.03623559685115433 \t 3.7896923749467715\n",
            "79     \t [0.84363628 0.39010202 0.36788822]. \t  0.20606981964993962 \t 3.7896923749467715\n",
            "80     \t [0.88060534 0.81553029 0.87952935]. \t  1.9182660692088596 \t 3.7896923749467715\n",
            "81     \t [0.89877928 0.3459699  0.20278669]. \t  0.22577900317369806 \t 3.7896923749467715\n",
            "82     \t [0.45857582 0.75096334 0.18121827]. \t  0.04132361494283127 \t 3.7896923749467715\n",
            "83     \t [0.63085465 0.39247245 0.10795824]. \t  0.1782535802299659 \t 3.7896923749467715\n",
            "84     \t [0.95316593 0.55188064 0.16964544]. \t  0.04218875364955372 \t 3.7896923749467715\n",
            "85     \t [0.85684935 0.75766339 0.033176  ]. \t  0.0017000811369795696 \t 3.7896923749467715\n",
            "86     \t [0.78716689 0.04587408 0.94080521]. \t  0.239651019612562 \t 3.7896923749467715\n",
            "87     \t [0.43911044 0.6981408  0.64567541]. \t  2.053757934020994 \t 3.7896923749467715\n",
            "88     \t [0.8694776  0.36424848 0.71022565]. \t  1.7943091250771404 \t 3.7896923749467715\n",
            "89     \t [0.13016603 0.16838239 0.61643476]. \t  0.4054373001335664 \t 3.7896923749467715\n",
            "90     \t [0.02729288 0.32701445 0.32086059]. \t  0.4466395844396876 \t 3.7896923749467715\n",
            "91     \t [0.37397447 0.81674824 0.92999278]. \t  1.7677722791097792 \t 3.7896923749467715\n",
            "92     \t [0.18208795 0.60697419 0.59659828]. \t  1.9675248424084217 \t 3.7896923749467715\n",
            "93     \t [0.23646816 0.25281473 0.09853109]. \t  0.3358113649828574 \t 3.7896923749467715\n",
            "94     \t [0.33836395 0.50746848 0.85995754]. \t  3.776428935520155 \t 3.7896923749467715\n",
            "95     \t [0.01149402 0.12150612 0.0622546 ]. \t  0.19307208285276578 \t 3.7896923749467715\n",
            "96     \t [0.54711447 0.76849605 0.56712612]. \t  1.70301075667317 \t 3.7896923749467715\n",
            "97     \t [0.89431243 0.73140871 0.6123873 ]. \t  0.803263834874783 \t 3.7896923749467715\n",
            "98     \t [0.18324495 0.1323387  0.43675612]. \t  0.404632292432565 \t 3.7896923749467715\n",
            "99     \t [0.38974735 0.09538962 0.96336092]. \t  0.3290183762762645 \t 3.7896923749467715\n",
            "100    \t [0.19820904 0.06434005 0.77592979]. \t  0.4449318315806874 \t 3.7896923749467715\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zciLjgNltda2",
        "outputId": "5c475ca3-4eed-4305-fb79-85138544cefb"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 4\r\n",
        "\r\n",
        "np.random.seed(run_num_4)\r\n",
        "surrogate_loser_4 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "loser_4 = dGPGO(surrogate_loser_4, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_4.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.83936105 0.82129497 0.64220716]. \t  0.8796727111154451 \t 1.9592421489197056\n",
            "init   \t [0.66722262 0.03913991 0.06567239]. \t  0.21283931850025994 \t 1.9592421489197056\n",
            "init   \t [0.27650064 0.3163605  0.19359241]. \t  0.5590312289203825 \t 1.9592421489197056\n",
            "init   \t [0.19709288 0.84920429 0.84235809]. \t  1.818649590001362 \t 1.9592421489197056\n",
            "init   \t [0.34736473 0.7397423  0.49340914]. \t  1.9592421489197056 \t 1.9592421489197056\n",
            "1      \t [0.22412884 0.99744654 0.22062981]. \t  0.08723427566372202 \t 1.9592421489197056\n",
            "2      \t [0.39834472 0.30674202 0.79309675]. \t  \u001b[92m2.1329197098545456\u001b[0m \t 2.1329197098545456\n",
            "3      \t [0.32515698 0.50644899 0.70570449]. \t  \u001b[92m2.4402824315155924\u001b[0m \t 2.4402824315155924\n",
            "4      \t [0.38751137 0.54288831 0.69362328]. \t  2.3274449687301217 \t 2.4402824315155924\n",
            "5      \t [0.03212668 0.37700128 0.80375987]. \t  \u001b[92m2.7728054967725395\u001b[0m \t 2.7728054967725395\n",
            "6      \t [0.         0.3999037  0.83482831]. \t  \u001b[92m3.0587004183830513\u001b[0m \t 3.0587004183830513\n",
            "7      \t [0.00765951 0.69443831 0.99483118]. \t  1.837984811087114 \t 3.0587004183830513\n",
            "8      \t [1.         0.33857737 1.        ]. \t  1.204825076053117 \t 3.0587004183830513\n",
            "9      \t [0.00241424 0.66657526 0.63371545]. \t  2.3452800172493675 \t 3.0587004183830513\n",
            "10     \t [0.20163624 0.3979214  0.94296382]. \t  2.3677471973333253 \t 3.0587004183830513\n",
            "11     \t [0.02415485 0.40795749 0.88236207]. \t  3.022914884107556 \t 3.0587004183830513\n",
            "12     \t [0.96710442 0.0449519  0.9665492 ]. \t  0.19275441918756014 \t 3.0587004183830513\n",
            "13     \t [0.84982688 0.97152581 0.83935076]. \t  0.7023593073136583 \t 3.0587004183830513\n",
            "14     \t [0.00507357 0.53594721 0.83557695]. \t  \u001b[92m3.774767984844263\u001b[0m \t 3.774767984844263\n",
            "15     \t [0.66777498 0.99831003 0.09312717]. \t  0.0018157895842971323 \t 3.774767984844263\n",
            "16     \t [0.2207714  0.01234288 0.10960952]. \t  0.39798770410144746 \t 3.774767984844263\n",
            "17     \t [0.40337365 0.59976506 0.042912  ]. \t  0.02181678121370963 \t 3.774767984844263\n",
            "18     \t [0.90243156 0.95637458 0.79314329]. \t  0.6979019807244994 \t 3.774767984844263\n",
            "19     \t [0.43816159 0.99249913 0.50382626]. \t  1.7506426999728708 \t 3.774767984844263\n",
            "20     \t [0.94383316 0.48004807 0.41985768]. \t  0.12403116135075266 \t 3.774767984844263\n",
            "21     \t [0.08966898 0.76237058 0.22546117]. \t  0.11425353032662568 \t 3.774767984844263\n",
            "22     \t [0.84234064 0.01920111 0.46515983]. \t  0.15661203492546594 \t 3.774767984844263\n",
            "23     \t [0.25056999 0.23593882 0.47352493]. \t  0.33300666000299306 \t 3.774767984844263\n",
            "24     \t [0.77736605 0.42924543 0.32339966]. \t  0.23232259737210692 \t 3.774767984844263\n",
            "25     \t [0.48323297 0.97953335 0.58107143]. \t  1.7743492083378223 \t 3.774767984844263\n",
            "26     \t [0.07390852 0.47026935 0.4418145 ]. \t  0.5388868547402469 \t 3.774767984844263\n",
            "27     \t [0.43391942 0.96545355 0.6232254 ]. \t  1.8531866038798177 \t 3.774767984844263\n",
            "28     \t [0.72373636 0.89800689 0.19750102]. \t  0.022219851170923014 \t 3.774767984844263\n",
            "29     \t [0.34682653 0.0185079  0.84163519]. \t  0.2873547834147019 \t 3.774767984844263\n",
            "30     \t [0.47646467 0.31690246 0.92015299]. \t  1.9335877861944526 \t 3.774767984844263\n",
            "31     \t [0.26793312 0.20835569 0.39616386]. \t  0.5675650435515753 \t 3.774767984844263\n",
            "32     \t [0.15449961 0.0739904  0.02383857]. \t  0.1444824248167775 \t 3.774767984844263\n",
            "33     \t [0.22224128 0.58835294 0.54227105]. \t  1.5620019707976203 \t 3.774767984844263\n",
            "34     \t [0.7624105  0.83740933 0.4174219 ]. \t  0.475673205419742 \t 3.774767984844263\n",
            "35     \t [0.38226498 0.56576795 0.31580928]. \t  0.2940891935717929 \t 3.774767984844263\n",
            "36     \t [0.42122065 0.03334281 0.87861693]. \t  0.2979774737400446 \t 3.774767984844263\n",
            "37     \t [0.95577186 0.2331745  0.18906347]. \t  0.25888078497782724 \t 3.774767984844263\n",
            "38     \t [0.77686775 0.19817622 0.50516741]. \t  0.20303404687426588 \t 3.774767984844263\n",
            "39     \t [0.02572599 0.03736815 0.77135148]. \t  0.34568347538666316 \t 3.774767984844263\n",
            "40     \t [0.53961985 0.04018105 0.36712348]. \t  0.6427412747817036 \t 3.774767984844263\n",
            "41     \t [0.09117621 0.44826011 0.64659412]. \t  1.6004301056836554 \t 3.774767984844263\n",
            "42     \t [0.69797215 0.92699324 0.60669458]. \t  1.068196523028918 \t 3.774767984844263\n",
            "43     \t [0.14716524 0.6846356  0.55273114]. \t  2.331277322632874 \t 3.774767984844263\n",
            "44     \t [0.94460183 0.39841865 0.20035792]. \t  0.14743471184946214 \t 3.774767984844263\n",
            "45     \t [0.02849758 0.52533294 0.99914079]. \t  2.0416369587013725 \t 3.774767984844263\n",
            "46     \t [0.59525019 0.69610244 0.04470115]. \t  0.0072205823831998734 \t 3.774767984844263\n",
            "47     \t [0.18193296 0.12830195 0.57593137]. \t  0.24216357204494532 \t 3.774767984844263\n",
            "48     \t [0.20414017 0.28332865 0.57594635]. \t  0.5157278007739652 \t 3.774767984844263\n",
            "49     \t [0.51788579 0.81395536 0.34779211]. \t  0.4928912781654505 \t 3.774767984844263\n",
            "50     \t [0.19029052 0.2022316  0.06057696]. \t  0.23450252136128893 \t 3.774767984844263\n",
            "51     \t [0.31404645 0.34552945 0.79524926]. \t  2.489876466905594 \t 3.774767984844263\n",
            "52     \t [0.38784961 0.87753762 0.74012079]. \t  1.6471620810320635 \t 3.774767984844263\n",
            "53     \t [0.86772633 0.88541181 0.69753257]. \t  0.7934431092025374 \t 3.774767984844263\n",
            "54     \t [0.46112737 0.30964653 0.05452543]. \t  0.17298332743033695 \t 3.774767984844263\n",
            "55     \t [0.26064166 0.75821426 0.03807247]. \t  0.0040868577728126795 \t 3.774767984844263\n",
            "56     \t [0.54614286 0.98388172 0.59399202]. \t  1.4879629764781568 \t 3.774767984844263\n",
            "57     \t [0.45408303 0.80825061 0.6365093 ]. \t  2.0677163745827047 \t 3.774767984844263\n",
            "58     \t [0.70741816 0.07928674 0.66920884]. \t  0.32737925886271296 \t 3.774767984844263\n",
            "59     \t [0.06057931 0.81118412 0.58648205]. \t  2.9844350007817253 \t 3.774767984844263\n",
            "60     \t [0.18077291 0.11516388 0.30186734]. \t  0.8693688111325402 \t 3.774767984844263\n",
            "61     \t [0.01147414 0.65918387 0.86218627]. \t  3.456136744660571 \t 3.774767984844263\n",
            "62     \t [0.20286778 0.48979907 0.2163436 ]. \t  0.2338871807716341 \t 3.774767984844263\n",
            "63     \t [0.5386693  0.27612135 0.13522479]. \t  0.4221550183648063 \t 3.774767984844263\n",
            "64     \t [0.22997948 0.52205122 0.23892099]. \t  0.220726732621446 \t 3.774767984844263\n",
            "65     \t [0.85129755 0.30898539 0.42632346]. \t  0.20407290980443352 \t 3.774767984844263\n",
            "66     \t [0.5290956  0.06631674 0.6350847 ]. \t  0.23937763515635238 \t 3.774767984844263\n",
            "67     \t [0.02116937 0.92722889 0.7482784 ]. \t  1.515646748001043 \t 3.774767984844263\n",
            "68     \t [0.70417805 0.87920468 0.75425424]. \t  1.185947839625982 \t 3.774767984844263\n",
            "69     \t [0.68707938 0.97773471 0.3911807 ]. \t  0.44383116056510996 \t 3.774767984844263\n",
            "70     \t [0.36894736 0.56217032 0.37926925]. \t  0.4739893548207636 \t 3.774767984844263\n",
            "71     \t [0.91906465 0.12271966 0.32488807]. \t  0.3661618263517013 \t 3.774767984844263\n",
            "72     \t [0.79562402 0.78140355 0.99146553]. \t  1.4024275536421427 \t 3.774767984844263\n",
            "73     \t [0.19952009 0.46582031 0.97637385]. \t  2.2772004000300323 \t 3.774767984844263\n",
            "74     \t [0.42921714 0.21734282 0.37283993]. \t  0.6569917965717333 \t 3.774767984844263\n",
            "75     \t [0.58785692 0.23925236 0.34858759]. \t  0.6224387919467267 \t 3.774767984844263\n",
            "76     \t [0.83910257 0.94330969 0.78787869]. \t  0.7862412827987283 \t 3.774767984844263\n",
            "77     \t [0.98293203 0.99547179 0.01586767]. \t  6.466411053342413e-05 \t 3.774767984844263\n",
            "78     \t [0.7662562  0.14790208 0.78952916]. \t  0.8577485172452937 \t 3.774767984844263\n",
            "79     \t [0.21712483 0.17339281 0.04403016]. \t  0.20263511578268303 \t 3.774767984844263\n",
            "80     \t [0.34812389 0.0410566  0.25745782]. \t  0.9402321835937894 \t 3.774767984844263\n",
            "81     \t [0.80787489 0.49984609 0.19371546]. \t  0.11358820295056764 \t 3.774767984844263\n",
            "82     \t [0.77992988 0.67256643 0.99579517]. \t  1.9093788624256178 \t 3.774767984844263\n",
            "83     \t [0.74233078 0.51867866 0.07880477]. \t  0.04543925368190092 \t 3.774767984844263\n",
            "84     \t [0.42375063 0.18059118 0.13005468]. \t  0.5409662204364257 \t 3.774767984844263\n",
            "85     \t [0.82159906 0.2629594  0.99301144]. \t  0.8829369483160052 \t 3.774767984844263\n",
            "86     \t [0.87004636 0.48103157 0.64604538]. \t  1.304640200884708 \t 3.774767984844263\n",
            "87     \t [0.00611828 0.39700184 0.56633479]. \t  0.7582299138150631 \t 3.774767984844263\n",
            "88     \t [0.66081675 0.24908963 0.36671335]. \t  0.49741112338686794 \t 3.774767984844263\n",
            "89     \t [0.37145448 0.12239177 0.85745563]. \t  0.6941282348959765 \t 3.774767984844263\n",
            "90     \t [0.50921909 0.11168804 0.67675224]. \t  0.44129452824360094 \t 3.774767984844263\n",
            "91     \t [0.12022965 0.95893321 0.59043519]. \t  2.751577852315629 \t 3.774767984844263\n",
            "92     \t [0.61692084 0.26266657 0.40584368]. \t  0.41060339066604384 \t 3.774767984844263\n",
            "93     \t [0.38132105 0.65477671 0.33897365]. \t  0.41884361706401735 \t 3.774767984844263\n",
            "94     \t [0.90274566 0.39309968 0.42848717]. \t  0.1538058248859996 \t 3.774767984844263\n",
            "95     \t [0.3874207  0.91625586 0.45246377]. \t  1.7139535209488614 \t 3.774767984844263\n",
            "96     \t [0.9037247  0.67953797 0.64761112]. \t  1.108383505018141 \t 3.774767984844263\n",
            "97     \t [0.2129587  0.5704972  0.34719778]. \t  0.42129348276748696 \t 3.774767984844263\n",
            "98     \t [0.5736689  0.22695589 0.48933553]. \t  0.27844968171053286 \t 3.774767984844263\n",
            "99     \t [0.27718184 0.48726663 0.59624794]. \t  1.2942319378416134 \t 3.774767984844263\n",
            "100    \t [0.77226186 0.20972114 0.79326813]. \t  1.2850883206415245 \t 3.774767984844263\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GH76hlE2tddm",
        "outputId": "12f7e091-fca1-4ebc-dc47-d13ccb31b06f"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 5\r\n",
        "\r\n",
        "np.random.seed(run_num_5)\r\n",
        "surrogate_loser_5 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "loser_5 = dGPGO(surrogate_loser_5, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_5.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.55735327 0.54176063 0.56042378]. \t  0.9599610140567721 \t 0.9810564697651996\n",
            "init   \t [0.38204809 0.11909418 0.84599912]. \t  0.6913364116577386 \t 0.9810564697651996\n",
            "init   \t [0.73974161 0.93235756 0.85932801]. \t  0.9810564697651996 \t 0.9810564697651996\n",
            "init   \t [0.70810463 0.68096875 0.4086023 ]. \t  0.4017443293787304 \t 0.9810564697651996\n",
            "init   \t [0.77458462 0.9844762  0.96548486]. \t  0.4769494155906823 \t 0.9810564697651996\n",
            "1      \t [0.0655006  0.90661895 0.81035589]. \t  \u001b[92m1.403888408720423\u001b[0m \t 1.403888408720423\n",
            "2      \t [0.41215072 0.7779011  0.74763494]. \t  \u001b[92m2.1757126346478852\u001b[0m \t 2.1757126346478852\n",
            "3      \t [0.33633606 0.78370041 0.79932727]. \t  \u001b[92m2.3419640547489906\u001b[0m \t 2.3419640547489906\n",
            "4      \t [0.30370554 0.5420168  0.98229967]. \t  \u001b[92m2.396700843210034\u001b[0m \t 2.396700843210034\n",
            "5      \t [0.31648233 0.75341797 0.90249718]. \t  \u001b[92m2.540868818111261\u001b[0m \t 2.540868818111261\n",
            "6      \t [0.24877353 0.70550839 0.93218964]. \t  \u001b[92m2.675295617402082\u001b[0m \t 2.675295617402082\n",
            "7      \t [0.22765288 0.65523033 0.98090669]. \t  2.2526020181259603 \t 2.675295617402082\n",
            "8      \t [0.88905259 0.9071195  0.09359558]. \t  0.0011609712890490993 \t 2.675295617402082\n",
            "9      \t [0.91213598 0.64848762 0.84940846]. \t  \u001b[92m3.3759969859236927\u001b[0m \t 3.3759969859236927\n",
            "10     \t [0.20537547 0.49376321 0.76620967]. \t  3.1799067774359235 \t 3.3759969859236927\n",
            "11     \t [0.98466418 0.54683349 0.96614916]. \t  2.595551041672285 \t 3.3759969859236927\n",
            "12     \t [0.96679483 0.71553251 0.59955893]. \t  0.6497481015154206 \t 3.3759969859236927\n",
            "13     \t [0.82359035 0.75215916 0.92537709]. \t  2.3011178733947304 \t 3.3759969859236927\n",
            "14     \t [5.87488041e-01 8.69935691e-01 2.86898181e-14]. \t  0.0004984957819070075 \t 3.3759969859236927\n",
            "15     \t [0.0580831  0.39109013 0.86632576]. \t  2.9589776717577707 \t 3.3759969859236927\n",
            "16     \t [0.82635183 0.37849214 0.82775201]. \t  2.8383706908177144 \t 3.3759969859236927\n",
            "17     \t [0.26164012 0.53591829 0.84217931]. \t  \u001b[92m3.8388737174774157\u001b[0m \t 3.8388737174774157\n",
            "18     \t [0.32726172 0.46664055 0.84778908]. \t  3.5999022792283437 \t 3.8388737174774157\n",
            "19     \t [0.85272299 0.58024084 0.03326872]. \t  0.01127557476336853 \t 3.8388737174774157\n",
            "20     \t [0.54787183 0.252763   0.54415079]. \t  0.33307288848698713 \t 3.8388737174774157\n",
            "21     \t [0.95895984 0.20211739 0.53649803]. \t  0.19459078704822094 \t 3.8388737174774157\n",
            "22     \t [0.53687692 0.57272348 0.98572679]. \t  2.3422281817251163 \t 3.8388737174774157\n",
            "23     \t [0.68867267 0.00765827 0.05716364]. \t  0.17359394534304726 \t 3.8388737174774157\n",
            "24     \t [0.65433974 0.02262776 0.44688952]. \t  0.28211827697204606 \t 3.8388737174774157\n",
            "25     \t [0.02006712 0.54361265 0.85674407]. \t  3.8106114172417715 \t 3.8388737174774157\n",
            "26     \t [0.04408369 0.57798277 0.84008442]. \t  3.792111144405652 \t 3.8388737174774157\n",
            "27     \t [0.73180974 0.54812754 0.78512463]. \t  3.319226189080209 \t 3.8388737174774157\n",
            "28     \t [0.03558969 0.66343753 0.93451965]. \t  2.8921565718998465 \t 3.8388737174774157\n",
            "29     \t [0.01404798 0.87419782 0.65725577]. \t  2.4792575908001058 \t 3.8388737174774157\n",
            "30     \t [0.5819394  0.26879597 0.05466687]. \t  0.17855837423948281 \t 3.8388737174774157\n",
            "31     \t [0.31797429 0.06123494 0.03694312]. \t  0.1957728032145529 \t 3.8388737174774157\n",
            "32     \t [0.97150247 0.69294377 0.22199652]. \t  0.019920135017441068 \t 3.8388737174774157\n",
            "33     \t [0.46085152 0.56645311 0.57553687]. \t  1.2863944466686195 \t 3.8388737174774157\n",
            "34     \t [0.82964158 0.57980175 0.14424181]. \t  0.041147105173632396 \t 3.8388737174774157\n",
            "35     \t [0.00842072 0.9616652  0.19379822]. \t  0.05452614106227584 \t 3.8388737174774157\n",
            "36     \t [0.01145241 0.64330292 0.80261839]. \t  3.3690139922537727 \t 3.8388737174774157\n",
            "37     \t [0.06013269 0.35026807 0.49586356]. \t  0.3954287956437817 \t 3.8388737174774157\n",
            "38     \t [0.65855735 0.7339546  0.17842301]. \t  0.027927969983747716 \t 3.8388737174774157\n",
            "39     \t [0.75833171 0.253738   0.0292391 ]. \t  0.09612694912065668 \t 3.8388737174774157\n",
            "40     \t [0.56494099 0.01288046 0.30065057]. \t  0.7736377060560666 \t 3.8388737174774157\n",
            "41     \t [0.44126691 0.35921091 0.76431648]. \t  2.3882885446221644 \t 3.8388737174774157\n",
            "42     \t [0.09772268 0.39730314 0.0571514 ]. \t  0.09736614631874568 \t 3.8388737174774157\n",
            "43     \t [0.04390447 0.99697316 0.73397787]. \t  1.2650016562313509 \t 3.8388737174774157\n",
            "44     \t [0.99299479 0.54594283 0.06288426]. \t  0.014164106367212341 \t 3.8388737174774157\n",
            "45     \t [0.83645947 0.11483328 0.79232853]. \t  0.671516469465977 \t 3.8388737174774157\n",
            "46     \t [0.25559256 0.4677096  0.20778315]. \t  0.26763763133381235 \t 3.8388737174774157\n",
            "47     \t [0.52134778 0.64412547 0.66476584]. \t  1.9387526401965522 \t 3.8388737174774157\n",
            "48     \t [0.97960237 0.58509312 0.39486473]. \t  0.0980438150330066 \t 3.8388737174774157\n",
            "49     \t [0.93425858 0.61751435 0.93636713]. \t  3.0007595366600857 \t 3.8388737174774157\n",
            "50     \t [0.93665032 0.83303117 0.3717566 ]. \t  0.14220193723144822 \t 3.8388737174774157\n",
            "51     \t [0.01043746 0.06645344 0.0582733 ]. \t  0.17874671309400159 \t 3.8388737174774157\n",
            "52     \t [0.42146344 0.43093162 0.21313343]. \t  0.34858766310202843 \t 3.8388737174774157\n",
            "53     \t [0.85740045 0.72358599 0.10595874]. \t  0.0067127424362161785 \t 3.8388737174774157\n",
            "54     \t [0.74536496 0.74030997 0.81142443]. \t  2.5236996121014146 \t 3.8388737174774157\n",
            "55     \t [0.48596649 0.05583247 0.71267398]. \t  0.34836328475177203 \t 3.8388737174774157\n",
            "56     \t [0.27943898 0.74737658 0.31328012]. \t  0.42639692017161174 \t 3.8388737174774157\n",
            "57     \t [0.7121061  0.70213367 0.23237757]. \t  0.05544100754439492 \t 3.8388737174774157\n",
            "58     \t [0.89629735 0.50531801 0.27715699]. \t  0.10830442188564005 \t 3.8388737174774157\n",
            "59     \t [0.81710177 0.67511305 0.7989285 ]. \t  2.909475046834837 \t 3.8388737174774157\n",
            "60     \t [0.27827065 0.70878759 0.22002966]. \t  0.10050040713970884 \t 3.8388737174774157\n",
            "61     \t [0.12217544 0.78496978 0.79026058]. \t  2.37095832378724 \t 3.8388737174774157\n",
            "62     \t [0.65576377 0.4536732  0.46632376]. \t  0.32528800321453955 \t 3.8388737174774157\n",
            "63     \t [0.78302288 0.13617894 0.70367363]. \t  0.5995147367720821 \t 3.8388737174774157\n",
            "64     \t [0.27565291 0.35679019 0.68211396]. \t  1.5721358806728192 \t 3.8388737174774157\n",
            "65     \t [0.50685866 0.05015652 0.57968208]. \t  0.1581913686140206 \t 3.8388737174774157\n",
            "66     \t [0.21613274 0.78748947 0.61979404]. \t  2.752520587780271 \t 3.8388737174774157\n",
            "67     \t [0.97683169 0.98537602 0.47034241]. \t  0.22879582575819743 \t 3.8388737174774157\n",
            "68     \t [0.8247913  0.00865746 0.77607339]. \t  0.2671791607087722 \t 3.8388737174774157\n",
            "69     \t [0.6154278  0.16102298 0.72380069]. \t  0.7868765266505973 \t 3.8388737174774157\n",
            "70     \t [0.44639332 0.52624246 0.94752527]. \t  2.9776268007255116 \t 3.8388737174774157\n",
            "71     \t [0.35265186 0.07195568 0.39129923]. \t  0.622943735926077 \t 3.8388737174774157\n",
            "72     \t [0.01915566 0.7514324  0.80350619]. \t  2.641516303394991 \t 3.8388737174774157\n",
            "73     \t [0.31166647 0.75193678 0.43966625]. \t  1.5656143444144268 \t 3.8388737174774157\n",
            "74     \t [0.29050797 0.99745077 0.38613176]. \t  0.9939936990149293 \t 3.8388737174774157\n",
            "75     \t [0.17449207 0.38170487 0.29543257]. \t  0.4687919094460895 \t 3.8388737174774157\n",
            "76     \t [0.18262366 0.40234101 0.78174162]. \t  2.853310454318601 \t 3.8388737174774157\n",
            "77     \t [0.23326724 0.12923415 0.21844802]. \t  0.8800126024126476 \t 3.8388737174774157\n",
            "78     \t [0.76291281 0.1939433  0.15790554]. \t  0.4132222812659086 \t 3.8388737174774157\n",
            "79     \t [0.09675512 0.75527026 0.79408307]. \t  2.6065132144451044 \t 3.8388737174774157\n",
            "80     \t [0.32685191 0.0862388  0.628197  ]. \t  0.2658594374175428 \t 3.8388737174774157\n",
            "81     \t [0.26988867 0.88735757 0.70824977]. \t  1.9302015782523108 \t 3.8388737174774157\n",
            "82     \t [0.84299487 0.1543878  0.18070156]. \t  0.4012743622864063 \t 3.8388737174774157\n",
            "83     \t [0.41130472 0.07171761 0.33969365]. \t  0.8345360731124587 \t 3.8388737174774157\n",
            "84     \t [7.35487395e-04 7.71143033e-01 7.78163031e-01]. \t  2.4185376828203515 \t 3.8388737174774157\n",
            "85     \t [0.27421342 0.3677449  0.62104297]. \t  1.042391069581968 \t 3.8388737174774157\n",
            "86     \t [0.02541217 0.25237601 0.45267658]. \t  0.29580436684080935 \t 3.8388737174774157\n",
            "87     \t [0.44317316 0.12260709 0.52398733]. \t  0.22575510537096177 \t 3.8388737174774157\n",
            "88     \t [0.57293859 0.99880182 0.4018766 ]. \t  0.6680010837087829 \t 3.8388737174774157\n",
            "89     \t [0.8022128  0.10624647 0.516911  ]. \t  0.15447209300219936 \t 3.8388737174774157\n",
            "90     \t [0.29358989 0.02299719 0.73290056]. \t  0.2828304241174419 \t 3.8388737174774157\n",
            "91     \t [0.54505805 0.52661682 0.47490715]. \t  0.5606793615622767 \t 3.8388737174774157\n",
            "92     \t [0.77323979 0.16021315 0.81059028]. \t  0.9486529552671926 \t 3.8388737174774157\n",
            "93     \t [0.45302664 0.21908675 0.15855595]. \t  0.618922530291222 \t 3.8388737174774157\n",
            "94     \t [0.82084058 0.40124898 0.96684758]. \t  2.0325833187159366 \t 3.8388737174774157\n",
            "95     \t [0.09455294 0.52870413 0.13348204]. \t  0.0900712093873404 \t 3.8388737174774157\n",
            "96     \t [0.44929156 0.89821783 0.27772   ]. \t  0.21315354397159983 \t 3.8388737174774157\n",
            "97     \t [0.28590341 0.55800049 0.51488452]. \t  1.1719963153531516 \t 3.8388737174774157\n",
            "98     \t [0.11525554 0.82021185 0.11016824]. \t  0.010565660616481393 \t 3.8388737174774157\n",
            "99     \t [0.13037669 0.19765145 0.80752148]. \t  1.2195795915290009 \t 3.8388737174774157\n",
            "100    \t [0.35073261 0.92595667 0.04860803]. \t  0.0014692202765782152 \t 3.8388737174774157\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fh_Amb8TtdgO",
        "outputId": "61829a04-114a-40cf-e875-6e7e79905d44"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 6\r\n",
        "\r\n",
        "np.random.seed(run_num_6)\r\n",
        "surrogate_loser_6 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "loser_6 = dGPGO(surrogate_loser_6, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_6.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.89286015 0.33197981 0.82122912]. \t  2.3879127825536575 \t 2.5106636917702634\n",
            "init   \t [0.04169663 0.10765668 0.59505206]. \t  0.2321934630074273 \t 2.5106636917702634\n",
            "init   \t [0.52981736 0.41880743 0.33540785]. \t  0.37989557867087576 \t 2.5106636917702634\n",
            "init   \t [0.62251943 0.43814143 0.73588211]. \t  2.5106636917702634 \t 2.5106636917702634\n",
            "init   \t [0.51803641 0.5788586  0.6453551 ]. \t  1.7289203948034264 \t 2.5106636917702634\n",
            "1      \t [0.62339102 0.39276721 0.97743955]. \t  1.8557550530508597 \t 2.5106636917702634\n",
            "2      \t [0.80698089 0.27589896 0.64454705]. \t  0.8287846757566074 \t 2.5106636917702634\n",
            "3      \t [0.97311854 0.60304551 0.92068359]. \t  \u001b[92m3.2276797271364055\u001b[0m \t 3.2276797271364055\n",
            "4      \t [0.89264006 0.58864963 0.95652948]. \t  2.776092518207796 \t 3.2276797271364055\n",
            "5      \t [0.96507948 0.60085005 0.82529255]. \t  \u001b[92m3.479322232477214\u001b[0m \t 3.479322232477214\n",
            "6      \t [0.95842992 0.48714277 0.81520669]. \t  3.4274222523101296 \t 3.479322232477214\n",
            "7      \t [0.95032843 0.60164973 0.75457865]. \t  2.678691870274754 \t 3.479322232477214\n",
            "8      \t [6.25995720e-15 2.26575588e-01 3.35697554e-14]. \t  0.06913143722665804 \t 3.479322232477214\n",
            "9      \t [0.23552488 0.16108274 0.03001412]. \t  0.17171654141876627 \t 3.479322232477214\n",
            "10     \t [0.12009714 0.6391512  0.00835189]. \t  0.007495681057260108 \t 3.479322232477214\n",
            "11     \t [0.9954704  0.56038921 0.8728146 ]. \t  \u001b[92m3.6483342701470103\u001b[0m \t 3.6483342701470103\n",
            "12     \t [0.01241981 0.45723369 0.89092866]. \t  3.3307523216977133 \t 3.6483342701470103\n",
            "13     \t [0.04797017 0.4609164  0.39606893]. \t  0.40804886814642155 \t 3.6483342701470103\n",
            "14     \t [0.08973749 0.02338467 0.97294633]. \t  0.15008313337918777 \t 3.6483342701470103\n",
            "15     \t [0.20653029 0.466105   0.94698721]. \t  2.7539848275938774 \t 3.6483342701470103\n",
            "16     \t [0.16387253 0.7650266  0.99087908]. \t  1.5307376242457789 \t 3.6483342701470103\n",
            "17     \t [0.26109554 0.84873653 0.17389629]. \t  0.039416154328806276 \t 3.6483342701470103\n",
            "18     \t [0.33709083 0.72144137 0.97784123]. \t  1.9710686570291658 \t 3.6483342701470103\n",
            "19     \t [0.07393115 0.24618375 0.99962104]. \t  0.7574894770758875 \t 3.6483342701470103\n",
            "20     \t [0.53871646 0.41225454 0.00120782]. \t  0.045869330604918386 \t 3.6483342701470103\n",
            "21     \t [0.22836016 0.89651158 0.89707304]. \t  1.2740290385689355 \t 3.6483342701470103\n",
            "22     \t [0.72328122 0.78476196 0.91791194]. \t  2.0917540572107587 \t 3.6483342701470103\n",
            "23     \t [0.00668549 0.48718495 0.84904449]. \t  \u001b[92m3.6560653766697033\u001b[0m \t 3.6560653766697033\n",
            "24     \t [0.80771617 0.77253684 0.56420031]. \t  0.8074089229060448 \t 3.6560653766697033\n",
            "25     \t [0.82280098 0.98264258 0.03503644]. \t  0.00023447364217242127 \t 3.6560653766697033\n",
            "26     \t [0.58890641 0.51674972 0.27519758]. \t  0.21561347145640256 \t 3.6560653766697033\n",
            "27     \t [0.61004954 0.97795743 0.5380338 ]. \t  1.2796657235618687 \t 3.6560653766697033\n",
            "28     \t [0.50645476 0.26659896 0.96461242]. \t  1.1510964734131832 \t 3.6560653766697033\n",
            "29     \t [5.34059538e-04 5.76547255e-01 7.78606268e-01]. \t  3.361170728423199 \t 3.6560653766697033\n",
            "30     \t [0.07455069 0.71285656 0.57962799]. \t  2.5839320858154777 \t 3.6560653766697033\n",
            "31     \t [0.94033288 0.99246043 0.72248221]. \t  0.41378534840136605 \t 3.6560653766697033\n",
            "32     \t [0.12975113 0.99882257 0.45191089]. \t  1.8670432475471535 \t 3.6560653766697033\n",
            "33     \t [0.33510451 0.26649245 0.92074688]. \t  1.4902139984299103 \t 3.6560653766697033\n",
            "34     \t [0.69362147 0.34044285 0.45542876]. \t  0.2582487569586175 \t 3.6560653766697033\n",
            "35     \t [0.27408507 0.37107772 0.1495041 ]. \t  0.33823696322386365 \t 3.6560653766697033\n",
            "36     \t [0.02756341 0.63338022 0.79994563]. \t  3.402608583174604 \t 3.6560653766697033\n",
            "37     \t [0.82915108 0.43047211 0.72997315]. \t  2.3337490349988457 \t 3.6560653766697033\n",
            "38     \t [0.00883835 0.6013543  0.83189179]. \t  \u001b[92m3.7006348348705727\u001b[0m \t 3.7006348348705727\n",
            "39     \t [0.11506585 0.08818824 0.01786167]. \t  0.1264166391740304 \t 3.7006348348705727\n",
            "40     \t [0.61125555 0.20673819 0.86929837]. \t  1.2342396193127003 \t 3.7006348348705727\n",
            "41     \t [0.11552913 0.937178   0.91319381]. \t  0.9229498004302739 \t 3.7006348348705727\n",
            "42     \t [0.73894287 0.33413814 0.6103939 ]. \t  0.7455446737025775 \t 3.7006348348705727\n",
            "43     \t [0.58721719 0.38510632 0.8374942 ]. \t  2.96017619594543 \t 3.7006348348705727\n",
            "44     \t [0.2924251  0.24847362 0.29458142]. \t  0.8162192034311126 \t 3.7006348348705727\n",
            "45     \t [0.99500173 0.54386195 0.87292778]. \t  3.6473823664605467 \t 3.7006348348705727\n",
            "46     \t [0.0131697  0.60408464 0.87412372]. \t  3.694936841153166 \t 3.7006348348705727\n",
            "47     \t [0.95946823 0.76273733 0.74217599]. \t  1.6013158015827202 \t 3.7006348348705727\n",
            "48     \t [0.00768901 0.57624884 0.45984971]. \t  1.006739146423282 \t 3.7006348348705727\n",
            "49     \t [0.98573139 0.35559012 0.01564991]. \t  0.027040927284828598 \t 3.7006348348705727\n",
            "50     \t [0.43681923 0.26771217 0.525546  ]. \t  0.3357713031640264 \t 3.7006348348705727\n",
            "51     \t [0.84650032 0.32968555 0.67756352]. \t  1.3030194973795366 \t 3.7006348348705727\n",
            "52     \t [0.29116669 0.1959375  0.57236862]. \t  0.33835299363392896 \t 3.7006348348705727\n",
            "53     \t [0.49086338 0.2402136  0.66310662]. \t  0.8580888362708491 \t 3.7006348348705727\n",
            "54     \t [0.67174643 0.13687066 0.29850672]. \t  0.7358259286026887 \t 3.7006348348705727\n",
            "55     \t [0.90824711 0.51686926 0.94068471]. \t  2.9773967891772 \t 3.7006348348705727\n",
            "56     \t [0.96313273 0.38871968 0.58126843]. \t  0.56783893931115 \t 3.7006348348705727\n",
            "57     \t [0.47678022 0.58904368 0.60675061]. \t  1.527985140761224 \t 3.7006348348705727\n",
            "58     \t [0.61939634 0.49537322 0.21904615]. \t  0.19585169709126193 \t 3.7006348348705727\n",
            "59     \t [0.00256339 0.6429676  0.81229958]. \t  3.4281474633316895 \t 3.7006348348705727\n",
            "60     \t [0.00883055 0.42079761 0.36488539]. \t  0.33722903429285517 \t 3.7006348348705727\n",
            "61     \t [0.89311458 0.0260461  0.41271687]. \t  0.21862146592906784 \t 3.7006348348705727\n",
            "62     \t [0.75365534 0.64361142 0.56627093]. \t  0.8495271886852758 \t 3.7006348348705727\n",
            "63     \t [0.10353124 0.02964342 0.15018766]. \t  0.4970848309262672 \t 3.7006348348705727\n",
            "64     \t [0.38652703 0.09628612 0.01688402]. \t  0.15160735034832432 \t 3.7006348348705727\n",
            "65     \t [0.90753459 0.45836172 0.5257814 ]. \t  0.3378386372294143 \t 3.7006348348705727\n",
            "66     \t [0.97786691 0.23969996 0.30313812]. \t  0.2737430610896056 \t 3.7006348348705727\n",
            "67     \t [0.2985376  0.45515157 0.72666153]. \t  2.5519283059711437 \t 3.7006348348705727\n",
            "68     \t [0.71962825 0.37640665 0.92594521]. \t  2.3788782644785234 \t 3.7006348348705727\n",
            "69     \t [0.91095124 0.26026786 0.08100662]. \t  0.11910272542970807 \t 3.7006348348705727\n",
            "70     \t [0.2007796  0.40410126 0.04414278]. \t  0.09057280874521183 \t 3.7006348348705727\n",
            "71     \t [0.98066823 0.17618395 0.30887276]. \t  0.29940407941322894 \t 3.7006348348705727\n",
            "72     \t [0.26486167 0.03322341 0.3011884 ]. \t  0.8724455649378886 \t 3.7006348348705727\n",
            "73     \t [0.19341254 0.9115757  0.8193853 ]. \t  1.3477910671526903 \t 3.7006348348705727\n",
            "74     \t [0.85383639 0.303693   0.58484458]. \t  0.4983868149187136 \t 3.7006348348705727\n",
            "75     \t [0.11261631 0.64295778 0.35291107]. \t  0.5654453165628954 \t 3.7006348348705727\n",
            "76     \t [0.25255531 0.58296163 0.5355288 ]. \t  1.4636644975046686 \t 3.7006348348705727\n",
            "77     \t [0.69044902 0.1928956  0.86895197]. \t  1.1234012728940739 \t 3.7006348348705727\n",
            "78     \t [0.06808589 0.44438348 0.31510407]. \t  0.33025696989363273 \t 3.7006348348705727\n",
            "79     \t [0.1535016  0.47575467 0.38377442]. \t  0.4277339308242742 \t 3.7006348348705727\n",
            "80     \t [0.88950651 0.15575891 0.5753877 ]. \t  0.23678865995367532 \t 3.7006348348705727\n",
            "81     \t [0.60563334 0.94996264 0.53201211]. \t  1.357208075848746 \t 3.7006348348705727\n",
            "82     \t [0.03640684 0.90685607 0.13933935]. \t  0.017359262999842685 \t 3.7006348348705727\n",
            "83     \t [0.83622549 0.00479024 0.15238126]. \t  0.30811922728605484 \t 3.7006348348705727\n",
            "84     \t [0.03047936 0.45130896 0.09831493]. \t  0.0994418319705768 \t 3.7006348348705727\n",
            "85     \t [0.56096791 0.42054039 0.53609475]. \t  0.5375442750723753 \t 3.7006348348705727\n",
            "86     \t [0.69089327 0.41434753 0.52576339]. \t  0.41222705943601856 \t 3.7006348348705727\n",
            "87     \t [0.33303682 0.85963986 0.88246611]. \t  1.6270249322213164 \t 3.7006348348705727\n",
            "88     \t [0.76694449 0.44906852 0.62817908]. \t  1.1229951985849844 \t 3.7006348348705727\n",
            "89     \t [0.38506973 0.2254537  0.89158306]. \t  1.3124017684339946 \t 3.7006348348705727\n",
            "90     \t [0.94760925 0.81277586 0.24814905]. \t  0.02384324621445827 \t 3.7006348348705727\n",
            "91     \t [0.22321952 0.85183336 0.71042249]. \t  2.116278041457136 \t 3.7006348348705727\n",
            "92     \t [0.63232264 0.79212972 0.83959709]. \t  2.2158432287888115 \t 3.7006348348705727\n",
            "93     \t [0.89311012 0.27852264 0.10452753]. \t  0.15260228430094222 \t 3.7006348348705727\n",
            "94     \t [0.65225194 0.56959118 0.85237454]. \t  \u001b[92m3.7923072368609834\u001b[0m \t 3.7923072368609834\n",
            "95     \t [0.45988746 0.15648535 0.31912992]. \t  0.889225446032996 \t 3.7923072368609834\n",
            "96     \t [0.91897039 0.23558463 0.23921164]. \t  0.34277234372304527 \t 3.7923072368609834\n",
            "97     \t [0.84404778 0.5475815  0.87225978]. \t  3.714655484967224 \t 3.7923072368609834\n",
            "98     \t [0.22581407 0.36439774 0.63953126]. \t  1.194698309876963 \t 3.7923072368609834\n",
            "99     \t [0.36129501 0.29655172 0.46130896]. \t  0.3620060373190372 \t 3.7923072368609834\n",
            "100    \t [0.21910047 0.55932934 0.67552251]. \t  2.2680994506603955 \t 3.7923072368609834\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XNmoDTuUtdi3",
        "outputId": "fbbd8acf-2bef-452e-dcde-1d226517c170"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 7\r\n",
        "\r\n",
        "np.random.seed(run_num_7)\r\n",
        "surrogate_loser_7 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "loser_7 = dGPGO(surrogate_loser_7, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_7.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.15266373 0.30235661 0.06203641]. \t  0.1742288077468587 \t 1.6237282255098657\n",
            "init   \t [0.45986034 0.83525338 0.92699705]. \t  1.6237282255098657 \t 1.6237282255098657\n",
            "init   \t [0.72698898 0.76849622 0.26920507]. \t  0.08405715787489784 \t 1.6237282255098657\n",
            "init   \t [0.64402929 0.09337326 0.07968589]. \t  0.2756481743251193 \t 1.6237282255098657\n",
            "init   \t [0.58961375 0.34334054 0.98887615]. \t  1.4020548914621052 \t 1.6237282255098657\n",
            "1      \t [0.32658881 0.90655956 0.99955954]. \t  0.6793796294845356 \t 1.6237282255098657\n",
            "2      \t [0.97122775 0.93199907 0.87171651]. \t  0.9308657054768225 \t 1.6237282255098657\n",
            "3      \t [0.57961628 0.58266912 0.79855763]. \t  \u001b[92m3.483788917709367\u001b[0m \t 3.483788917709367\n",
            "4      \t [0.49520411 0.3912425  0.52481494]. \t  0.4786781247129547 \t 3.483788917709367\n",
            "5      \t [0.96188519 0.49079304 0.94964007]. \t  2.739322132683216 \t 3.483788917709367\n",
            "6      \t [0.98059547 0.6421952  0.42555104]. \t  0.1417294677187748 \t 3.483788917709367\n",
            "7      \t [0.70280052 0.55033441 0.98704683]. \t  2.2942076552340476 \t 3.483788917709367\n",
            "8      \t [0.068876   0.71925152 0.90263977]. \t  2.8258711843250186 \t 3.483788917709367\n",
            "9      \t [0.73690732 0.60265172 0.72918553]. \t  2.450488476072661 \t 3.483788917709367\n",
            "10     \t [0.31914295 0.52209303 0.87566286]. \t  \u001b[92m3.7677622256952326\u001b[0m \t 3.7677622256952326\n",
            "11     \t [0.3557968  0.55817769 0.83931655]. \t  \u001b[92m3.839718943012042\u001b[0m \t 3.839718943012042\n",
            "12     \t [0.93922254 0.81747143 0.05364584]. \t  0.0009060527890463743 \t 3.839718943012042\n",
            "13     \t [0.37994118 0.54553686 0.0404556 ]. \t  0.03432358159356053 \t 3.839718943012042\n",
            "14     \t [0.43973222 0.06629741 0.00092467]. \t  0.11424263638089514 \t 3.839718943012042\n",
            "15     \t [0.25314001 0.65919145 0.90377159]. \t  3.2840777187020302 \t 3.839718943012042\n",
            "16     \t [0.88588849 0.01225523 0.05902045]. \t  0.10937692698621664 \t 3.839718943012042\n",
            "17     \t [0.82268541 0.16811429 0.02726336]. \t  0.0932560464366292 \t 3.839718943012042\n",
            "18     \t [0.13342852 0.97479596 0.06696674]. \t  0.0023106132522125286 \t 3.839718943012042\n",
            "19     \t [0.08166382 0.99937123 0.83049291]. \t  0.7736758598017694 \t 3.839718943012042\n",
            "20     \t [0.98661072 0.93167181 0.26349885]. \t  0.023051931611116974 \t 3.839718943012042\n",
            "21     \t [0.00289244 0.94091602 0.01974607]. \t  0.0006374894819092091 \t 3.839718943012042\n",
            "22     \t [0.58330803 0.98589533 0.04048264]. \t  0.0005808995806584593 \t 3.839718943012042\n",
            "23     \t [0.99982028 0.68388686 0.92351034]. \t  2.7955600260423945 \t 3.839718943012042\n",
            "24     \t [0.05911271 0.55455605 0.78651416]. \t  3.472572086525786 \t 3.839718943012042\n",
            "25     \t [0.45611335 0.03406024 0.61887374]. \t  0.1704663937816772 \t 3.839718943012042\n",
            "26     \t [0.04753001 0.21858082 0.71403751]. \t  1.0492607520494066 \t 3.839718943012042\n",
            "27     \t [0.83943545 0.77914392 0.08044774]. \t  0.0029026804888624066 \t 3.839718943012042\n",
            "28     \t [0.20846012 0.53348377 0.51992205]. \t  1.1202841961222842 \t 3.839718943012042\n",
            "29     \t [0.38151606 0.64937266 0.67849598]. \t  2.2736534531887607 \t 3.839718943012042\n",
            "30     \t [0.40986174 0.49504099 0.8598248 ]. \t  3.727721661366961 \t 3.839718943012042\n",
            "31     \t [0.66666872 0.9879886  0.97856614]. \t  0.4295463085574229 \t 3.839718943012042\n",
            "32     \t [0.41565213 0.97229675 0.87826132]. \t  0.7827205261169002 \t 3.839718943012042\n",
            "33     \t [0.44280206 0.63711349 0.86091196]. \t  3.611710810406915 \t 3.839718943012042\n",
            "34     \t [0.92294047 0.83548231 0.27643884]. \t  0.042120310718118566 \t 3.839718943012042\n",
            "35     \t [0.29234884 0.38878995 0.02698622]. \t  0.08307067463903102 \t 3.839718943012042\n",
            "36     \t [0.62391776 0.3903563  0.68428808]. \t  1.6717627713433323 \t 3.839718943012042\n",
            "37     \t [0.1908268  0.52539051 0.95233098]. \t  2.8886375190571583 \t 3.839718943012042\n",
            "38     \t [0.0497901  0.70348162 0.40504456]. \t  1.1607301643572914 \t 3.839718943012042\n",
            "39     \t [0.28167736 0.09390389 0.82019296]. \t  0.5800442303293563 \t 3.839718943012042\n",
            "40     \t [0.71455558 0.69017883 0.69063379]. \t  1.73744604695138 \t 3.839718943012042\n",
            "41     \t [0.0498949  0.78122685 0.55605748]. \t  2.879384773660573 \t 3.839718943012042\n",
            "42     \t [0.45781108 0.26174588 0.94199186]. \t  1.2974802200354303 \t 3.839718943012042\n",
            "43     \t [0.80913553 0.35598418 0.04574654]. \t  0.07245009830167098 \t 3.839718943012042\n",
            "44     \t [0.39885319 0.30427766 0.15303229]. \t  0.4753994581941886 \t 3.839718943012042\n",
            "45     \t [0.83497665 0.25408047 0.74194063]. \t  1.4079264679802885 \t 3.839718943012042\n",
            "46     \t [0.0013753  0.68001559 0.7350727 ]. \t  2.7183458351778986 \t 3.839718943012042\n",
            "47     \t [0.22768617 0.54106823 0.82321837]. \t  3.7748914228213573 \t 3.839718943012042\n",
            "48     \t [0.86384956 0.12288378 0.74114512]. \t  0.6383704123542232 \t 3.839718943012042\n",
            "49     \t [0.79372708 0.35345592 0.53009712]. \t  0.32798013380319835 \t 3.839718943012042\n",
            "50     \t [0.97791284 0.38396962 0.41962348]. \t  0.1246402598256812 \t 3.839718943012042\n",
            "51     \t [0.64270236 0.75777217 0.68904066]. \t  1.6447147713498858 \t 3.839718943012042\n",
            "52     \t [0.59139013 0.74227825 0.5157312 ]. \t  1.2993767437115655 \t 3.839718943012042\n",
            "53     \t [0.0323653 0.3292039 0.1426402]. \t  0.2856535148959515 \t 3.839718943012042\n",
            "54     \t [0.14383193 0.24141099 0.57065719]. \t  0.40695734141747664 \t 3.839718943012042\n",
            "55     \t [0.89946978 0.08606762 0.85056182]. \t  0.5104618044370799 \t 3.839718943012042\n",
            "56     \t [0.02531627 0.98529951 0.2911975 ]. \t  0.32308348826062566 \t 3.839718943012042\n",
            "57     \t [0.34184268 0.52468794 0.68221237]. \t  2.1899374017129496 \t 3.839718943012042\n",
            "58     \t [0.8893635  0.6882076  0.46426269]. \t  0.3181762541715076 \t 3.839718943012042\n",
            "59     \t [0.24686786 0.27282795 0.58364829]. \t  0.5249811572852917 \t 3.839718943012042\n",
            "60     \t [0.70830618 0.04660946 0.32106337]. \t  0.6183033089960998 \t 3.839718943012042\n",
            "61     \t [0.65447025 0.27748361 0.42481929]. \t  0.33403303258545897 \t 3.839718943012042\n",
            "62     \t [0.37683677 0.11525415 0.83630861]. \t  0.6800501825362579 \t 3.839718943012042\n",
            "63     \t [0.6491765  0.27384865 0.4162992 ]. \t  0.3572171488569962 \t 3.839718943012042\n",
            "64     \t [0.56966596 0.46622878 0.01058221]. \t  0.036280012296470564 \t 3.839718943012042\n",
            "65     \t [0.45698753 0.27685337 0.04188856]. \t  0.16480651689762704 \t 3.839718943012042\n",
            "66     \t [0.78088958 0.25464311 0.24869187]. \t  0.49327180578225444 \t 3.839718943012042\n",
            "67     \t [0.61253894 0.36137669 0.63838783]. \t  1.0866471780065128 \t 3.839718943012042\n",
            "68     \t [0.00981779 0.00824872 0.03410437]. \t  0.11806847800629534 \t 3.839718943012042\n",
            "69     \t [0.598202   0.82681672 0.4747132 ]. \t  1.208649785030718 \t 3.839718943012042\n",
            "70     \t [0.46091194 0.43140288 0.24909247]. \t  0.377258888183895 \t 3.839718943012042\n",
            "71     \t [0.30980585 0.49272807 0.36154795]. \t  0.3955171481806316 \t 3.839718943012042\n",
            "72     \t [0.96726563 0.89399728 0.96826019]. \t  0.8901370600589371 \t 3.839718943012042\n",
            "73     \t [0.94643135 0.6245116  0.67643341]. \t  1.505106316787355 \t 3.839718943012042\n",
            "74     \t [0.60589302 0.39114068 0.79202229]. \t  2.8228352062288518 \t 3.839718943012042\n",
            "75     \t [0.82920723 0.53231563 0.33423074]. \t  0.13146137802733474 \t 3.839718943012042\n",
            "76     \t [0.40343961 0.54806134 0.28058575]. \t  0.23948447099976833 \t 3.839718943012042\n",
            "77     \t [0.69097311 0.11912294 0.61050187]. \t  0.27857093327605525 \t 3.839718943012042\n",
            "78     \t [0.25755065 0.94546291 0.61372001]. \t  2.5138857218938946 \t 3.839718943012042\n",
            "79     \t [0.09052689 0.51850689 0.63509928]. \t  1.7795558465731527 \t 3.839718943012042\n",
            "80     \t [0.08479463 0.43788564 0.85789907]. \t  3.3742250385770323 \t 3.839718943012042\n",
            "81     \t [0.70973134 0.52561147 0.93043662]. \t  3.200528830327633 \t 3.839718943012042\n",
            "82     \t [0.07887098 0.62444953 0.75322688]. \t  3.047981909596093 \t 3.839718943012042\n",
            "83     \t [0.43462338 0.85841275 0.40296966]. \t  1.097896948788411 \t 3.839718943012042\n",
            "84     \t [0.07294121 0.01268299 0.75749395]. \t  0.27074762988187523 \t 3.839718943012042\n",
            "85     \t [0.44020526 0.37645287 0.00968987]. \t  0.06863582851943462 \t 3.839718943012042\n",
            "86     \t [0.96569043 0.0641876  0.43360957]. \t  0.15547161313291824 \t 3.839718943012042\n",
            "87     \t [0.9198246  0.75940701 0.3705377 ]. \t  0.14052092081423212 \t 3.839718943012042\n",
            "88     \t [0.22721022 0.26302799 0.52202655]. \t  0.3376790780837995 \t 3.839718943012042\n",
            "89     \t [0.49843932 0.33475206 0.39853827]. \t  0.4193223537377658 \t 3.839718943012042\n",
            "90     \t [0.88988416 0.29059567 0.72660669]. \t  1.5339975868716995 \t 3.839718943012042\n",
            "91     \t [0.72225574 0.33315413 0.03442952]. \t  0.08471335964138589 \t 3.839718943012042\n",
            "92     \t [0.43095451 0.12771229 0.9532665 ]. \t  0.46890119473138564 \t 3.839718943012042\n",
            "93     \t [0.89322154 0.53536312 0.32746133]. \t  0.10282724257581978 \t 3.839718943012042\n",
            "94     \t [0.77737971 0.27780371 0.64434659]. \t  0.8377082090623511 \t 3.839718943012042\n",
            "95     \t [0.35604936 0.29752458 0.55250137]. \t  0.44763709345367153 \t 3.839718943012042\n",
            "96     \t [0.30570847 0.99814714 0.78796821]. \t  0.8833022204035772 \t 3.839718943012042\n",
            "97     \t [0.00304443 0.23869801 0.80155107]. \t  1.5294429905123614 \t 3.839718943012042\n",
            "98     \t [0.23894336 0.95513749 0.52767956]. \t  2.6336435430210194 \t 3.839718943012042\n",
            "99     \t [0.96334931 0.53329215 0.96711868]. \t  2.5687415955445196 \t 3.839718943012042\n",
            "100    \t [0.07733048 0.16211484 0.24032112]. \t  0.7439261134149052 \t 3.839718943012042\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_21yVprtdlu",
        "outputId": "95cd3832-e842-4144-c0bb-2d207923dca7"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 8\r\n",
        "\r\n",
        "np.random.seed(run_num_8)\r\n",
        "surrogate_loser_8 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "loser_8 = dGPGO(surrogate_loser_8, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_8.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.10949987 0.04221501 0.59969258]. \t  0.15732379460832624 \t 0.8830091449513892\n",
            "init   \t [0.29987071 0.79661178 0.36619613]. \t  0.8830091449513892 \t 0.8830091449513892\n",
            "init   \t [0.88060834 0.29784163 0.32910326]. \t  0.2992873424905509 \t 0.8830091449513892\n",
            "init   \t [0.56871692 0.74392742 0.05385289]. \t  0.0051668955191694075 \t 0.8830091449513892\n",
            "init   \t [0.60207437 0.42931858 0.20547361]. \t  0.2908249205255991 \t 0.8830091449513892\n",
            "1      \t [0.19816249 0.92572195 0.98825083]. \t  0.653164991364588 \t 0.8830091449513892\n",
            "2      \t [0.03762084 0.88738442 0.32955494]. \t  0.6464733859302134 \t 0.8830091449513892\n",
            "3      \t [0.58539768 0.712554   0.72501424]. \t  \u001b[92m2.1494445934413724\u001b[0m \t 2.1494445934413724\n",
            "4      \t [0.99676486 0.73834616 0.85474135]. \t  \u001b[92m2.622597831543382\u001b[0m \t 2.622597831543382\n",
            "5      \t [0.93909608 0.58044829 0.98333517]. \t  2.313423464876202 \t 2.622597831543382\n",
            "6      \t [0.94032455 0.97516964 0.92680391]. \t  0.6077146441222263 \t 2.622597831543382\n",
            "7      \t [0.94991741 0.71609051 0.64858771]. \t  0.974460058395742 \t 2.622597831543382\n",
            "8      \t [0.10619384 0.57838683 0.78637684]. \t  \u001b[92m3.4676199411224444\u001b[0m \t 3.4676199411224444\n",
            "9      \t [0.00761835 0.5482665  0.78027156]. \t  3.3916277796569814 \t 3.4676199411224444\n",
            "10     \t [0.08678321 0.5568385  0.82576685]. \t  \u001b[92m3.7716425791092787\u001b[0m \t 3.7716425791092787\n",
            "11     \t [0.00807206 0.50652397 0.98226247]. \t  2.2968166284834375 \t 3.7716425791092787\n",
            "12     \t [0.08418942 0.38948961 0.79376446]. \t  2.8290180552533193 \t 3.7716425791092787\n",
            "13     \t [0.65458059 0.09756017 0.08878127]. \t  0.29979025409772375 \t 3.7716425791092787\n",
            "14     \t [0.37753901 0.9214998  0.04799899]. \t  0.0014317586025206425 \t 3.7716425791092787\n",
            "15     \t [0.9853598  0.31563753 0.06335115]. \t  0.061893833647541675 \t 3.7716425791092787\n",
            "16     \t [0.97845074 0.84655631 0.27202393]. \t  0.029757767403704203 \t 3.7716425791092787\n",
            "17     \t [0.01392183 0.91419384 0.71602659]. \t  1.8091533287402832 \t 3.7716425791092787\n",
            "18     \t [0.26053791 0.66587223 0.91009376]. \t  3.185003706979344 \t 3.7716425791092787\n",
            "19     \t [0.37153686 0.9801946  0.63072668]. \t  1.9360855314429737 \t 3.7716425791092787\n",
            "20     \t [0.64564286 0.78688721 0.88130829]. \t  2.2592397773222395 \t 3.7716425791092787\n",
            "21     \t [0.26806168 0.92072454 0.57991583]. \t  2.7504536542836218 \t 3.7716425791092787\n",
            "22     \t [0.99969474 0.59475775 0.17639792]. \t  0.02589209249577102 \t 3.7716425791092787\n",
            "23     \t [0.92705413 0.12267879 0.87792272]. \t  0.6413473884501208 \t 3.7716425791092787\n",
            "24     \t [0.54587708 0.31433642 0.57322102]. \t  0.5247106459907515 \t 3.7716425791092787\n",
            "25     \t [0.21076196 0.77023363 0.76059542]. \t  2.416597308428657 \t 3.7716425791092787\n",
            "26     \t [0.01110837 0.99382329 0.37453144]. \t  0.9523950007537811 \t 3.7716425791092787\n",
            "27     \t [0. 0. 0.]. \t  0.06797411659013229 \t 3.7716425791092787\n",
            "28     \t [0.28722319 0.60847062 0.00676597]. \t  0.011592754208355942 \t 3.7716425791092787\n",
            "29     \t [0.14683982 0.04030608 0.72601804]. \t  0.3195768296436863 \t 3.7716425791092787\n",
            "30     \t [0.6154916  0.22834525 0.33458037]. \t  0.649724837300166 \t 3.7716425791092787\n",
            "31     \t [0.69742778 0.02048288 0.70169544]. \t  0.2427294149060997 \t 3.7716425791092787\n",
            "32     \t [0.80518828 0.21820939 0.23047591]. \t  0.4900798351740443 \t 3.7716425791092787\n",
            "33     \t [0.49180021 0.67289655 0.09640609]. \t  0.020477956936715228 \t 3.7716425791092787\n",
            "34     \t [0.18869189 0.24753615 0.03695648]. \t  0.15575914337109642 \t 3.7716425791092787\n",
            "35     \t [0.2574242  0.34725895 0.79140007]. \t  2.4829744875783115 \t 3.7716425791092787\n",
            "36     \t [0.56853046 0.35320536 0.40936881]. \t  0.3551657529796507 \t 3.7716425791092787\n",
            "37     \t [0.45222453 0.34502697 0.26254166]. \t  0.5921939442113907 \t 3.7716425791092787\n",
            "38     \t [0.37352425 0.49742226 0.6706283 ]. \t  1.9661671443461928 \t 3.7716425791092787\n",
            "39     \t [0.33441288 0.55681958 0.67261602]. \t  2.1628781305061278 \t 3.7716425791092787\n",
            "40     \t [0.8265361  0.4549186  0.12056968]. \t  0.08966259473256484 \t 3.7716425791092787\n",
            "41     \t [0.01969536 0.35498542 0.88037469]. \t  2.548617770891581 \t 3.7716425791092787\n",
            "42     \t [0.37671893 0.37852214 0.4858566 ]. \t  0.41712957272792933 \t 3.7716425791092787\n",
            "43     \t [0.94598866 0.17874027 0.78746887]. \t  1.0359074807983142 \t 3.7716425791092787\n",
            "44     \t [0.49561461 0.44173046 0.18714477]. \t  0.27897904173413063 \t 3.7716425791092787\n",
            "45     \t [0.65245155 0.41173725 0.90895695]. \t  2.858627770566837 \t 3.7716425791092787\n",
            "46     \t [0.32817392 0.43930842 0.76576021]. \t  2.93773136755562 \t 3.7716425791092787\n",
            "47     \t [0.47310704 0.73225946 0.77967682]. \t  2.567374504345819 \t 3.7716425791092787\n",
            "48     \t [0.9492419  0.6437365  0.83270298]. \t  3.3208203778497323 \t 3.7716425791092787\n",
            "49     \t [0.77699426 0.9952028  0.02650218]. \t  0.00020481136958916906 \t 3.7716425791092787\n",
            "50     \t [0.63776864 0.13811317 0.10232408]. \t  0.3542316780422566 \t 3.7716425791092787\n",
            "51     \t [0.0226272  0.46039584 0.39306004]. \t  0.39248366552278097 \t 3.7716425791092787\n",
            "52     \t [0.74226553 0.68447382 0.74109946]. \t  2.258201061656978 \t 3.7716425791092787\n",
            "53     \t [0.73671863 0.51163238 0.43451977]. \t  0.2618360570450024 \t 3.7716425791092787\n",
            "54     \t [0.28537774 0.79787034 0.66787496]. \t  2.4079245833904652 \t 3.7716425791092787\n",
            "55     \t [0.52755091 0.87057573 0.11727033]. \t  0.007314436154976636 \t 3.7716425791092787\n",
            "56     \t [0.29346589 0.65910737 0.11163608]. \t  0.029896622802024427 \t 3.7716425791092787\n",
            "57     \t [0.2895177  0.92186079 0.05601142]. \t  0.0019237374875095135 \t 3.7716425791092787\n",
            "58     \t [0.88082954 0.71100467 0.07799923]. \t  0.004986407593590679 \t 3.7716425791092787\n",
            "59     \t [0.73226542 0.65192332 0.47391306]. \t  0.5463663067102064 \t 3.7716425791092787\n",
            "60     \t [0.02333221 0.35878397 0.34093755]. \t  0.38738388979188215 \t 3.7716425791092787\n",
            "61     \t [0.98356167 0.53763874 0.63483676]. \t  1.1181476260591983 \t 3.7716425791092787\n",
            "62     \t [0.31406297 0.02351087 0.46790084]. \t  0.28743459428368334 \t 3.7716425791092787\n",
            "63     \t [0.88684696 0.12634136 0.08895455]. \t  0.17206619872067863 \t 3.7716425791092787\n",
            "64     \t [0.18218602 0.45036847 0.09175881]. \t  0.1184082809363772 \t 3.7716425791092787\n",
            "65     \t [0.34292031 0.68519737 0.09262511]. \t  0.018785249890164495 \t 3.7716425791092787\n",
            "66     \t [0.7317639  0.74224622 0.95671144]. \t  2.077065612238471 \t 3.7716425791092787\n",
            "67     \t [0.87137829 0.5695713  0.62329453]. \t  1.0607784657356756 \t 3.7716425791092787\n",
            "68     \t [0.40862621 0.14201035 0.85863338]. \t  0.8060903991006787 \t 3.7716425791092787\n",
            "69     \t [0.16367498 0.61225246 0.02678576]. \t  0.013724982809582762 \t 3.7716425791092787\n",
            "70     \t [0.22148134 0.37110448 0.85048554]. \t  2.8364247037422916 \t 3.7716425791092787\n",
            "71     \t [0.13958531 0.2856406  0.7196961 ]. \t  1.5031167436047488 \t 3.7716425791092787\n",
            "72     \t [0.07848445 0.86996977 0.15270764]. \t  0.02527200462831359 \t 3.7716425791092787\n",
            "73     \t [0.58561207 0.76780412 0.07594845]. \t  0.0055932062345625125 \t 3.7716425791092787\n",
            "74     \t [0.43456286 0.62971159 0.41523235]. \t  0.7294054153517803 \t 3.7716425791092787\n",
            "75     \t [0.70086648 0.12372512 0.64641762]. \t  0.38204950898303597 \t 3.7716425791092787\n",
            "76     \t [0.91643672 0.5876769  0.04810105]. \t  0.010587036778547476 \t 3.7716425791092787\n",
            "77     \t [0.51723626 0.13900936 0.15596207]. \t  0.6423475702077569 \t 3.7716425791092787\n",
            "78     \t [0.08522595 0.01635146 0.36754278]. \t  0.5270746580713294 \t 3.7716425791092787\n",
            "79     \t [0.78306988 0.08787704 0.37528958]. \t  0.4211244425314013 \t 3.7716425791092787\n",
            "80     \t [0.19974885 0.85121287 0.97113527]. \t  1.1809105868252145 \t 3.7716425791092787\n",
            "81     \t [0.90777519 0.74850197 0.25926154]. \t  0.03547903149967979 \t 3.7716425791092787\n",
            "82     \t [0.1885716  0.12356481 0.45988309]. \t  0.3315845871223555 \t 3.7716425791092787\n",
            "83     \t [0.63074269 0.83889432 0.31506937]. \t  0.23857930741350988 \t 3.7716425791092787\n",
            "84     \t [0.7426569  0.81448563 0.09728448]. \t  0.003764932984621751 \t 3.7716425791092787\n",
            "85     \t [0.15288894 0.84947162 0.48361687]. \t  2.5748938282286176 \t 3.7716425791092787\n",
            "86     \t [0.14127014 0.02953937 0.32421009]. \t  0.7204917512255963 \t 3.7716425791092787\n",
            "87     \t [0.89069861 0.32297623 0.05610986]. \t  0.07585405621148908 \t 3.7716425791092787\n",
            "88     \t [0.89113566 0.81003517 0.79241111]. \t  1.7044650571138988 \t 3.7716425791092787\n",
            "89     \t [0.29976282 0.72376121 0.38408178]. \t  0.9206494684935415 \t 3.7716425791092787\n",
            "90     \t [0.21801703 0.69174281 0.33881151]. \t  0.5459252904266572 \t 3.7716425791092787\n",
            "91     \t [0.32550266 0.96841791 0.94725396]. \t  0.6263481766596568 \t 3.7716425791092787\n",
            "92     \t [0.67032488 0.96302862 0.87442259]. \t  0.7919522796466973 \t 3.7716425791092787\n",
            "93     \t [0.16572149 0.0161577  0.12188675]. \t  0.4232172729959182 \t 3.7716425791092787\n",
            "94     \t [0.31948346 0.05319824 0.01070769]. \t  0.13222931057489118 \t 3.7716425791092787\n",
            "95     \t [0.8957484  0.2366864  0.66129404]. \t  0.7942865476787239 \t 3.7716425791092787\n",
            "96     \t [0.6343747  0.42482422 0.42699233]. \t  0.28859725211452913 \t 3.7716425791092787\n",
            "97     \t [0.09414835 0.47320232 0.48502237]. \t  0.6944438440132831 \t 3.7716425791092787\n",
            "98     \t [0.35682058 0.62212976 0.54162852]. \t  1.5788029312392569 \t 3.7716425791092787\n",
            "99     \t [0.48811289 0.59955038 0.18383557]. \t  0.09066010487259671 \t 3.7716425791092787\n",
            "100    \t [0.52697289 0.4467923  0.3546839 ]. \t  0.34114377715747707 \t 3.7716425791092787\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vkq32q1utdoO",
        "outputId": "b4faedea-d61f-4706-afe0-d7e63ee2c52a"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 9\r\n",
        "\r\n",
        "np.random.seed(run_num_9)\r\n",
        "surrogate_loser_9 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "loser_9 = dGPGO(surrogate_loser_9, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_9.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.67227856 0.4880784  0.82549517]. \t  3.595021899183128 \t 3.595021899183128\n",
            "init   \t [0.03144639 0.80804996 0.56561742]. \t  2.9633561694281085 \t 3.595021899183128\n",
            "init   \t [0.2976225  0.04669572 0.9906274 ]. \t  0.16382388103073592 \t 3.595021899183128\n",
            "init   \t [0.00682573 0.76979303 0.7467671 ]. \t  2.382987807172393 \t 3.595021899183128\n",
            "init   \t [0.37743894 0.49414745 0.92894839]. \t  3.1588932929069533 \t 3.595021899183128\n",
            "1      \t [0.58399395 0.99040527 0.49790057]. \t  1.2164468038749363 \t 3.595021899183128\n",
            "2      \t [0.00254849 0.51933085 0.00362572]. \t  0.01654759503343932 \t 3.595021899183128\n",
            "3      \t [0.74154712 0.47672217 0.94815008]. \t  2.7597734226906323 \t 3.595021899183128\n",
            "4      \t [0.48354376 0.44835907 0.61793077]. \t  1.1987271223179583 \t 3.595021899183128\n",
            "5      \t [0.         0.72798121 0.04117472]. \t  0.004289365160862021 \t 3.595021899183128\n",
            "6      \t [0.6679822  0.90842185 0.81306364]. \t  1.146292548378177 \t 3.595021899183128\n",
            "7      \t [0.91709837 0.66012726 0.65001173]. \t  1.1566740281404257 \t 3.595021899183128\n",
            "8      \t [0.52331729 0.6302517  0.79204039]. \t  3.277489543826432 \t 3.595021899183128\n",
            "9      \t [0.62987244 0.28234509 0.92984965]. \t  1.546418256553609 \t 3.595021899183128\n",
            "10     \t [0.         0.14867332 0.85264187]. \t  0.8427545018626041 \t 3.595021899183128\n",
            "11     \t [0.07632655 0.07933738 0.02896344]. \t  0.1387471600120119 \t 3.595021899183128\n",
            "12     \t [0.03878991 0.9764706  0.3580824 ]. \t  0.8338018431687888 \t 3.595021899183128\n",
            "13     \t [0. 0. 0.]. \t  0.06797411659013229 \t 3.595021899183128\n",
            "14     \t [0.66215534 0.55381474 0.79115333]. \t  3.4151752717333546 \t 3.595021899183128\n",
            "15     \t [0.01365384 0.58729588 0.67588959]. \t  2.3240893231140256 \t 3.595021899183128\n",
            "16     \t [0.84039244 0.01788344 0.00620267]. \t  0.06018633325917739 \t 3.595021899183128\n",
            "17     \t [0.55642169 0.56607551 0.88048217]. \t  \u001b[92m3.760422081936679\u001b[0m \t 3.760422081936679\n",
            "18     \t [0.54520167 0.52135039 0.90221158]. \t  3.5576649552957407 \t 3.760422081936679\n",
            "19     \t [0.24561221 0.33233123 0.41387818]. \t  0.42211076022411576 \t 3.760422081936679\n",
            "20     \t [0.007436   0.0250568  0.48281179]. \t  0.17503379126264504 \t 3.760422081936679\n",
            "21     \t [0.97989571 0.04386623 0.45282803]. \t  0.12255157534987737 \t 3.760422081936679\n",
            "22     \t [0.54654219 0.3634862  0.64921966]. \t  1.2165482191162211 \t 3.760422081936679\n",
            "23     \t [0.25528111 0.48415676 0.36037925]. \t  0.39866790747783953 \t 3.760422081936679\n",
            "24     \t [0.07569981 0.13697431 0.85839891]. \t  0.7673788323050574 \t 3.760422081936679\n",
            "25     \t [0.27036096 0.91077795 0.38093927]. \t  1.1084812363376046 \t 3.760422081936679\n",
            "26     \t [0.79705018 0.02438926 0.25197161]. \t  0.525917058461396 \t 3.760422081936679\n",
            "27     \t [0.87335527 0.39530656 0.35284998]. \t  0.19324353876702555 \t 3.760422081936679\n",
            "28     \t [0.39445906 0.51019958 0.43097223]. \t  0.5296688656203746 \t 3.760422081936679\n",
            "29     \t [0.45014408 0.52611058 0.99092281]. \t  2.2163135656543136 \t 3.760422081936679\n",
            "30     \t [0.94153639 0.92860498 0.00195884]. \t  0.00010038785415261656 \t 3.760422081936679\n",
            "31     \t [0.70344285 0.22611978 0.55108985]. \t  0.2906825411389715 \t 3.760422081936679\n",
            "32     \t [0.04934746 0.9126289  0.30728679]. \t  0.46718190652705865 \t 3.760422081936679\n",
            "33     \t [0.63083083 0.80771709 0.94964549]. \t  1.66189665408206 \t 3.760422081936679\n",
            "34     \t [0.6992185  0.98398579 0.09641763]. \t  0.00187628453708249 \t 3.760422081936679\n",
            "35     \t [0.61687711 0.09510488 0.91547047]. \t  0.44474067889320423 \t 3.760422081936679\n",
            "36     \t [0.78219097 0.51341686 0.56040375]. \t  0.632511600681982 \t 3.760422081936679\n",
            "37     \t [0.45342712 0.29246917 0.08607081]. \t  0.26866794578786274 \t 3.760422081936679\n",
            "38     \t [0.79231077 0.41076059 0.45925204]. \t  0.21737004359724799 \t 3.760422081936679\n",
            "39     \t [0.54742893 0.33386352 0.6356025 ]. \t  0.9920591266767318 \t 3.760422081936679\n",
            "40     \t [0.30756416 0.97485586 0.24073759]. \t  0.12554771190166797 \t 3.760422081936679\n",
            "41     \t [0.60765123 0.16909337 0.90400786]. \t  0.8513283924330438 \t 3.760422081936679\n",
            "42     \t [0.02609647 0.25664567 0.42214936]. \t  0.34216163993403137 \t 3.760422081936679\n",
            "43     \t [0.53773788 0.36460807 0.23858312]. \t  0.49174845219631486 \t 3.760422081936679\n",
            "44     \t [0.96152213 0.82641581 0.10945664]. \t  0.0019440563357673208 \t 3.760422081936679\n",
            "45     \t [0.79290613 0.85126246 0.36330886]. \t  0.2480413986069324 \t 3.760422081936679\n",
            "46     \t [0.36693992 0.5510474  0.54730084]. \t  1.207137595299256 \t 3.760422081936679\n",
            "47     \t [0.15396336 0.15144365 0.04018996]. \t  0.1830869129518747 \t 3.760422081936679\n",
            "48     \t [0.83564559 0.59730719 0.3737175 ]. \t  0.15190635573334832 \t 3.760422081936679\n",
            "49     \t [0.84113746 0.21218613 0.48897705]. \t  0.18643348600582219 \t 3.760422081936679\n",
            "50     \t [0.67957759 0.62638588 0.90313422]. \t  3.416340324741837 \t 3.760422081936679\n",
            "51     \t [0.15868669 0.58519333 0.11021103]. \t  0.05012181383179967 \t 3.760422081936679\n",
            "52     \t [0.52305601 0.70289761 0.29157513]. \t  0.19819230143825314 \t 3.760422081936679\n",
            "53     \t [0.6400917  0.45691768 0.67169099]. \t  1.712734043008807 \t 3.760422081936679\n",
            "54     \t [0.28146427 0.33584932 0.58268669]. \t  0.6754966180919455 \t 3.760422081936679\n",
            "55     \t [0.51813264 0.72474541 0.2269723 ]. \t  0.08029306698449311 \t 3.760422081936679\n",
            "56     \t [0.19934762 0.49715661 0.04974183]. \t  0.052606553273321296 \t 3.760422081936679\n",
            "57     \t [0.91096375 0.47376646 0.8982599 ]. \t  3.3133478775835847 \t 3.760422081936679\n",
            "58     \t [0.96119224 0.1761734  0.50729654]. \t  0.1456110852282695 \t 3.760422081936679\n",
            "59     \t [0.28176789 0.67036869 0.25019929]. \t  0.15806582287163762 \t 3.760422081936679\n",
            "60     \t [0.85359838 0.82557268 0.75249523]. \t  1.355038799317163 \t 3.760422081936679\n",
            "61     \t [0.35716294 0.50607502 0.95971011]. \t  2.722902045977438 \t 3.760422081936679\n",
            "62     \t [0.42153072 0.83281484 0.35735336]. \t  0.6905915555116233 \t 3.760422081936679\n",
            "63     \t [0.14581925 0.96649475 0.98547217]. \t  0.4930788184241869 \t 3.760422081936679\n",
            "64     \t [0.24815173 0.73691098 0.79981383]. \t  2.7596901300978987 \t 3.760422081936679\n",
            "65     \t [0.9781367  0.84839682 0.41277241]. \t  0.17489809045160942 \t 3.760422081936679\n",
            "66     \t [0.3251493  0.73720585 0.12627524]. \t  0.02049352192856097 \t 3.760422081936679\n",
            "67     \t [0.7100949  0.0961176  0.64271501]. \t  0.30709097445666406 \t 3.760422081936679\n",
            "68     \t [0.65824962 0.69439832 0.72715427]. \t  2.152139382115979 \t 3.760422081936679\n",
            "69     \t [0.87557455 0.74930866 0.33004303]. \t  0.1057796659536137 \t 3.760422081936679\n",
            "70     \t [0.15115266 0.3255929  0.11656677]. \t  0.28441747665591366 \t 3.760422081936679\n",
            "71     \t [0.40161095 0.14330153 0.07912775]. \t  0.34220197048906614 \t 3.760422081936679\n",
            "72     \t [0.42432498 0.83660477 0.62693789]. \t  2.190133108141262 \t 3.760422081936679\n",
            "73     \t [0.07849063 0.98087423 0.6746157 ]. \t  1.9158326207869543 \t 3.760422081936679\n",
            "74     \t [0.07995938 0.42066868 0.51111481]. \t  0.6046239929769441 \t 3.760422081936679\n",
            "75     \t [0.59839891 0.84095705 0.65613497]. \t  1.5002171944513882 \t 3.760422081936679\n",
            "76     \t [0.89083205 0.30444691 0.91605057]. \t  1.810568198950642 \t 3.760422081936679\n",
            "77     \t [0.62443328 0.85865884 0.42726402]. \t  0.8373908278862687 \t 3.760422081936679\n",
            "78     \t [0.50969006 0.90259795 0.4377578 ]. \t  1.2261056077922827 \t 3.760422081936679\n",
            "79     \t [0.77621412 0.58111152 0.05490291]. \t  0.018409940709691444 \t 3.760422081936679\n",
            "80     \t [0.4826354  0.22150444 0.13440055]. \t  0.5078311586541444 \t 3.760422081936679\n",
            "81     \t [0.28505127 0.33246699 0.83330575]. \t  2.4724974913509064 \t 3.760422081936679\n",
            "82     \t [0.99137479 0.72353283 0.21374937]. \t  0.01436863814474256 \t 3.760422081936679\n",
            "83     \t [0.25536089 0.2923841  0.95535917]. \t  1.4120529804189925 \t 3.760422081936679\n",
            "84     \t [0.68554222 0.94377583 0.3566769 ]. \t  0.32595809242863 \t 3.760422081936679\n",
            "85     \t [0.00257283 0.98064215 0.46917557]. \t  2.0802613281659084 \t 3.760422081936679\n",
            "86     \t [0.76426008 0.22618128 0.17754331]. \t  0.4363121439109615 \t 3.760422081936679\n",
            "87     \t [0.1904641  0.81242336 0.43071404]. \t  1.80071597008606 \t 3.760422081936679\n",
            "88     \t [0.0906485  0.02573558 0.0191743 ]. \t  0.11503131228834358 \t 3.760422081936679\n",
            "89     \t [0.39433212 0.39069809 0.85675531]. \t  3.014316632906568 \t 3.760422081936679\n",
            "90     \t [0.67749328 0.72883553 0.7487094 ]. \t  2.1450116026714365 \t 3.760422081936679\n",
            "91     \t [0.86893183 0.83153693 0.7423304 ]. \t  1.2417906801469307 \t 3.760422081936679\n",
            "92     \t [0.22134224 0.60622318 0.00599918]. \t  0.011200136663916813 \t 3.760422081936679\n",
            "93     \t [0.52759496 0.24928289 0.83633028]. \t  1.6666359908101493 \t 3.760422081936679\n",
            "94     \t [0.48191241 0.90687268 0.65217406]. \t  1.732280853468756 \t 3.760422081936679\n",
            "95     \t [0.22739074 0.69964776 0.51761201]. \t  2.1686681936323704 \t 3.760422081936679\n",
            "96     \t [0.85732851 0.88204729 0.4189794 ]. \t  0.326832007774965 \t 3.760422081936679\n",
            "97     \t [0.27436566 0.47429614 0.33407217]. \t  0.3713947852116168 \t 3.760422081936679\n",
            "98     \t [0.80144005 0.62222005 0.48333435]. \t  0.42246043421277657 \t 3.760422081936679\n",
            "99     \t [0.11137178 0.24205983 0.83749476]. \t  1.5897244175653458 \t 3.760422081936679\n",
            "100    \t [0.76642793 0.13416178 0.66281868]. \t  0.45882963914336505 \t 3.760422081936679\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5Aur-aRtdq-",
        "outputId": "88adb821-9614-4bcb-91cb-57ddf6122d54"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 10\r\n",
        "\r\n",
        "np.random.seed(run_num_10)\r\n",
        "surrogate_loser_10 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "loser_10 = dGPGO(surrogate_loser_10, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_10.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.65358959 0.11500694 0.95028286]. \t  0.42730315147591735 \t 1.1029187088185965\n",
            "init   \t [0.4821914  0.87247454 0.21233268]. \t  0.06161964400032635 \t 1.1029187088185965\n",
            "init   \t [0.04070962 0.39719446 0.2331322 ]. \t  0.33269334660262956 \t 1.1029187088185965\n",
            "init   \t [0.84174072 0.20708234 0.74246953]. \t  1.1029187088185965 \t 1.1029187088185965\n",
            "init   \t [0.39215413 0.18225652 0.74353941]. \t  0.9779763535009853 \t 1.1029187088185965\n",
            "1      \t [0.85259632 0.05715442 0.33936127]. \t  0.4102376796793569 \t 1.1029187088185965\n",
            "2      \t [0.92518579 0.78050514 0.87881645]. \t  \u001b[92m2.2443644989048672\u001b[0m \t 2.2443644989048672\n",
            "3      \t [0.98338289 0.93392737 0.81033881]. \t  0.8330483590093913 \t 2.2443644989048672\n",
            "4      \t [0.24984974 0.81883701 0.99874938]. \t  1.135383544316759 \t 2.2443644989048672\n",
            "5      \t [0.84241011 0.46743734 0.97175067]. \t  \u001b[92m2.3195199773796715\u001b[0m \t 2.3195199773796715\n",
            "6      \t [0.57885752 0.71268607 0.92271476]. \t  \u001b[92m2.7017934403131854\u001b[0m \t 2.7017934403131854\n",
            "7      \t [0.61818574 0.63122128 0.97495209]. \t  2.426452498349205 \t 2.7017934403131854\n",
            "8      \t [0.57218725 0.8304376  0.80287284]. \t  1.7985258361310805 \t 2.7017934403131854\n",
            "9      \t [0.46284468 0.45960559 0.00413455]. \t  0.03774755754852872 \t 2.7017934403131854\n",
            "10     \t [0.98644438 0.00642055 0.87846481]. \t  0.22106378720336184 \t 2.7017934403131854\n",
            "11     \t [0.03582635 0.81040024 0.04907321]. \t  0.0027269977286783513 \t 2.7017934403131854\n",
            "12     \t [0.90426261 0.71700086 0.00753295]. \t  0.0015716828782100572 \t 2.7017934403131854\n",
            "13     \t [0.3208246  0.5816177  0.62439972]. \t  1.8569073198218415 \t 2.7017934403131854\n",
            "14     \t [0.7615828  0.63212196 0.97738212]. \t  2.360200553099713 \t 2.7017934403131854\n",
            "15     \t [1.75694365e-15 1.44802672e-15 1.35663767e-15]. \t  0.06797411659013423 \t 2.7017934403131854\n",
            "16     \t [0.68848169 0.5933104  0.88714662]. \t  \u001b[92m3.6474143939785204\u001b[0m \t 3.6474143939785204\n",
            "17     \t [0.7601571  0.52191695 0.76560069]. \t  3.0590985216883135 \t 3.6474143939785204\n",
            "18     \t [0.04312409 0.53788678 0.80260989]. \t  3.6034105941107564 \t 3.6474143939785204\n",
            "19     \t [0.68924464 0.92590455 0.01509171]. \t  0.0003279225325621969 \t 3.6474143939785204\n",
            "20     \t [0.02092836 0.47132655 0.92880135]. \t  3.0128633197361006 \t 3.6474143939785204\n",
            "21     \t [0.98612122 0.00741282 0.11980953]. \t  0.1472567483905558 \t 3.6474143939785204\n",
            "22     \t [0.00446584 0.68915886 0.89330189]. \t  3.1172149095247015 \t 3.6474143939785204\n",
            "23     \t [0.01641043 0.5160622  0.81323481]. \t  3.6364719088743143 \t 3.6474143939785204\n",
            "24     \t [0.4493366  0.05227787 0.00579074]. \t  0.12088568375860852 \t 3.6474143939785204\n",
            "25     \t [0.27980633 0.46148506 0.1752227 ]. \t  0.2378344066527602 \t 3.6474143939785204\n",
            "26     \t [0.06094368 0.67953594 0.21114809]. \t  0.08829263937905073 \t 3.6474143939785204\n",
            "27     \t [0.73893811 0.68093479 0.04192289]. \t  0.006243422255177505 \t 3.6474143939785204\n",
            "28     \t [5.32172750e-04 7.41632626e-02 5.84257932e-01]. \t  0.17127161528889503 \t 3.6474143939785204\n",
            "29     \t [0.39040893 0.32429144 0.01740015]. \t  0.0998205204225004 \t 3.6474143939785204\n",
            "30     \t [0.71265456 0.6122366  0.96764277]. \t  2.588658085473996 \t 3.6474143939785204\n",
            "31     \t [0.53783974 0.37586609 0.11422795]. \t  0.23297846283288623 \t 3.6474143939785204\n",
            "32     \t [0.19079304 0.5492814  0.85276274]. \t  \u001b[92m3.8540176131000927\u001b[0m \t 3.8540176131000927\n",
            "33     \t [0.15416496 0.58132998 0.86309978]. \t  3.817218129837225 \t 3.8540176131000927\n",
            "34     \t [0.92196866 0.20235519 0.22032218]. \t  0.34779631994302274 \t 3.8540176131000927\n",
            "35     \t [0.41268804 0.21677931 0.07668479]. \t  0.3026346220661753 \t 3.8540176131000927\n",
            "36     \t [0.28033917 0.9940199  0.44363132]. \t  1.6428151604376122 \t 3.8540176131000927\n",
            "37     \t [0.91502671 0.69756974 0.91725515]. \t  2.787194121186799 \t 3.8540176131000927\n",
            "38     \t [0.2326055  0.52617439 0.8524771 ]. \t  3.8300230674427915 \t 3.8540176131000927\n",
            "39     \t [0.84554329 0.19749171 0.53590731]. \t  0.21072762937007228 \t 3.8540176131000927\n",
            "40     \t [0.19321478 0.63427392 0.85924375]. \t  3.6423060541937153 \t 3.8540176131000927\n",
            "41     \t [0.54897872 0.42031727 0.66842942]. \t  1.6163134760994067 \t 3.8540176131000927\n",
            "42     \t [0.56867221 0.45234487 0.80205391]. \t  3.306796720955105 \t 3.8540176131000927\n",
            "43     \t [0.0250935  0.89844767 0.76071283]. \t  1.6139226024109985 \t 3.8540176131000927\n",
            "44     \t [0.26367193 0.5250609  0.87941416]. \t  3.7533332823869046 \t 3.8540176131000927\n",
            "45     \t [0.32103194 0.78721674 0.28324319]. \t  0.2781771042452093 \t 3.8540176131000927\n",
            "46     \t [0.73990251 0.38721673 0.73946956]. \t  2.273626035083554 \t 3.8540176131000927\n",
            "47     \t [0.61276184 0.23705712 0.22676888]. \t  0.6905085975860578 \t 3.8540176131000927\n",
            "48     \t [0.79204942 0.18643486 0.93466089]. \t  0.8194979038041548 \t 3.8540176131000927\n",
            "49     \t [0.2182822  0.82045885 0.34387207]. \t  0.7485559328161449 \t 3.8540176131000927\n",
            "50     \t [0.79434093 0.75456143 0.80972959]. \t  2.3576544924574 \t 3.8540176131000927\n",
            "51     \t [0.24552268 0.59409748 0.84235125]. \t  3.7955548568028683 \t 3.8540176131000927\n",
            "52     \t [0.33678804 0.22014992 0.00684788]. \t  0.11712495717363505 \t 3.8540176131000927\n",
            "53     \t [0.41028139 0.5719381  0.26311632]. \t  0.19774787863788867 \t 3.8540176131000927\n",
            "54     \t [0.60077419 0.77319619 0.97632717]. \t  1.6470965209265724 \t 3.8540176131000927\n",
            "55     \t [0.95857662 0.47617844 0.80837472]. \t  3.338517307679115 \t 3.8540176131000927\n",
            "56     \t [0.16839819 0.09440242 0.80593045]. \t  0.5819460844192055 \t 3.8540176131000927\n",
            "57     \t [0.77126344 0.27787038 0.57095094]. \t  0.40678651464373805 \t 3.8540176131000927\n",
            "58     \t [0.23622091 0.68363914 0.77638665]. \t  2.9986286541218723 \t 3.8540176131000927\n",
            "59     \t [0.98691583 0.16799015 0.96810489]. \t  0.5602547142275782 \t 3.8540176131000927\n",
            "60     \t [0.09323872 0.98788564 0.33246815]. \t  0.5978453646759858 \t 3.8540176131000927\n",
            "61     \t [0.20585617 0.57446848 0.03650038]. \t  0.02341783038978431 \t 3.8540176131000927\n",
            "62     \t [0.91977899 0.02827927 0.32915799]. \t  0.33213211617093846 \t 3.8540176131000927\n",
            "63     \t [0.1744179  0.49781195 0.54077387]. \t  1.051091442156995 \t 3.8540176131000927\n",
            "64     \t [0.99869586 0.95597948 0.00719737]. \t  6.747924827195337e-05 \t 3.8540176131000927\n",
            "65     \t [0.05923735 0.65355236 0.73897894]. \t  2.847661076986637 \t 3.8540176131000927\n",
            "66     \t [0.01269454 0.01430481 0.53274047]. \t  0.11687636895554003 \t 3.8540176131000927\n",
            "67     \t [0.83162222 0.16189688 0.6821508 ]. \t  0.6204044967331243 \t 3.8540176131000927\n",
            "68     \t [0.10307635 0.98704216 0.62230977]. \t  2.3851373682146537 \t 3.8540176131000927\n",
            "69     \t [0.81310371 0.96461204 0.89116056]. \t  0.7398602162679827 \t 3.8540176131000927\n",
            "70     \t [0.96392299 0.0732633  0.82991453]. \t  0.4700895298354437 \t 3.8540176131000927\n",
            "71     \t [0.18947797 0.52662333 0.87202074]. \t  3.784839321722959 \t 3.8540176131000927\n",
            "72     \t [0.20147301 0.83427847 0.02792526]. \t  0.0016588547472213234 \t 3.8540176131000927\n",
            "73     \t [0.96753452 0.40978965 0.46139521]. \t  0.14787896680659463 \t 3.8540176131000927\n",
            "74     \t [0.24613548 0.25790695 0.36722679]. \t  0.6088691497095003 \t 3.8540176131000927\n",
            "75     \t [0.61869238 0.30960007 0.75805921]. \t  1.9422037888060717 \t 3.8540176131000927\n",
            "76     \t [0.06499414 0.21232159 0.37895357]. \t  0.497457472021624 \t 3.8540176131000927\n",
            "77     \t [0.03885248 0.33913528 0.45254913]. \t  0.33612032404781683 \t 3.8540176131000927\n",
            "78     \t [0.55379685 0.80032251 0.19912658]. \t  0.04278284053876489 \t 3.8540176131000927\n",
            "79     \t [0.02021057 0.15528238 0.76939561]. \t  0.8700524306842632 \t 3.8540176131000927\n",
            "80     \t [0.322192   0.09348805 0.37668228]. \t  0.6953889615688063 \t 3.8540176131000927\n",
            "81     \t [0.09095127 0.93642432 0.61762491]. \t  2.685359651785647 \t 3.8540176131000927\n",
            "82     \t [0.10159781 0.36047065 0.96154357]. \t  1.8417153323280768 \t 3.8540176131000927\n",
            "83     \t [0.57007758 0.64436134 0.09782204]. \t  0.02497798409309806 \t 3.8540176131000927\n",
            "84     \t [0.07585703 0.84169618 0.31840343]. \t  0.5586293380163749 \t 3.8540176131000927\n",
            "85     \t [0.26852638 0.32867212 0.25219684]. \t  0.625031988431732 \t 3.8540176131000927\n",
            "86     \t [0.03915346 0.47901975 0.39966145]. \t  0.4367926667230772 \t 3.8540176131000927\n",
            "87     \t [0.79613901 0.22285573 0.8464027 ]. \t  1.401327231034421 \t 3.8540176131000927\n",
            "88     \t [0.78634327 0.08272775 0.40411308]. \t  0.34048301845184237 \t 3.8540176131000927\n",
            "89     \t [0.25075283 0.42774475 0.05874369]. \t  0.09926859078580283 \t 3.8540176131000927\n",
            "90     \t [0.52020758 0.06642892 0.61019363]. \t  0.20301745333843152 \t 3.8540176131000927\n",
            "91     \t [0.04145178 0.9982077  0.04650104]. \t  0.0011636459714390384 \t 3.8540176131000927\n",
            "92     \t [0.6463775  0.3034504  0.95923746]. \t  1.4498903641763436 \t 3.8540176131000927\n",
            "93     \t [0.21348632 0.41152009 0.08247436]. \t  0.14062703500889415 \t 3.8540176131000927\n",
            "94     \t [0.52393133 0.71775999 0.69267821]. \t  2.0244943228113934 \t 3.8540176131000927\n",
            "95     \t [0.92048307 0.02856064 0.60107968]. \t  0.12706036817059735 \t 3.8540176131000927\n",
            "96     \t [0.55966008 0.00459803 0.75177789]. \t  0.2510841678353941 \t 3.8540176131000927\n",
            "97     \t [0.06118166 0.51334582 0.17268385]. \t  0.12985995150060514 \t 3.8540176131000927\n",
            "98     \t [0.67207969 0.29327723 0.95465068]. \t  1.416516516636988 \t 3.8540176131000927\n",
            "99     \t [0.21093958 0.37233381 0.00276439]. \t  0.05926409704295324 \t 3.8540176131000927\n",
            "100    \t [0.06026911 0.76609599 0.98141921]. \t  1.630173690362098 \t 3.8540176131000927\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MMu7U9NNtdtn",
        "outputId": "579e9451-9b70-457a-e25e-77836550a952"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 11\r\n",
        "\r\n",
        "np.random.seed(run_num_11)\r\n",
        "surrogate_loser_11 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "loser_11 = dGPGO(surrogate_loser_11, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_11.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.98103566 0.82124785 0.64589361]. \t  0.6570240879762975 \t 0.687459437576373\n",
            "init   \t [0.42368801 0.20231098 0.49190677]. \t  0.29901296656621285 \t 0.687459437576373\n",
            "init   \t [0.13855833 0.45252104 0.11373463]. \t  0.1378633538075966 \t 0.687459437576373\n",
            "init   \t [0.00292449 0.52342617 0.18997116]. \t  0.12299763462541902 \t 0.687459437576373\n",
            "init   \t [0.14171979 0.33586637 0.58369219]. \t  0.687459437576373 \t 0.687459437576373\n",
            "1      \t [0.23267662 0.55632866 0.98247629]. \t  \u001b[92m2.4014175052443334\u001b[0m \t 2.4014175052443334\n",
            "2      \t [0.1018956  0.95528749 0.95450529]. \t  0.6625688909370097 \t 2.4014175052443334\n",
            "3      \t [0.49069589 0.32394686 0.99166679]. \t  1.2576491073928158 \t 2.4014175052443334\n",
            "4      \t [0.99622896 0.50984357 0.00987988]. \t  0.00899849618592385 \t 2.4014175052443334\n",
            "5      \t [0.17147829 0.44852386 0.99855404]. \t  1.8327027793895603 \t 2.4014175052443334\n",
            "6      \t [0.52755702 0.61099275 0.96085651]. \t  \u001b[92m2.731594261003499\u001b[0m \t 2.731594261003499\n",
            "7      \t [0.50454698 0.68368804 0.91827122]. \t  \u001b[92m2.973325813164906\u001b[0m \t 2.973325813164906\n",
            "8      \t [0.45499982 0.74056849 0.7320783 ]. \t  2.2578040189840607 \t 2.973325813164906\n",
            "9      \t [0.53897689 0.95646638 0.88685629]. \t  0.8406578977033827 \t 2.973325813164906\n",
            "10     \t [0.59988711 0.56442319 0.80722352]. \t  \u001b[92m3.588878654081515\u001b[0m \t 3.588878654081515\n",
            "11     \t [0.8720919  0.50663956 0.61651099]. \t  0.9950572561296357 \t 3.588878654081515\n",
            "12     \t [0.41328528 0.46568929 0.82937505]. \t  3.5573768835752326 \t 3.588878654081515\n",
            "13     \t [0.37143313 0.52284198 0.79913322]. \t  3.5738586105828203 \t 3.588878654081515\n",
            "14     \t [0.45197443 0.5223837  0.79936033]. \t  3.5587515329528716 \t 3.588878654081515\n",
            "15     \t [0.51668635 0.54416624 0.80920986]. \t  \u001b[92m3.6446249191072395\u001b[0m \t 3.6446249191072395\n",
            "16     \t [0.80300647 0.22342926 0.03056251]. \t  0.09442709719293398 \t 3.6446249191072395\n",
            "17     \t [0.00086147 0.61131219 0.8133503 ]. \t  3.5709476777142406 \t 3.6446249191072395\n",
            "18     \t [0.84744422 0.43299283 0.60527535]. \t  0.8425924470350133 \t 3.6446249191072395\n",
            "19     \t [0.01092959 0.80559214 0.02907683]. \t  0.0017824761667485985 \t 3.6446249191072395\n",
            "20     \t [0.96624075 0.7970065  0.98087211]. \t  1.3940054087467613 \t 3.6446249191072395\n",
            "21     \t [0.48075247 0.4868558  0.84329412]. \t  \u001b[92m3.6882293140094626\u001b[0m \t 3.6882293140094626\n",
            "22     \t [0.16029874 0.8895327  0.85099335]. \t  1.4576903395520366 \t 3.6882293140094626\n",
            "23     \t [0.02516017 0.58371562 0.30762436]. \t  0.28033234832751935 \t 3.6882293140094626\n",
            "24     \t [0.48944405 0.6559092  0.83877525]. \t  3.459362722987617 \t 3.6882293140094626\n",
            "25     \t [0.42029516 0.21429986 0.30920755]. \t  0.8618548387131872 \t 3.6882293140094626\n",
            "26     \t [0.3958507  0.05303969 0.06829623]. \t  0.29195510283100545 \t 3.6882293140094626\n",
            "27     \t [0.46530289 0.56618438 0.82141748]. \t  \u001b[92m3.7356284054236553\u001b[0m \t 3.7356284054236553\n",
            "28     \t [0.77891632 0.65166351 0.98580516]. \t  2.150894348089524 \t 3.7356284054236553\n",
            "29     \t [0.65255185 0.37046885 0.60672359]. \t  0.8229914938001928 \t 3.7356284054236553\n",
            "30     \t [0.92828663 0.58486899 0.87941434]. \t  3.621992127145754 \t 3.7356284054236553\n",
            "31     \t [0.47931034 0.84014484 0.67227953]. \t  1.8305430269273972 \t 3.7356284054236553\n",
            "32     \t [0.99368625 0.46727277 0.87533105]. \t  3.396403462607304 \t 3.7356284054236553\n",
            "33     \t [0.58409319 0.95293018 0.05602783]. \t  0.00103398458818912 \t 3.7356284054236553\n",
            "34     \t [0.98333652 0.08432273 0.06130314]. \t  0.08925024837207238 \t 3.7356284054236553\n",
            "35     \t [0.75331242 0.27518024 0.74191406]. \t  1.5632474996030026 \t 3.7356284054236553\n",
            "36     \t [3.36437397e-04 5.92900497e-01 8.76078865e-01]. \t  3.714162490553864 \t 3.7356284054236553\n",
            "37     \t [0.03117542 0.99269051 0.18117536]. \t  0.039118174704452655 \t 3.7356284054236553\n",
            "38     \t [0.47540019 0.50247134 0.84723974]. \t  \u001b[92m3.7544355481009624\u001b[0m \t 3.7544355481009624\n",
            "39     \t [0.14965119 0.83004101 0.03678273]. \t  0.0020265420497056937 \t 3.7544355481009624\n",
            "40     \t [0.65825822 0.03070688 0.99081436]. \t  0.13820090658858034 \t 3.7544355481009624\n",
            "41     \t [0.81284622 0.54746547 0.93318464]. \t  3.1695915074968855 \t 3.7544355481009624\n",
            "42     \t [0.06732819 0.25070027 0.34887756]. \t  0.5420588033558534 \t 3.7544355481009624\n",
            "43     \t [0.56644517 0.56731915 0.82431938]. \t  3.723116416141599 \t 3.7544355481009624\n",
            "44     \t [0.94486544 0.02738538 0.98606256]. \t  0.13644349615061416 \t 3.7544355481009624\n",
            "45     \t [0.51908844 0.55629807 0.82279569]. \t  3.7379308193542053 \t 3.7544355481009624\n",
            "46     \t [0.60059134 0.57304807 0.82515093]. \t  3.708296014518343 \t 3.7544355481009624\n",
            "47     \t [0.57775579 0.25342925 0.84516579]. \t  1.6930206158719372 \t 3.7544355481009624\n",
            "48     \t [0.64910131 0.22567323 0.61880267]. \t  0.5435876480199389 \t 3.7544355481009624\n",
            "49     \t [0.01604554 0.33670665 0.18461822]. \t  0.3487165227529128 \t 3.7544355481009624\n",
            "50     \t [0.79408129 0.56766075 0.71696715]. \t  2.3064305635288314 \t 3.7544355481009624\n",
            "51     \t [0.7402549  0.3655645  0.54355901]. \t  0.406519646040674 \t 3.7544355481009624\n",
            "52     \t [0.3419478  0.37953654 0.70966463]. \t  1.978436778247392 \t 3.7544355481009624\n",
            "53     \t [0.4836289  0.56703853 0.840715  ]. \t  \u001b[92m3.8176219835117435\u001b[0m \t 3.8176219835117435\n",
            "54     \t [0.12063825 0.28654585 0.99348593]. \t  1.0168883573826288 \t 3.8176219835117435\n",
            "55     \t [0.60034281 0.33793455 0.11347122]. \t  0.2572321192658406 \t 3.8176219835117435\n",
            "56     \t [0.39737068 0.82353818 0.94749653]. \t  1.5740956107911792 \t 3.8176219835117435\n",
            "57     \t [0.41435179 0.284656   0.94117003]. \t  1.4810304390715325 \t 3.8176219835117435\n",
            "58     \t [0.28734283 0.70631795 0.44272901]. \t  1.4552116958940275 \t 3.8176219835117435\n",
            "59     \t [0.64582064 0.27048736 0.60267387]. \t  0.5685328343977809 \t 3.8176219835117435\n",
            "60     \t [0.92654285 0.91771414 0.60255869]. \t  0.48812552914376145 \t 3.8176219835117435\n",
            "61     \t [0.79067284 0.4889186  0.34277043]. \t  0.1720656681141432 \t 3.8176219835117435\n",
            "62     \t [0.73614964 0.08873081 0.08123764]. \t  0.23429503064823837 \t 3.8176219835117435\n",
            "63     \t [0.06546414 0.0247577  0.30992506]. \t  0.6604402137049257 \t 3.8176219835117435\n",
            "64     \t [0.37505358 0.98290616 0.57097691]. \t  2.1753048118193075 \t 3.8176219835117435\n",
            "65     \t [0.95340121 0.51222226 0.23034747]. \t  0.07641901044959246 \t 3.8176219835117435\n",
            "66     \t [0.66780369 0.70433721 0.20511346]. \t  0.04426141336992313 \t 3.8176219835117435\n",
            "67     \t [0.1718238  0.95266348 0.21957189]. \t  0.0965598026581265 \t 3.8176219835117435\n",
            "68     \t [0.00668649 0.08940948 0.4725365 ]. \t  0.21997486144380612 \t 3.8176219835117435\n",
            "69     \t [0.50161296 0.95814489 0.13143229]. \t  0.008605766789104212 \t 3.8176219835117435\n",
            "70     \t [0.89116531 0.99014832 0.88225675]. \t  0.5995239394899143 \t 3.8176219835117435\n",
            "71     \t [0.52987778 0.82918509 0.07807094]. \t  0.003878824742720693 \t 3.8176219835117435\n",
            "72     \t [0.46976662 0.07514661 0.19917063]. \t  0.8292924932649325 \t 3.8176219835117435\n",
            "73     \t [0.01185891 0.48020866 0.89149413]. \t  3.4585103790982075 \t 3.8176219835117435\n",
            "74     \t [0.8785674  0.28201439 0.88658166]. \t  1.7952803792715935 \t 3.8176219835117435\n",
            "75     \t [0.11661158 0.57658081 0.89141671]. \t  3.6830251509630694 \t 3.8176219835117435\n",
            "76     \t [0.59244815 0.46942989 0.82981494]. \t  3.5506139509376005 \t 3.8176219835117435\n",
            "77     \t [0.15000929 0.83777228 0.81145404]. \t  1.947239797927471 \t 3.8176219835117435\n",
            "78     \t [0.03822197 0.16401539 0.05399344]. \t  0.17994921078638834 \t 3.8176219835117435\n",
            "79     \t [0.47399276 0.3863694  0.51186274]. \t  0.4426504618250182 \t 3.8176219835117435\n",
            "80     \t [0.07164216 0.1899446  0.51708782]. \t  0.2455051318790618 \t 3.8176219835117435\n",
            "81     \t [0.38550927 0.95537209 0.6460422 ]. \t  1.8994677417985926 \t 3.8176219835117435\n",
            "82     \t [0.62526033 0.68415627 0.34354801]. \t  0.2776421583690714 \t 3.8176219835117435\n",
            "83     \t [0.26733556 0.17048883 0.76671184]. \t  0.9703752437022648 \t 3.8176219835117435\n",
            "84     \t [0.0319908  0.90880231 0.79468209]. \t  1.423155602228559 \t 3.8176219835117435\n",
            "85     \t [0.89718587 0.84522385 0.93800549]. \t  1.412830789733427 \t 3.8176219835117435\n",
            "86     \t [0.45140418 0.1109365  0.8919736 ]. \t  0.5691786665236267 \t 3.8176219835117435\n",
            "87     \t [0.88238899 0.29590736 0.9282799 ]. \t  1.6420849187800834 \t 3.8176219835117435\n",
            "88     \t [0.51218717 0.00332253 0.90375347]. \t  0.1970479087957312 \t 3.8176219835117435\n",
            "89     \t [0.93373041 0.30008258 0.2868365 ]. \t  0.2738027942787708 \t 3.8176219835117435\n",
            "90     \t [0.82243595 0.81083886 0.19101491]. \t  0.015539666017811682 \t 3.8176219835117435\n",
            "91     \t [0.41487269 0.86355672 0.87969103]. \t  1.5871950857407389 \t 3.8176219835117435\n",
            "92     \t [0.85695397 0.31416628 0.88882694]. \t  2.089645940612437 \t 3.8176219835117435\n",
            "93     \t [0.30654317 0.68735427 0.25532736]. \t  0.1666041551928666 \t 3.8176219835117435\n",
            "94     \t [0.67643792 0.00669018 0.92556499]. \t  0.17923468057010708 \t 3.8176219835117435\n",
            "95     \t [0.41761799 0.16804838 0.19966897]. \t  0.8437008962970893 \t 3.8176219835117435\n",
            "96     \t [0.79041122 0.13903144 0.95899516]. \t  0.4865923987032969 \t 3.8176219835117435\n",
            "97     \t [0.27664727 0.08700359 0.81163344]. \t  0.5498838079004507 \t 3.8176219835117435\n",
            "98     \t [0.29130459 0.70467093 0.66824017]. \t  2.401895065242285 \t 3.8176219835117435\n",
            "99     \t [0.60433714 0.04489796 0.1987617 ]. \t  0.6982724795764844 \t 3.8176219835117435\n",
            "100    \t [0.92270353 0.34908719 0.75873467]. \t  2.179299016401862 \t 3.8176219835117435\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UEjKllHqtdwW",
        "outputId": "0771cc55-c4fc-40d9-adc7-7c9a6d3d53a4"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 12\r\n",
        "\r\n",
        "np.random.seed(run_num_12)\r\n",
        "surrogate_loser_12 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "loser_12 = dGPGO(surrogate_loser_12, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_12.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.19151945 0.62210877 0.43772774]. \t  1.1006281843679786 \t 1.6482992955272024\n",
            "init   \t [0.78535858 0.77997581 0.27259261]. \t  0.07153771680480671 \t 1.6482992955272024\n",
            "init   \t [0.27646426 0.80187218 0.95813935]. \t  1.6482992955272024 \t 1.6482992955272024\n",
            "init   \t [0.87593263 0.35781727 0.50099513]. \t  0.2282556248207173 \t 1.6482992955272024\n",
            "init   \t [0.68346294 0.71270203 0.37025075]. \t  0.33032494760932407 \t 1.6482992955272024\n",
            "1      \t [0.01398415 0.60897714 0.98856638]. \t  \u001b[92m2.228026380269567\u001b[0m \t 2.228026380269567\n",
            "2      \t [0.09098655 0.16693846 0.95141238]. \t  0.6431040211578418 \t 2.228026380269567\n",
            "3      \t [0.00761147 0.89633798 0.99755295]. \t  0.7296905183544732 \t 2.228026380269567\n",
            "4      \t [0.87278659 0.50804111 0.98303484]. \t  \u001b[92m2.2637740820310066\u001b[0m \t 2.2637740820310066\n",
            "5      \t [0.6615157  0.61727972 0.98784207]. \t  2.2346948616653965 \t 2.2637740820310066\n",
            "6      \t [0.84395992 0.6470589  0.96467316]. \t  \u001b[92m2.4977128438483738\u001b[0m \t 2.4977128438483738\n",
            "7      \t [0.92391915 0.77073359 0.8956314 ]. \t  2.291665201139339 \t 2.4977128438483738\n",
            "8      \t [0.20132189 0.20164407 0.00697849]. \t  0.11204254954905647 \t 2.4977128438483738\n",
            "9      \t [0.02579296 0.41608071 0.65981911]. \t  1.5926596429081494 \t 2.4977128438483738\n",
            "10     \t [0.99904362 0.81350028 0.97049399]. \t  1.3792952580622315 \t 2.4977128438483738\n",
            "11     \t [0.         0.62086029 0.        ]. \t  0.006305031521604348 \t 2.4977128438483738\n",
            "12     \t [0. 0. 0.]. \t  0.06797411659013229 \t 2.4977128438483738\n",
            "13     \t [0.83414773 0.67641845 0.78327469]. \t  \u001b[92m2.7274644364724274\u001b[0m \t 2.7274644364724274\n",
            "14     \t [0.97673988 0.56264251 0.90150951]. \t  \u001b[92m3.4848509099025375\u001b[0m \t 3.4848509099025375\n",
            "15     \t [0.97116965 0.66728227 0.49654319]. \t  0.28674280245577843 \t 3.4848509099025375\n",
            "16     \t [0.55587017 0.98258199 0.09546704]. \t  0.0028214604569373847 \t 3.4848509099025375\n",
            "17     \t [0.96659155 0.66956823 0.01459458]. \t  0.0024141465368047715 \t 3.4848509099025375\n",
            "18     \t [0.9735731  0.13550594 0.88536298]. \t  0.6892097803315539 \t 3.4848509099025375\n",
            "19     \t [0.35657319 0.69697249 0.11065344]. \t  0.021505444175504602 \t 3.4848509099025375\n",
            "20     \t [0.87923961 0.92027939 0.02115801]. \t  0.00021401256688099456 \t 3.4848509099025375\n",
            "21     \t [0.97360337 0.5001345  0.86784386]. \t  \u001b[92m3.581366682231876\u001b[0m \t 3.581366682231876\n",
            "22     \t [0.02295395 0.96711675 0.35569796]. \t  0.8195862057351571 \t 3.581366682231876\n",
            "23     \t [0.90657175 0.18849839 0.00408659]. \t  0.049945512571984804 \t 3.581366682231876\n",
            "24     \t [0.95934255 0.13483385 0.58978918]. \t  0.23503660187818645 \t 3.581366682231876\n",
            "25     \t [0.75257917 0.76666177 0.77475694]. \t  2.0249713671027507 \t 3.581366682231876\n",
            "26     \t [0.9979112  0.57526963 0.87863872]. \t  \u001b[92m3.6116078388705395\u001b[0m \t 3.6116078388705395\n",
            "27     \t [0.99382924 0.5379479  0.85526552]. \t  \u001b[92m3.670098913582027\u001b[0m \t 3.670098913582027\n",
            "28     \t [0.84445786 0.33819294 0.87260182]. \t  2.4044592050677114 \t 3.670098913582027\n",
            "29     \t [0.43817054 0.11696402 0.3778372 ]. \t  0.6896644481940144 \t 3.670098913582027\n",
            "30     \t [0.06346045 0.63738689 0.33134359]. \t  0.4292681917320401 \t 3.670098913582027\n",
            "31     \t [0.2639856  0.42438551 0.83427328]. \t  3.2996335115671536 \t 3.670098913582027\n",
            "32     \t [0.82033973 0.79716428 0.08591558]. \t  0.0028307305418085256 \t 3.670098913582027\n",
            "33     \t [0.45938261 0.75250997 0.30932276]. \t  0.3116825248599445 \t 3.670098913582027\n",
            "34     \t [0.14857774 0.26493928 0.00777946]. \t  0.09209710617458892 \t 3.670098913582027\n",
            "35     \t [0.60832238 0.55700038 0.81874419]. \t  \u001b[92m3.683568939368378\u001b[0m \t 3.683568939368378\n",
            "36     \t [0.42652492 0.52862037 0.77247941]. \t  3.2912807532496595 \t 3.683568939368378\n",
            "37     \t [0.88857253 0.13831501 0.99785855]. \t  0.3461011837569411 \t 3.683568939368378\n",
            "38     \t [0.36751943 0.04738586 0.8720085 ]. \t  0.34860173861156735 \t 3.683568939368378\n",
            "39     \t [0.05922251 0.19985131 0.18276069]. \t  0.5656147149471785 \t 3.683568939368378\n",
            "40     \t [0.17134421 0.86725833 0.08159063]. \t  0.004730173871140647 \t 3.683568939368378\n",
            "41     \t [0.66681684 0.56372273 0.69434298]. \t  2.09518839689952 \t 3.683568939368378\n",
            "42     \t [0.18872382 0.51514048 0.22488052]. \t  0.20745664175205186 \t 3.683568939368378\n",
            "43     \t [0.20044312 0.58545254 0.84540233]. \t  \u001b[92m3.8191482293802874\u001b[0m \t 3.8191482293802874\n",
            "44     \t [0.01743162 0.75653637 0.75437458]. \t  2.4712722235429574 \t 3.8191482293802874\n",
            "45     \t [0.52873134 0.48130385 0.74748806]. \t  2.8460017521193435 \t 3.8191482293802874\n",
            "46     \t [0.85544522 0.01913392 0.08899941]. \t  0.17209892135155425 \t 3.8191482293802874\n",
            "47     \t [0.94509336 0.52652727 0.52765881]. \t  0.355756562588529 \t 3.8191482293802874\n",
            "48     \t [0.26774887 0.59244424 0.828676  ]. \t  3.755093621486673 \t 3.8191482293802874\n",
            "49     \t [0.49512097 0.96958735 0.07743071]. \t  0.0021082109387776248 \t 3.8191482293802874\n",
            "50     \t [0.86690057 0.3390747  0.56690559]. \t  0.4488104603551687 \t 3.8191482293802874\n",
            "51     \t [0.43918792 0.76714066 0.46667066]. \t  1.5678915085471918 \t 3.8191482293802874\n",
            "52     \t [0.10554099 0.13941053 0.86069795]. \t  0.7794254933929609 \t 3.8191482293802874\n",
            "53     \t [0.47038939 0.8521396  0.9974098 ]. \t  0.9628504485489497 \t 3.8191482293802874\n",
            "54     \t [0.77998072 0.34352677 0.73868039]. \t  1.9942376878878827 \t 3.8191482293802874\n",
            "55     \t [0.68813783 0.09659201 0.92520935]. \t  0.4257988157189452 \t 3.8191482293802874\n",
            "56     \t [0.50350165 0.82297981 0.52319174]. \t  1.8476111133622661 \t 3.8191482293802874\n",
            "57     \t [0.70155505 0.48281139 0.71915125]. \t  2.381860570908207 \t 3.8191482293802874\n",
            "58     \t [0.55925288 0.44245054 0.32789276]. \t  0.3357905619753193 \t 3.8191482293802874\n",
            "59     \t [0.49978886 0.66873457 0.25703783]. \t  0.13282998981277314 \t 3.8191482293802874\n",
            "60     \t [0.84105879 0.5629506  0.34627985]. \t  0.1243490147263995 \t 3.8191482293802874\n",
            "61     \t [0.12603402 0.63989199 0.71126356]. \t  2.6777207524288986 \t 3.8191482293802874\n",
            "62     \t [0.39964433 0.99658814 0.45459576]. \t  1.484373314649264 \t 3.8191482293802874\n",
            "63     \t [0.44241317 0.17151992 0.84918487]. \t  1.0160410107130888 \t 3.8191482293802874\n",
            "64     \t [0.18133544 0.67887524 0.60018155]. \t  2.3929382970522894 \t 3.8191482293802874\n",
            "65     \t [0.81359004 0.76129887 0.49975164]. \t  0.6095207961373732 \t 3.8191482293802874\n",
            "66     \t [0.06363343 0.5422766  0.31167274]. \t  0.2877952793680176 \t 3.8191482293802874\n",
            "67     \t [0.63145594 0.62557868 0.9352136 ]. \t  3.0671100960742277 \t 3.8191482293802874\n",
            "68     \t [0.50259193 0.833834   0.27032893]. \t  0.16973072351208715 \t 3.8191482293802874\n",
            "69     \t [0.86423622 0.32921701 0.78258116]. \t  2.2149879950678066 \t 3.8191482293802874\n",
            "70     \t [0.09070466 0.39075461 0.30718995]. \t  0.40511813609791275 \t 3.8191482293802874\n",
            "71     \t [0.92027701 0.90035824 0.55900314]. \t  0.4813358852570495 \t 3.8191482293802874\n",
            "72     \t [0.07396428 0.72569041 0.44195972]. \t  1.6725069055792048 \t 3.8191482293802874\n",
            "73     \t [0.38433519 0.8620133  0.47753883]. \t  2.017900529756957 \t 3.8191482293802874\n",
            "74     \t [0.70212302 0.08661498 0.22402478]. \t  0.671412664775161 \t 3.8191482293802874\n",
            "75     \t [0.43888827 0.73929972 0.25561194]. \t  0.14414037023096218 \t 3.8191482293802874\n",
            "76     \t [0.75182032 0.06531014 0.26174813]. \t  0.626724796742964 \t 3.8191482293802874\n",
            "77     \t [0.09764391 0.93899129 0.53823406]. \t  2.882858654763731 \t 3.8191482293802874\n",
            "78     \t [0.77977615 0.7150441  0.1566065 ]. \t  0.01689399623485657 \t 3.8191482293802874\n",
            "79     \t [0.1475432  0.68460258 0.02741935]. \t  0.006626606260917234 \t 3.8191482293802874\n",
            "80     \t [0.10978135 0.16938244 0.0893402 ]. \t  0.30763139312569293 \t 3.8191482293802874\n",
            "81     \t [0.91764548 0.85015027 0.7222717 ]. \t  0.9821056508060743 \t 3.8191482293802874\n",
            "82     \t [0.36101895 0.51007173 0.34447252]. \t  0.35858889577856073 \t 3.8191482293802874\n",
            "83     \t [0.55837414 0.61348561 0.73980716]. \t  2.7160211996294548 \t 3.8191482293802874\n",
            "84     \t [0.00460694 0.52167147 0.3961229 ]. \t  0.49170667493883247 \t 3.8191482293802874\n",
            "85     \t [0.31677985 0.2602402  0.8693305 ]. \t  1.7042213605703371 \t 3.8191482293802874\n",
            "86     \t [0.18255073 0.2406186  0.76028492]. \t  1.433703322006356 \t 3.8191482293802874\n",
            "87     \t [0.41735198 0.83179659 0.40232147]. \t  1.1129711832789138 \t 3.8191482293802874\n",
            "88     \t [0.79543712 0.53168433 0.72799723]. \t  2.503079766226982 \t 3.8191482293802874\n",
            "89     \t [0.53838599 0.95932856 0.43034474]. \t  1.0109630170128627 \t 3.8191482293802874\n",
            "90     \t [0.25203354 0.22257782 0.88271513]. \t  1.3217363185741255 \t 3.8191482293802874\n",
            "91     \t [0.60582574 0.32749757 0.65001159]. \t  1.0835379561877838 \t 3.8191482293802874\n",
            "92     \t [0.90664635 0.39572821 0.15957417]. \t  0.13677434322670798 \t 3.8191482293802874\n",
            "93     \t [0.0285213  0.76993271 0.50713399]. \t  2.540627034382265 \t 3.8191482293802874\n",
            "94     \t [0.55089586 0.71819762 0.38418863]. \t  0.5712272229635553 \t 3.8191482293802874\n",
            "95     \t [0.90917994 0.50707355 0.56146547]. \t  0.5385477977482964 \t 3.8191482293802874\n",
            "96     \t [0.41922349 0.57565327 0.70638927]. \t  2.4807373000665214 \t 3.8191482293802874\n",
            "97     \t [0.92676762 0.83831109 0.71541354]. \t  0.9938525433078214 \t 3.8191482293802874\n",
            "98     \t [0.67512551 0.55953668 0.47623847]. \t  0.47444785276726686 \t 3.8191482293802874\n",
            "99     \t [0.14963312 0.34455413 0.24765072]. \t  0.5208219028417973 \t 3.8191482293802874\n",
            "100    \t [0.72125536 0.92491891 0.11602707]. \t  0.0034592068050073 \t 3.8191482293802874\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QoONg4VEtdy-",
        "outputId": "62608ffe-cfcd-42c1-80ef-5966bcc5fc09"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 13\r\n",
        "\r\n",
        "np.random.seed(run_num_13)\r\n",
        "surrogate_loser_13 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "loser_13 = dGPGO(surrogate_loser_13, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_13.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.66064431 0.41360065 0.66810256]. \t  1.5457680899590245 \t 2.6697919207500047\n",
            "init   \t [0.22994342 0.80767834 0.63681846]. \t  2.6697919207500047 \t 2.6697919207500047\n",
            "init   \t [0.17219385 0.26038587 0.91531999]. \t  1.4685599870430508 \t 2.6697919207500047\n",
            "init   \t [0.46281551 0.12019095 0.88088551]. \t  0.639297028201682 \t 2.6697919207500047\n",
            "init   \t [0.22621895 0.81144033 0.44587892]. \t  1.960293029067079 \t 2.6697919207500047\n",
            "1      \t [0.54200533 0.91610447 0.94072107]. \t  0.9345749070922738 \t 2.6697919207500047\n",
            "2      \t [0.02210984 0.92408402 0.84182838]. \t  1.1986876793966825 \t 2.6697919207500047\n",
            "3      \t [0.2551603  0.64940006 0.63764251]. \t  2.2608836438455646 \t 2.6697919207500047\n",
            "4      \t [0.54585488 0.88620007 0.61783277]. \t  1.6931270368695779 \t 2.6697919207500047\n",
            "5      \t [0.96079806 0.29216074 0.95264917]. \t  1.3919250763683957 \t 2.6697919207500047\n",
            "6      \t [0.2047804  0.9433318  0.58983856]. \t  \u001b[92m2.7575076770906346\u001b[0m \t 2.7575076770906346\n",
            "7      \t [0.44158377 0.59676932 0.85413505]. \t  \u001b[92m3.7859887510423507\u001b[0m \t 3.7859887510423507\n",
            "8      \t [0.69029983 0.42345295 0.95445601]. \t  2.3745904957199278 \t 3.7859887510423507\n",
            "9      \t [0.22868386 0.66917995 0.98838502]. \t  2.0730833308255656 \t 3.7859887510423507\n",
            "10     \t [0.05283888 0.87230097 0.11290663]. \t  0.009718695164118827 \t 3.7859887510423507\n",
            "11     \t [2.81281769e-01 9.18261116e-01 7.13252403e-04]. \t  0.00045855666905634016 \t 3.7859887510423507\n",
            "12     \t [0.58199399 0.5939967  0.81004786]. \t  3.556737177444406 \t 3.7859887510423507\n",
            "13     \t [0.49871725 0.62039365 0.85432931]. \t  3.685151399572989 \t 3.7859887510423507\n",
            "14     \t [0.47734314 0.34762654 0.39486908]. \t  0.42153003151846963 \t 3.7859887510423507\n",
            "15     \t [0.45955332 0.60551176 0.82710401]. \t  3.6763756178432723 \t 3.7859887510423507\n",
            "16     \t [0.98749352 0.56828758 0.79943945]. \t  3.334091654880357 \t 3.7859887510423507\n",
            "17     \t [0.43523602 0.30944264 0.01400769]. \t  0.09944893382802913 \t 3.7859887510423507\n",
            "18     \t [0.94512015 0.52884703 0.9536055 ]. \t  2.791466511578881 \t 3.7859887510423507\n",
            "19     \t [0.32848772 0.0309685  0.00328881]. \t  0.11418116782522938 \t 3.7859887510423507\n",
            "20     \t [0.07470961 0.89719757 0.51290199]. \t  2.8512319266465767 \t 3.7859887510423507\n",
            "21     \t [0.85605406 0.66166925 0.00903148]. \t  0.003462123803062882 \t 3.7859887510423507\n",
            "22     \t [0.95697545 0.02967185 0.07296311]. \t  0.10574102614056077 \t 3.7859887510423507\n",
            "23     \t [0.67393751 0.46596782 0.65886108]. \t  1.552198018826415 \t 3.7859887510423507\n",
            "24     \t [0.02976144 0.99005347 0.34261586]. \t  0.6666677541474395 \t 3.7859887510423507\n",
            "25     \t [0.00883771 0.04680592 0.41489659]. \t  0.3427992518669401 \t 3.7859887510423507\n",
            "26     \t [0.98998461 0.10589034 0.5205187 ]. \t  0.11420391030535892 \t 3.7859887510423507\n",
            "27     \t [0.40166306 0.0073971  0.2971015 ]. \t  0.861048034139009 \t 3.7859887510423507\n",
            "28     \t [0.44765897 0.52101607 0.99548192]. \t  2.124536785267271 \t 3.7859887510423507\n",
            "29     \t [0.07775889 0.32134065 0.08101328]. \t  0.1805078909640293 \t 3.7859887510423507\n",
            "30     \t [1.         0.88229598 0.04018327]. \t  0.00028280570102994206 \t 3.7859887510423507\n",
            "31     \t [0.63695442 0.19345711 0.67316354]. \t  0.7158065553199365 \t 3.7859887510423507\n",
            "32     \t [0.08364875 0.70324552 0.18000765]. \t  0.053297210490026 \t 3.7859887510423507\n",
            "33     \t [0.71862106 0.57869395 0.48791097]. \t  0.4877635725523503 \t 3.7859887510423507\n",
            "34     \t [0.71015982 0.06240761 0.63233308]. \t  0.22214275190933627 \t 3.7859887510423507\n",
            "35     \t [0.29537222 0.35230617 0.91184422]. \t  2.3294800034017893 \t 3.7859887510423507\n",
            "36     \t [0.93264058 0.79466263 0.80134094]. \t  1.8719682539032476 \t 3.7859887510423507\n",
            "37     \t [0.99370289 0.94780616 0.6772205 ]. \t  0.4206480841228158 \t 3.7859887510423507\n",
            "38     \t [0.79155526 0.75895341 0.35415099]. \t  0.20442672556046407 \t 3.7859887510423507\n",
            "39     \t [0.41819026 0.01110224 0.42672782]. \t  0.42020265486539904 \t 3.7859887510423507\n",
            "40     \t [0.78397085 0.65300818 0.7031766 ]. \t  1.901811970012256 \t 3.7859887510423507\n",
            "41     \t [0.00212608 0.68961232 0.72529341]. \t  2.638644485498675 \t 3.7859887510423507\n",
            "42     \t [0.23444754 0.59936677 0.83483382]. \t  3.7597520195818515 \t 3.7859887510423507\n",
            "43     \t [0.04359117 0.24712361 0.53820461]. \t  0.3207940100321773 \t 3.7859887510423507\n",
            "44     \t [0.92602388 0.994073   0.32371181]. \t  0.07090802659903218 \t 3.7859887510423507\n",
            "45     \t [0.57344005 0.61197114 0.18922437]. \t  0.07785242306504275 \t 3.7859887510423507\n",
            "46     \t [0.12012265 0.14761798 0.26838165]. \t  0.8242627741310308 \t 3.7859887510423507\n",
            "47     \t [0.27362221 0.36267462 0.50258601]. \t  0.44167122163239614 \t 3.7859887510423507\n",
            "48     \t [0.34693366 0.68782871 0.87437651]. \t  3.2544861467751325 \t 3.7859887510423507\n",
            "49     \t [0.76667116 0.6392036  0.80705852]. \t  3.2443713047496106 \t 3.7859887510423507\n",
            "50     \t [0.13767661 0.66241837 0.82792657]. \t  3.425531063299217 \t 3.7859887510423507\n",
            "51     \t [0.95793867 0.66615194 0.60097035]. \t  0.7192507904766757 \t 3.7859887510423507\n",
            "52     \t [0.05145473 0.64851336 0.21348459]. \t  0.09474949345676345 \t 3.7859887510423507\n",
            "53     \t [0.48623021 0.77377589 0.411413  ]. \t  0.9734191014544429 \t 3.7859887510423507\n",
            "54     \t [0.78882542 0.13270961 0.90543571]. \t  0.6333344930577843 \t 3.7859887510423507\n",
            "55     \t [0.24220402 0.25654683 0.99013943]. \t  0.8858897160755633 \t 3.7859887510423507\n",
            "56     \t [0.22829701 0.05339116 0.54151275]. \t  0.16354623570708623 \t 3.7859887510423507\n",
            "57     \t [0.76016192 0.74881222 0.56770784]. \t  0.9373503703113361 \t 3.7859887510423507\n",
            "58     \t [0.2795376  0.58991046 0.2037006 ]. \t  0.12301042643614361 \t 3.7859887510423507\n",
            "59     \t [0.31980344 0.82750827 0.11581836]. \t  0.011157555616638023 \t 3.7859887510423507\n",
            "60     \t [0.20118285 0.96464589 0.24263923]. \t  0.14556622456810533 \t 3.7859887510423507\n",
            "61     \t [0.51970053 0.84549219 0.4922885 ]. \t  1.6293240662063382 \t 3.7859887510423507\n",
            "62     \t [0.82379254 0.81675594 0.85550943]. \t  1.9278312884785196 \t 3.7859887510423507\n",
            "63     \t [0.40139966 0.0505134  0.44444517]. \t  0.38492138667391107 \t 3.7859887510423507\n",
            "64     \t [0.41306454 0.59975919 0.95354358]. \t  2.885245533239425 \t 3.7859887510423507\n",
            "65     \t [0.90762895 0.09043539 0.35045986]. \t  0.33954753185910946 \t 3.7859887510423507\n",
            "66     \t [0.10936729 0.10358713 0.96204372]. \t  0.3545374226997494 \t 3.7859887510423507\n",
            "67     \t [0.04035773 0.82036326 0.80315336]. \t  2.0799583721374217 \t 3.7859887510423507\n",
            "68     \t [0.17023444 0.85258575 0.91493466]. \t  1.5565545405274104 \t 3.7859887510423507\n",
            "69     \t [0.87734285 0.27734693 0.62186276]. \t  0.6661400065856827 \t 3.7859887510423507\n",
            "70     \t [0.85442665 0.36765057 0.29538047]. \t  0.2636416785367353 \t 3.7859887510423507\n",
            "71     \t [0.65398105 0.18685371 0.2556733 ]. \t  0.7441875730102886 \t 3.7859887510423507\n",
            "72     \t [0.16483906 0.02722539 0.07838903]. \t  0.27911575215119816 \t 3.7859887510423507\n",
            "73     \t [0.88555469 0.94382983 0.28432556]. \t  0.05271556107938427 \t 3.7859887510423507\n",
            "74     \t [0.68988585 0.09996215 0.53871476]. \t  0.1713866911101003 \t 3.7859887510423507\n",
            "75     \t [0.0180464  0.85478503 0.24884179]. \t  0.17919690815782152 \t 3.7859887510423507\n",
            "76     \t [0.78321123 0.04993389 0.03396936]. \t  0.11155547807075329 \t 3.7859887510423507\n",
            "77     \t [0.83488428 0.17038079 0.32578126]. \t  0.4593399119267525 \t 3.7859887510423507\n",
            "78     \t [0.00337352 0.81377532 0.47141789]. \t  2.3007426133611486 \t 3.7859887510423507\n",
            "79     \t [0.44659059 0.34840865 0.05190856]. \t  0.14299750465571026 \t 3.7859887510423507\n",
            "80     \t [0.82874866 0.46665737 0.88077643]. \t  3.426233378521397 \t 3.7859887510423507\n",
            "81     \t [0.44960303 0.01595051 0.18324486]. \t  0.716359984072404 \t 3.7859887510423507\n",
            "82     \t [0.24540929 0.96089703 0.58222289]. \t  2.6288618583053487 \t 3.7859887510423507\n",
            "83     \t [0.60404809 0.72598803 0.35607217]. \t  0.37369385522183546 \t 3.7859887510423507\n",
            "84     \t [0.48995829 0.47442484 0.90045138]. \t  3.391988206652351 \t 3.7859887510423507\n",
            "85     \t [0.86483266 0.77760985 0.20264433]. \t  0.017360610657446037 \t 3.7859887510423507\n",
            "86     \t [0.96077586 0.70534906 0.83842714]. \t  2.8847770418582708 \t 3.7859887510423507\n",
            "87     \t [0.46111202 0.66460214 0.78780807]. \t  3.0911449545854603 \t 3.7859887510423507\n",
            "88     \t [0.17321434 0.20357407 0.55873031]. \t  0.3164822550587725 \t 3.7859887510423507\n",
            "89     \t [0.03865646 0.34254306 0.56508583]. \t  0.5978670329887741 \t 3.7859887510423507\n",
            "90     \t [0.79060194 0.25759698 0.43351173]. \t  0.24940990598258378 \t 3.7859887510423507\n",
            "91     \t [0.32236939 0.42199787 0.28337111]. \t  0.42707895490724906 \t 3.7859887510423507\n",
            "92     \t [0.67223202 0.36264268 0.17064459]. \t  0.31461328905573827 \t 3.7859887510423507\n",
            "93     \t [0.62734209 0.15873131 0.39466884]. \t  0.505366693689942 \t 3.7859887510423507\n",
            "94     \t [0.86554748 0.19661497 0.24919996]. \t  0.4438912830916337 \t 3.7859887510423507\n",
            "95     \t [0.98606367 0.93178489 0.99407399]. \t  0.5702372182084068 \t 3.7859887510423507\n",
            "96     \t [0.13527758 0.02125589 0.12677246]. \t  0.42834497445761877 \t 3.7859887510423507\n",
            "97     \t [0.6710944  0.88514049 0.09061597]. \t  0.0026314471908069825 \t 3.7859887510423507\n",
            "98     \t [0.13400302 0.73596897 0.96113073]. \t  2.091712838707545 \t 3.7859887510423507\n",
            "99     \t [0.8586582  0.53204264 0.85304921]. \t  3.721611532889103 \t 3.7859887510423507\n",
            "100    \t [0.86204299 0.60518289 0.98541934]. \t  2.265846033509685 \t 3.7859887510423507\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CoQHSe2Dtd1m",
        "outputId": "bdc0d1db-7fd6-405a-9f3c-87ec742ddbdf"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 14\r\n",
        "\r\n",
        "np.random.seed(run_num_14)\r\n",
        "surrogate_loser_14 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "loser_14 = dGPGO(surrogate_loser_14, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_14.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.64755105 0.50714969 0.52834138]. \t  0.5963212059988954 \t 2.610000357863649\n",
            "init   \t [0.8962852  0.69999119 0.7142971 ]. \t  1.7197848290620104 \t 2.610000357863649\n",
            "init   \t [0.71733838 0.22281946 0.17515452]. \t  0.48166052848103497 \t 2.610000357863649\n",
            "init   \t [0.45684149 0.92873843 0.00988589]. \t  0.0004588015757462679 \t 2.610000357863649\n",
            "init   \t [0.08992219 0.85020027 0.48562106]. \t  2.610000357863649 \t 2.610000357863649\n",
            "1      \t [0.07426471 0.99752623 0.88021278]. \t  0.6630933245090485 \t 2.610000357863649\n",
            "2      \t [0.00183165 0.47030174 0.11424444]. \t  0.0965701557020153 \t 2.610000357863649\n",
            "3      \t [0.04945322 0.96746298 0.34336512]. \t  0.71192198606886 \t 2.610000357863649\n",
            "4      \t [1.         1.         0.34022154]. \t  0.05968457037811824 \t 2.610000357863649\n",
            "5      \t [0.02018382 0.66869984 0.65233773]. \t  2.405736826463803 \t 2.610000357863649\n",
            "6      \t [0.98316389 0.95230271 0.91112149]. \t  0.7577541915119886 \t 2.610000357863649\n",
            "7      \t [0.90704093 0.48954436 0.08174793]. \t  0.037394353710536885 \t 2.610000357863649\n",
            "8      \t [0.22114138 0.79060245 0.4871729 ]. \t  2.3968131562451074 \t 2.610000357863649\n",
            "9      \t [0.11973516 0.79385856 0.47220507]. \t  2.327657029595589 \t 2.610000357863649\n",
            "10     \t [4.43715863e-16 8.57457020e-16 3.54981701e-16]. \t  0.06797411659013285 \t 2.610000357863649\n",
            "11     \t [0.54065881 0.94923953 0.55300518]. \t  1.6604480885303885 \t 2.610000357863649\n",
            "12     \t [0.00359052 0.82103204 0.66262341]. \t  2.517650573420291 \t 2.610000357863649\n",
            "13     \t [0.83290042 0.01398428 0.99121239]. \t  0.11416467958709316 \t 2.610000357863649\n",
            "14     \t [0.99633872 0.31485976 0.71516283]. \t  1.5604264387790878 \t 2.610000357863649\n",
            "15     \t [0.23012355 0.97789034 0.56348271]. \t  2.6047985483443408 \t 2.610000357863649\n",
            "16     \t [0.19780628 0.84579831 0.61246098]. \t  \u001b[92m2.8699833606292318\u001b[0m \t 2.8699833606292318\n",
            "17     \t [0.92241755 0.02069622 0.06753755]. \t  0.10980622757934344 \t 2.8699833606292318\n",
            "18     \t [0.96873907 0.56322882 0.9383951 ]. \t  \u001b[92m3.050598972616365\u001b[0m \t 3.050598972616365\n",
            "19     \t [0.50238505 0.17847192 0.0068099 ]. \t  0.11920865160208076 \t 3.050598972616365\n",
            "20     \t [0.         0.         0.52522608]. \t  0.11212774814645654 \t 3.050598972616365\n",
            "21     \t [0.19454059 0.18159557 0.72237632]. \t  0.8914520401243811 \t 3.050598972616365\n",
            "22     \t [0.16724033 0.25522918 0.97108959]. \t  1.022888839175624 \t 3.050598972616365\n",
            "23     \t [0.40019455 0.99076213 0.36883095]. \t  0.7195529257060763 \t 3.050598972616365\n",
            "24     \t [0.20310957 0.77766367 0.58326912]. \t  2.8418115627537075 \t 3.050598972616365\n",
            "25     \t [0.94908331 0.93559586 0.01052216]. \t  0.00011013250890670079 \t 3.050598972616365\n",
            "26     \t [0.9697394  0.46710464 0.97359054]. \t  2.257700375199257 \t 3.050598972616365\n",
            "27     \t [0.89615666 0.00768718 0.33627956]. \t  0.33469361102942724 \t 3.050598972616365\n",
            "28     \t [0.38149194 0.03150917 0.95003229]. \t  0.19751289845086012 \t 3.050598972616365\n",
            "29     \t [0.27445978 0.91477366 0.68346722]. \t  2.009026079099388 \t 3.050598972616365\n",
            "30     \t [0.97345089 0.61327362 0.84793905]. \t  \u001b[92m3.5325036368275318\u001b[0m \t 3.5325036368275318\n",
            "31     \t [0.96803703 0.49412009 0.97246297]. \t  2.3800799393998964 \t 3.5325036368275318\n",
            "32     \t [0.99076004 0.64244821 0.99111959]. \t  2.048367904818647 \t 3.5325036368275318\n",
            "33     \t [0.99807668 0.53227242 0.89848652]. \t  3.481955888934822 \t 3.5325036368275318\n",
            "34     \t [0.95841897 0.99084348 0.68758232]. \t  0.3695955295760476 \t 3.5325036368275318\n",
            "35     \t [0.96232362 0.52546593 0.40224017]. \t  0.10604193758342897 \t 3.5325036368275318\n",
            "36     \t [0.17653826 0.21738289 0.96215749]. \t  0.8595982659733136 \t 3.5325036368275318\n",
            "37     \t [0.28318428 0.71248563 0.53696315]. \t  2.2598699965857656 \t 3.5325036368275318\n",
            "38     \t [0.91865577 0.36275365 0.1912791 ]. \t  0.1862391981836753 \t 3.5325036368275318\n",
            "39     \t [0.41527166 0.64813462 0.16672562]. \t  0.058604953951955356 \t 3.5325036368275318\n",
            "40     \t [0.79917452 0.98775728 0.7286888 ]. \t  0.5596406347526663 \t 3.5325036368275318\n",
            "41     \t [0.39789933 0.57798059 0.91184543]. \t  3.509310013992248 \t 3.5325036368275318\n",
            "42     \t [0.44117221 0.43970576 0.85138463]. \t  3.421117537006776 \t 3.5325036368275318\n",
            "43     \t [0.49862776 0.47957223 0.90592127]. \t  3.364667946063466 \t 3.5325036368275318\n",
            "44     \t [0.3546162  0.51018387 0.8547629 ]. \t  \u001b[92m3.790962458669707\u001b[0m \t 3.790962458669707\n",
            "45     \t [0.4413456  0.07346512 0.83536804]. \t  0.48157224856520825 \t 3.790962458669707\n",
            "46     \t [0.42542469 0.33852438 0.46777118]. \t  0.35843440517809333 \t 3.790962458669707\n",
            "47     \t [0.45205372 0.86171738 0.8772896 ]. \t  1.6020784611943335 \t 3.790962458669707\n",
            "48     \t [0.06500231 0.74732896 0.95425446]. \t  2.0866199126866483 \t 3.790962458669707\n",
            "49     \t [0.40675235 0.20604412 0.32647895]. \t  0.835147485697324 \t 3.790962458669707\n",
            "50     \t [0.51629496 0.44238532 0.11428872]. \t  0.1618554098435086 \t 3.790962458669707\n",
            "51     \t [0.01841433 0.66213843 0.44030181]. \t  1.3100501508645412 \t 3.790962458669707\n",
            "52     \t [0.42750177 0.08095008 0.22881845]. \t  0.934718699821992 \t 3.790962458669707\n",
            "53     \t [0.48386171 0.71325686 0.86100882]. \t  3.04148585418282 \t 3.790962458669707\n",
            "54     \t [0.19229006 0.27583495 0.78979266]. \t  1.8392142767687603 \t 3.790962458669707\n",
            "55     \t [0.99607341 0.95060923 0.07884714]. \t  0.00040076247048857424 \t 3.790962458669707\n",
            "56     \t [0.72236324 0.20397123 0.68392677]. \t  0.809154011084921 \t 3.790962458669707\n",
            "57     \t [0.82726544 0.56984414 0.91488448]. \t  3.402460992571767 \t 3.790962458669707\n",
            "58     \t [0.4496368  0.09232231 0.65260425]. \t  0.3286036761854578 \t 3.790962458669707\n",
            "59     \t [0.66686898 0.36122784 0.22139213]. \t  0.3992469319683874 \t 3.790962458669707\n",
            "60     \t [0.84343985 0.47811337 0.0260408 ]. \t  0.024123015022092978 \t 3.790962458669707\n",
            "61     \t [0.50786571 0.30890895 0.09449962]. \t  0.26672971047047095 \t 3.790962458669707\n",
            "62     \t [0.15524704 0.78149988 0.03254962]. \t  0.002786228877873154 \t 3.790962458669707\n",
            "63     \t [0.29530089 0.8267771  0.50007408]. \t  2.4621184302121764 \t 3.790962458669707\n",
            "64     \t [0.73916819 0.92720124 0.92738278]. \t  0.8970108912251867 \t 3.790962458669707\n",
            "65     \t [0.67444173 0.44494752 0.87215447]. \t  3.372482011479299 \t 3.790962458669707\n",
            "66     \t [0.56649389 0.50233033 0.51945521]. \t  0.6376902576687437 \t 3.790962458669707\n",
            "67     \t [0.28076717 0.11678024 0.47315713]. \t  0.3132157828560709 \t 3.790962458669707\n",
            "68     \t [0.34975716 0.55735162 0.44800429]. \t  0.7645979471726574 \t 3.790962458669707\n",
            "69     \t [0.79661882 0.26738063 0.53155921]. \t  0.2656598661584797 \t 3.790962458669707\n",
            "70     \t [0.14360166 0.87285564 0.22607183]. \t  0.11979374408608724 \t 3.790962458669707\n",
            "71     \t [0.20273741 0.68270988 0.68133879]. \t  2.5067715263777925 \t 3.790962458669707\n",
            "72     \t [0.10236774 0.30777171 0.08361548]. \t  0.2042348819686146 \t 3.790962458669707\n",
            "73     \t [0.67858085 0.6219549  0.55325466]. \t  0.9070086533155038 \t 3.790962458669707\n",
            "74     \t [0.14445184 0.18590658 0.93168239]. \t  0.8378508615508395 \t 3.790962458669707\n",
            "75     \t [0.33188654 0.9244444  0.23513361]. \t  0.11906325846609561 \t 3.790962458669707\n",
            "76     \t [0.48097666 0.13009226 0.29282477]. \t  0.9441138008571816 \t 3.790962458669707\n",
            "77     \t [0.32990587 0.58124315 0.16500403]. \t  0.09588791985837758 \t 3.790962458669707\n",
            "78     \t [0.55142752 0.00357512 0.70672671]. \t  0.21495598343963285 \t 3.790962458669707\n",
            "79     \t [0.49708493 0.55071354 0.15142348]. \t  0.10212397980838571 \t 3.790962458669707\n",
            "80     \t [0.83839049 0.61356518 0.90770868]. \t  3.37999894933908 \t 3.790962458669707\n",
            "81     \t [0.10514481 0.86268084 0.29104502]. \t  0.3755481141119861 \t 3.790962458669707\n",
            "82     \t [0.77926204 0.03190372 0.28453967]. \t  0.5564609157951522 \t 3.790962458669707\n",
            "83     \t [0.0844148  0.96013624 0.30520214]. \t  0.42971674692981515 \t 3.790962458669707\n",
            "84     \t [0.91312378 0.47041951 0.20526919]. \t  0.1073063230476932 \t 3.790962458669707\n",
            "85     \t [0.90271173 0.30248265 0.60457918]. \t  0.6061828342528154 \t 3.790962458669707\n",
            "86     \t [0.56818296 0.13607622 0.98090366]. \t  0.40439730435804294 \t 3.790962458669707\n",
            "87     \t [0.02036094 0.91618381 0.91600987]. \t  1.0458528763005754 \t 3.790962458669707\n",
            "88     \t [0.58408506 0.22139976 0.15178313]. \t  0.5231401031426556 \t 3.790962458669707\n",
            "89     \t [0.7460948 0.9881278 0.4693557]. \t  0.6309118965643825 \t 3.790962458669707\n",
            "90     \t [0.64502969 0.7036153  0.33497261]. \t  0.24718341667757182 \t 3.790962458669707\n",
            "91     \t [0.81336943 0.7955296  0.2408704 ]. \t  0.038691247046843534 \t 3.790962458669707\n",
            "92     \t [0.22908182 0.34958393 0.70349495]. \t  1.7538685921576378 \t 3.790962458669707\n",
            "93     \t [0.02556752 0.98745888 0.72362373]. \t  1.3883167758583452 \t 3.790962458669707\n",
            "94     \t [0.59237836 0.93434985 0.51741072]. \t  1.4000596558201313 \t 3.790962458669707\n",
            "95     \t [0.07210507 0.7999767  0.69767257]. \t  2.4017222558308857 \t 3.790962458669707\n",
            "96     \t [0.01540501 0.18169681 0.53582139]. \t  0.23750339174042914 \t 3.790962458669707\n",
            "97     \t [0.38957153 0.32283297 0.30482957]. \t  0.6454892445601828 \t 3.790962458669707\n",
            "98     \t [0.38533378 0.41887816 0.54780256]. \t  0.6877116008288234 \t 3.790962458669707\n",
            "99     \t [0.14901495 0.8771433  0.04026063]. \t  0.001634100766366568 \t 3.790962458669707\n",
            "100    \t [0.62120192 0.76496167 0.20277625]. \t  0.04053907768932854 \t 3.790962458669707\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aq4J_FTytd4O",
        "outputId": "b009b3c1-8233-4215-b511-d5f06be673c4"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 15\r\n",
        "\r\n",
        "np.random.seed(run_num_15)\r\n",
        "surrogate_loser_15 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "loser_15 = dGPGO(surrogate_loser_15, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_15.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.57788186 0.25550133 0.25252687]. \t  0.7217338409961529 \t 1.540625560354162\n",
            "init   \t [0.70990435 0.44755236 0.22694296]. \t  0.23199690080875204 \t 1.540625560354162\n",
            "init   \t [0.40170957 0.88225774 0.43721347]. \t  1.540625560354162 \t 1.540625560354162\n",
            "init   \t [0.87842518 0.78052775 0.53421429]. \t  0.5653855252190516 \t 1.540625560354162\n",
            "init   \t [0.01173301 0.26575648 0.3311941 ]. \t  0.5019682819961726 \t 1.540625560354162\n",
            "1      \t [0.45870705 0.97826428 0.97993811]. \t  0.46749494069699443 \t 1.540625560354162\n",
            "2      \t [0.57415019 0.         0.74787924]. \t  0.2382252101275239 \t 1.540625560354162\n",
            "3      \t [0.29062705 0.98880101 0.29539427]. \t  0.31682912945841557 \t 1.540625560354162\n",
            "4      \t [0.43535676 0.57154161 0.57219925]. \t  1.3316104943205571 \t 1.540625560354162\n",
            "5      \t [0.57655701 0.96655517 0.45190655]. \t  1.044454263539026 \t 1.540625560354162\n",
            "6      \t [0.1320059  0.88475329 0.73070609]. \t  \u001b[92m1.8815619089044315\u001b[0m \t 1.8815619089044315\n",
            "7      \t [0.20225734 0.91762416 0.5993738 ]. \t  \u001b[92m2.8143502335642547\u001b[0m \t 2.8143502335642547\n",
            "8      \t [0.16555403 0.97361575 0.60531603]. \t  2.5650740349200474 \t 2.8143502335642547\n",
            "9      \t [0.96497544 0.07697573 0.01139424]. \t  0.04752173213497603 \t 2.8143502335642547\n",
            "10     \t [0.03327723 0.64188914 0.3369034 ]. \t  0.45761760123727435 \t 2.8143502335642547\n",
            "11     \t [0.41813569 0.02117882 0.4779319 ]. \t  0.25775969864154175 \t 2.8143502335642547\n",
            "12     \t [0.95879744 0.05984281 0.94784765]. \t  0.2571601167758112 \t 2.8143502335642547\n",
            "13     \t [0.11014298 0.         0.        ]. \t  0.08364007088942793 \t 2.8143502335642547\n",
            "14     \t [0.35527983 0.90916712 0.67741579]. \t  1.9203936704155313 \t 2.8143502335642547\n",
            "15     \t [0.99283118 0.9511519  0.62298994]. \t  0.3552350859464189 \t 2.8143502335642547\n",
            "16     \t [0.18147782 0.8508714  0.52985713]. \t  \u001b[92m2.9458934218402075\u001b[0m \t 2.9458934218402075\n",
            "17     \t [0.01623769 0.51466513 0.63429125]. \t  1.7332314038985324 \t 2.9458934218402075\n",
            "18     \t [0.10335547 0.78073509 0.56438992]. \t  2.9208937301662594 \t 2.9458934218402075\n",
            "19     \t [0.96122811 0.99723448 0.34352912]. \t  0.07674862353276775 \t 2.9458934218402075\n",
            "20     \t [0.07841726 0.94982537 0.54372164]. \t  2.8450922021504472 \t 2.9458934218402075\n",
            "21     \t [0.47470863 0.05941278 0.98107859]. \t  0.20276338663977286 \t 2.9458934218402075\n",
            "22     \t [0.09348781 0.91066751 0.51469921]. \t  2.8459236162875943 \t 2.9458934218402075\n",
            "23     \t [0.99943586 0.24544196 0.37414875]. \t  0.19099191679550337 \t 2.9458934218402075\n",
            "24     \t [0.86090899 0.82391403 0.9345957 ]. \t  1.6120319357268127 \t 2.9458934218402075\n",
            "25     \t [0.94502108 0.39990792 0.46043258]. \t  0.1539566760694928 \t 2.9458934218402075\n",
            "26     \t [0.08282586 0.77020464 0.55400076]. \t  2.8494528624919098 \t 2.9458934218402075\n",
            "27     \t [0.74102955 0.10310999 0.55334669]. \t  0.17062597575798433 \t 2.9458934218402075\n",
            "28     \t [0.18374988 0.8686762  0.55497609]. \t  \u001b[92m3.032624131081467\u001b[0m \t 3.032624131081467\n",
            "29     \t [0.64977247 0.99241348 0.0401207 ]. \t  0.0004630520725081235 \t 3.032624131081467\n",
            "30     \t [0.95305461 0.98549338 0.93676392]. \t  0.5362971674464517 \t 3.032624131081467\n",
            "31     \t [0.37857704 0.4008916  0.57095718]. \t  0.7635747625375403 \t 3.032624131081467\n",
            "32     \t [0.09185293 0.74550382 0.61485494]. \t  2.7270945552334847 \t 3.032624131081467\n",
            "33     \t [0.81464089 0.20410306 0.19092096]. \t  0.42888437233317794 \t 3.032624131081467\n",
            "34     \t [0.85842146 0.29924661 0.15859815]. \t  0.24543263868828105 \t 3.032624131081467\n",
            "35     \t [0.24974013 0.69021691 0.23669921]. \t  0.1322942127617831 \t 3.032624131081467\n",
            "36     \t [0.09799486 0.0101965  0.80726401]. \t  0.27442220587581845 \t 3.032624131081467\n",
            "37     \t [0.09235068 0.59453282 0.5299634 ]. \t  1.5829305238168287 \t 3.032624131081467\n",
            "38     \t [0.13771917 0.43744959 0.88943158]. \t  \u001b[92m3.232777206779901\u001b[0m \t 3.232777206779901\n",
            "39     \t [0.1641477  0.51364821 0.94131128]. \t  3.0322764746781763 \t 3.232777206779901\n",
            "40     \t [0.3636355  0.46116569 0.98184479]. \t  2.1722178828530816 \t 3.232777206779901\n",
            "41     \t [0.02034258 0.44509854 0.87865083]. \t  \u001b[92m3.3324225114014894\u001b[0m \t 3.3324225114014894\n",
            "42     \t [0.05687073 0.35584674 0.95015972]. \t  1.9379969571552922 \t 3.3324225114014894\n",
            "43     \t [0.07681371 0.52740433 0.90294471]. \t  \u001b[92m3.547890552981594\u001b[0m \t 3.547890552981594\n",
            "44     \t [0.95675721 0.92668882 0.00587561]. \t  0.000105073781984434 \t 3.547890552981594\n",
            "45     \t [0.55513085 0.78585576 0.17222904]. \t  0.026846057895913045 \t 3.547890552981594\n",
            "46     \t [0.32640468 0.99936566 0.57634998]. \t  2.227868485959561 \t 3.547890552981594\n",
            "47     \t [0.10589905 0.5469575  0.0158781 ]. \t  0.019376708817783993 \t 3.547890552981594\n",
            "48     \t [0.72025457 0.42821853 0.89108759]. \t  3.1298633504098006 \t 3.547890552981594\n",
            "49     \t [0.88471708 0.05032498 0.84873591]. \t  0.37411211371285885 \t 3.547890552981594\n",
            "50     \t [0.37463577 0.52589471 0.87634371]. \t  \u001b[92m3.7726339942689924\u001b[0m \t 3.7726339942689924\n",
            "51     \t [0.01588795 0.49292129 0.87888474]. \t  3.6040856502904433 \t 3.7726339942689924\n",
            "52     \t [0.04936964 0.76643056 0.27365458]. \t  0.25866039619980574 \t 3.7726339942689924\n",
            "53     \t [0.20524069 0.76502703 0.87035564]. \t  2.5771584611667055 \t 3.7726339942689924\n",
            "54     \t [0.44579854 0.10044851 0.46713745]. \t  0.32543707520716264 \t 3.7726339942689924\n",
            "55     \t [0.76353136 0.02023589 0.33067034]. \t  0.5065642586272455 \t 3.7726339942689924\n",
            "56     \t [0.05689745 0.55302702 0.21793852]. \t  0.139296626823076 \t 3.7726339942689924\n",
            "57     \t [0.93704072 0.31603255 0.75654058]. \t  1.9235605761300654 \t 3.7726339942689924\n",
            "58     \t [0.92634453 0.55250125 0.37701181]. \t  0.10624015191215301 \t 3.7726339942689924\n",
            "59     \t [0.98566888 0.93332484 0.38748483]. \t  0.12626546681121031 \t 3.7726339942689924\n",
            "60     \t [0.52099922 0.16674173 0.54409066]. \t  0.2500879587637723 \t 3.7726339942689924\n",
            "61     \t [0.45686738 0.7185135  0.97018309]. \t  2.0936312622869466 \t 3.7726339942689924\n",
            "62     \t [0.38554434 0.52484819 0.3410793 ]. \t  0.3446974468763136 \t 3.7726339942689924\n",
            "63     \t [0.48744492 0.75878045 0.40262775]. \t  0.8724685464007343 \t 3.7726339942689924\n",
            "64     \t [0.32494146 0.55551894 0.11106917]. \t  0.07247095796871711 \t 3.7726339942689924\n",
            "65     \t [0.22437916 0.18511023 0.39884955]. \t  0.554974513879724 \t 3.7726339942689924\n",
            "66     \t [0.66195118 0.83031165 0.1263408 ]. \t  0.007416368719395564 \t 3.7726339942689924\n",
            "67     \t [0.45277534 0.90837962 0.37948517]. \t  0.8302928560503484 \t 3.7726339942689924\n",
            "68     \t [0.11030779 0.93358032 0.79663281]. \t  1.2673113215586038 \t 3.7726339942689924\n",
            "69     \t [0.75689381 0.26626376 0.40769996]. \t  0.3093906641862753 \t 3.7726339942689924\n",
            "70     \t [0.25249093 0.86169084 0.93775836]. \t  1.3487665227408552 \t 3.7726339942689924\n",
            "71     \t [0.07417741 0.78342391 0.77384638]. \t  2.3549929045130416 \t 3.7726339942689924\n",
            "72     \t [0.47843585 0.0947401  0.45417235]. \t  0.35892328654689665 \t 3.7726339942689924\n",
            "73     \t [0.0028475  0.80972295 0.83880453]. \t  2.1616271984147324 \t 3.7726339942689924\n",
            "74     \t [0.18412308 0.40611329 0.4560012 ]. \t  0.4478611342438572 \t 3.7726339942689924\n",
            "75     \t [0.72015424 0.33382483 0.25526604]. \t  0.43357330396654686 \t 3.7726339942689924\n",
            "76     \t [0.63260961 0.21704913 0.29129282]. \t  0.7245423555057698 \t 3.7726339942689924\n",
            "77     \t [0.55578338 0.43008045 0.87967857]. \t  3.2470912244888375 \t 3.7726339942689924\n",
            "78     \t [0.12989691 0.37828551 0.63240513]. \t  1.1871928238269298 \t 3.7726339942689924\n",
            "79     \t [0.53736537 0.60544502 0.00838678]. \t  0.01141987847330488 \t 3.7726339942689924\n",
            "80     \t [0.53955518 0.92113759 0.71411087]. \t  1.251299842896169 \t 3.7726339942689924\n",
            "81     \t [0.78266961 0.80364241 0.95875221]. \t  1.5914169852142543 \t 3.7726339942689924\n",
            "82     \t [0.98124241 0.27066251 0.49880464]. \t  0.16785373269959117 \t 3.7726339942689924\n",
            "83     \t [0.86908715 0.7723944  0.27931136]. \t  0.05584375404926373 \t 3.7726339942689924\n",
            "84     \t [0.18191085 0.87279776 0.06389839]. \t  0.0030067773074108337 \t 3.7726339942689924\n",
            "85     \t [0.68267667 0.23806959 0.53615028]. \t  0.27687692743916364 \t 3.7726339942689924\n",
            "86     \t [0.51580088 0.11030196 0.4589715 ]. \t  0.33837634771568686 \t 3.7726339942689924\n",
            "87     \t [0.96852447 0.15481291 0.01517424]. \t  0.049788937359754426 \t 3.7726339942689924\n",
            "88     \t [0.57673346 0.28305391 0.46648613]. \t  0.3037232717970031 \t 3.7726339942689924\n",
            "89     \t [0.74398022 0.26145588 0.77234634]. \t  1.630304906430959 \t 3.7726339942689924\n",
            "90     \t [0.9634963  0.27587429 0.56715608]. \t  0.35600452235267915 \t 3.7726339942689924\n",
            "91     \t [0.73619882 0.71342445 0.18271428]. \t  0.026606237937590572 \t 3.7726339942689924\n",
            "92     \t [0.57409829 0.41833827 0.3448306 ]. \t  0.3538976036382143 \t 3.7726339942689924\n",
            "93     \t [0.56102639 0.06679605 0.64037298]. \t  0.2483887387972291 \t 3.7726339942689924\n",
            "94     \t [0.93882985 0.30550525 0.23690212]. \t  0.25813917911444156 \t 3.7726339942689924\n",
            "95     \t [0.05289148 0.90095206 0.18316854]. \t  0.04819088234293035 \t 3.7726339942689924\n",
            "96     \t [0.47581993 0.98761237 0.71369248]. \t  1.0932033178536977 \t 3.7726339942689924\n",
            "97     \t [0.79326914 0.88886517 0.95242994]. \t  1.0291534985796806 \t 3.7726339942689924\n",
            "98     \t [0.90504706 0.25651419 0.73046914]. \t  1.3447868969921708 \t 3.7726339942689924\n",
            "99     \t [0.35541618 0.87374982 0.17466265]. \t  0.03535399187079531 \t 3.7726339942689924\n",
            "100    \t [0.61162121 0.13982352 0.67154308]. \t  0.513200668672658 \t 3.7726339942689924\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4tJ-ta9Gtd7H",
        "outputId": "c3b6ddce-f929-41f3-dddd-5ddf3508b883"
      },
      "source": [
        "\r\n",
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 16\r\n",
        "\r\n",
        "np.random.seed(run_num_16)\r\n",
        "surrogate_loser_16 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "loser_16 = dGPGO(surrogate_loser_16, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_16.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.86394833 0.316366   0.67215078]. \t  1.1947633822631252 \t 3.8084053754826726\n",
            "init   \t [0.50791167 0.78166043 0.28368168]. \t  0.20052765717741963 \t 3.8084053754826726\n",
            "init   \t [0.23370878 0.56379969 0.87502436]. \t  3.8084053754826726 \t 3.8084053754826726\n",
            "init   \t [0.71894292 0.18213174 0.24380041]. \t  0.6532826728302027 \t 3.8084053754826726\n",
            "init   \t [0.08605673 0.53424539 0.83969965]. \t  3.8072565344663847 \t 3.8084053754826726\n",
            "1      \t [0.12378065 0.3168353  0.9881258 ]. \t  1.2451880833929392 \t 3.8084053754826726\n",
            "2      \t [0.27777137 0.94321124 0.96390235]. \t  0.6864412627036871 \t 3.8084053754826726\n",
            "3      \t [0.04424604 0.50197838 0.49031238]. \t  0.8231348239757807 \t 3.8084053754826726\n",
            "4      \t [0.00628562 0.62733695 0.98683809]. \t  2.216178855902818 \t 3.8084053754826726\n",
            "5      \t [0.24302952 0.56104258 0.71810923]. \t  2.714059998275844 \t 3.8084053754826726\n",
            "6      \t [0.96778771 0.7812586  0.04735995]. \t  0.0011023859383045742 \t 3.8084053754826726\n",
            "7      \t [0.94597768 0.01177013 0.21221742]. \t  0.30096480916717455 \t 3.8084053754826726\n",
            "8      \t [0.8171649  0.73766983 0.95086348]. \t  2.1626172365292398 \t 3.8084053754826726\n",
            "9      \t [0.61496661 0.46734526 0.01942058]. \t  0.03873121346787046 \t 3.8084053754826726\n",
            "10     \t [0.80321937 0.9517176  0.02576245]. \t  0.0002434110966197108 \t 3.8084053754826726\n",
            "11     \t [0.50795096 0.70412141 0.95623856]. \t  2.374229919305513 \t 3.8084053754826726\n",
            "12     \t [0.16239385 0.55378967 0.90205333]. \t  3.608086628159596 \t 3.8084053754826726\n",
            "13     \t [0.06460378 0.91264441 0.19694417]. \t  0.06429592639676497 \t 3.8084053754826726\n",
            "14     \t [0.89224049 0.99299674 0.53740514]. \t  0.42915213568812904 \t 3.8084053754826726\n",
            "15     \t [0.84816517 0.39323114 0.99170065]. \t  1.6321149312091097 \t 3.8084053754826726\n",
            "16     \t [4.37312646e-04 5.53803144e-01 7.95326299e-01]. \t  3.5341512321783823 \t 3.8084053754826726\n",
            "17     \t [4.24339113e-14 2.81227772e-14 3.36368343e-14]. \t  0.06797411659017981 \t 3.8084053754826726\n",
            "18     \t [0.93697085 0.6048629  0.69077647]. \t  1.7505391354700963 \t 3.8084053754826726\n",
            "19     \t [0.06196911 0.9379467  0.18851526]. \t  0.05190659427104055 \t 3.8084053754826726\n",
            "20     \t [0.12966175 0.61321656 0.85641987]. \t  3.731291338843464 \t 3.8084053754826726\n",
            "21     \t [0.01141799 0.7958181  0.04024629]. \t  0.0024257263715949007 \t 3.8084053754826726\n",
            "22     \t [0.47378794 0.95181223 0.60819171]. \t  1.8355609890458182 \t 3.8084053754826726\n",
            "23     \t [0.90603606 0.25244893 0.40295125]. \t  0.21999200267847413 \t 3.8084053754826726\n",
            "24     \t [0.67644299 0.17377249 0.64549339]. \t  0.5184753684240689 \t 3.8084053754826726\n",
            "25     \t [0.0918845  0.35222564 0.4215209 ]. \t  0.3685063985974734 \t 3.8084053754826726\n",
            "26     \t [0.52488244 0.425859   0.11355576]. \t  0.17691971367879086 \t 3.8084053754826726\n",
            "27     \t [0.0796643  0.536108   0.88064967]. \t  3.7395446042988794 \t 3.8084053754826726\n",
            "28     \t [0.22418673 0.0710134  0.17935842]. \t  0.7291417604546632 \t 3.8084053754826726\n",
            "29     \t [0.23621156 0.77273158 0.72314536]. \t  2.3689595603050706 \t 3.8084053754826726\n",
            "30     \t [0.08934153 0.48178943 0.84929768]. \t  3.6533056075788517 \t 3.8084053754826726\n",
            "31     \t [0.45034395 0.31184179 0.49118223]. \t  0.34019795084950083 \t 3.8084053754826726\n",
            "32     \t [0.46134655 0.08631294 0.92137812]. \t  0.4002426920815167 \t 3.8084053754826726\n",
            "33     \t [0.19064436 0.41679247 0.29594489]. \t  0.4111794760569056 \t 3.8084053754826726\n",
            "34     \t [0.75430526 0.46631548 0.45988832]. \t  0.2589634023281027 \t 3.8084053754826726\n",
            "35     \t [0.13569583 0.80181312 0.26061919]. \t  0.2203440587860694 \t 3.8084053754826726\n",
            "36     \t [0.0210929  0.27206192 0.20612286]. \t  0.490978954096385 \t 3.8084053754826726\n",
            "37     \t [0.02629512 0.25758702 0.77117475]. \t  1.5962871548899078 \t 3.8084053754826726\n",
            "38     \t [0.0730912  0.3120373  0.68606544]. \t  1.373801062907681 \t 3.8084053754826726\n",
            "39     \t [0.60526256 0.23723161 0.17647999]. \t  0.5717865799947605 \t 3.8084053754826726\n",
            "40     \t [0.72925291 0.17985652 0.1038405 ]. \t  0.292122585001201 \t 3.8084053754826726\n",
            "41     \t [0.79270642 0.632309   0.21490531]. \t  0.050736446991021644 \t 3.8084053754826726\n",
            "42     \t [0.74023708 0.53345148 0.69811026]. \t  2.097988154667948 \t 3.8084053754826726\n",
            "43     \t [0.93793031 0.3497155  0.18397546]. \t  0.1792599539864782 \t 3.8084053754826726\n",
            "44     \t [0.86734098 0.14622375 0.46534638]. \t  0.1795843969354928 \t 3.8084053754826726\n",
            "45     \t [0.32042685 0.21356485 0.69082321]. \t  0.909072075181484 \t 3.8084053754826726\n",
            "46     \t [0.93574892 0.27989918 0.01027377]. \t  0.04031023584034215 \t 3.8084053754826726\n",
            "47     \t [0.81706479 0.81081815 0.35408109]. \t  0.1967663213350807 \t 3.8084053754826726\n",
            "48     \t [0.87105988 0.83328345 0.27765357]. \t  0.054612293237242315 \t 3.8084053754826726\n",
            "49     \t [0.80936618 0.41185184 0.98653548]. \t  1.8141680762717804 \t 3.8084053754826726\n",
            "50     \t [0.1355641  0.54577091 0.8484696 ]. \t  \u001b[92m3.8424198169073027\u001b[0m \t 3.8424198169073027\n",
            "51     \t [0.96048891 0.86684945 0.3033962 ]. \t  0.05267070671118978 \t 3.8424198169073027\n",
            "52     \t [0.12271592 0.1474196  0.20314984]. \t  0.7305270817652886 \t 3.8424198169073027\n",
            "53     \t [0.17349131 0.99299808 0.26561668]. \t  0.20964188064412295 \t 3.8424198169073027\n",
            "54     \t [0.39313108 0.06006951 0.06830073]. \t  0.29458594150999273 \t 3.8424198169073027\n",
            "55     \t [0.11058698 0.97447688 0.3993736 ]. \t  1.3143788435188235 \t 3.8424198169073027\n",
            "56     \t [0.87576718 0.49481146 0.55244252]. \t  0.5009209227040916 \t 3.8424198169073027\n",
            "57     \t [0.02274508 0.95137123 0.03512004]. \t  0.0009698546243275209 \t 3.8424198169073027\n",
            "58     \t [0.38999471 0.77210159 0.1036472 ]. \t  0.010896907604143387 \t 3.8424198169073027\n",
            "59     \t [0.21582292 0.48375857 0.98977518]. \t  2.1204190952489697 \t 3.8424198169073027\n",
            "60     \t [0.73989052 0.66110466 0.04476188]. \t  0.007994844924721287 \t 3.8424198169073027\n",
            "61     \t [0.6488681  0.22512697 0.23753612]. \t  0.6857983232125278 \t 3.8424198169073027\n",
            "62     \t [0.14359178 0.42114215 0.06835466]. \t  0.10418286952472619 \t 3.8424198169073027\n",
            "63     \t [0.68130055 0.62446541 0.87330595]. \t  3.595549698048504 \t 3.8424198169073027\n",
            "64     \t [0.01731029 0.90173671 0.83410066]. \t  1.3793650749973891 \t 3.8424198169073027\n",
            "65     \t [0.13804909 0.39060505 0.22599701]. \t  0.3945123996524405 \t 3.8424198169073027\n",
            "66     \t [0.97278609 0.91514663 0.22301319]. \t  0.01214201106358989 \t 3.8424198169073027\n",
            "67     \t [0.47805976 0.44904643 0.69911163]. \t  2.1228685999717154 \t 3.8424198169073027\n",
            "68     \t [0.77591688 0.99971211 0.4089057 ]. \t  0.3571896826106656 \t 3.8424198169073027\n",
            "69     \t [0.20130951 0.18090192 0.01361808]. \t  0.1280031127309551 \t 3.8424198169073027\n",
            "70     \t [0.17274771 0.89530659 0.84319324]. \t  1.4260445211381512 \t 3.8424198169073027\n",
            "71     \t [0.44817697 0.83618646 0.21842817]. \t  0.07567909537410877 \t 3.8424198169073027\n",
            "72     \t [0.63705785 0.06685462 0.37546693]. \t  0.5564580121519138 \t 3.8424198169073027\n",
            "73     \t [0.34473282 0.70500744 0.23790905]. \t  0.12499189981633492 \t 3.8424198169073027\n",
            "74     \t [0.15550048 0.0224995  0.95052102]. \t  0.17842015907358422 \t 3.8424198169073027\n",
            "75     \t [0.8195422  0.67247422 0.25780942]. \t  0.056289828958829925 \t 3.8424198169073027\n",
            "76     \t [0.45033804 0.7720043  0.64231289]. \t  2.072030997389843 \t 3.8424198169073027\n",
            "77     \t [0.95281713 0.4814589  0.13972786]. \t  0.05889852967393788 \t 3.8424198169073027\n",
            "78     \t [0.21029326 0.08949757 0.61269778]. \t  0.24228973974732076 \t 3.8424198169073027\n",
            "79     \t [0.97854859 0.24215962 0.51359038]. \t  0.17767339073826924 \t 3.8424198169073027\n",
            "80     \t [0.27415217 0.58686992 0.96646935]. \t  2.6831589861332583 \t 3.8424198169073027\n",
            "81     \t [0.38697907 0.29536366 0.09405804]. \t  0.29552336972369 \t 3.8424198169073027\n",
            "82     \t [0.38041894 0.28113978 0.19138337]. \t  0.6436855569505461 \t 3.8424198169073027\n",
            "83     \t [0.73644648 0.74409984 0.0593703 ]. \t  0.004067044093233711 \t 3.8424198169073027\n",
            "84     \t [0.43974686 0.25576428 0.76727338]. \t  1.5860201622412489 \t 3.8424198169073027\n",
            "85     \t [0.91716939 0.23206351 0.2338737 ]. \t  0.3441867231335853 \t 3.8424198169073027\n",
            "86     \t [0.928106   0.36398451 0.2170875 ]. \t  0.19819281824603735 \t 3.8424198169073027\n",
            "87     \t [0.55496943 0.67175689 0.39539542]. \t  0.5496100823873051 \t 3.8424198169073027\n",
            "88     \t [0.10679858 0.88619389 0.87102049]. \t  1.436988468303008 \t 3.8424198169073027\n",
            "89     \t [0.42338151 0.41695061 0.16612036]. \t  0.2995029456411814 \t 3.8424198169073027\n",
            "90     \t [0.89484097 0.34203198 0.19629526]. \t  0.22656051317356457 \t 3.8424198169073027\n",
            "91     \t [0.17349676 0.41012199 0.94005107]. \t  2.4938462280056983 \t 3.8424198169073027\n",
            "92     \t [0.19771752 0.31239557 0.2045076 ]. \t  0.558662218988743 \t 3.8424198169073027\n",
            "93     \t [0.33178305 0.02142237 0.89524347]. \t  0.24704410756571965 \t 3.8424198169073027\n",
            "94     \t [0.33225438 0.53569611 0.07988812]. \t  0.06111318267713783 \t 3.8424198169073027\n",
            "95     \t [0.65412864 0.03015545 0.90688896]. \t  0.2532532659952299 \t 3.8424198169073027\n",
            "96     \t [0.96827564 0.09360594 0.34086681]. \t  0.28907922102817385 \t 3.8424198169073027\n",
            "97     \t [0.50114949 0.9151915  0.28738893]. \t  0.21956923397295164 \t 3.8424198169073027\n",
            "98     \t [0.4219465  0.87052368 0.10971162]. \t  0.007494253147581695 \t 3.8424198169073027\n",
            "99     \t [0.64684064 0.68675623 0.41936934]. \t  0.5458447520385002 \t 3.8424198169073027\n",
            "100    \t [0.28560638 0.15564314 0.33671172]. \t  0.8403431815373047 \t 3.8424198169073027\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NqjDLo0otd9-",
        "outputId": "9816b0f4-d279-4b9e-c74f-4ba24c59a8a0"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 17\r\n",
        "\r\n",
        "np.random.seed(run_num_17)\r\n",
        "surrogate_loser_17 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "loser_17 = dGPGO(surrogate_loser_17, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_17.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.65182403 0.73051962 0.19336736]. \t  0.03562358530655929 \t 3.1179188940604616\n",
            "init   \t [0.51967724 0.67638327 0.80444487]. \t  3.1179188940604616 \t 3.1179188940604616\n",
            "init   \t [0.53351286 0.90803418 0.44348113]. \t  1.1984486017622078 \t 3.1179188940604616\n",
            "init   \t [0.25597221 0.27866502 0.04332186]. \t  0.1645724004847893 \t 3.1179188940604616\n",
            "init   \t [0.46897499 0.9416138  0.60040624]. \t  1.9192522196289552 \t 3.1179188940604616\n",
            "1      \t [0.24286651 0.38597556 0.96454773]. \t  1.996297956656514 \t 3.1179188940604616\n",
            "2      \t [0.78722    0.79439557 0.98615055]. \t  1.382220738619041 \t 3.1179188940604616\n",
            "3      \t [0.48164726 0.49181657 0.69335958]. \t  2.165805360529691 \t 3.1179188940604616\n",
            "4      \t [0.4607031  0.72735048 0.97289599]. \t  2.000642401584277 \t 3.1179188940604616\n",
            "5      \t [0.         0.50533149 0.77857357]. \t  \u001b[92m3.3068755025235426\u001b[0m \t 3.3068755025235426\n",
            "6      \t [0.84992296 0.83200048 0.62767412]. \t  0.8027149864585417 \t 3.3068755025235426\n",
            "7      \t [0.31440273 0.71467703 0.56281211]. \t  2.297619566563724 \t 3.3068755025235426\n",
            "8      \t [0.02687805 0.43540554 0.74617149]. \t  2.676479718276525 \t 3.3068755025235426\n",
            "9      \t [0.05277616 0.51582688 0.94760749]. \t  2.920841279450375 \t 3.3068755025235426\n",
            "10     \t [0.772095   0.51143209 0.9217907 ]. \t  3.263249569583606 \t 3.3068755025235426\n",
            "11     \t [0.02874189 0.58448618 0.82689163]. \t  \u001b[92m3.7296108369895147\u001b[0m \t 3.7296108369895147\n",
            "12     \t [0.10288782 0.7969761  0.88363143]. \t  2.214759000019335 \t 3.7296108369895147\n",
            "13     \t [0.89234347 0.01595942 0.00143224]. \t  0.047613428692671905 \t 3.7296108369895147\n",
            "14     \t [0.9380584  0.45124303 0.92985098]. \t  2.8321646340031372 \t 3.7296108369895147\n",
            "15     \t [0.91097032 0.94135118 0.03397971]. \t  0.00021251829636137717 \t 3.7296108369895147\n",
            "16     \t [0.09885817 0.66452421 0.75403163]. \t  2.936617958754953 \t 3.7296108369895147\n",
            "17     \t [0.84249502 0.08146543 0.99593134]. \t  0.21416823622395875 \t 3.7296108369895147\n",
            "18     \t [0.60594296 0.43841293 0.9958068 ]. \t  1.8307902811957812 \t 3.7296108369895147\n",
            "19     \t [0.03863129 0.97053758 0.10718181]. \t  0.006838843999504983 \t 3.7296108369895147\n",
            "20     \t [0.23482374 0.0235891  0.87837889]. \t  0.2702690833319212 \t 3.7296108369895147\n",
            "21     \t [0.14224361 0.64099833 0.02530469]. \t  0.009886745309677786 \t 3.7296108369895147\n",
            "22     \t [0.93709034 0.40042116 0.74436374]. \t  2.3433515064437827 \t 3.7296108369895147\n",
            "23     \t [0.13352918 0.94357418 0.30028195]. \t  0.4097630621694289 \t 3.7296108369895147\n",
            "24     \t [0.25015144 0.42310362 0.01396667]. \t  0.054826780946359246 \t 3.7296108369895147\n",
            "25     \t [0.76576681 0.43251344 0.11100496]. \t  0.111032154670387 \t 3.7296108369895147\n",
            "26     \t [0.71302946 0.65428918 0.85710419]. \t  3.4312930079457056 \t 3.7296108369895147\n",
            "27     \t [0.71440553 0.77407405 0.36272561]. \t  0.3093169795364798 \t 3.7296108369895147\n",
            "28     \t [0.70784051 0.97993163 0.67028909]. \t  0.7892816451607718 \t 3.7296108369895147\n",
            "29     \t [0.51800861 0.54551968 0.26487654]. \t  0.19936679977656882 \t 3.7296108369895147\n",
            "30     \t [0.0535083  0.02209474 0.14427398]. \t  0.4306261442312767 \t 3.7296108369895147\n",
            "31     \t [0.23072881 0.85476936 0.97600168]. \t  1.1203921741915033 \t 3.7296108369895147\n",
            "32     \t [0.78903081 0.82775419 0.1624179 ]. \t  0.009969463208157872 \t 3.7296108369895147\n",
            "33     \t [0.34639487 0.50220626 0.23128726]. \t  0.24563542914317918 \t 3.7296108369895147\n",
            "34     \t [0.73921642 0.46775549 0.67290683]. \t  1.6954635060896477 \t 3.7296108369895147\n",
            "35     \t [0.78764978 0.06042556 0.8302592 ]. \t  0.4267724380537717 \t 3.7296108369895147\n",
            "36     \t [0.70383577 0.57096565 0.17725708]. \t  0.0771285228845155 \t 3.7296108369895147\n",
            "37     \t [0.08224507 0.52206253 0.92520623]. \t  3.274999790654435 \t 3.7296108369895147\n",
            "38     \t [0.0620436  0.42801267 0.25147661]. \t  0.3106822479949355 \t 3.7296108369895147\n",
            "39     \t [0.20240843 0.223458   0.35299039]. \t  0.6751471570789034 \t 3.7296108369895147\n",
            "40     \t [0.78411066 0.99428025 0.80895205]. \t  0.5882277001490892 \t 3.7296108369895147\n",
            "41     \t [0.99896311 0.52984322 0.07359522]. \t  0.018018260800360366 \t 3.7296108369895147\n",
            "42     \t [0.82027636 0.03502204 0.24575806]. \t  0.5004688146837654 \t 3.7296108369895147\n",
            "43     \t [0.85121567 0.22893522 0.27913839]. \t  0.438480566219128 \t 3.7296108369895147\n",
            "44     \t [0.26433048 0.1597651  0.85691673]. \t  0.9205910915577684 \t 3.7296108369895147\n",
            "45     \t [0.76691622 0.51337129 0.38827815]. \t  0.19408491849855966 \t 3.7296108369895147\n",
            "46     \t [0.55298012 0.12193723 0.8363212 ]. \t  0.7153673496834476 \t 3.7296108369895147\n",
            "47     \t [0.98453676 0.40768679 0.23852634]. \t  0.13626639106252103 \t 3.7296108369895147\n",
            "48     \t [0.25856198 0.52084654 0.77467825]. \t  3.3395602090894543 \t 3.7296108369895147\n",
            "49     \t [0.45296663 0.78267689 0.08376546]. \t  0.006740911098779007 \t 3.7296108369895147\n",
            "50     \t [0.58567443 0.59245542 0.05416328]. \t  0.02355784289594982 \t 3.7296108369895147\n",
            "51     \t [0.59592109 0.97796742 0.08147182]. \t  0.0017791627784233446 \t 3.7296108369895147\n",
            "52     \t [0.21162752 0.13948396 0.00148574]. \t  0.11091806741123574 \t 3.7296108369895147\n",
            "53     \t [0.81432498 0.1020756  0.43962605]. \t  0.2412751609616757 \t 3.7296108369895147\n",
            "54     \t [0.64815103 0.29806231 0.44364782]. \t  0.2969447837152171 \t 3.7296108369895147\n",
            "55     \t [0.56051781 0.51183617 0.49888347]. \t  0.5888244819252172 \t 3.7296108369895147\n",
            "56     \t [0.48517218 0.99134974 0.49422206]. \t  1.5385809578318683 \t 3.7296108369895147\n",
            "57     \t [0.05816553 0.45763475 0.69376408]. \t  2.153215054180613 \t 3.7296108369895147\n",
            "58     \t [0.26909419 0.74643037 0.04197797]. \t  0.004915713440641152 \t 3.7296108369895147\n",
            "59     \t [0.25910482 0.33170441 0.73636218]. \t  1.9550234372979356 \t 3.7296108369895147\n",
            "60     \t [0.86860015 0.22907423 0.02897167]. \t  0.07587325391055157 \t 3.7296108369895147\n",
            "61     \t [0.26429574 0.80177263 0.59003997]. \t  2.7847853706041117 \t 3.7296108369895147\n",
            "62     \t [0.3464018  0.83103848 0.63003205]. \t  2.4356311999821503 \t 3.7296108369895147\n",
            "63     \t [0.33390264 0.9444325  0.98442306]. \t  0.5888996464899308 \t 3.7296108369895147\n",
            "64     \t [0.71600419 0.11410615 0.07313597]. \t  0.22480895578597396 \t 3.7296108369895147\n",
            "65     \t [0.86916049 0.615922   0.70458205]. \t  1.9599722542591864 \t 3.7296108369895147\n",
            "66     \t [0.53336509 0.84486317 0.59531914]. \t  1.8388020734686554 \t 3.7296108369895147\n",
            "67     \t [0.73978342 0.65624186 0.80203376]. \t  3.109129084832267 \t 3.7296108369895147\n",
            "68     \t [0.46908484 0.29016108 0.48622976]. \t  0.32457236537058454 \t 3.7296108369895147\n",
            "69     \t [0.8055574  0.75264993 0.77470985]. \t  2.0918877698505494 \t 3.7296108369895147\n",
            "70     \t [0.71930082 0.69451456 0.87152847]. \t  3.121077480785742 \t 3.7296108369895147\n",
            "71     \t [0.59214386 0.4087555  0.43154413]. \t  0.3113929738693751 \t 3.7296108369895147\n",
            "72     \t [0.1040646  0.38199153 0.39295203]. \t  0.3873126590831428 \t 3.7296108369895147\n",
            "73     \t [0.30928002 0.42841219 0.01410685]. \t  0.054880482195661766 \t 3.7296108369895147\n",
            "74     \t [0.97649113 0.50950127 0.40146832]. \t  0.1005319691968133 \t 3.7296108369895147\n",
            "75     \t [0.8338393  0.74114505 0.04995728]. \t  0.002826701835849269 \t 3.7296108369895147\n",
            "76     \t [0.03921711 0.24384318 0.59904097]. \t  0.5135943190545953 \t 3.7296108369895147\n",
            "77     \t [0.66007928 0.85332333 0.76544453]. \t  1.4290146320216675 \t 3.7296108369895147\n",
            "78     \t [0.89236848 0.35997672 0.13321747]. \t  0.14218906731735056 \t 3.7296108369895147\n",
            "79     \t [0.78621447 0.86215128 0.27846026]. \t  0.07912757823474001 \t 3.7296108369895147\n",
            "80     \t [0.80024897 0.32727763 0.05355603]. \t  0.09341169596865143 \t 3.7296108369895147\n",
            "81     \t [0.93128779 0.68743986 0.38818922]. \t  0.13885246555159952 \t 3.7296108369895147\n",
            "82     \t [0.47095339 0.96251407 0.29195505]. \t  0.23654792306024855 \t 3.7296108369895147\n",
            "83     \t [0.14768116 0.94840828 0.44541929]. \t  1.9774934950818177 \t 3.7296108369895147\n",
            "84     \t [0.86850914 0.70841359 0.22112106]. \t  0.027852132025478437 \t 3.7296108369895147\n",
            "85     \t [0.3953896  0.57065299 0.56860651]. \t  1.3740985016426173 \t 3.7296108369895147\n",
            "86     \t [0.45464463 0.93392166 0.39989463]. \t  0.9872177351317891 \t 3.7296108369895147\n",
            "87     \t [0.83370591 0.80028876 0.03974119]. \t  0.0012445130735553001 \t 3.7296108369895147\n",
            "88     \t [0.3954636 0.4760783 0.9305186]. \t  3.0539262472087465 \t 3.7296108369895147\n",
            "89     \t [0.20014603 0.64918314 0.30455259]. \t  0.32378189219673037 \t 3.7296108369895147\n",
            "90     \t [0.81105369 0.1425165  0.27709311]. \t  0.5516169326058925 \t 3.7296108369895147\n",
            "91     \t [0.51933244 0.17350192 0.01046866]. \t  0.12509653075594532 \t 3.7296108369895147\n",
            "92     \t [0.20641137 0.07008326 0.74426415]. \t  0.43692938093535527 \t 3.7296108369895147\n",
            "93     \t [0.26813454 0.33962522 0.85618197]. \t  2.5218159508499016 \t 3.7296108369895147\n",
            "94     \t [0.34518094 0.31846439 0.72401148]. \t  1.7566215036092712 \t 3.7296108369895147\n",
            "95     \t [0.72152361 0.5511943  0.11487202]. \t  0.05310531285038199 \t 3.7296108369895147\n",
            "96     \t [0.88422074 0.77691097 0.8119611 ]. \t  2.122276340715377 \t 3.7296108369895147\n",
            "97     \t [0.88363888 0.3470758  0.85211001]. \t  2.540494522342128 \t 3.7296108369895147\n",
            "98     \t [0.8354768  0.44173748 0.23095813]. \t  0.17848714396589418 \t 3.7296108369895147\n",
            "99     \t [0.04158365 0.95493571 0.1420074 ]. \t  0.01712143442770344 \t 3.7296108369895147\n",
            "100    \t [0.87034875 0.38981482 0.60959142]. \t  0.8117138118498288 \t 3.7296108369895147\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jOqNmgSIteAw",
        "outputId": "35aa79cf-8adf-41cc-c71e-abe5923f5962"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 18\r\n",
        "\r\n",
        "np.random.seed(run_num_18)\r\n",
        "surrogate_loser_18 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "loser_18 = dGPGO(surrogate_loser_18, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_18.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.8734294  0.96854066 0.86919454]. \t  0.722531189286755 \t 1.1210522139432408\n",
            "init   \t [0.53085569 0.23272833 0.0113988 ]. \t  0.1133646794700679 \t 1.1210522139432408\n",
            "init   \t [0.43046882 0.40235136 0.52267467]. \t  0.5232646985053151 \t 1.1210522139432408\n",
            "init   \t [0.4783918  0.55535647 0.54338602]. \t  1.0390748854353227 \t 1.1210522139432408\n",
            "init   \t [0.76089558 0.71237457 0.6196821 ]. \t  1.1210522139432408 \t 1.1210522139432408\n",
            "1      \t [0.28982083 0.95201409 0.18404427]. \t  0.04220734680078784 \t 1.1210522139432408\n",
            "2      \t [0.12883542 0.98258307 0.90180597]. \t  0.6910738837771888 \t 1.1210522139432408\n",
            "3      \t [0.49148239 0.8099037  0.6976846 ]. \t  \u001b[92m1.8373634823765186\u001b[0m \t 1.8373634823765186\n",
            "4      \t [0.46136123 0.85569499 0.88610921]. \t  1.63167944843026 \t 1.8373634823765186\n",
            "5      \t [0.49454713 0.93328953 0.56170327]. \t  \u001b[92m1.9054625615284428\u001b[0m \t 1.9054625615284428\n",
            "6      \t [0.41435422 0.93277915 0.58517174]. \t  \u001b[92m2.210826503581639\u001b[0m \t 2.210826503581639\n",
            "7      \t [0.80890325 0.0109094  0.32544314]. \t  0.45214607547989116 \t 2.210826503581639\n",
            "8      \t [0.34819219 0.99739422 0.56097608]. \t  2.1934316775664433 \t 2.210826503581639\n",
            "9      \t [0. 0. 0.]. \t  0.06797411659013229 \t 2.210826503581639\n",
            "10     \t [0.43963141 0.98143234 0.73039565]. \t  1.0945002305811715 \t 2.210826503581639\n",
            "11     \t [0.02531226 0.36418907 0.95948581]. \t  1.8848202094361524 \t 2.210826503581639\n",
            "12     \t [1.         0.59373356 1.        ]. \t  1.9950302006748892 \t 2.210826503581639\n",
            "13     \t [0.99974218 0.73629479 0.16915152]. \t  0.0075698155392647 \t 2.210826503581639\n",
            "14     \t [0.02495978 0.79442693 0.58527601]. \t  \u001b[92m2.9049684298182346\u001b[0m \t 2.9049684298182346\n",
            "15     \t [0.08520035 0.90585596 0.49694613]. \t  2.701977635267954 \t 2.9049684298182346\n",
            "16     \t [0.45494122 0.         0.        ]. \t  0.10000081256820421 \t 2.9049684298182346\n",
            "17     \t [0.12426119 0.84411526 0.52541272]. \t  \u001b[92m2.9563972883678864\u001b[0m \t 2.9563972883678864\n",
            "18     \t [0.02639366 0.68481539 0.75100875]. \t  2.8131050833123785 \t 2.9563972883678864\n",
            "19     \t [0.02758352 0.02020285 0.38162083]. \t  0.43651901494056317 \t 2.9563972883678864\n",
            "20     \t [0.28915653 0.55854136 0.96225688]. \t  2.768662534236792 \t 2.9563972883678864\n",
            "21     \t [0.12624466 0.77769878 0.6270266 ]. \t  2.7789264762997097 \t 2.9563972883678864\n",
            "22     \t [1.         0.10827526 0.06546373]. \t  0.08912173062371706 \t 2.9563972883678864\n",
            "23     \t [0.09844136 0.73638296 0.99651424]. \t  1.6148421635159205 \t 2.9563972883678864\n",
            "24     \t [0.49045263 0.03648856 0.99270562]. \t  0.14488304068598584 \t 2.9563972883678864\n",
            "25     \t [0.37075679 0.40636174 0.99390997]. \t  1.7080737298449449 \t 2.9563972883678864\n",
            "26     \t [0.0408982  0.50164123 0.5916972 ]. \t  1.3623365345201217 \t 2.9563972883678864\n",
            "27     \t [0.18770015 0.01323407 0.78394959]. \t  0.28358527861716876 \t 2.9563972883678864\n",
            "28     \t [0.07863691 0.91863354 0.61030717]. \t  2.8043982616240037 \t 2.9563972883678864\n",
            "29     \t [0.98391598 0.92875561 0.41143334]. \t  0.16085776241783298 \t 2.9563972883678864\n",
            "30     \t [0.15450228 0.78418128 0.71203099]. \t  2.394328901027423 \t 2.9563972883678864\n",
            "31     \t [0.99662963 0.70457646 0.99318033]. \t  1.7670751183737858 \t 2.9563972883678864\n",
            "32     \t [0.99827755 0.18154592 0.7676135 ]. \t  1.008242186747991 \t 2.9563972883678864\n",
            "33     \t [0.80542036 0.93073344 0.58505498]. \t  0.7409700208735187 \t 2.9563972883678864\n",
            "34     \t [0.51853295 0.1580878  0.65790907]. \t  0.527067878688211 \t 2.9563972883678864\n",
            "35     \t [0.64262746 0.50518779 0.79424085]. \t  \u001b[92m3.4202876002328155\u001b[0m \t 3.4202876002328155\n",
            "36     \t [0.71764436 0.50120913 0.89630177]. \t  \u001b[92m3.513416336552514\u001b[0m \t 3.513416336552514\n",
            "37     \t [0.79404411 0.97984595 0.11606075]. \t  0.002211339423337698 \t 3.513416336552514\n",
            "38     \t [0.73714587 0.45329374 0.88416363]. \t  3.3506148979386787 \t 3.513416336552514\n",
            "39     \t [0.96662041 0.17014501 0.54127832]. \t  0.17694808258495845 \t 3.513416336552514\n",
            "40     \t [0.72206819 0.52574194 0.86187993]. \t  \u001b[92m3.755946347264982\u001b[0m \t 3.755946347264982\n",
            "41     \t [0.15815988 0.67675116 0.59988159]. \t  2.396099008710307 \t 3.755946347264982\n",
            "42     \t [0.55307155 0.83113468 0.65867287]. \t  1.659746827776479 \t 3.755946347264982\n",
            "43     \t [0.10993072 0.74821578 0.46707571]. \t  2.077590778069325 \t 3.755946347264982\n",
            "44     \t [0.22024099 0.74329319 0.1828966 ]. \t  0.05357232306344689 \t 3.755946347264982\n",
            "45     \t [0.19339606 0.4635285  0.98352696]. \t  2.1467115145102658 \t 3.755946347264982\n",
            "46     \t [0.45353138 0.15999598 0.48404493]. \t  0.29717680074648767 \t 3.755946347264982\n",
            "47     \t [0.22384738 0.1550626  0.19208477]. \t  0.7812168459064405 \t 3.755946347264982\n",
            "48     \t [0.48298156 0.28940149 0.37160245]. \t  0.5464451043545906 \t 3.755946347264982\n",
            "49     \t [0.8965415  0.66114057 0.13985342]. \t  0.015502662617992406 \t 3.755946347264982\n",
            "50     \t [0.60261663 0.44378932 0.49433609]. \t  0.40890372618870746 \t 3.755946347264982\n",
            "51     \t [0.5967522  0.98799453 0.06163179]. \t  0.0009981258980179624 \t 3.755946347264982\n",
            "52     \t [0.71091254 0.69983393 0.23784189]. \t  0.059899164199538016 \t 3.755946347264982\n",
            "53     \t [0.20273128 0.58872086 0.10767453]. \t  0.04954895770137224 \t 3.755946347264982\n",
            "54     \t [0.66317934 0.93525471 0.68586192]. \t  0.9970043927416965 \t 3.755946347264982\n",
            "55     \t [0.58764246 0.33637294 0.31484846]. \t  0.5169073769604036 \t 3.755946347264982\n",
            "56     \t [0.86629224 0.04252853 0.05942087]. \t  0.1231852269548608 \t 3.755946347264982\n",
            "57     \t [0.57283024 0.38759573 0.9278396 ]. \t  2.4697606835699473 \t 3.755946347264982\n",
            "58     \t [0.27813955 0.37995999 0.75926053]. \t  2.5025501735589653 \t 3.755946347264982\n",
            "59     \t [0.41186934 0.23513372 0.39403079]. \t  0.562511567165831 \t 3.755946347264982\n",
            "60     \t [0.32108626 0.38704859 0.13114269]. \t  0.27577883497062416 \t 3.755946347264982\n",
            "61     \t [0.30102575 0.18571067 0.69279238]. \t  0.7864759815082917 \t 3.755946347264982\n",
            "62     \t [0.65089824 0.92138798 0.79668312]. \t  1.0511102182778886 \t 3.755946347264982\n",
            "63     \t [0.0107059  0.76416342 0.50836861]. \t  2.499686990361062 \t 3.755946347264982\n",
            "64     \t [0.9855434  0.58877487 0.82257687]. \t  3.491008348926851 \t 3.755946347264982\n",
            "65     \t [0.36787302 0.17657212 0.1280795 ]. \t  0.5396635722306211 \t 3.755946347264982\n",
            "66     \t [0.79104461 0.10531206 0.92057209]. \t  0.4680412116273659 \t 3.755946347264982\n",
            "67     \t [0.81682994 0.62089582 0.08571168]. \t  0.01656009674352917 \t 3.755946347264982\n",
            "68     \t [0.88480013 0.54827671 0.58088944]. \t  0.6976109027858441 \t 3.755946347264982\n",
            "69     \t [0.25229893 0.85664729 0.91213272]. \t  1.5378716112082156 \t 3.755946347264982\n",
            "70     \t [0.50164155 0.6093058  0.23688983]. \t  0.12733227359775642 \t 3.755946347264982\n",
            "71     \t [0.68805769 0.04971805 0.07463285]. \t  0.2312012001698145 \t 3.755946347264982\n",
            "72     \t [0.16447824 0.05546928 0.91134554]. \t  0.31617261110468226 \t 3.755946347264982\n",
            "73     \t [0.67234226 0.0349185  0.15057444]. \t  0.4712658901470614 \t 3.755946347264982\n",
            "74     \t [0.3134     0.67794811 0.1465898 ]. \t  0.03974076975009795 \t 3.755946347264982\n",
            "75     \t [0.30640295 0.92612678 0.86974016]. \t  1.119825071673948 \t 3.755946347264982\n",
            "76     \t [0.17382888 0.31318679 0.10265048]. \t  0.26947136899597673 \t 3.755946347264982\n",
            "77     \t [0.75990013 0.21311832 0.99415522]. \t  0.6388463637914399 \t 3.755946347264982\n",
            "78     \t [0.89047902 0.64491292 0.32921552]. \t  0.08812881948180289 \t 3.755946347264982\n",
            "79     \t [0.98696351 0.18431559 0.75848459]. \t  1.0022842918950319 \t 3.755946347264982\n",
            "80     \t [0.35343701 0.18275473 0.59440178]. \t  0.3720688142423593 \t 3.755946347264982\n",
            "81     \t [0.31539192 0.7792746  0.79955247]. \t  2.3896674916440004 \t 3.755946347264982\n",
            "82     \t [0.14508814 0.18143484 0.64846608]. \t  0.5653094985500519 \t 3.755946347264982\n",
            "83     \t [0.40737253 0.82678737 0.820015  ]. \t  1.9662984601658695 \t 3.755946347264982\n",
            "84     \t [0.92766229 0.18593304 0.06503856]. \t  0.10954397157902122 \t 3.755946347264982\n",
            "85     \t [0.32869146 0.05835603 0.47108057]. \t  0.2996102459165014 \t 3.755946347264982\n",
            "86     \t [0.02684041 0.70539556 0.80612601]. \t  3.0104249940434444 \t 3.755946347264982\n",
            "87     \t [0.05193446 0.06965373 0.43434944]. \t  0.3262841943372766 \t 3.755946347264982\n",
            "88     \t [0.81004504 0.44320829 0.10766096]. \t  0.0898625192921795 \t 3.755946347264982\n",
            "89     \t [0.61289151 0.38869684 0.43681509]. \t  0.29952453091027376 \t 3.755946347264982\n",
            "90     \t [0.86832659 0.12322599 0.86359209]. \t  0.6746635330832248 \t 3.755946347264982\n",
            "91     \t [0.86847025 0.1642101  0.21030744]. \t  0.4197267969060901 \t 3.755946347264982\n",
            "92     \t [0.09465095 0.7470169  0.71808077]. \t  2.503304736154796 \t 3.755946347264982\n",
            "93     \t [0.29236438 0.26645893 0.82895882]. \t  1.829975409816589 \t 3.755946347264982\n",
            "94     \t [0.92417251 0.24429314 0.62430697]. \t  0.5906192293138568 \t 3.755946347264982\n",
            "95     \t [0.55557028 0.3141471  0.73791255]. \t  1.8287978040338055 \t 3.755946347264982\n",
            "96     \t [0.90070846 0.18634617 0.11644003]. \t  0.20613038331249936 \t 3.755946347264982\n",
            "97     \t [0.92852018 0.18809886 0.72942986]. \t  0.9291753024647236 \t 3.755946347264982\n",
            "98     \t [0.98478221 0.01874909 0.39303239]. \t  0.18371725118779617 \t 3.755946347264982\n",
            "99     \t [0.5440732  0.8788577  0.07824594]. \t  0.002815235884962438 \t 3.755946347264982\n",
            "100    \t [0.12195859 0.52278805 0.62882059]. \t  1.749290705274293 \t 3.755946347264982\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i4HnKuoqteDo",
        "outputId": "3a4a6d31-213f-4070-826a-eba58ce0ca0e"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 19\r\n",
        "\r\n",
        "np.random.seed(run_num_19)\r\n",
        "surrogate_loser_19 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "loser_19 = dGPGO(surrogate_loser_19, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_19.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.55944558 0.76149199 0.24453155]. \t  0.09433080659504385 \t 2.524990008735946\n",
            "init   \t [0.77168947 0.35447208 0.7966974 ]. \t  2.524990008735946 \t 2.524990008735946\n",
            "init   \t [0.34600002 0.44387444 0.30330359]. \t  0.3914194506273641 \t 2.524990008735946\n",
            "init   \t [0.44211392 0.57320936 0.06566402]. \t  0.036928944981920925 \t 2.524990008735946\n",
            "init   \t [0.02408889 0.82062263 0.36513765]. \t  0.9772047942100802 \t 2.524990008735946\n",
            "1      \t [0.96499061 0.19185662 0.98563058]. \t  0.5815385407774518 \t 2.524990008735946\n",
            "2      \t [0.35021227 0.89949578 0.95419986]. \t  0.9824452595236306 \t 2.524990008735946\n",
            "3      \t [0.83943674 0.47248815 0.71189353]. \t  2.1954869424194308 \t 2.524990008735946\n",
            "4      \t [0.46047313 0.25352007 0.98033154]. \t  0.9460814398415793 \t 2.524990008735946\n",
            "5      \t [0.87921595 0.09179931 0.4612539 ]. \t  0.1689760951805058 \t 2.524990008735946\n",
            "6      \t [0.70683766 0.42442774 0.83999819]. \t  \u001b[92m3.263915625235409\u001b[0m \t 3.263915625235409\n",
            "7      \t [0.74803474 0.60533411 0.93779535]. \t  3.0703841077028233 \t 3.263915625235409\n",
            "8      \t [0.57560304 0.47085767 0.99523987]. \t  1.978350792868597 \t 3.263915625235409\n",
            "9      \t [0.40777968 0.81249746 0.68887458]. \t  2.039298715115512 \t 3.263915625235409\n",
            "10     \t [0.81021848 0.03471006 0.0376359 ]. \t  0.10705973553781457 \t 3.263915625235409\n",
            "11     \t [0.5413991  0.5482306  0.73314717]. \t  2.7336547845565304 \t 3.263915625235409\n",
            "12     \t [1.80259711e-13 1.37939672e-13 2.07645727e-13]. \t  0.06797411659040772 \t 3.263915625235409\n",
            "13     \t [0.         0.90373522 0.        ]. \t  0.0004408446666478906 \t 3.263915625235409\n",
            "14     \t [0.66475684 0.0382532  0.96400626]. \t  0.18867003607216465 \t 3.263915625235409\n",
            "15     \t [0.06583469 0.95808291 0.57514725]. \t  2.795998179155653 \t 3.263915625235409\n",
            "16     \t [0.06677576 0.98821057 0.84641376]. \t  0.7896893879358474 \t 3.263915625235409\n",
            "17     \t [0.06460414 0.00856095 0.5507647 ]. \t  0.11307571360736571 \t 3.263915625235409\n",
            "18     \t [0.74305045 0.94567471 0.91399927]. \t  0.8206436917127765 \t 3.263915625235409\n",
            "19     \t [0.91931399 0.43446071 0.03269371]. \t  0.028232747946401054 \t 3.263915625235409\n",
            "20     \t [0.424762   0.17982051 0.277162  ]. \t  0.9516468019622124 \t 3.263915625235409\n",
            "21     \t [0.28314539 0.00285815 0.07177572]. \t  0.27274097784952667 \t 3.263915625235409\n",
            "22     \t [0.68655261 0.47258864 0.84533407]. \t  \u001b[92m3.58096390901099\u001b[0m \t 3.58096390901099\n",
            "23     \t [0.09645332 0.95171827 0.70824437]. \t  1.7335812415162595 \t 3.58096390901099\n",
            "24     \t [0.29698544 0.17077213 0.37241439]. \t  0.6982054319324602 \t 3.58096390901099\n",
            "25     \t [0.85557198 0.57367251 0.68673724]. \t  1.819144295018671 \t 3.58096390901099\n",
            "26     \t [0.01307524 0.98248115 0.5492361 ]. \t  2.6150610041996902 \t 3.58096390901099\n",
            "27     \t [0.1101793  0.97400703 0.37545393]. \t  1.0346566781035018 \t 3.58096390901099\n",
            "28     \t [0.05486895 0.31756584 0.13401811]. \t  0.2926632371936038 \t 3.58096390901099\n",
            "29     \t [0.50329508 0.22209549 0.64275971]. \t  0.6699701094542871 \t 3.58096390901099\n",
            "30     \t [0.61332035 0.09483347 0.84810284]. \t  0.563234333851149 \t 3.58096390901099\n",
            "31     \t [0.8527929  0.94871274 0.00283516]. \t  0.00011822558074294612 \t 3.58096390901099\n",
            "32     \t [0.7076945  0.55551048 0.83049328]. \t  \u001b[92m3.7205360934649\u001b[0m \t 3.7205360934649\n",
            "33     \t [0.24364365 0.35900578 0.90424936]. \t  2.4538321445628757 \t 3.7205360934649\n",
            "34     \t [0.02149832 0.39687062 0.85587215]. \t  3.031221431323753 \t 3.7205360934649\n",
            "35     \t [0.01497557 0.28253221 0.8763065 ]. \t  1.8628285818339796 \t 3.7205360934649\n",
            "36     \t [0.04503366 0.22594075 0.7378816 ]. \t  1.2153461880632406 \t 3.7205360934649\n",
            "37     \t [0.28985676 0.99895607 0.97563001]. \t  0.40933801513369666 \t 3.7205360934649\n",
            "38     \t [0.82309201 0.72860314 0.42102276]. \t  0.3286530780947166 \t 3.7205360934649\n",
            "39     \t [0.42547655 0.0600468  0.47533445]. \t  0.28650314809862054 \t 3.7205360934649\n",
            "40     \t [0.12523936 0.71993174 0.23134444]. \t  0.12419919234462012 \t 3.7205360934649\n",
            "41     \t [0.96160152 0.36463765 0.76840526]. \t  2.3586836952155927 \t 3.7205360934649\n",
            "42     \t [0.19076268 0.78430101 0.32881315]. \t  0.5982041585220237 \t 3.7205360934649\n",
            "43     \t [0.40813466 0.66810217 0.94607042]. \t  2.739423609810528 \t 3.7205360934649\n",
            "44     \t [0.9962501  0.90959763 0.21364914]. \t  0.009053813831887955 \t 3.7205360934649\n",
            "45     \t [0.89691926 0.09320847 0.15243687]. \t  0.2900082957339025 \t 3.7205360934649\n",
            "46     \t [0.08587334 0.74904582 0.5635006 ]. \t  2.7671860688773733 \t 3.7205360934649\n",
            "47     \t [0.90428046 0.59773861 0.69326085]. \t  1.8257945304360101 \t 3.7205360934649\n",
            "48     \t [0.77119201 0.03859479 0.34265034]. \t  0.4890468547987831 \t 3.7205360934649\n",
            "49     \t [0.23163943 0.83329578 0.03876278]. \t  0.0021264459665956 \t 3.7205360934649\n",
            "50     \t [0.22967869 0.13702735 0.05796596]. \t  0.2523975141805462 \t 3.7205360934649\n",
            "51     \t [0.11237253 0.65966214 0.42607608]. \t  1.1995499965462453 \t 3.7205360934649\n",
            "52     \t [0.40230869 0.63370855 0.44560545]. \t  0.9785715601738311 \t 3.7205360934649\n",
            "53     \t [0.63854359 0.86124614 0.40485685]. \t  0.6643958568560102 \t 3.7205360934649\n",
            "54     \t [0.08013076 0.54133813 0.42400428]. \t  0.6870736161624138 \t 3.7205360934649\n",
            "55     \t [0.47446429 0.75289979 0.96713271]. \t  1.9044832473946198 \t 3.7205360934649\n",
            "56     \t [0.63207101 0.35509364 0.00158214]. \t  0.055427795107536884 \t 3.7205360934649\n",
            "57     \t [0.06661471 0.95018614 0.80741635]. \t  1.1220850988448952 \t 3.7205360934649\n",
            "58     \t [0.96799084 0.6911544  0.91876818]. \t  2.8023998550294 \t 3.7205360934649\n",
            "59     \t [0.9030704  0.69926889 0.30648406]. \t  0.06705165906432335 \t 3.7205360934649\n",
            "60     \t [0.37250422 0.30495954 0.09332783]. \t  0.28344377600126297 \t 3.7205360934649\n",
            "61     \t [0.1500338  0.2151008  0.13895462]. \t  0.48014866379633925 \t 3.7205360934649\n",
            "62     \t [0.72429142 0.87225717 0.0389365 ]. \t  0.0008069837359671329 \t 3.7205360934649\n",
            "63     \t [0.86377943 0.88216093 0.87175809]. \t  1.329082134525974 \t 3.7205360934649\n",
            "64     \t [0.52780961 0.26488685 0.19576341]. \t  0.6398531580442676 \t 3.7205360934649\n",
            "65     \t [0.60830622 0.53552546 0.48498891]. \t  0.5388088371927547 \t 3.7205360934649\n",
            "66     \t [0.43756068 0.48625546 0.57947842]. \t  1.0478624634428502 \t 3.7205360934649\n",
            "67     \t [0.5947457  0.26791511 0.43075178]. \t  0.3584817684980289 \t 3.7205360934649\n",
            "68     \t [0.3857746  0.28585579 0.81923471]. \t  2.0091479654578412 \t 3.7205360934649\n",
            "69     \t [0.36378236 0.43626916 0.27521403]. \t  0.3957733306512729 \t 3.7205360934649\n",
            "70     \t [0.31011784 0.11476409 0.94703201]. \t  0.4384406165894201 \t 3.7205360934649\n",
            "71     \t [0.96106242 0.25484353 0.2852708 ]. \t  0.28735363514336076 \t 3.7205360934649\n",
            "72     \t [0.67275161 0.90584988 0.19440821]. \t  0.024581555798376124 \t 3.7205360934649\n",
            "73     \t [0.42856543 0.24123473 0.06572751]. \t  0.2506139230144343 \t 3.7205360934649\n",
            "74     \t [0.11795803 0.82308563 0.88447371]. \t  1.9627816582310302 \t 3.7205360934649\n",
            "75     \t [0.63786628 0.39515356 0.74653427]. \t  2.4223442758118603 \t 3.7205360934649\n",
            "76     \t [0.24354281 0.26780065 0.85639567]. \t  1.8094612962111003 \t 3.7205360934649\n",
            "77     \t [0.66767182 0.18181134 0.57205183]. \t  0.28984354895333153 \t 3.7205360934649\n",
            "78     \t [0.30974426 0.6692461  0.01863442]. \t  0.007649779620783134 \t 3.7205360934649\n",
            "79     \t [0.39396999 0.01359379 0.42209846]. \t  0.44288926847849175 \t 3.7205360934649\n",
            "80     \t [0.00622778 0.70489465 0.76489267]. \t  2.78904023721274 \t 3.7205360934649\n",
            "81     \t [0.18417019 0.65820939 0.99740273]. \t  1.9631796307825014 \t 3.7205360934649\n",
            "82     \t [0.58584054 0.3550544  0.10439691]. \t  0.2224736789213607 \t 3.7205360934649\n",
            "83     \t [0.87431571 0.63954261 0.41195434]. \t  0.19574496210540726 \t 3.7205360934649\n",
            "84     \t [0.69349169 0.58534044 0.62972631]. \t  1.3302088711861788 \t 3.7205360934649\n",
            "85     \t [0.15209794 0.15202414 0.68155881]. \t  0.5943639086588272 \t 3.7205360934649\n",
            "86     \t [0.79200461 0.63794397 0.74960302]. \t  2.5584025352025037 \t 3.7205360934649\n",
            "87     \t [0.34459962 0.41262583 0.27340987]. \t  0.4449259337296729 \t 3.7205360934649\n",
            "88     \t [0.20535494 0.65975039 0.7603884 ]. \t  3.0016424961664874 \t 3.7205360934649\n",
            "89     \t [0.46426209 0.42973631 0.24545965]. \t  0.3772305907675988 \t 3.7205360934649\n",
            "90     \t [0.19736085 0.98220752 0.1427098 ]. \t  0.016314629956979514 \t 3.7205360934649\n",
            "91     \t [0.66980287 0.41687428 0.04706623]. \t  0.07243416592273004 \t 3.7205360934649\n",
            "92     \t [0.04429268 0.08927876 0.88714029]. \t  0.47583883365716306 \t 3.7205360934649\n",
            "93     \t [0.79558746 0.93614399 0.12043335]. \t  0.0028177863330248122 \t 3.7205360934649\n",
            "94     \t [0.41132162 0.13810393 0.80963589]. \t  0.8209804264571146 \t 3.7205360934649\n",
            "95     \t [0.6393976  0.57288184 0.79072809]. \t  3.3957518147380443 \t 3.7205360934649\n",
            "96     \t [0.40173052 0.16642951 0.80461023]. \t  1.0008823398027678 \t 3.7205360934649\n",
            "97     \t [0.91045104 0.13395119 0.72920788]. \t  0.6572687768834146 \t 3.7205360934649\n",
            "98     \t [0.98245798 0.35708702 0.41811415]. \t  0.12973870484146707 \t 3.7205360934649\n",
            "99     \t [0.38571383 0.86707224 0.2595399 ]. \t  0.17830519849965276 \t 3.7205360934649\n",
            "100    \t [0.00128665 0.16274988 0.01869641]. \t  0.10224037162700153 \t 3.7205360934649\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FfiDmH_bteGg",
        "outputId": "bcb5b0e5-d4c5-4aee-d693-0b8882932df7"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 20\r\n",
        "\r\n",
        "np.random.seed(run_num_20)\r\n",
        "surrogate_loser_20 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "loser_20 = dGPGO(surrogate_loser_20, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_20.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.57051729 0.56452876 0.48844183]. \t  0.675391399411646 \t 0.675391399411646\n",
            "init   \t [0.33647775 0.37586818 0.53203587]. \t  0.5331349538596052 \t 0.675391399411646\n",
            "init   \t [0.06810629 0.58452906 0.23789776]. \t  0.14747335095307315 \t 0.675391399411646\n",
            "init   \t [0.16075658 0.15211915 0.12706922]. \t  0.48089725804912437 \t 0.675391399411646\n",
            "init   \t [0.32744117 0.69415387 0.35896647]. \t  0.6289408047543804 \t 0.675391399411646\n",
            "1      \t [0.837611   0.97862966 0.10314233]. \t  0.001341877938468012 \t 0.675391399411646\n",
            "2      \t [0.4192146  0.99013113 0.80759551]. \t  \u001b[92m0.8040311121195758\u001b[0m \t 0.8040311121195758\n",
            "3      \t [0.58490171 0.         0.        ]. \t  0.08889155882157923 \t 0.8040311121195758\n",
            "4      \t [0.56803068 0.99098437 0.50767036]. \t  \u001b[92m1.310904866486966\u001b[0m \t 1.310904866486966\n",
            "5      \t [0.84822682 0.9788853  0.85466061]. \t  0.6702188994918671 \t 1.310904866486966\n",
            "6      \t [0.84509259 0.         0.89862994]. \t  0.19194384543556486 \t 1.310904866486966\n",
            "7      \t [0.67407812 0.99571082 0.58414561]. \t  1.010438400626982 \t 1.310904866486966\n",
            "8      \t [0.25706606 0.99173014 0.15932733]. \t  0.02275764294278305 \t 1.310904866486966\n",
            "9      \t [0.03546958 0.         0.28410152]. \t  0.6197286265376367 \t 1.310904866486966\n",
            "10     \t [0.97947894 0.27434002 0.93560419]. \t  \u001b[92m1.3958911240572458\u001b[0m \t 1.3958911240572458\n",
            "11     \t [0.87557865 0.50601557 0.95614793]. \t  \u001b[92m2.7139777944223322\u001b[0m \t 2.7139777944223322\n",
            "12     \t [0.75793425 0.69772209 0.98756189]. \t  1.927540683132728 \t 2.7139777944223322\n",
            "13     \t [0.12414169 0.99832635 0.576206  ]. \t  2.566437553023592 \t 2.7139777944223322\n",
            "14     \t [0.75104891 0.2645525  0.9551541 ]. \t  1.199937865485019 \t 2.7139777944223322\n",
            "15     \t [0.97780808 0.58615943 0.81664382]. \t  \u001b[92m3.457183993275378\u001b[0m \t 3.457183993275378\n",
            "16     \t [0.98736861 0.72749542 0.80627068]. \t  2.471770357321162 \t 3.457183993275378\n",
            "17     \t [0.89238206 0.48690832 0.72463641]. \t  2.3802531328000907 \t 3.457183993275378\n",
            "18     \t [0.0105289  0.05708276 0.89134199]. \t  0.3491269144620891 \t 3.457183993275378\n",
            "19     \t [0.44027839 0.02479109 0.93932283]. \t  0.19894546674484326 \t 3.457183993275378\n",
            "20     \t [0.01937257 0.67545384 0.73411381]. \t  2.736759042810903 \t 3.457183993275378\n",
            "21     \t [0.614879   0.01897242 0.51500566]. \t  0.15367501249511611 \t 3.457183993275378\n",
            "22     \t [0.00792764 0.8642164  0.54512418]. \t  2.9705057868024403 \t 3.457183993275378\n",
            "23     \t [0.1392827  0.79366321 0.04088121]. \t  0.0029033334872716663 \t 3.457183993275378\n",
            "24     \t [0.51179929 0.7341067  0.00566347]. \t  0.002856584824382445 \t 3.457183993275378\n",
            "25     \t [0.19277311 0.22411141 0.23644552]. \t  0.7916688656986456 \t 3.457183993275378\n",
            "26     \t [0.01884545 0.8846453  0.80561362]. \t  1.563571742337803 \t 3.457183993275378\n",
            "27     \t [0.99834345 0.76091015 0.04426641]. \t  0.001183423337691984 \t 3.457183993275378\n",
            "28     \t [0.80232729 0.52554835 0.70874912]. \t  2.213201506160826 \t 3.457183993275378\n",
            "29     \t [0.20739069 0.06601235 0.08339898]. \t  0.3266638276141411 \t 3.457183993275378\n",
            "30     \t [0.09177414 0.25874783 0.46582282]. \t  0.3098176725068744 \t 3.457183993275378\n",
            "31     \t [0.59223853 0.49019339 0.514645  ]. \t  0.5636584653373499 \t 3.457183993275378\n",
            "32     \t [0.72667423 0.33162898 0.13623505]. \t  0.25692809763748053 \t 3.457183993275378\n",
            "33     \t [0.99759854 0.56746187 0.86151219]. \t  \u001b[92m3.6621067295652265\u001b[0m \t 3.6621067295652265\n",
            "34     \t [0.68789817 0.31596021 0.63752013]. \t  0.9215222723989691 \t 3.6621067295652265\n",
            "35     \t [0.57752401 0.80350725 0.18556134]. \t  0.0312731366325119 \t 3.6621067295652265\n",
            "36     \t [0.19787002 0.1977767  0.83017448]. \t  1.2277701990258327 \t 3.6621067295652265\n",
            "37     \t [0.16067943 0.02817582 0.75723975]. \t  0.3137567177172803 \t 3.6621067295652265\n",
            "38     \t [0.72541457 0.57787923 0.33583694]. \t  0.16900693522232732 \t 3.6621067295652265\n",
            "39     \t [0.47827067 0.1364753  0.25670492]. \t  0.9585843576107357 \t 3.6621067295652265\n",
            "40     \t [0.23343479 0.59138584 0.30642539]. \t  0.3000119022807604 \t 3.6621067295652265\n",
            "41     \t [0.43976237 0.92250638 0.44201957]. \t  1.447177776885933 \t 3.6621067295652265\n",
            "42     \t [0.01103052 0.19701301 0.73397797]. \t  1.013933980043001 \t 3.6621067295652265\n",
            "43     \t [0.24307251 0.26563776 0.48045205]. \t  0.33360810781241124 \t 3.6621067295652265\n",
            "44     \t [0.26541441 0.20442255 0.21154234]. \t  0.8182007654284428 \t 3.6621067295652265\n",
            "45     \t [0.17598597 0.39637633 0.93595256]. \t  2.4420997621465594 \t 3.6621067295652265\n",
            "46     \t [0.37043523 0.84876641 0.75157091]. \t  1.8070542759678743 \t 3.6621067295652265\n",
            "47     \t [0.65497411 0.21224035 0.76263526]. \t  1.2312315716964708 \t 3.6621067295652265\n",
            "48     \t [0.40726517 0.24250585 0.70979656]. \t  1.1742944616992397 \t 3.6621067295652265\n",
            "49     \t [0.37505632 0.11231425 0.63764472]. \t  0.3404462066127 \t 3.6621067295652265\n",
            "50     \t [0.06155421 0.86108384 0.97645898]. \t  1.069465683646719 \t 3.6621067295652265\n",
            "51     \t [0.64534578 0.18739021 0.25470415]. \t  0.7539909915086813 \t 3.6621067295652265\n",
            "52     \t [0.28313966 0.41353302 0.27735699]. \t  0.4382617694804395 \t 3.6621067295652265\n",
            "53     \t [0.38503878 0.50627476 0.87408068]. \t  \u001b[92m3.727265148396073\u001b[0m \t 3.727265148396073\n",
            "54     \t [0.65515322 0.48343052 0.95854661]. \t  2.6390659543517048 \t 3.727265148396073\n",
            "55     \t [0.35869692 0.89270219 0.01228564]. \t  0.0007101787817227387 \t 3.727265148396073\n",
            "56     \t [0.61181179 0.36417187 0.10662379]. \t  0.20987490090205413 \t 3.727265148396073\n",
            "57     \t [0.93400767 0.1910905  0.00216429]. \t  0.04407628331929119 \t 3.727265148396073\n",
            "58     \t [0.26830483 0.30305693 0.19889753]. \t  0.5988107042239119 \t 3.727265148396073\n",
            "59     \t [0.27043163 0.2608227  0.27860543]. \t  0.7938419092915375 \t 3.727265148396073\n",
            "60     \t [0.34468722 0.65659973 0.86325841]. \t  3.5060239611180912 \t 3.727265148396073\n",
            "61     \t [0.29287666 0.55526847 0.92213394]. \t  3.3921387662091185 \t 3.727265148396073\n",
            "62     \t [0.8598495  0.09687106 0.70544899]. \t  0.45449322998722325 \t 3.727265148396073\n",
            "63     \t [0.16356561 0.91951492 0.89582497]. \t  1.104444384511528 \t 3.727265148396073\n",
            "64     \t [0.09449532 0.3713226  0.38575202]. \t  0.3889635066766884 \t 3.727265148396073\n",
            "65     \t [0.95270096 0.07614477 0.80783348]. \t  0.4895141696876869 \t 3.727265148396073\n",
            "66     \t [0.97256952 0.53652074 0.09495664]. \t  0.023834490049896122 \t 3.727265148396073\n",
            "67     \t [0.04743825 0.43780258 0.59339554]. \t  1.092959659135892 \t 3.727265148396073\n",
            "68     \t [0.37743864 0.46176713 0.30895875]. \t  0.36325840663334485 \t 3.727265148396073\n",
            "69     \t [0.20933675 0.14745675 0.3301066 ]. \t  0.8199469925635752 \t 3.727265148396073\n",
            "70     \t [0.09873953 0.08344149 0.80692709]. \t  0.5296855557748921 \t 3.727265148396073\n",
            "71     \t [0.81629061 0.76645914 0.85429327]. \t  2.4207171980182767 \t 3.727265148396073\n",
            "72     \t [0.50729649 0.54241612 0.82395397]. \t  \u001b[92m3.7483005109548158\u001b[0m \t 3.7483005109548158\n",
            "73     \t [0.84389819 0.06320748 0.9842202 ]. \t  0.20065465243633962 \t 3.7483005109548158\n",
            "74     \t [0.23833763 0.14661407 0.50546705]. \t  0.25480671405943567 \t 3.7483005109548158\n",
            "75     \t [0.42865986 0.34679638 0.33070755]. \t  0.5504560083351115 \t 3.7483005109548158\n",
            "76     \t [0.99103575 0.05213942 0.16551671]. \t  0.22002866049970732 \t 3.7483005109548158\n",
            "77     \t [0.41202769 0.53309334 0.45145642]. \t  0.6403108764028697 \t 3.7483005109548158\n",
            "78     \t [0.96633121 0.2334966  0.14465115]. \t  0.19060165348368643 \t 3.7483005109548158\n",
            "79     \t [0.83820039 0.8749031  0.41691695]. \t  0.3498651032944783 \t 3.7483005109548158\n",
            "80     \t [0.86449676 0.79579375 0.91435755]. \t  1.9827797305941421 \t 3.7483005109548158\n",
            "81     \t [0.93400194 0.03894668 0.94800781]. \t  0.2100098811139738 \t 3.7483005109548158\n",
            "82     \t [0.00313698 0.46054941 0.47015367]. \t  0.5733329196058139 \t 3.7483005109548158\n",
            "83     \t [0.29671487 0.24949484 0.68979245]. \t  1.0839801349581415 \t 3.7483005109548158\n",
            "84     \t [0.76412824 0.85355953 0.26043253]. \t  0.06421503493392128 \t 3.7483005109548158\n",
            "85     \t [0.72595035 0.82629906 0.78520751]. \t  1.643169958970201 \t 3.7483005109548158\n",
            "86     \t [0.21010636 0.94073467 0.28099994]. \t  0.2948632701610779 \t 3.7483005109548158\n",
            "87     \t [0.47684429 0.79057807 0.68083692]. \t  1.937658177452781 \t 3.7483005109548158\n",
            "88     \t [7.93175689e-01 2.53034714e-01 1.34429246e-04]. \t  0.05690454852728574 \t 3.7483005109548158\n",
            "89     \t [0.20677644 0.64188221 0.66110577]. \t  2.3710065060725087 \t 3.7483005109548158\n",
            "90     \t [0.59187883 0.47656673 0.53049912]. \t  0.5999908176225892 \t 3.7483005109548158\n",
            "91     \t [0.01786845 0.04463263 0.66603289]. \t  0.24235692844368947 \t 3.7483005109548158\n",
            "92     \t [0.58293223 0.27863226 0.96653618]. \t  1.2122270629110494 \t 3.7483005109548158\n",
            "93     \t [0.86670758 0.64452272 0.24326527]. \t  0.04633779647468605 \t 3.7483005109548158\n",
            "94     \t [0.23969353 0.81773721 0.53651922]. \t  2.8242970536633676 \t 3.7483005109548158\n",
            "95     \t [0.30600453 0.74269544 0.00722497]. \t  0.002870525365569 \t 3.7483005109548158\n",
            "96     \t [0.84062088 0.07715726 0.05192499]. \t  0.1255534664247606 \t 3.7483005109548158\n",
            "97     \t [0.0562497  0.94139369 0.10719894]. \t  0.007371544767094565 \t 3.7483005109548158\n",
            "98     \t [0.10695383 0.05917109 0.42824318]. \t  0.37253454997322105 \t 3.7483005109548158\n",
            "99     \t [0.16625909 0.72276611 0.68183679]. \t  2.5378207743566707 \t 3.7483005109548158\n",
            "100    \t [0.49060885 0.6520218  0.17341417]. \t  0.05712173825425049 \t 3.7483005109548158\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9kM4bcwSteJO",
        "outputId": "abb3dab6-90e9-48fa-be27-19c0c2837196"
      },
      "source": [
        "end_lose = time.time()\r\n",
        "end_lose\r\n",
        "\r\n",
        "time_lose = end_lose - start_lose\r\n",
        "time_lose\r\n",
        "\r\n",
        "start_win = time.time()\r\n",
        "start_win"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1613495414.4635003"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8NMzLM7nteQH",
        "outputId": "1bc38fa5-7e6c-4652-f220-63dcd0d0246c"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 1 \r\n",
        "\r\n",
        "np.random.seed(run_num_1)\r\n",
        "surrogate_winner_1 = dtStudentProcess(d_cov_func, nu = df)\r\n",
        "\r\n",
        "winner_1 = dGPGO_stp(surrogate_winner_1, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_1.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.61217018 0.16906975 0.43605902]. \t  0.37345684462559386 \t 2.3951473341797507\n",
            "init   \t [0.76926247 0.2953253  0.14916296]. \t  0.296162062221701 \t 2.3951473341797507\n",
            "init   \t [0.02247832 0.42022449 0.23868214]. \t  0.2904113287153621 \t 2.3951473341797507\n",
            "init   \t [0.33765619 0.99071246 0.23772645]. \t  0.11013785080555143 \t 2.3951473341797507\n",
            "init   \t [0.08119266 0.66960024 0.62124292]. \t  2.3951473341797507 \t 2.3951473341797507\n",
            "1      \t [1. 1. 1.]. \t  0.31688362070415665 \t 2.3951473341797507\n",
            "2      \t [0.06043968 0.93959724 0.93680048]. \t  0.8159694620909308 \t 2.3951473341797507\n",
            "3      \t [0.15721395 0.18840441 0.9937264 ]. \t  0.542400596386888 \t 2.3951473341797507\n",
            "4      \t [0.99271916 0.97953791 0.19194934]. \t  0.0051249034030868 \t 2.3951473341797507\n",
            "5      \t [0.89539121 0.31310218 0.96935805]. \t  1.3903958518762796 \t 2.3951473341797507\n",
            "6      \t [0.54505566 0.63114103 0.96723501]. \t  \u001b[92m2.564960371535323\u001b[0m \t 2.564960371535323\n",
            "7      \t [0.98008859 0.54430467 0.67191   ]. \t  1.573154033071845 \t 2.564960371535323\n",
            "8      \t [0.54323338 0.94439984 0.69117173]. \t  1.2343288447736682 \t 2.564960371535323\n",
            "9      \t [0.03524238 0.59065387 0.92159026]. \t  \u001b[92m3.336800214725045\u001b[0m \t 3.336800214725045\n",
            "10     \t [0.94166773 0.11653376 0.52048176]. \t  0.12865874932753893 \t 3.336800214725045\n",
            "11     \t [0.01762465 0.02724597 0.44571668]. \t  0.2560086992016979 \t 3.336800214725045\n",
            "12     \t [0.38250177 0.04518189 0.08113646]. \t  0.3356017180695572 \t 3.336800214725045\n",
            "13     \t [0.26491173 0.56309669 0.72108411]. \t  2.7427497265577006 \t 3.336800214725045\n",
            "14     \t [0.6837988  0.76565075 0.00529513]. \t  0.00152557431203651 \t 3.336800214725045\n",
            "15     \t [1.         0.65794383 1.        ]. \t  1.8528009578537707 \t 3.336800214725045\n",
            "16     \t [0.82690081 0.01340752 0.0053981 ]. \t  0.0611524349747567 \t 3.336800214725045\n",
            "17     \t [0.14774672 0.6126935  0.99370423]. \t  2.147956385646059 \t 3.336800214725045\n",
            "18     \t [0.1646096  0.94745132 0.0826047 ]. \t  0.003831243638259969 \t 3.336800214725045\n",
            "19     \t [0.01178606 0.55571505 0.77280802]. \t  3.3163882248099226 \t 3.336800214725045\n",
            "20     \t [0.95860609 0.43402201 0.06402803]. \t  0.03736973280896683 \t 3.336800214725045\n",
            "21     \t [0.70119517 0.58811023 0.71959916]. \t  2.3803972437510152 \t 3.336800214725045\n",
            "22     \t [0.05273503 0.11569335 0.00356714]. \t  0.09194754470669135 \t 3.336800214725045\n",
            "23     \t [0.7533428  0.07848185 0.96653955]. \t  0.2714977689185242 \t 3.336800214725045\n",
            "24     \t [0.0223879  0.06813152 0.92685919]. \t  0.32379950916161804 \t 3.336800214725045\n",
            "25     \t [0.98561487 0.94373963 0.69633007]. \t  0.4727101326828849 \t 3.336800214725045\n",
            "26     \t [0.97579735 0.68626357 0.34346175]. \t  0.0713698039763077 \t 3.336800214725045\n",
            "27     \t [0.0461432  0.75494237 0.82730806]. \t  2.673281346095761 \t 3.336800214725045\n",
            "28     \t [0.39247148 0.29114421 0.00422846]. \t  0.09245522862306457 \t 3.336800214725045\n",
            "29     \t [0.99077136 0.04353467 0.90378298]. \t  0.28515236923171866 \t 3.336800214725045\n",
            "30     \t [0.74020513 0.88398175 0.96902451]. \t  0.9678096154569737 \t 3.336800214725045\n",
            "31     \t [0.51136527 0.50755174 0.84063075]. \t  \u001b[92m3.754640956742592\u001b[0m \t 3.754640956742592\n",
            "32     \t [0.34527363 0.13528551 0.81426946]. \t  0.8039439479755055 \t 3.754640956742592\n",
            "33     \t [0.07571583 0.97313717 0.57593994]. \t  2.7177671014407396 \t 3.754640956742592\n",
            "34     \t [0.56108518 0.33582697 0.86028363]. \t  2.46824592416084 \t 3.754640956742592\n",
            "35     \t [2.69364050e-14 3.49164069e-14 4.92267219e-14]. \t  0.06797411659019557 \t 3.754640956742592\n",
            "36     \t [0.99460572 0.0392557  0.27173388]. \t  0.2907858380300578 \t 3.754640956742592\n",
            "37     \t [0.00881421 0.79745136 0.10327641]. \t  0.009031229234290892 \t 3.754640956742592\n",
            "38     \t [0.6193481  0.00937012 0.24377972]. \t  0.7257759899724099 \t 3.754640956742592\n",
            "39     \t [0.00744657 0.40301213 0.76650824]. \t  2.693456869905508 \t 3.754640956742592\n",
            "40     \t [0.00106946 0.92708285 0.53961605]. \t  2.833565857022644 \t 3.754640956742592\n",
            "41     \t [0.07930791 0.53720475 0.87115126]. \t  \u001b[92m3.786174010598919\u001b[0m \t 3.786174010598919\n",
            "42     \t [0.97763611 0.70926418 0.01521341]. \t  0.0015035484549101256 \t 3.786174010598919\n",
            "43     \t [0.28910529 0.71804324 0.76574603]. \t  2.713657137495339 \t 3.786174010598919\n",
            "44     \t [0.62835206 0.97820225 0.03741701]. \t  0.0004911823693306104 \t 3.786174010598919\n",
            "45     \t [0.32033164 0.47214386 0.88394431]. \t  3.5161957811219158 \t 3.786174010598919\n",
            "46     \t [0.17829735 0.51570212 0.85921338]. \t  \u001b[92m3.7926167394436487\u001b[0m \t 3.7926167394436487\n",
            "47     \t [0.42461378 0.5371151  0.82623226]. \t  3.7740810623311365 \t 3.7926167394436487\n",
            "48     \t [0.04255517 0.51430089 0.88235136]. \t  3.670358754364442 \t 3.7926167394436487\n",
            "49     \t [0.3392115  0.44472185 0.82801049]. \t  3.4303351554775556 \t 3.7926167394436487\n",
            "50     \t [0.00394933 0.5055215  0.8749002 ]. \t  3.6705998800273782 \t 3.7926167394436487\n",
            "51     \t [0.45033973 0.6061437  0.85493005]. \t  3.753688653015401 \t 3.7926167394436487\n",
            "52     \t [0.04025001 0.99124637 0.76920759]. \t  1.0556774066727113 \t 3.7926167394436487\n",
            "53     \t [0.06021738 0.92770897 0.27680425]. \t  0.28615887254037903 \t 3.7926167394436487\n",
            "54     \t [0.51575824 0.96321219 0.99909687]. \t  0.4512120912234741 \t 3.7926167394436487\n",
            "55     \t [9.80557894e-07 2.66326702e-06 1.69340615e-01]. \t  0.434750339064883 \t 3.7926167394436487\n",
            "56     \t [0.25904957 0.54354925 0.02666472]. \t  0.02774143745435685 \t 3.7926167394436487\n",
            "57     \t [0.00281792 0.9722953  0.04792056]. \t  0.0012897774933416997 \t 3.7926167394436487\n",
            "58     \t [0.36271447 0.51916759 0.80328236]. \t  3.6040767388610275 \t 3.7926167394436487\n",
            "59     \t [0.01024699 0.46253471 0.84142553]. \t  3.524568133501758 \t 3.7926167394436487\n",
            "60     \t [0.39677235 0.58929595 0.83675867]. \t  3.7816262322931196 \t 3.7926167394436487\n",
            "61     \t [0.26323177 0.00244195 0.96635107]. \t  0.12818186042277518 \t 3.7926167394436487\n",
            "62     \t [0.02938756 0.45830168 0.01424345]. \t  0.03241094855247772 \t 3.7926167394436487\n",
            "63     \t [0.664164   0.53008855 0.87558074]. \t  3.7405780076484016 \t 3.7926167394436487\n",
            "64     \t [0.14182676 0.53652662 0.81608895]. \t  3.721048553212503 \t 3.7926167394436487\n",
            "65     \t [0.40411024 0.57824194 0.87792896]. \t  3.7806400908045896 \t 3.7926167394436487\n",
            "66     \t [0.77008245 0.53099902 0.86040924]. \t  3.7510208254463655 \t 3.7926167394436487\n",
            "67     \t [0.85662347 0.02185238 0.78117389]. \t  0.30292013605185314 \t 3.7926167394436487\n",
            "68     \t [0.58255578 0.53398273 0.85126821]. \t  \u001b[92m3.8111087628106937\u001b[0m \t 3.8111087628106937\n",
            "69     \t [0.00374122 0.3694047  0.99813288]. \t  1.4252519754207145 \t 3.8111087628106937\n",
            "70     \t [0.38721004 0.57612644 0.92320677]. \t  3.373790110281817 \t 3.8111087628106937\n",
            "71     \t [0.65307643 0.53164678 0.84179267]. \t  3.7742758670214096 \t 3.8111087628106937\n",
            "72     \t [0.54798683 0.5741338  0.84689683]. \t  3.8074427943879274 \t 3.8111087628106937\n",
            "73     \t [0.58275276 0.47073152 0.84948426]. \t  3.5974570139292736 \t 3.8111087628106937\n",
            "74     \t [0.29363794 0.57300769 0.84862673]. \t  \u001b[92m3.847654975964267\u001b[0m \t 3.847654975964267\n",
            "75     \t [0.138323   0.52254164 0.8501613 ]. \t  3.809821244444544 \t 3.847654975964267\n",
            "76     \t [0.70231016 0.52513953 0.85248588]. \t  3.7657203105538692 \t 3.847654975964267\n",
            "77     \t [0.47428761 0.46172344 0.78457677]. \t  3.221447481034328 \t 3.847654975964267\n",
            "78     \t [0.6492858  0.52591035 0.85575184]. \t  3.7825333534833385 \t 3.847654975964267\n",
            "79     \t [0.1811546  0.5344763  0.85180163]. \t  3.838945886800932 \t 3.847654975964267\n",
            "80     \t [0.77719329 0.57452852 0.83800398]. \t  3.7052374702591084 \t 3.847654975964267\n",
            "81     \t [0.05551929 0.53579737 0.8296917 ]. \t  3.769977960502577 \t 3.847654975964267\n",
            "82     \t [0.59831253 0.53472728 0.86312455]. \t  3.801522718193876 \t 3.847654975964267\n",
            "83     \t [0.66583416 0.54829458 0.83689413]. \t  3.7653176522640317 \t 3.847654975964267\n",
            "84     \t [0.60944791 0.48675893 0.91331785]. \t  3.3051194944552025 \t 3.847654975964267\n",
            "85     \t [0.53335572 0.46638946 0.81415275]. \t  3.470500567163201 \t 3.847654975964267\n",
            "86     \t [0.23332317 0.62877359 0.83387434]. \t  3.641111763735332 \t 3.847654975964267\n",
            "87     \t [0.51722882 0.58432904 0.83685049]. \t  3.7676790177637294 \t 3.847654975964267\n",
            "88     \t [0.60008067 0.57210571 0.83050324]. \t  3.7393826979364015 \t 3.847654975964267\n",
            "89     \t [0.13260162 0.59310493 0.81964827]. \t  3.6997032595650317 \t 3.847654975964267\n",
            "90     \t [0.00906319 0.61115908 0.72902227]. \t  2.8123972838008773 \t 3.847654975964267\n",
            "91     \t [0.70822321 0.5796581  0.85518904]. \t  3.759830622179515 \t 3.847654975964267\n",
            "92     \t [0.11424412 0.58460589 0.83076457]. \t  3.76870026980633 \t 3.847654975964267\n",
            "93     \t [0.39429329 0.5761656  0.87345389]. \t  3.8035064243724515 \t 3.847654975964267\n",
            "94     \t [0.69995093 0.56763672 0.81860078]. \t  3.636912639924504 \t 3.847654975964267\n",
            "95     \t [0.76011488 0.54870394 0.84833786]. \t  3.7642339922580446 \t 3.847654975964267\n",
            "96     \t [0.25285798 0.48575918 0.79804943]. \t  3.470030171023258 \t 3.847654975964267\n",
            "97     \t [0.51412724 0.57570998 0.84288936]. \t  3.8045114976989547 \t 3.847654975964267\n",
            "98     \t [0.56423646 0.57459162 0.85335692]. \t  3.8103382631045006 \t 3.847654975964267\n",
            "99     \t [0.1831383  0.4877795  0.81688142]. \t  3.6046308049339433 \t 3.847654975964267\n",
            "100    \t [0.47980728 0.56727405 0.82241079]. \t  3.7367396691700407 \t 3.847654975964267\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBLcW6tlteS2",
        "outputId": "5961ab68-023e-4949-bca0-f5b846f91081"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 2 \r\n",
        "\r\n",
        "np.random.seed(run_num_2)\r\n",
        "surrogate_winner_2 = dtStudentProcess(d_cov_func, nu = df)\r\n",
        "\r\n",
        "winner_2 = dGPGO_stp(surrogate_winner_2, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_2.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.85198549 0.0739036  0.89493176]. \t  0.3999083566189884 \t 2.6229838112516717\n",
            "init   \t [0.43649355 0.12767773 0.57585787]. \t  0.24461577848211966 \t 2.6229838112516717\n",
            "init   \t [0.84047092 0.43512055 0.69591056]. \t  1.8897715258798413 \t 2.6229838112516717\n",
            "init   \t [0.6846381  0.70064837 0.77969426]. \t  2.6229838112516717 \t 2.6229838112516717\n",
            "init   \t [0.64274937 0.96102617 0.10846489]. \t  0.003309399496320042 \t 2.6229838112516717\n",
            "1      \t [1. 1. 1.]. \t  0.31688362070415665 \t 2.6229838112516717\n",
            "2      \t [0.06235371 0.93877586 0.51099026]. \t  \u001b[92m2.716489798166292\u001b[0m \t 2.716489798166292\n",
            "3      \t [0.09831145 0.99372006 0.99535858]. \t  0.3647801677562718 \t 2.716489798166292\n",
            "4      \t [0.12387761 0.14906975 0.98769341]. \t  0.42212207280543435 \t 2.716489798166292\n",
            "5      \t [0.09184097 0.60864856 0.10010983]. \t  0.03364448068974112 \t 2.716489798166292\n",
            "6      \t [0.03846391 0.98441727 0.13398256]. \t  0.01313336214932907 \t 2.716489798166292\n",
            "7      \t [0.76861707 1.         0.59162661]. \t  0.7133720131273626 \t 2.716489798166292\n",
            "8      \t [0.87533228 0.01580846 0.00721452]. \t  0.054959917562459513 \t 2.716489798166292\n",
            "9      \t [0.02851891 0.49081837 0.58086585]. \t  1.2342513397521742 \t 2.716489798166292\n",
            "10     \t [0.05593192 0.10927067 0.08477948]. \t  0.2742228220374764 \t 2.716489798166292\n",
            "11     \t [0.9449635  0.59504613 0.16844385]. \t  0.029987329500278628 \t 2.716489798166292\n",
            "12     \t [0.97197754 0.65101064 0.98052675]. \t  2.197108899856513 \t 2.716489798166292\n",
            "13     \t [0.56176075 0.29070203 0.02061851]. \t  0.1065901609487191 \t 2.716489798166292\n",
            "14     \t [0.33919865 0.99374473 0.6389444 ]. \t  1.894104395534279 \t 2.716489798166292\n",
            "15     \t [0.46634423 0.49474369 0.95112195]. \t  \u001b[92m2.8240391947183188\u001b[0m \t 2.8240391947183188\n",
            "16     \t [0.76590001 0.39117677 0.99716402]. \t  1.556835999608739 \t 2.8240391947183188\n",
            "17     \t [0.04780484 0.61976502 0.97653457]. \t  2.421126490501701 \t 2.8240391947183188\n",
            "18     \t [0.36380832 0.7647438  0.42439058]. \t  1.338532491575134 \t 2.8240391947183188\n",
            "19     \t [1.         0.7761063  0.72091364]. \t  1.294130552278178 \t 2.8240391947183188\n",
            "20     \t [0.63191854 0.92616438 0.99990808]. \t  0.5857190033967211 \t 2.8240391947183188\n",
            "21     \t [0.94370685 0.97636958 0.06620126]. \t  0.00032801869311639074 \t 2.8240391947183188\n",
            "22     \t [0.98043751 0.03895684 0.70167592]. \t  0.2770860042543458 \t 2.8240391947183188\n",
            "23     \t [0.11727524 0.82433257 0.75833468]. \t  2.099667794047401 \t 2.8240391947183188\n",
            "24     \t [0.47156709 0.0101798  0.01699426]. \t  0.13195234337419431 \t 2.8240391947183188\n",
            "25     \t [0.73937039 0.02832851 0.23446747]. \t  0.592967202791494 \t 2.8240391947183188\n",
            "26     \t [0.02229331 0.26447125 0.02477353]. \t  0.09610689621087938 \t 2.8240391947183188\n",
            "27     \t [0.30029802 0.68778263 0.97068274]. \t  2.267838253718934 \t 2.8240391947183188\n",
            "28     \t [0.002897   0.90104093 0.530301  ]. \t  \u001b[92m2.867814143620765\u001b[0m \t 2.867814143620765\n",
            "29     \t [0.47808053 0.00645933 0.98660663]. \t  0.11205696453106141 \t 2.867814143620765\n",
            "30     \t [0.82660579 0.70808024 0.01406449]. \t  0.002442931043854671 \t 2.867814143620765\n",
            "31     \t [0.03015946 0.99303442 0.62789117]. \t  2.2613897130440437 \t 2.867814143620765\n",
            "32     \t [0.95886553 0.36915751 0.01143307]. \t  0.02615014716964419 \t 2.867814143620765\n",
            "33     \t [0.0505469  0.05563414 0.7249141 ]. \t  0.3598724288447533 \t 2.867814143620765\n",
            "34     \t [0.63688598 0.5982654  0.85681648]. \t  \u001b[92m3.734857263704983\u001b[0m \t 3.734857263704983\n",
            "35     \t [0.78057593 0.00257239 0.5764698 ]. \t  0.09897457816177216 \t 3.734857263704983\n",
            "36     \t [4.12998780e-11 3.26600991e-11 7.22711036e-02]. \t  0.18521600345602215 \t 3.734857263704983\n",
            "37     \t [0.         0.         0.23789193]. \t  0.564994911372362 \t 3.734857263704983\n",
            "38     \t [0.10432524 0.00971215 0.93798579]. \t  0.17027601385234997 \t 3.734857263704983\n",
            "39     \t [0.99904854 0.50274235 0.94842907]. \t  2.7881883669175704 \t 3.734857263704983\n",
            "40     \t [0.01251232 0.79293192 0.97599544]. \t  1.5110250306442063 \t 3.734857263704983\n",
            "41     \t [0.40483813 0.50621771 0.8540522 ]. \t  \u001b[92m3.7759662972578503\u001b[0m \t 3.7759662972578503\n",
            "42     \t [0.52346705 0.51201989 0.81559658]. \t  3.6502173480581046 \t 3.7759662972578503\n",
            "43     \t [0.51238902 0.42452018 0.86029246]. \t  3.286680305139343 \t 3.7759662972578503\n",
            "44     \t [2.44554540e-05 9.45010841e-01 4.50270672e-01]. \t  1.987709275535531 \t 3.7759662972578503\n",
            "45     \t [0.419179   0.86814513 0.0470324 ]. \t  0.0018067160136387031 \t 3.7759662972578503\n",
            "46     \t [0.32351518 0.99435592 0.22980604]. \t  0.09554184684741875 \t 3.7759662972578503\n",
            "47     \t [0.99808851 0.92689217 0.36916672]. \t  0.098058275003588 \t 3.7759662972578503\n",
            "48     \t [0.98438078 0.163602   0.27465949]. \t  0.31396181751225294 \t 3.7759662972578503\n",
            "49     \t [0.43918671 0.55893195 0.75736797]. \t  3.1096441596308564 \t 3.7759662972578503\n",
            "50     \t [0.52116116 0.48450534 0.83035427]. \t  3.63781309029848 \t 3.7759662972578503\n",
            "51     \t [0.34772685 0.47739235 0.84160722]. \t  3.6525152956174463 \t 3.7759662972578503\n",
            "52     \t [0.2708965  0.58006795 0.81544472]. \t  3.7094719147979998 \t 3.7759662972578503\n",
            "53     \t [0.19671548 0.56321736 0.76949772]. \t  3.315550104661594 \t 3.7759662972578503\n",
            "54     \t [0.01618986 0.51084954 0.87141043]. \t  3.7073294648343524 \t 3.7759662972578503\n",
            "55     \t [1.44227894e-01 3.71799072e-08 3.30373943e-01]. \t  0.6659204179538226 \t 3.7759662972578503\n",
            "56     \t [0.45170147 0.5511251  0.87941599]. \t  \u001b[92m3.7840831582066343\u001b[0m \t 3.7840831582066343\n",
            "57     \t [0.16455212 0.44559074 0.8621925 ]. \t  3.433822939548725 \t 3.7840831582066343\n",
            "58     \t [0.32848473 0.56089239 0.82608435]. \t  \u001b[92m3.7895306386791714\u001b[0m \t 3.7895306386791714\n",
            "59     \t [0.00809847 0.28414369 0.98196513]. \t  1.0981716590688286 \t 3.7895306386791714\n",
            "60     \t [0.41674291 0.48720115 0.89205597]. \t  3.534770769663548 \t 3.7895306386791714\n",
            "61     \t [0.39771281 0.52139211 0.8534065 ]. \t  \u001b[92m3.8191462985624614\u001b[0m \t 3.8191462985624614\n",
            "62     \t [0.23440065 0.45449832 0.87703708]. \t  3.443609459158491 \t 3.8191462985624614\n",
            "63     \t [0.39817486 0.58005276 0.83433534]. \t  3.7931188027951435 \t 3.8191462985624614\n",
            "64     \t [0.01435176 0.52547561 0.83829134]. \t  3.768761101297958 \t 3.8191462985624614\n",
            "65     \t [0.52274403 0.51962947 0.8536116 ]. \t  3.798563828757483 \t 3.8191462985624614\n",
            "66     \t [0.38370591 0.4966606  0.85610603]. \t  3.740980848299051 \t 3.8191462985624614\n",
            "67     \t [0.00176955 0.60148416 0.76782822]. \t  3.2130745548162505 \t 3.8191462985624614\n",
            "68     \t [0.80634859 0.56189773 0.88877142]. \t  3.647660107323671 \t 3.8191462985624614\n",
            "69     \t [0.16320906 0.56551466 0.83080553]. \t  3.80387620738451 \t 3.8191462985624614\n",
            "70     \t [3.02216486e-01 1.23406980e-06 7.53754929e-01]. \t  0.24143949938164763 \t 3.8191462985624614\n",
            "71     \t [0.99094508 0.50862054 0.80956234]. \t  3.4282707914014914 \t 3.8191462985624614\n",
            "72     \t [0.44022372 0.57925218 0.81458965]. \t  3.6767496655408016 \t 3.8191462985624614\n",
            "73     \t [0.30557368 0.58689026 0.82214487]. \t  3.7335266186702007 \t 3.8191462985624614\n",
            "74     \t [0.49054481 0.56098157 0.84422749]. \t  \u001b[92m3.829994126077763\u001b[0m \t 3.829994126077763\n",
            "75     \t [0.42898219 0.51949923 0.80476793]. \t  3.6055963966360483 \t 3.829994126077763\n",
            "76     \t [0.5994864  0.56164009 0.84181164]. \t  3.796592528039027 \t 3.829994126077763\n",
            "77     \t [0.25757223 0.6169793  0.77768968]. \t  3.3001393317127663 \t 3.829994126077763\n",
            "78     \t [1.         1.         0.78965761]. \t  0.46316101916578345 \t 3.829994126077763\n",
            "79     \t [0.52691881 0.49400746 0.87172712]. \t  3.6766733085800327 \t 3.829994126077763\n",
            "80     \t [0.1650223  0.5177287  0.85267058]. \t  3.802464987575077 \t 3.829994126077763\n",
            "81     \t [0.53190601 0.52217681 0.85051334]. \t  3.801817876089271 \t 3.829994126077763\n",
            "82     \t [0.32694696 0.60521335 0.81633808]. \t  3.6445153898440723 \t 3.829994126077763\n",
            "83     \t [0.98464349 0.01389699 0.17163263]. \t  0.2190903238151828 \t 3.829994126077763\n",
            "84     \t [0.02019641 0.53480755 0.84675949]. \t  3.8005187604281683 \t 3.829994126077763\n",
            "85     \t [0.68607335 0.5014541  0.86532743]. \t  3.693338866865438 \t 3.829994126077763\n",
            "86     \t [0.37645627 0.55672803 0.84957034]. \t  \u001b[92m3.8564339058468313\u001b[0m \t 3.8564339058468313\n",
            "87     \t [0.24843684 0.53621948 0.8361031 ]. \t  3.8239437735272697 \t 3.8564339058468313\n",
            "88     \t [0.72823669 0.38187888 0.84710777]. \t  2.907333923061829 \t 3.8564339058468313\n",
            "89     \t [0.615279   0.60966221 0.84561653]. \t  3.6853627732359264 \t 3.8564339058468313\n",
            "90     \t [0.45186236 0.48396145 0.8430969 ]. \t  3.6786992582837295 \t 3.8564339058468313\n",
            "91     \t [0.43400174 0.51779348 0.90062109]. \t  3.5748206725885154 \t 3.8564339058468313\n",
            "92     \t [0.2487902  0.53445664 0.84385264]. \t  3.839260211508245 \t 3.8564339058468313\n",
            "93     \t [0.01932072 0.52937935 0.84913795]. \t  3.7933666372370993 \t 3.8564339058468313\n",
            "94     \t [0.38860446 0.46837475 0.83750795]. \t  3.5964210775267005 \t 3.8564339058468313\n",
            "95     \t [0.5111522  0.57531485 0.85049957]. \t  3.819371704108217 \t 3.8564339058468313\n",
            "96     \t [0.69678425 0.57684907 0.85686088]. \t  3.769447378975577 \t 3.8564339058468313\n",
            "97     \t [0.21091029 0.56126685 0.84176039]. \t  3.8448638464760885 \t 3.8564339058468313\n",
            "98     \t [0.40845898 0.58476867 0.84004326]. \t  3.8006562879918855 \t 3.8564339058468313\n",
            "99     \t [0.45539351 0.50990634 0.86215213]. \t  3.7728301935181174 \t 3.8564339058468313\n",
            "100    \t [0.59574071 0.70897791 0.76924611]. \t  2.542052229950288 \t 3.8564339058468313\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "emgjXwfeuvRY",
        "outputId": "9bed6f91-731a-4df9-bc33-0381484d1933"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 3 \r\n",
        "\r\n",
        "np.random.seed(run_num_3)\r\n",
        "surrogate_winner_3 = dtStudentProcess(d_cov_func, nu = df)\r\n",
        "\r\n",
        "winner_3 = dGPGO_stp(surrogate_winner_3, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_3.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.75157561 0.10925128 0.48612128]. \t  0.19395982560567754 \t 0.5647137279144399\n",
            "init   \t [0.49983118 0.65711    0.23588471]. \t  0.10633623603471726 \t 0.5647137279144399\n",
            "init   \t [0.61279489 0.1196524  0.7122023 ]. \t  0.5647137279144399 \t 0.5647137279144399\n",
            "init   \t [0.37256054 0.52476827 0.26328708]. \t  0.24645583811739444 \t 0.5647137279144399\n",
            "init   \t [0.62163122 0.44909976 0.21906361]. \t  0.2633149120115605 \t 0.5647137279144399\n",
            "1      \t [1. 1. 1.]. \t  0.31688362070415665 \t 0.5647137279144399\n",
            "2      \t [0.00227319 0.27141826 0.95291977]. \t  \u001b[92m1.263004128201673\u001b[0m \t 1.263004128201673\n",
            "3      \t [0.00798031 0.03065721 0.14136081]. \t  0.39018974818183505 \t 1.263004128201673\n",
            "4      \t [0.24328895 0.80755318 0.95497633]. \t  \u001b[92m1.634385399805955\u001b[0m \t 1.634385399805955\n",
            "5      \t [0.05018267 0.95018324 0.96278164]. \t  0.6517797142407744 \t 1.634385399805955\n",
            "6      \t [1.         0.40742056 0.95048795]. \t  \u001b[92m2.2572448938609546\u001b[0m \t 2.2572448938609546\n",
            "7      \t [1.         0.66497601 0.71080351]. \t  1.76170738162672 \t 2.2572448938609546\n",
            "8      \t [0.99404649 0.51223225 0.00765043]. \t  0.008600763782223483 \t 2.2572448938609546\n",
            "9      \t [0.70357405 0.61134807 1.        ]. \t  2.02708110044123 \t 2.2572448938609546\n",
            "10     \t [0.92447327 0.12892628 0.98241231]. \t  0.36675430636579204 \t 2.2572448938609546\n",
            "11     \t [0.01976412 0.97583946 0.04670764]. \t  0.001245865418005175 \t 2.2572448938609546\n",
            "12     \t [0.99589215 0.97307727 0.15347992]. \t  0.0021876462663275357 \t 2.2572448938609546\n",
            "13     \t [0.58927601 0.08985364 0.03044746]. \t  0.15944848433183503 \t 2.2572448938609546\n",
            "14     \t [0.06767933 0.00777126 0.91377213]. \t  0.1933982485879563 \t 2.2572448938609546\n",
            "15     \t [1.         0.63675089 1.        ]. \t  1.9156936391391615 \t 2.2572448938609546\n",
            "16     \t [0.20523207 0.99721531 0.72449449]. \t  1.327173515899098 \t 2.2572448938609546\n",
            "17     \t [0.05735131 0.45030718 0.68449192]. \t  2.0147267348170605 \t 2.2572448938609546\n",
            "18     \t [0.51492964 1.         1.        ]. \t  0.3320507672627641 \t 2.2572448938609546\n",
            "19     \t [0.43530605 0.57709843 0.75531213]. \t  \u001b[92m3.070717582202245\u001b[0m \t 3.070717582202245\n",
            "20     \t [0.33081669 0.44639571 0.89910636]. \t  \u001b[92m3.235760772058998\u001b[0m \t 3.235760772058998\n",
            "21     \t [0.12297912 0.00867872 0.40634635]. \t  0.4194267410590804 \t 3.235760772058998\n",
            "22     \t [0.8298889 1.        0.6027259]. \t  0.5589678100531476 \t 3.235760772058998\n",
            "23     \t [0.36593104 0.42105333 0.75747987]. \t  2.743736010047984 \t 3.235760772058998\n",
            "24     \t [0.04405112 0.48913539 0.04031171]. \t  0.0391272481766946 \t 3.235760772058998\n",
            "25     \t [0.01178214 0.86567377 0.51600564]. \t  2.8281982104250556 \t 3.235760772058998\n",
            "26     \t [0.95195299 0.16592518 0.16018336]. \t  0.24959756995046442 \t 3.235760772058998\n",
            "27     \t [0.3061382  0.36748874 0.99187397]. \t  1.5151951587247 \t 3.235760772058998\n",
            "28     \t [0.47007285 0.36121295 0.00855656]. \t  0.07169854131066096 \t 3.235760772058998\n",
            "29     \t [0.25926261 0.03992217 0.00407196]. \t  0.11370609085632767 \t 3.235760772058998\n",
            "30     \t [0.7015261  0.84857889 0.02481276]. \t  0.0008111554908137231 \t 3.235760772058998\n",
            "31     \t [0.92387052 0.01768501 0.48462414]. \t  0.1059321086590386 \t 3.235760772058998\n",
            "32     \t [0.92504032 0.05264358 0.0329761 ]. \t  0.07305648085593545 \t 3.235760772058998\n",
            "33     \t [0.80373941 0.81402396 0.87764993]. \t  1.9571292126163495 \t 3.235760772058998\n",
            "34     \t [1.        1.        0.8096207]. \t  0.4941188176237432 \t 3.235760772058998\n",
            "35     \t [0.11209056 0.59154455 0.8991356 ]. \t  \u001b[92m3.593896333426767\u001b[0m \t 3.593896333426767\n",
            "36     \t [0.00217535 0.95155078 0.26184913]. \t  0.20871490035074294 \t 3.593896333426767\n",
            "37     \t [1.04887620e-13 1.57552492e-13 2.86918622e-13]. \t  0.06797411659048597 \t 3.593896333426767\n",
            "38     \t [0.46924464 0.97364051 0.03875098]. \t  0.0007567948829400931 \t 3.593896333426767\n",
            "39     \t [0.06195201 0.53234796 0.83450004]. \t  \u001b[92m3.784335945032053\u001b[0m \t 3.784335945032053\n",
            "40     \t [0.04178397 0.59267377 0.92193804]. \t  3.330199967183004 \t 3.784335945032053\n",
            "41     \t [0.01827025 0.76027581 0.81219259]. \t  2.589673621407247 \t 3.784335945032053\n",
            "42     \t [0.77812732 0.44504792 0.82660076]. \t  3.3518744068214152 \t 3.784335945032053\n",
            "43     \t [0.09363681 0.72749106 0.00112408]. \t  0.002535240236507474 \t 3.784335945032053\n",
            "44     \t [0.92768517 0.1724717  0.74351412]. \t  0.8929944333064983 \t 3.784335945032053\n",
            "45     \t [0.9926441  0.73741467 0.39396525]. \t  0.12124231653898068 \t 3.784335945032053\n",
            "46     \t [0.17376495 0.75652314 0.49396818]. \t  2.3754852129584725 \t 3.784335945032053\n",
            "47     \t [0.88486947 0.0036365  0.92049923]. \t  0.17630887377802912 \t 3.784335945032053\n",
            "48     \t [0.25831848 0.58639197 0.73242817]. \t  2.877408456577788 \t 3.784335945032053\n",
            "49     \t [0.         0.0212663  0.55669995]. \t  0.11264920627644255 \t 3.784335945032053\n",
            "50     \t [0.16524407 0.62649384 0.80146112]. \t  3.472276418562876 \t 3.784335945032053\n",
            "51     \t [0.03941128 0.61628933 0.47813135]. \t  1.3758757166298674 \t 3.784335945032053\n",
            "52     \t [0.74586011 0.5839689  0.7097277 ]. \t  2.2117523749432206 \t 3.784335945032053\n",
            "53     \t [0.12091813 0.51126617 0.88834461]. \t  3.638935163512565 \t 3.784335945032053\n",
            "54     \t [0.05268882 0.56929355 0.94501458]. \t  3.028222158125816 \t 3.784335945032053\n",
            "55     \t [0.73379974 0.35669198 0.85834208]. \t  2.6535493333650715 \t 3.784335945032053\n",
            "56     \t [0.03607688 0.99887897 0.70123989]. \t  1.5356311241123297 \t 3.784335945032053\n",
            "57     \t [0.36291565 0.60634218 0.88231056]. \t  3.6924544247186954 \t 3.784335945032053\n",
            "58     \t [0.00668353 0.56511481 0.83717143]. \t  \u001b[92m3.7870977234542003\u001b[0m \t 3.7870977234542003\n",
            "59     \t [0.28321513 0.50327565 0.86146394]. \t  3.7581642824722348 \t 3.7870977234542003\n",
            "60     \t [0.20691844 0.51180559 0.85556985]. \t  \u001b[92m3.789586639775824\u001b[0m \t 3.789586639775824\n",
            "61     \t [0.18622191 0.59626348 0.80441652]. \t  3.5969659975590673 \t 3.789586639775824\n",
            "62     \t [0.29898329 0.60900892 0.82762339]. \t  3.696689850643865 \t 3.789586639775824\n",
            "63     \t [0.1364943  0.63367921 0.7807492 ]. \t  3.2765961987614105 \t 3.789586639775824\n",
            "64     \t [0.97544744 0.00149616 0.17888262]. \t  0.22955320008350583 \t 3.789586639775824\n",
            "65     \t [0.27780765 0.58601264 0.84091386]. \t  \u001b[92m3.8122551094623125\u001b[0m \t 3.8122551094623125\n",
            "66     \t [0.23116246 0.6128204  0.84219784]. \t  3.7330593713623346 \t 3.8122551094623125\n",
            "67     \t [0.02154755 0.68861644 0.7580104 ]. \t  2.8377925555052066 \t 3.8122551094623125\n",
            "68     \t [0.34688249 0.46735149 0.86771699]. \t  3.5705717541078696 \t 3.8122551094623125\n",
            "69     \t [0.03938366 0.52973537 0.86604798]. \t  3.7802333413071714 \t 3.8122551094623125\n",
            "70     \t [0.31342976 0.63406281 0.79878925]. \t  3.4049409910874653 \t 3.8122551094623125\n",
            "71     \t [0.22654905 0.45510068 0.83638162]. \t  3.514251297505198 \t 3.8122551094623125\n",
            "72     \t [0.21245092 0.52924976 0.83410813]. \t  3.805479542215944 \t 3.8122551094623125\n",
            "73     \t [0.1561071  0.5379695  0.86703572]. \t  \u001b[92m3.817069881059605\u001b[0m \t 3.817069881059605\n",
            "74     \t [0.09826155 0.61800861 0.87075857]. \t  3.6788271978515388 \t 3.817069881059605\n",
            "75     \t [0.27447533 0.6386287  0.90675307]. \t  3.372624338083212 \t 3.817069881059605\n",
            "76     \t [0.56888399 0.56103622 0.82513537]. \t  3.733947339681703 \t 3.817069881059605\n",
            "77     \t [0.23863015 0.59204473 0.86968537]. \t  3.7874536955201155 \t 3.817069881059605\n",
            "78     \t [0.16027789 0.55694087 0.83317501]. \t  3.8165136607904513 \t 3.817069881059605\n",
            "79     \t [0.35340917 0.5272625  0.85541704]. \t  \u001b[92m3.8332365135907693\u001b[0m \t 3.8332365135907693\n",
            "80     \t [0.98902996 0.96897608 0.02265054]. \t  9.1874116116191e-05 \t 3.8332365135907693\n",
            "81     \t [0.37133005 0.50762501 0.87410252]. \t  3.7322761984047457 \t 3.8332365135907693\n",
            "82     \t [0.52337678 0.572788   0.9018212 ]. \t  3.6059766250623935 \t 3.8332365135907693\n",
            "83     \t [0.23203233 0.54799944 0.86230775]. \t  \u001b[92m3.847765129418307\u001b[0m \t 3.847765129418307\n",
            "84     \t [0.65207084 0.53968638 0.83782163]. \t  3.770190912244353 \t 3.847765129418307\n",
            "85     \t [0.10780313 0.52705459 0.88335403]. \t  3.712943582748775 \t 3.847765129418307\n",
            "86     \t [0.26054006 0.62774591 0.85186853]. \t  3.6802388525548717 \t 3.847765129418307\n",
            "87     \t [0.42325143 0.51275926 0.86073251]. \t  3.7873070245278697 \t 3.847765129418307\n",
            "88     \t [0.57804705 0.56860837 0.81050767]. \t  3.619203875822228 \t 3.847765129418307\n",
            "89     \t [0.11939832 0.57014288 0.84684619]. \t  3.8330790861202715 \t 3.847765129418307\n",
            "90     \t [0.98657146 0.02784431 0.86891841]. \t  0.28309213515724746 \t 3.847765129418307\n",
            "91     \t [0.46642822 0.53798483 0.8852377 ]. \t  3.7372490242008842 \t 3.847765129418307\n",
            "92     \t [0.388928   0.5841938  0.84577787]. \t  3.8177997913599127 \t 3.847765129418307\n",
            "93     \t [0.42915819 0.61072831 0.90253483]. \t  3.5264387343117605 \t 3.847765129418307\n",
            "94     \t [0.50030156 0.59097788 0.83259794]. \t  3.7381114681588334 \t 3.847765129418307\n",
            "95     \t [0.13231119 0.54870078 0.85949531]. \t  3.8394094342051264 \t 3.847765129418307\n",
            "96     \t [0.4592948  0.57702196 0.78762949]. \t  3.4340754429104545 \t 3.847765129418307\n",
            "97     \t [0.83058345 0.53894186 0.8491738 ]. \t  3.7354871508032104 \t 3.847765129418307\n",
            "98     \t [0.17201736 0.51903519 0.85411131]. \t  3.806207438477421 \t 3.847765129418307\n",
            "99     \t [0.28150751 0.56842682 0.84713686]. \t  \u001b[92m3.851354345108315\u001b[0m \t 3.851354345108315\n",
            "100    \t [0.44711506 0.60559085 0.85921244]. \t  3.7553093492139897 \t 3.851354345108315\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8riJpBBKuvT4",
        "outputId": "6a3a0430-7b2b-4780-ca29-4b198fb7c205"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 4 \r\n",
        "\r\n",
        "np.random.seed(run_num_4)\r\n",
        "surrogate_winner_4 = dtStudentProcess(d_cov_func, nu = df)\r\n",
        "\r\n",
        "winner_4 = dGPGO_stp(surrogate_winner_4, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_4.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.83936105 0.82129497 0.64220716]. \t  0.8796727111154451 \t 1.9592421489197056\n",
            "init   \t [0.66722262 0.03913991 0.06567239]. \t  0.21283931850025994 \t 1.9592421489197056\n",
            "init   \t [0.27650064 0.3163605  0.19359241]. \t  0.5590312289203825 \t 1.9592421489197056\n",
            "init   \t [0.19709288 0.84920429 0.84235809]. \t  1.818649590001362 \t 1.9592421489197056\n",
            "init   \t [0.34736473 0.7397423  0.49340914]. \t  1.9592421489197056 \t 1.9592421489197056\n",
            "1      \t [0.72375065 0.15415572 0.98627436]. \t  0.4436681531252645 \t 1.9592421489197056\n",
            "2      \t [0.00568574 0.90362017 0.0072839 ]. \t  0.0005407899212332239 \t 1.9592421489197056\n",
            "3      \t [0.91444374 0.93067625 0.16188928]. \t  0.004441114262621566 \t 1.9592421489197056\n",
            "4      \t [0.21130366 0.05657913 0.88927897]. \t  0.35479026588444773 \t 1.9592421489197056\n",
            "5      \t [0.98597041 0.2856864  0.41065736]. \t  0.15284929019920618 \t 1.9592421489197056\n",
            "6      \t [0.01263547 0.84000117 0.48748252]. \t  \u001b[92m2.5493250331261685\u001b[0m \t 2.5493250331261685\n",
            "7      \t [0.02972088 0.6284749  0.84237526]. \t  \u001b[92m3.6309488543491604\u001b[0m \t 3.6309488543491604\n",
            "8      \t [1. 1. 1.]. \t  0.31688362070415665 \t 3.6309488543491604\n",
            "9      \t [0.04681794 0.56517096 0.99093683]. \t  2.2288293321682677 \t 3.6309488543491604\n",
            "10     \t [0.99883605 0.01930831 0.92265041]. \t  0.20215238778135372 \t 3.6309488543491604\n",
            "11     \t [0.99265674 0.54082755 0.97192954]. \t  2.4878236273254775 \t 3.6309488543491604\n",
            "12     \t [0.13846881 0.0228485  0.34934838]. \t  0.6391425027910097 \t 3.6309488543491604\n",
            "13     \t [0.77219006 0.36598797 0.0043671 ]. \t  0.041514068514461565 \t 3.6309488543491604\n",
            "14     \t [0.00670169 0.51111502 0.53536439]. \t  1.0681249311115697 \t 3.6309488543491604\n",
            "15     \t [0.58558966 0.92598694 0.05314251]. \t  0.0010947524295280414 \t 3.6309488543491604\n",
            "16     \t [0.40166896 0.01279959 0.60813828]. \t  0.13802504266533774 \t 3.6309488543491604\n",
            "17     \t [0.00082813 0.0504218  0.02602301]. \t  0.11111517830261598 \t 3.6309488543491604\n",
            "18     \t [0.16345544 0.99651625 0.49329637]. \t  2.289188729025954 \t 3.6309488543491604\n",
            "19     \t [0.99554361 0.9576235  0.69802017]. \t  0.4300570038815998 \t 3.6309488543491604\n",
            "20     \t [0.07216167 0.88356224 0.96215796]. \t  1.0298298769580736 \t 3.6309488543491604\n",
            "21     \t [0.31174701 0.0189721  0.06093377]. \t  0.2506996030806425 \t 3.6309488543491604\n",
            "22     \t [0.50511242 0.42718633 0.6652271 ]. \t  1.619889141070305 \t 3.6309488543491604\n",
            "23     \t [0.75911008 0.70332007 0.99692331]. \t  1.7612440817386843 \t 3.6309488543491604\n",
            "24     \t [0.96127081 0.2878493  0.05250767]. \t  0.06531141076926203 \t 3.6309488543491604\n",
            "25     \t [0.05206217 0.11645052 0.99586248]. \t  0.2965464520344992 \t 3.6309488543491604\n",
            "26     \t [0.00148381 0.89117079 0.77956044]. \t  1.5755200861115546 \t 3.6309488543491604\n",
            "27     \t [0.66701577 0.99475404 0.42204306]. \t  0.6018000873213677 \t 3.6309488543491604\n",
            "28     \t [0.97050783 0.01399257 0.35476047]. \t  0.24233260345106145 \t 3.6309488543491604\n",
            "29     \t [0.94543233 0.99309845 0.03756964]. \t  0.0001396609701745804 \t 3.6309488543491604\n",
            "30     \t [0.0344202  0.46647514 0.00382817]. \t  0.02632873684750242 \t 3.6309488543491604\n",
            "31     \t [0.98375164 0.2641491  0.71365308]. \t  1.266231901195165 \t 3.6309488543491604\n",
            "32     \t [0.17960826 0.92275689 0.22534381]. \t  0.11264362345331402 \t 3.6309488543491604\n",
            "33     \t [0.73560847 0.97068809 0.99633509]. \t  0.4285888813432009 \t 3.6309488543491604\n",
            "34     \t [0.03570225 0.02547101 0.66599813]. \t  0.2068472304888523 \t 3.6309488543491604\n",
            "35     \t [0.92166738 0.03765294 0.01455281]. \t  0.055241103567133146 \t 3.6309488543491604\n",
            "36     \t [0.91051901 0.69517691 0.00841486]. \t  0.002003572332844767 \t 3.6309488543491604\n",
            "37     \t [0.19079759 0.31517693 0.75994811]. \t  2.0141934310880183 \t 3.6309488543491604\n",
            "38     \t [1.         0.79391408 0.90771552]. \t  2.00039524230632 \t 3.6309488543491604\n",
            "39     \t [0.40904628 0.47036777 0.98169653]. \t  2.2142061002493882 \t 3.6309488543491604\n",
            "40     \t [0.18693465 0.61638322 0.69766788]. \t  2.5663349592956637 \t 3.6309488543491604\n",
            "41     \t [0.63664881 0.54040182 0.25612258]. \t  0.16367431334025673 \t 3.6309488543491604\n",
            "42     \t [1.         0.60819636 0.82149336]. \t  3.4063671261908053 \t 3.6309488543491604\n",
            "43     \t [0.49714381 0.62382606 0.81382168]. \t  3.505217710950571 \t 3.6309488543491604\n",
            "44     \t [0.40926743 0.96439656 0.99551949]. \t  0.4627141401708138 \t 3.6309488543491604\n",
            "45     \t [0.49781244 0.69369385 0.92364071]. \t  2.8473409854332705 \t 3.6309488543491604\n",
            "46     \t [0.83103544 0.51873903 0.8638766 ]. \t  \u001b[92m3.701553632509258\u001b[0m \t 3.701553632509258\n",
            "47     \t [0.57850634 0.98023404 0.7822688 ]. \t  0.7839495122442697 \t 3.701553632509258\n",
            "48     \t [0.25733757 0.58571116 0.00991058]. \t  0.014840371706917755 \t 3.701553632509258\n",
            "49     \t [0.00445695 0.35262249 0.9032259 ]. \t  2.371251851312346 \t 3.701553632509258\n",
            "50     \t [1.         0.59285531 0.78232236]. \t  3.0629149080794438 \t 3.701553632509258\n",
            "51     \t [0.00948813 0.9712942  0.23165897]. \t  0.1159995089818237 \t 3.701553632509258\n",
            "52     \t [0.60468849 0.42265256 0.89881956]. \t  3.0459145710786655 \t 3.701553632509258\n",
            "53     \t [0.74440084 0.04192803 0.87212283]. \t  0.32753599661096083 \t 3.701553632509258\n",
            "54     \t [0.90780221 0.44226067 0.88151531]. \t  3.2419331309892554 \t 3.701553632509258\n",
            "55     \t [1.         0.47428937 0.87144828]. \t  3.446804850076829 \t 3.701553632509258\n",
            "56     \t [0.00351238 0.59999642 0.83025028]. \t  3.696460714094023 \t 3.701553632509258\n",
            "57     \t [0.54250533 0.56087358 0.86506576]. \t  \u001b[92m3.821051786704831\u001b[0m \t 3.821051786704831\n",
            "58     \t [0.43768156 0.56225835 0.8405229 ]. \t  \u001b[92m3.8301609252842317\u001b[0m \t 3.8301609252842317\n",
            "59     \t [0.73016979 0.53664367 0.86839763]. \t  3.756532117271755 \t 3.8301609252842317\n",
            "60     \t [0.83602269 0.51563571 0.8673579 ]. \t  3.684480371351953 \t 3.8301609252842317\n",
            "61     \t [0.81100378 0.52253719 0.87098609]. \t  3.698930141328803 \t 3.8301609252842317\n",
            "62     \t [0.59975742 0.51851392 0.87740789]. \t  3.722054727553758 \t 3.8301609252842317\n",
            "63     \t [0.80568374 0.54247429 0.8469672 ]. \t  3.7434385610518968 \t 3.8301609252842317\n",
            "64     \t [0.75351371 0.50650237 0.8743581 ]. \t  3.661532020356973 \t 3.8301609252842317\n",
            "65     \t [0.64875691 0.52510965 0.87653196]. \t  3.730656526132589 \t 3.8301609252842317\n",
            "66     \t [0.71737027 0.53135791 0.870634  ]. \t  3.7465767628366553 \t 3.8301609252842317\n",
            "67     \t [0.66829316 0.53149171 0.84642205]. \t  3.7794070779280755 \t 3.8301609252842317\n",
            "68     \t [0.02171548 0.621316   0.78215796]. \t  3.303843737623709 \t 3.8301609252842317\n",
            "69     \t [0.35724103 0.54371728 0.84378915]. \t  \u001b[92m3.8475227381750674\u001b[0m \t 3.8475227381750674\n",
            "70     \t [0.7107777  0.46696616 0.88483553]. \t  3.43662872807062 \t 3.8475227381750674\n",
            "71     \t [0.6040031  0.52985733 0.86978696]. \t  3.7762677424272058 \t 3.8475227381750674\n",
            "72     \t [0.67581356 0.614054   0.82845125]. \t  3.5741855126218294 \t 3.8475227381750674\n",
            "73     \t [0.68769134 0.53922519 0.86937495]. \t  3.769208504454233 \t 3.8475227381750674\n",
            "74     \t [0.49259409 0.59671566 0.83372501]. \t  3.7283175573203713 \t 3.8475227381750674\n",
            "75     \t [0.49174875 0.54798702 0.85589277]. \t  3.841930330902385 \t 3.8475227381750674\n",
            "76     \t [0.37403981 0.55624756 0.82850852]. \t  3.7983609113903163 \t 3.8475227381750674\n",
            "77     \t [0.         0.00408596 0.91067674]. \t  0.188381546592008 \t 3.8475227381750674\n",
            "78     \t [0.70884709 0.52484845 0.85023785]. \t  3.76172033546427 \t 3.8475227381750674\n",
            "79     \t [0.57709108 0.56381359 0.8375736 ]. \t  3.7875110109897756 \t 3.8475227381750674\n",
            "80     \t [0.56057448 0.58361861 0.84224976]. \t  3.775070549769986 \t 3.8475227381750674\n",
            "81     \t [0.67301844 0.55146542 0.8302248 ]. \t  3.7331846004435394 \t 3.8475227381750674\n",
            "82     \t [0.51612351 0.5257331  0.86975947]. \t  3.785301677941842 \t 3.8475227381750674\n",
            "83     \t [0.62329405 0.54209334 0.83056858]. \t  3.7504828253316966 \t 3.8475227381750674\n",
            "84     \t [0.72757756 0.56533097 0.84228587]. \t  3.7527503138033835 \t 3.8475227381750674\n",
            "85     \t [0.75066658 0.55180077 0.85611975]. \t  3.7742768530048787 \t 3.8475227381750674\n",
            "86     \t [0.32629945 0.57331395 0.83213308]. \t  3.8045993332353225 \t 3.8475227381750674\n",
            "87     \t [0.47220288 0.55701233 0.85144197]. \t  3.8450784768901642 \t 3.8475227381750674\n",
            "88     \t [0.53729746 0.57156097 0.8380264 ]. \t  3.7906414534353607 \t 3.8475227381750674\n",
            "89     \t [0.36483765 0.58846546 0.80832966]. \t  3.628891470800893 \t 3.8475227381750674\n",
            "90     \t [0.34869005 0.63930502 0.80576007]. \t  3.4225543688498283 \t 3.8475227381750674\n",
            "91     \t [0.50253557 0.50700962 0.86631574]. \t  3.747491019868731 \t 3.8475227381750674\n",
            "92     \t [0.62204354 0.56575081 0.84809643]. \t  3.800328606259198 \t 3.8475227381750674\n",
            "93     \t [0.52550648 0.54844766 0.87174479]. \t  3.805846299042614 \t 3.8475227381750674\n",
            "94     \t [0.43986083 0.53476223 0.85568208]. \t  3.838275847862579 \t 3.8475227381750674\n",
            "95     \t [0.67757702 0.53393626 0.86501751]. \t  3.7765982799344466 \t 3.8475227381750674\n",
            "96     \t [0.54138866 0.61731153 0.82513233]. \t  3.592262684059002 \t 3.8475227381750674\n",
            "97     \t [0.54702958 0.57571179 0.84581631]. \t  3.803025681477789 \t 3.8475227381750674\n",
            "98     \t [0.64812398 0.61584449 0.82363423]. \t  3.5482993354785455 \t 3.8475227381750674\n",
            "99     \t [0.8896368  0.50664936 0.82397301]. \t  3.5682762386498714 \t 3.8475227381750674\n",
            "100    \t [0.62601012 0.53489693 0.8552705 ]. \t  3.8023853663769396 \t 3.8475227381750674\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mrvkDXP0uvWU",
        "outputId": "2ba16fc3-9abf-48ae-b380-6c0d39d0da15"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 5 \r\n",
        "\r\n",
        "np.random.seed(run_num_5)\r\n",
        "surrogate_winner_5 = dtStudentProcess(d_cov_func, nu = df)\r\n",
        "\r\n",
        "winner_5 = dGPGO_stp(surrogate_winner_5, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_5.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.55735327 0.54176063 0.56042378]. \t  0.9599610140567721 \t 0.9810564697651996\n",
            "init   \t [0.38204809 0.11909418 0.84599912]. \t  0.6913364116577386 \t 0.9810564697651996\n",
            "init   \t [0.73974161 0.93235756 0.85932801]. \t  0.9810564697651996 \t 0.9810564697651996\n",
            "init   \t [0.70810463 0.68096875 0.4086023 ]. \t  0.4017443293787304 \t 0.9810564697651996\n",
            "init   \t [0.77458462 0.9844762  0.96548486]. \t  0.4769494155906823 \t 0.9810564697651996\n",
            "1      \t [0.07134165 0.9939609  0.10403246]. \t  0.0059887472761662895 \t 0.9810564697651996\n",
            "2      \t [0.11239019 0.98624253 0.96690795]. \t  0.4814827550321023 \t 0.9810564697651996\n",
            "3      \t [0.07745819 0.0603163  0.06574566]. \t  0.22187171495503513 \t 0.9810564697651996\n",
            "4      \t [0.89457872 0.03755953 0.99520822]. \t  0.13961802934493878 \t 0.9810564697651996\n",
            "5      \t [0.9479872  0.06571009 0.2313906 ]. \t  0.34270972626851454 \t 0.9810564697651996\n",
            "6      \t [0.00198933 0.4672197  0.7091874 ]. \t  \u001b[92m2.360146381618021\u001b[0m \t 2.360146381618021\n",
            "7      \t [0.99794715 0.66907171 0.81399976]. \t  \u001b[92m2.9971657603610686\u001b[0m \t 2.9971657603610686\n",
            "8      \t [0.30370554 0.5420168  0.98229967]. \t  2.396700843210232 \t 2.9971657603610686\n",
            "9      \t [1.         1.         0.70240993]. \t  0.3303429464838314 \t 2.9971657603610686\n",
            "10     \t [0.97522562 0.65940892 0.02021827]. \t  0.0028426920862521625 \t 2.9971657603610686\n",
            "11     \t [1.         0.58490282 1.        ]. \t  2.002684768029195 \t 2.9971657603610686\n",
            "12     \t [0.07631256 0.50026653 0.03546057]. \t  0.03572796001323271 \t 2.9971657603610686\n",
            "13     \t [0.00606278 0.01286773 0.47787347]. \t  0.1769367137903308 \t 2.9971657603610686\n",
            "14     \t [0.06927026 0.24993547 0.9685129 ]. \t  1.004531629071045 \t 2.9971657603610686\n",
            "15     \t [0.00679996 0.896335   0.40330367]. \t  1.4574502679336796 \t 2.9971657603610686\n",
            "16     \t [0.96810876 0.46382381 0.49782987]. \t  0.2241629663591477 \t 2.9971657603610686\n",
            "17     \t [0.6958187  0.31151117 0.04928452]. \t  0.11946683270259309 \t 2.9971657603610686\n",
            "18     \t [0.57768495 0.00508031 0.63452166]. \t  0.14609475714756284 \t 2.9971657603610686\n",
            "19     \t [0.99465672 0.13736072 0.03392424]. \t  0.0600377896520842 \t 2.9971657603610686\n",
            "20     \t [0.38478479 0.76762911 0.12844494]. \t  0.01730083356633278 \t 2.9971657603610686\n",
            "21     \t [0.02734636 0.71505344 0.8450068 ]. \t  \u001b[92m3.0402945778637527\u001b[0m \t 3.0402945778637527\n",
            "22     \t [0.82081709 0.73462637 0.99719437]. \t  1.5920681215686494 \t 3.0402945778637527\n",
            "23     \t [0.7386477  0.9933911  0.04125856]. \t  0.00035676874950450613 \t 3.0402945778637527\n",
            "24     \t [1. 1. 1.]. \t  0.31688362070415665 \t 3.0402945778637527\n",
            "25     \t [0.50697902 0.11870248 0.01530655]. \t  0.14054322918148698 \t 3.0402945778637527\n",
            "26     \t [0.34556287 0.00286701 0.95799062]. \t  0.1379963025980146 \t 3.0402945778637527\n",
            "27     \t [0.22151571 0.31161992 0.39291693]. \t  0.4687886454017968 \t 3.0402945778637527\n",
            "28     \t [0.94618147 0.0836968  0.74161946]. \t  0.47199525810083515 \t 3.0402945778637527\n",
            "29     \t [0.21184824 0.60968849 0.85556657]. \t  \u001b[92m3.7554870491292887\u001b[0m \t 3.7554870491292887\n",
            "30     \t [0.35854505 0.85749082 0.90627049]. \t  1.5538270663633966 \t 3.7554870491292887\n",
            "31     \t [0.95977387 0.0093455  0.04075006]. \t  0.06700179984117596 \t 3.7554870491292887\n",
            "32     \t [0.99013577 0.99708475 0.11202119]. \t  0.0007673607871951586 \t 3.7554870491292887\n",
            "33     \t [0.00955729 0.76170703 0.1491596 ]. \t  0.025508948654623418 \t 3.7554870491292887\n",
            "34     \t [0.03094172 0.87523856 0.67796475]. \t  2.309612792604284 \t 3.7554870491292887\n",
            "35     \t [0.18311435 0.53058885 0.78795495]. \t  3.492386237105402 \t 3.7554870491292887\n",
            "36     \t [0.12810846 0.02482348 0.7471355 ]. \t  0.2969398136858685 \t 3.7554870491292887\n",
            "37     \t [0.31012337 0.97240421 0.42599398]. \t  1.467858017677049 \t 3.7554870491292887\n",
            "38     \t [0.17050083 0.54440598 0.82419517]. \t  \u001b[92m3.7771101381321923\u001b[0m \t 3.7771101381321923\n",
            "39     \t [0.07135131 0.5521731  0.90222905]. \t  3.5878218358937572 \t 3.7771101381321923\n",
            "40     \t [0.1449255  0.76389107 0.64546764]. \t  2.668192544851366 \t 3.7771101381321923\n",
            "41     \t [1.         0.78452759 1.        ]. \t  1.2622193117196352 \t 3.7771101381321923\n",
            "42     \t [0.844298   0.39411385 0.91698503]. \t  2.5961149986135275 \t 3.7771101381321923\n",
            "43     \t [0.04090394 0.52620533 0.91142692]. \t  3.4466030436604824 \t 3.7771101381321923\n",
            "44     \t [0.2433432  0.00032535 0.26876214]. \t  0.8325409883376914 \t 3.7771101381321923\n",
            "45     \t [0.86486816 0.62252367 0.81426395]. \t  3.3485665477274535 \t 3.7771101381321923\n",
            "46     \t [0.22561874 0.57129025 0.79607107]. \t  3.5744867672873033 \t 3.7771101381321923\n",
            "47     \t [0.20841005 0.59172609 0.82556347]. \t  3.7420087509682736 \t 3.7771101381321923\n",
            "48     \t [0.25627821 0.57648939 0.794913  ]. \t  3.556210236274466 \t 3.7771101381321923\n",
            "49     \t [0.09632851 0.58412765 0.85936482]. \t  \u001b[92m3.8067041319356\u001b[0m \t 3.8067041319356\n",
            "50     \t [0.24258084 0.5663828  0.81838586]. \t  3.746766656763554 \t 3.8067041319356\n",
            "51     \t [0.20787197 0.61004825 0.74845788]. \t  3.0360366726171604 \t 3.8067041319356\n",
            "52     \t [0.11968447 0.60381124 0.81206561]. \t  3.6217582766739453 \t 3.8067041319356\n",
            "53     \t [0.9264433  0.54218401 0.84224577]. \t  3.6823212720831604 \t 3.8067041319356\n",
            "54     \t [0.78411996 0.55741271 0.83929774]. \t  3.7290364567605825 \t 3.8067041319356\n",
            "55     \t [0.12657943 0.67130052 0.901974  ]. \t  3.2082307557298373 \t 3.8067041319356\n",
            "56     \t [0.63620365 0.53437853 0.87737403]. \t  3.746299273706839 \t 3.8067041319356\n",
            "57     \t [0.19454397 0.53528026 0.8474065 ]. \t  \u001b[92m3.839991415924233\u001b[0m \t 3.839991415924233\n",
            "58     \t [0.47623898 0.55670178 0.85475801]. \t  \u001b[92m3.8454090641210503\u001b[0m \t 3.8454090641210503\n",
            "59     \t [0.75195433 0.57181346 0.84799904]. \t  3.7493772669148355 \t 3.8454090641210503\n",
            "60     \t [0.19420819 0.55732495 0.81099204]. \t  3.7019458755643804 \t 3.8454090641210503\n",
            "61     \t [0.73688447 0.54094846 0.87125533]. \t  3.750491650260928 \t 3.8454090641210503\n",
            "62     \t [0.10088532 0.99872864 0.72341623]. \t  1.3566130712432254 \t 3.8454090641210503\n",
            "63     \t [0.74694254 0.55447644 0.86284751]. \t  3.7711833239335624 \t 3.8454090641210503\n",
            "64     \t [0.68194877 0.52993171 0.86679873]. \t  3.765330339013696 \t 3.8454090641210503\n",
            "65     \t [0.4020162  0.56809271 0.83663952]. \t  3.818808368560962 \t 3.8454090641210503\n",
            "66     \t [0.66655938 0.56624842 0.85422732]. \t  3.7931328612171007 \t 3.8454090641210503\n",
            "67     \t [0.03888814 0.56174112 0.86850402]. \t  3.7975679108906952 \t 3.8454090641210503\n",
            "68     \t [0.81264504 0.5495295  0.86151521]. \t  3.749589860743982 \t 3.8454090641210503\n",
            "69     \t [0.56078845 0.5147957  0.87510636]. \t  3.730153972238868 \t 3.8454090641210503\n",
            "70     \t [0.33098987 0.49938816 0.82968088]. \t  3.715149171612586 \t 3.8454090641210503\n",
            "71     \t [0.         0.41688982 0.18235786]. \t  0.22348015220485404 \t 3.8454090641210503\n",
            "72     \t [0.53202267 0.48917237 0.8735447 ]. \t  3.6472901894429626 \t 3.8454090641210503\n",
            "73     \t [0.6439138  0.5257653  0.83685086]. \t  3.7538387283683234 \t 3.8454090641210503\n",
            "74     \t [0.31050325 0.55399512 0.85039946]. \t  \u001b[92m3.860766251888525\u001b[0m \t 3.860766251888525\n",
            "75     \t [0.59809423 0.56141009 0.85566041]. \t  3.817042626599405 \t 3.860766251888525\n",
            "76     \t [0.4875754  0.56161067 0.85394736]. \t  3.841309431557688 \t 3.860766251888525\n",
            "77     \t [0.62232482 0.51699487 0.80869304]. \t  3.5811508635494267 \t 3.860766251888525\n",
            "78     \t [0.28709605 0.53669912 0.821877  ]. \t  3.764468979471485 \t 3.860766251888525\n",
            "79     \t [0.47384999 0.50340392 0.86439383]. \t  3.7439620885262825 \t 3.860766251888525\n",
            "80     \t [0.64304161 0.54280029 0.86700132]. \t  3.7908347708399845 \t 3.860766251888525\n",
            "81     \t [0.69627423 0.56544349 0.8609535 ]. \t  3.783602108231304 \t 3.860766251888525\n",
            "82     \t [0.31244468 0.58520426 0.83601424]. \t  3.79862594091444 \t 3.860766251888525\n",
            "83     \t [0.35376306 0.52447228 0.86013607]. \t  3.822279141138181 \t 3.860766251888525\n",
            "84     \t [0.43644121 0.51743734 0.85996854]. \t  3.8001948356499464 \t 3.860766251888525\n",
            "85     \t [0.02426015 0.61858178 0.87477697]. \t  3.6429125038397583 \t 3.860766251888525\n",
            "86     \t [0.59973923 0.55115096 0.86412857]. \t  3.8116313291276906 \t 3.860766251888525\n",
            "87     \t [0.48497046 0.58251031 0.83938163]. \t  3.788405879565137 \t 3.860766251888525\n",
            "88     \t [0.2577863  0.47916484 0.87194726]. \t  3.616156167330601 \t 3.860766251888525\n",
            "89     \t [0.01176232 0.64341475 0.80268027]. \t  3.369006078768125 \t 3.860766251888525\n",
            "90     \t [0.45643057 0.52092595 0.86782469]. \t  3.7887728754887733 \t 3.860766251888525\n",
            "91     \t [0.7322166  0.51426526 0.87290417]. \t  3.6971328625156596 \t 3.860766251888525\n",
            "92     \t [0.23710595 0.4645423  0.87195568]. \t  3.531999377740975 \t 3.860766251888525\n",
            "93     \t [0.73953302 0.57670688 0.83122639]. \t  3.684476267620366 \t 3.860766251888525\n",
            "94     \t [0.51105352 0.49256185 0.82946637]. \t  3.668665251427584 \t 3.860766251888525\n",
            "95     \t [0.71229072 0.50727985 0.87428878]. \t  3.6762556226386147 \t 3.860766251888525\n",
            "96     \t [0.64223613 0.56600161 0.85523757]. \t  3.801042707669963 \t 3.860766251888525\n",
            "97     \t [0.48399174 0.49963179 0.87686326]. \t  3.6819723648111458 \t 3.860766251888525\n",
            "98     \t [0.64660778 0.57782244 0.85187134]. \t  3.780561644523847 \t 3.860766251888525\n",
            "99     \t [0.43471201 0.53618752 0.8606982 ]. \t  3.835736881111432 \t 3.860766251888525\n",
            "100    \t [0.60735979 0.57922128 0.83455977]. \t  3.7429202831575448 \t 3.860766251888525\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cO3I_9cbuvY2",
        "outputId": "aa14b123-f173-4a33-f5b2-bd46b34ef2b9"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 6 \r\n",
        "\r\n",
        "np.random.seed(run_num_6)\r\n",
        "surrogate_winner_6 = dtStudentProcess(d_cov_func, nu = df)\r\n",
        "\r\n",
        "winner_6 = dGPGO_stp(surrogate_winner_6, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_6.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.89286015 0.33197981 0.82122912]. \t  2.3879127825536575 \t 2.5106636917702634\n",
            "init   \t [0.04169663 0.10765668 0.59505206]. \t  0.2321934630074273 \t 2.5106636917702634\n",
            "init   \t [0.52981736 0.41880743 0.33540785]. \t  0.37989557867087576 \t 2.5106636917702634\n",
            "init   \t [0.62251943 0.43814143 0.73588211]. \t  2.5106636917702634 \t 2.5106636917702634\n",
            "init   \t [0.51803641 0.5788586  0.6453551 ]. \t  1.7289203948034264 \t 2.5106636917702634\n",
            "1      \t [1. 1. 1.]. \t  0.31688362070415665 \t 2.5106636917702634\n",
            "2      \t [0.11083969 0.91322847 0.90307713]. \t  1.123323445246224 \t 2.5106636917702634\n",
            "3      \t [0.01697197 0.14168357 0.98707342]. \t  0.3969072864759877 \t 2.5106636917702634\n",
            "4      \t [0.09379159 0.98855725 0.0273139 ]. \t  0.0006952036589698887 \t 2.5106636917702634\n",
            "5      \t [0.74319789 0.0628957  0.97628683]. \t  0.21620990668345164 \t 2.5106636917702634\n",
            "6      \t [0.97215077 0.8958194  0.09299307]. \t  0.0008465811043153739 \t 2.5106636917702634\n",
            "7      \t [0.98681603 0.0835005  0.02567241]. \t  0.054574035106798306 \t 2.5106636917702634\n",
            "8      \t [0.00676464 0.36240414 0.07004012]. \t  0.11516576142652592 \t 2.5106636917702634\n",
            "9      \t [0.72375268 0.63203218 0.9925582 ]. \t  2.1077219299935255 \t 2.5106636917702634\n",
            "10     \t [0.01988521 0.01189723 0.06101082]. \t  0.17333141682944478 \t 2.5106636917702634\n",
            "11     \t [0.97722762 0.03223192 0.52737838]. \t  0.0839599072440842 \t 2.5106636917702634\n",
            "12     \t [0.99906773 0.7141954  0.57253678]. \t  0.4903471047831218 \t 2.5106636917702634\n",
            "13     \t [0.07305831 0.45631126 0.82566278]. \t  \u001b[92m3.466461427676241\u001b[0m \t 3.466461427676241\n",
            "14     \t [0.00817638 0.97136293 0.47940026]. \t  2.2370161255253267 \t 3.466461427676241\n",
            "15     \t [0.53497861 0.01629817 0.02246142]. \t  0.13772020547689695 \t 3.466461427676241\n",
            "16     \t [0.3157091  0.40187096 0.94498277]. \t  2.377733089641431 \t 3.466461427676241\n",
            "17     \t [0.00181174 0.71057357 0.80412084]. \t  2.9536435463175263 \t 3.466461427676241\n",
            "18     \t [0.72750326 0.99925983 0.42865694]. \t  0.5065014346092718 \t 3.466461427676241\n",
            "19     \t [0.63593854 0.98031684 0.99793017]. \t  0.39475235442008466 \t 3.466461427676241\n",
            "20     \t [0.70498935 0.39205752 0.00647504]. \t  0.04345458769299887 \t 3.466461427676241\n",
            "21     \t [0.02794024 0.34365678 0.98701654]. \t  1.4132611698550561 \t 3.466461427676241\n",
            "22     \t [0.2257051  0.15030674 0.01755453]. \t  0.14316279130893814 \t 3.466461427676241\n",
            "23     \t [0.98675967 0.29509735 0.36040645]. \t  0.18719436139362414 \t 3.466461427676241\n",
            "24     \t [0.48728646 0.84391268 0.0785993 ]. \t  0.003827740063175662 \t 3.466461427676241\n",
            "25     \t [0.0234094  0.44221226 0.51545609]. \t  0.679271359480746 \t 3.466461427676241\n",
            "26     \t [0.4459969  0.05026986 0.73829296]. \t  0.3656385784512614 \t 3.466461427676241\n",
            "27     \t [0.75895959 0.97278289 0.02992014]. \t  0.000274677758001912 \t 3.466461427676241\n",
            "28     \t [0.93827082 0.46726479 0.95900372]. \t  2.4996096103562477 \t 3.466461427676241\n",
            "29     \t [0.22071066 0.88832268 0.75701605]. \t  1.6862613896276513 \t 3.466461427676241\n",
            "30     \t [0.96560213 0.10521037 0.97269916]. \t  0.32286671042984516 \t 3.466461427676241\n",
            "31     \t [0.01451426 0.39265687 0.82833811]. \t  2.989735591331901 \t 3.466461427676241\n",
            "32     \t [0.99739753 0.98528164 0.5819246 ]. \t  0.28918491231238735 \t 3.466461427676241\n",
            "33     \t [0.8072043  0.95835663 0.83574739]. \t  0.7862825676364825 \t 3.466461427676241\n",
            "34     \t [0.11691971 0.37104105 0.8392368 ]. \t  2.829028745284669 \t 3.466461427676241\n",
            "35     \t [0.7676108  0.00234254 0.30024111]. \t  0.5270277182435539 \t 3.466461427676241\n",
            "36     \t [0.06265631 0.70756724 0.06128802]. \t  0.007983889150649803 \t 3.466461427676241\n",
            "37     \t [0.25514473 0.60111262 0.83835749]. \t  \u001b[92m3.7653691785087933\u001b[0m \t 3.7653691785087933\n",
            "38     \t [0.35400735 0.99559786 0.19781469]. \t  0.04764253238052792 \t 3.7653691785087933\n",
            "39     \t [0.00843811 0.938003   0.23038274]. \t  0.11970596966725204 \t 3.7653691785087933\n",
            "40     \t [0.31784374 0.65759754 1.        ]. \t  1.9284694447133572 \t 3.7653691785087933\n",
            "41     \t [0.30318792 0.51600281 0.00396187]. \t  0.025170831891173546 \t 3.7653691785087933\n",
            "42     \t [0.99035812 0.34332604 0.02029418]. \t  0.0301638741090389 \t 3.7653691785087933\n",
            "43     \t [0.20788514 0.51789755 0.72637511]. \t  2.7561643729902716 \t 3.7653691785087933\n",
            "44     \t [1.         0.76734261 0.88750063]. \t  2.32746243507702 \t 3.7653691785087933\n",
            "45     \t [0.27563715 0.03290546 0.21407547]. \t  0.8338535095141165 \t 3.7653691785087933\n",
            "46     \t [0.98783829 0.58528227 0.84942989]. \t  3.625100033818599 \t 3.7653691785087933\n",
            "47     \t [0.5430287  0.62248852 0.87089541]. \t  3.6481824403125525 \t 3.7653691785087933\n",
            "48     \t [0.37493684 0.50954483 0.87325884]. \t  3.7422709930098095 \t 3.7653691785087933\n",
            "49     \t [0.53449003 0.68016982 0.82124535]. \t  3.1940970758837546 \t 3.7653691785087933\n",
            "50     \t [0.17360701 0.44526989 0.83956758]. \t  3.449311794524764 \t 3.7653691785087933\n",
            "51     \t [0.99150598 0.49730625 0.82223183]. \t  3.489225544982256 \t 3.7653691785087933\n",
            "52     \t [0.37608318 0.63929914 0.83737164]. \t  3.5837982025938127 \t 3.7653691785087933\n",
            "53     \t [0.48953935 0.53194079 0.88897361]. \t  3.6997188689722265 \t 3.7653691785087933\n",
            "54     \t [0.50906521 0.54371457 0.85145602]. \t  \u001b[92m3.8361343720778263\u001b[0m \t 3.8361343720778263\n",
            "55     \t [0.93712803 0.0273238  0.87319603]. \t  0.27903777848697564 \t 3.8361343720778263\n",
            "56     \t [0.99889097 0.51687934 0.89725524]. \t  3.45892899631979 \t 3.8361343720778263\n",
            "57     \t [0.00451024 0.97251683 0.64435071]. \t  2.202195752821407 \t 3.8361343720778263\n",
            "58     \t [0.38140732 0.54588611 0.86016207]. \t  \u001b[92m3.850715135481134\u001b[0m \t 3.850715135481134\n",
            "59     \t [0.45281498 0.53674927 0.80896081]. \t  3.656231781063186 \t 3.850715135481134\n",
            "60     \t [0.57497039 0.5709261  0.8141426 ]. \t  3.6456032039363855 \t 3.850715135481134\n",
            "61     \t [0.36269696 0.51876539 0.86689145]. \t  3.7929468599603426 \t 3.850715135481134\n",
            "62     \t [0.62868777 0.46406777 0.88551406]. \t  3.432844809664248 \t 3.850715135481134\n",
            "63     \t [0.43988143 0.51582299 0.82389881]. \t  3.725626193476958 \t 3.850715135481134\n",
            "64     \t [0.47815335 0.56248104 0.86099301]. \t  3.838638647885199 \t 3.850715135481134\n",
            "65     \t [0.20845487 0.00664935 0.99985623]. \t  0.09840279932518448 \t 3.850715135481134\n",
            "66     \t [0.38764925 0.5191934  0.8708879 ]. \t  3.7795464400148155 \t 3.850715135481134\n",
            "67     \t [0.36916302 0.59597211 0.84166774]. \t  3.7819096150095106 \t 3.850715135481134\n",
            "68     \t [0.44342653 0.54250651 0.86826597]. \t  3.824798108190726 \t 3.850715135481134\n",
            "69     \t [0.59914483 0.51391665 0.84060939]. \t  3.7529568879025343 \t 3.850715135481134\n",
            "70     \t [0.49578238 0.57201072 0.81273172]. \t  3.65959481056413 \t 3.850715135481134\n",
            "71     \t [0.46519544 0.57512996 0.85320522]. \t  3.830664463666026 \t 3.850715135481134\n",
            "72     \t [0.49415368 0.58578922 0.80371589]. \t  3.55881053794153 \t 3.850715135481134\n",
            "73     \t [0.25443429 0.58064606 0.83913118]. \t  3.818549362588666 \t 3.850715135481134\n",
            "74     \t [0.3006343  0.60394168 0.83455085]. \t  3.7424832005987465 \t 3.850715135481134\n",
            "75     \t [0.99981983 0.50882172 0.87080496]. \t  3.5889768062599066 \t 3.850715135481134\n",
            "76     \t [0.38962076 0.53039704 0.86821853]. \t  3.813176544683107 \t 3.850715135481134\n",
            "77     \t [0.49834375 0.61746693 0.83949436]. \t  3.670344763919486 \t 3.850715135481134\n",
            "78     \t [0.47688614 0.55864984 0.83915922]. \t  3.8211466981007547 \t 3.850715135481134\n",
            "79     \t [0.53855185 0.54586035 0.8555798 ]. \t  3.8321668169305663 \t 3.850715135481134\n",
            "80     \t [0.21222165 0.5727372  0.84893967]. \t  3.8453905298502664 \t 3.850715135481134\n",
            "81     \t [0.39317133 0.58401147 0.85621298]. \t  3.8251663747491698 \t 3.850715135481134\n",
            "82     \t [1.         0.47507149 0.92012466]. \t  3.062453236289557 \t 3.850715135481134\n",
            "83     \t [0.5807738  0.49683189 0.86249187]. \t  3.7068388104102334 \t 3.850715135481134\n",
            "84     \t [0.         0.         0.84313098]. \t  0.23439608812035584 \t 3.850715135481134\n",
            "85     \t [0.50304764 0.49122807 0.86755584]. \t  3.6830809888631135 \t 3.850715135481134\n",
            "86     \t [0.42460855 0.55325208 0.85316383]. \t  \u001b[92m3.85329032101475\u001b[0m \t 3.85329032101475\n",
            "87     \t [0.42086236 0.52537459 0.86076663]. \t  3.818903305420399 \t 3.85329032101475\n",
            "88     \t [0.3605544  0.5170587  0.86719087]. \t  3.787573545378501 \t 3.85329032101475\n",
            "89     \t [0.34906637 0.58209158 0.83505178]. \t  3.7985296042010983 \t 3.85329032101475\n",
            "90     \t [0.56593787 0.50877768 0.87291903]. \t  3.720319880209328 \t 3.85329032101475\n",
            "91     \t [0.54044453 0.55087753 0.83297681]. \t  3.7862674106087018 \t 3.85329032101475\n",
            "92     \t [0.3303049  0.62556129 0.84349235]. \t  3.676862555298176 \t 3.85329032101475\n",
            "93     \t [0.48921859 0.56871413 0.83988339]. \t  3.812093885124132 \t 3.85329032101475\n",
            "94     \t [0.50592943 0.57834977 0.85644786]. \t  3.817995737223474 \t 3.85329032101475\n",
            "95     \t [0.00884152 0.60135747 0.83189377]. \t  3.700633616823379 \t 3.85329032101475\n",
            "96     \t [0.57582777 0.54045813 0.8605159 ]. \t  3.8168855455898774 \t 3.85329032101475\n",
            "97     \t [0.42384121 0.49852597 0.87048541]. \t  3.7112171394313718 \t 3.85329032101475\n",
            "98     \t [0.39876093 0.57985251 0.8554587 ]. \t  3.8327694166823756 \t 3.85329032101475\n",
            "99     \t [0.41792529 0.54685201 0.85313525]. \t  3.8524455761901084 \t 3.85329032101475\n",
            "100    \t [0.37777632 0.51863135 0.85017549]. \t  3.813901486043156 \t 3.85329032101475\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8rQbLZD8uvbI",
        "outputId": "c17fe382-ac79-40eb-ed50-fc6a4b25afa6"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 7 \r\n",
        "\r\n",
        "np.random.seed(run_num_7)\r\n",
        "surrogate_winner_7 = dtStudentProcess(d_cov_func, nu = df)\r\n",
        "\r\n",
        "winner_7 = dGPGO_stp(surrogate_winner_7, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_7.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.15266373 0.30235661 0.06203641]. \t  0.1742288077468587 \t 1.6237282255098657\n",
            "init   \t [0.45986034 0.83525338 0.92699705]. \t  1.6237282255098657 \t 1.6237282255098657\n",
            "init   \t [0.72698898 0.76849622 0.26920507]. \t  0.08405715787489784 \t 1.6237282255098657\n",
            "init   \t [0.64402929 0.09337326 0.07968589]. \t  0.2756481743251193 \t 1.6237282255098657\n",
            "init   \t [0.58961375 0.34334054 0.98887615]. \t  1.4020548914621052 \t 1.6237282255098657\n",
            "1      \t [0.0095305  0.06847696 0.97789675]. \t  0.22422993094430546 \t 1.6237282255098657\n",
            "2      \t [1. 1. 1.]. \t  0.31688362070415665 \t 1.6237282255098657\n",
            "3      \t [0.08306623 0.99668859 0.81981745]. \t  0.8193325731144557 \t 1.6237282255098657\n",
            "4      \t [0.4572754  0.07470741 0.65360152]. \t  0.29053610296817073 \t 1.6237282255098657\n",
            "5      \t [0.08456997 0.97839894 0.015549  ]. \t  0.0005077397962715806 \t 1.6237282255098657\n",
            "6      \t [0.16238714 0.62716538 0.93687526]. \t  \u001b[92m3.0519370698729644\u001b[0m \t 3.0519370698729644\n",
            "7      \t [0.99247569 0.07531813 0.7594942 ]. \t  0.4605729057121457 \t 3.0519370698729644\n",
            "8      \t [0.99064128 0.92226591 0.02931425]. \t  0.00015963016353151625 \t 3.0519370698729644\n",
            "9      \t [0.00659273 0.6433503  0.44524754]. \t  1.2484292564247035 \t 3.0519370698729644\n",
            "10     \t [0.95731884 0.58825239 0.62627064]. \t  1.0040471572486132 \t 3.0519370698729644\n",
            "11     \t [0.03795813 0.72045593 0.98192351]. \t  1.8979577221551773 \t 3.0519370698729644\n",
            "12     \t [0.98382715 0.27993618 0.29751896]. \t  0.24196930797294067 \t 3.0519370698729644\n",
            "13     \t [0.08226407 0.01771363 0.22733878]. \t  0.6751477131588729 \t 3.0519370698729644\n",
            "14     \t [0.96075948 0.99580786 0.60910769]. \t  0.33315225252768066 \t 3.0519370698729644\n",
            "15     \t [0.13696602 0.27408661 0.64454242]. \t  0.8780867073831925 \t 3.0519370698729644\n",
            "16     \t [0.29181005 0.95535828 0.41580062]. \t  1.4247484537410664 \t 3.0519370698729644\n",
            "17     \t [0.32142506 0.66089138 0.6950395 ]. \t  2.453982914221601 \t 3.0519370698729644\n",
            "18     \t [0.99697033 0.05149246 0.33198442]. \t  0.2594678878462106 \t 3.0519370698729644\n",
            "19     \t [0.98567311 0.1337062  0.01089334]. \t  0.044318838551756924 \t 3.0519370698729644\n",
            "20     \t [0.23925055 0.39723304 0.99332711]. \t  1.6632645849678045 \t 3.0519370698729644\n",
            "21     \t [0.94480338 0.62380806 0.99799704]. \t  1.9933906660030045 \t 3.0519370698729644\n",
            "22     \t [0.64523991 0.97887702 0.05208564]. \t  0.0006968538462589734 \t 3.0519370698729644\n",
            "23     \t [0.40430706 0.61989275 0.00133896]. \t  0.009640088753541444 \t 3.0519370698729644\n",
            "24     \t [0.10405701 0.52492683 0.83190774]. \t  \u001b[92m3.7735280924223744\u001b[0m \t 3.7735280924223744\n",
            "25     \t [0.44767277 0.00332131 0.04095836]. \t  0.18548934830832456 \t 3.7735280924223744\n",
            "26     \t [0.71487537 0.69632619 0.90668262]. \t  2.942185664251553 \t 3.7735280924223744\n",
            "27     \t [0.07945579 0.65221788 0.80850526]. \t  3.3820754902533805 \t 3.7735280924223744\n",
            "28     \t [0.69413127 0.01205997 0.99197631]. \t  0.11215624222922088 \t 3.7735280924223744\n",
            "29     \t [0.85765387 0.01362896 0.58046789]. \t  0.10251190752124838 \t 3.7735280924223744\n",
            "30     \t [0.00085432 0.14822443 0.06834129]. \t  0.20117157798792903 \t 3.7735280924223744\n",
            "31     \t [0.67095661 0.97872728 0.78266946]. \t  0.7161053270240159 \t 3.7735280924223744\n",
            "32     \t [0.02595943 0.94717378 0.41192615]. \t  1.511724144453188 \t 3.7735280924223744\n",
            "33     \t [1.         0.81601924 0.91768541]. \t  1.7518649563307267 \t 3.7735280924223744\n",
            "34     \t [0.00381478 0.55861788 0.97447272]. \t  2.5168138985982247 \t 3.7735280924223744\n",
            "35     \t [2.41076767e-12 5.68102973e-02 5.10752301e-01]. \t  0.1522311735096632 \t 3.7735280924223744\n",
            "36     \t [0. 0. 0.]. \t  0.06797411659013229 \t 3.7735280924223744\n",
            "37     \t [0.86070436 0.36330274 0.0290715 ]. \t  0.048092199540914884 \t 3.7735280924223744\n",
            "38     \t [0.08635886 0.63462348 0.00672149]. \t  0.0072494639921289825 \t 3.7735280924223744\n",
            "39     \t [0.7212604  0.50502922 0.83233727]. \t  3.6671431458169974 \t 3.7735280924223744\n",
            "40     \t [0.84514095 0.41232829 0.88811835]. \t  2.994497769478048 \t 3.7735280924223744\n",
            "41     \t [0.49670866 0.54301235 0.84841034]. \t  \u001b[92m3.8354620582478582\u001b[0m \t 3.8354620582478582\n",
            "42     \t [0.58345875 0.57782182 0.80868207]. \t  3.585758229671717 \t 3.8354620582478582\n",
            "43     \t [0.44064076 0.48892524 0.82732568]. \t  3.6568304634973523 \t 3.8354620582478582\n",
            "44     \t [0.61736699 0.51012853 0.85875488]. \t  3.750637957506302 \t 3.8354620582478582\n",
            "45     \t [0.03143703 0.44524632 0.84131799]. \t  3.421730423536375 \t 3.8354620582478582\n",
            "46     \t [0.39797226 0.47237127 0.85871986]. \t  3.6216238577662985 \t 3.8354620582478582\n",
            "47     \t [0.57336041 0.55248463 0.84819289]. \t  3.8211179614666073 \t 3.8354620582478582\n",
            "48     \t [0.4630046  0.47913957 0.88059813]. \t  3.570603924620608 \t 3.8354620582478582\n",
            "49     \t [0.64875435 0.47945148 0.85300878]. \t  3.627286376761378 \t 3.8354620582478582\n",
            "50     \t [0.02408657 0.55274885 0.83374198]. \t  3.786253372732978 \t 3.8354620582478582\n",
            "51     \t [0.97370799 0.0593317  0.99867744]. \t  0.1663898696808663 \t 3.8354620582478582\n",
            "52     \t [0.36219628 0.59554929 0.83437659]. \t  3.7615149144787976 \t 3.8354620582478582\n",
            "53     \t [0.3757999  0.57303433 0.84150039]. \t  3.8302526380678996 \t 3.8354620582478582\n",
            "54     \t [0.49924503 0.49361871 0.86267526]. \t  3.7070202224130497 \t 3.8354620582478582\n",
            "55     \t [0.53712018 0.57504138 0.84575052]. \t  3.806510132903016 \t 3.8354620582478582\n",
            "56     \t [0.46437067 0.56226378 0.85698961]. \t  \u001b[92m3.844438767745747\u001b[0m \t 3.844438767745747\n",
            "57     \t [0.49787956 0.53120576 0.86990041]. \t  3.7978849207535137 \t 3.844438767745747\n",
            "58     \t [0.59552562 0.52987474 0.86108686]. \t  3.798223861853165 \t 3.844438767745747\n",
            "59     \t [0.30814012 0.53967259 0.86299139]. \t  3.842608922743818 \t 3.844438767745747\n",
            "60     \t [0.59960276 0.52265286 0.85031877]. \t  3.787897907660199 \t 3.844438767745747\n",
            "61     \t [0.05144739 0.47534251 0.82990756]. \t  3.5800057217842807 \t 3.844438767745747\n",
            "62     \t [0.58322328 0.56125698 0.84328894]. \t  3.805363921303427 \t 3.844438767745747\n",
            "63     \t [0.51665621 0.50037927 0.8955014 ]. \t  3.5549369071608727 \t 3.844438767745747\n",
            "64     \t [0.50564426 0.52260146 0.86634197]. \t  3.790442974512631 \t 3.844438767745747\n",
            "65     \t [0.01959051 0.62550864 0.85034983]. \t  3.651352388460514 \t 3.844438767745747\n",
            "66     \t [0.66075097 0.53274963 0.84022782]. \t  3.7690148994837447 \t 3.844438767745747\n",
            "67     \t [0.17557164 0.62158851 0.83819343]. \t  3.6838929989050966 \t 3.844438767745747\n",
            "68     \t [0.61618611 0.46059274 0.85911142]. \t  3.5252548658054854 \t 3.844438767745747\n",
            "69     \t [0.3851875  0.5299292  0.81930576]. \t  3.7335061518732875 \t 3.844438767745747\n",
            "70     \t [0.50291845 0.52175016 0.88138484]. \t  3.7255074458404525 \t 3.844438767745747\n",
            "71     \t [0.56124103 0.51268857 0.86847007]. \t  3.7491993725206343 \t 3.844438767745747\n",
            "72     \t [0.60431644 0.54738106 0.83159223]. \t  3.7623143073634666 \t 3.844438767745747\n",
            "73     \t [0.37279163 0.57673166 0.8535315 ]. \t  3.8407437063170233 \t 3.844438767745747\n",
            "74     \t [0.40985167 0.49502883 0.85981112]. \t  3.727696487115214 \t 3.844438767745747\n",
            "75     \t [0.35074019 0.59052376 0.847303  ]. \t  3.809585046532577 \t 3.844438767745747\n",
            "76     \t [0.0492968  0.57812659 0.8327869 ]. \t  3.771636945241978 \t 3.844438767745747\n",
            "77     \t [0.52204892 0.53953174 0.86197228]. \t  3.8248125269256477 \t 3.844438767745747\n",
            "78     \t [0.15879687 0.52078269 0.85031846]. \t  3.809308278398592 \t 3.844438767745747\n",
            "79     \t [0.4629876  0.51369323 0.83823482]. \t  3.7739385702420223 \t 3.844438767745747\n",
            "80     \t [0.58870883 0.50104101 0.82824863]. \t  3.6757981803406903 \t 3.844438767745747\n",
            "81     \t [0.50802979 0.51817269 0.82054849]. \t  3.6985483667033203 \t 3.844438767745747\n",
            "82     \t [0.6300134  0.50591639 0.85439756]. \t  3.7377988830007824 \t 3.844438767745747\n",
            "83     \t [0.22465746 0.51701443 0.85003354]. \t  3.808106396866097 \t 3.844438767745747\n",
            "84     \t [0.22343934 0.49960566 0.82614201]. \t  3.6994351844813487 \t 3.844438767745747\n",
            "85     \t [0.58119989 0.57858327 0.81608121]. \t  3.643926381925053 \t 3.844438767745747\n",
            "86     \t [0.50683967 0.5394394  0.87247604]. \t  3.798997485212353 \t 3.844438767745747\n",
            "87     \t [0.21168551 0.52200039 0.86317154]. \t  3.805356448182632 \t 3.844438767745747\n",
            "88     \t [0.35249445 0.59442864 0.84475366]. \t  3.794899499528392 \t 3.844438767745747\n",
            "89     \t [0.36004867 0.58109571 0.79377918]. \t  3.520579994216594 \t 3.844438767745747\n",
            "90     \t [0.51389283 0.52786369 0.82694423]. \t  3.7490769809892055 \t 3.844438767745747\n",
            "91     \t [0.30417059 0.62207091 0.826242  ]. \t  3.6371040040791476 \t 3.844438767745747\n",
            "92     \t [0.39380405 0.49376568 0.86684411]. \t  3.7061651900698425 \t 3.844438767745747\n",
            "93     \t [0.49662185 0.53280577 0.87263077]. \t  3.790847767126519 \t 3.844438767745747\n",
            "94     \t [0.49693915 0.53010327 0.86531301]. \t  3.8093274210393395 \t 3.844438767745747\n",
            "95     \t [0.15778553 0.5452065  0.85809382]. \t  3.843516688459091 \t 3.844438767745747\n",
            "96     \t [0.34441208 0.56199954 0.80759496]. \t  3.667650256902603 \t 3.844438767745747\n",
            "97     \t [0.56534413 0.50062071 0.86749982]. \t  3.711382998775074 \t 3.844438767745747\n",
            "98     \t [0.53942663 0.53584191 0.92560623]. \t  3.315989817040016 \t 3.844438767745747\n",
            "99     \t [0.37279166 0.5624033  0.82791596]. \t  3.7925255344516837 \t 3.844438767745747\n",
            "100    \t [0.58094368 0.49302463 0.83419638]. \t  3.673334225133943 \t 3.844438767745747\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6YRio_skuvd2",
        "outputId": "29b634a0-de4d-41a5-cef6-4bf40073f531"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 8 \r\n",
        "\r\n",
        "np.random.seed(run_num_8)\r\n",
        "surrogate_winner_8 = dtStudentProcess(d_cov_func, nu = df)\r\n",
        "\r\n",
        "winner_8 = dGPGO_stp(surrogate_winner_8, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_8.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.10949987 0.04221501 0.59969258]. \t  0.15732379460832624 \t 0.8830091449513892\n",
            "init   \t [0.29987071 0.79661178 0.36619613]. \t  0.8830091449513892 \t 0.8830091449513892\n",
            "init   \t [0.88060834 0.29784163 0.32910326]. \t  0.2992873424905509 \t 0.8830091449513892\n",
            "init   \t [0.56871692 0.74392742 0.05385289]. \t  0.0051668955191694075 \t 0.8830091449513892\n",
            "init   \t [0.60207437 0.42931858 0.20547361]. \t  0.2908249205255991 \t 0.8830091449513892\n",
            "1      \t [1. 1. 1.]. \t  0.31688362070415665 \t 0.8830091449513892\n",
            "2      \t [0.09250474 0.65862611 0.98455533]. \t  \u001b[92m2.16574947683527\u001b[0m \t 2.16574947683527\n",
            "3      \t [0.69332868 0.16959446 0.9783922 ]. \t  0.5362860668682835 \t 2.16574947683527\n",
            "4      \t [0.58210726 0.9454608  0.86643983]. \t  0.9308592193228079 \t 2.16574947683527\n",
            "5      \t [0.05085969 0.93845901 0.71419179]. \t  1.7308871085289008 \t 2.16574947683527\n",
            "6      \t [0.05078821 0.86384699 0.92516961]. \t  1.3973687799811472 \t 2.16574947683527\n",
            "7      \t [0.21170364 0.03719789 0.02995932]. \t  0.16077819332419288 \t 2.16574947683527\n",
            "8      \t [0.77490742 0.03566679 0.09658235]. \t  0.23811379462256194 \t 2.16574947683527\n",
            "9      \t [0.00979584 0.76071194 0.04844819]. \t  0.003736200648899856 \t 2.16574947683527\n",
            "10     \t [0.97332082 0.99909138 0.02867902]. \t  9.292333815586823e-05 \t 2.16574947683527\n",
            "11     \t [0.98554599 0.60470456 0.98710451]. \t  \u001b[92m2.2072483598501327\u001b[0m \t 2.2072483598501327\n",
            "12     \t [0.98429884 0.49684792 0.07838057]. \t  0.026079811175337562 \t 2.2072483598501327\n",
            "13     \t [0.02298079 0.05297391 0.89388603]. \t  0.33264238562001525 \t 2.2072483598501327\n",
            "14     \t [0.98140044 0.85635616 0.62811295]. \t  0.5280747937569644 \t 2.2072483598501327\n",
            "15     \t [0.01922824 0.45660988 0.41613412]. \t  0.4294863884479162 \t 2.2072483598501327\n",
            "16     \t [0.59329175 0.63815878 1.        ]. \t  1.9788992644136862 \t 2.2072483598501327\n",
            "17     \t [0.31438112 0.43163966 0.82656065]. \t  \u001b[92m3.3358486115294\u001b[0m \t 3.3358486115294\n",
            "18     \t [5.03573770e-04 9.96775013e-01 1.32864028e-01]. \t  0.01210868274653652 \t 3.3358486115294\n",
            "19     \t [0.29060652 0.40627267 0.98301611]. \t  1.8660791584131686 \t 3.3358486115294\n",
            "20     \t [0.59407832 0.021001   0.61974219]. \t  0.1517590416583478 \t 3.3358486115294\n",
            "21     \t [0.9912329  0.11575526 0.72554814]. \t  0.5644209465739388 \t 3.3358486115294\n",
            "22     \t [0.66280816 0.58432907 0.69302564]. \t  2.0652609363466894 \t 3.3358486115294\n",
            "23     \t [0.33123917 0.61233095 0.79662908]. \t  \u001b[92m3.4707194874995744\u001b[0m \t 3.4707194874995744\n",
            "24     \t [0.09185195 0.6460716  0.70126   ]. \t  2.6057457429877395 \t 3.4707194874995744\n",
            "25     \t [0.33649809 0.32496334 0.01526063]. \t  0.09621801796102435 \t 3.4707194874995744\n",
            "26     \t [0.98626404 0.1052157  0.00447135]. \t  0.04006835713353459 \t 3.4707194874995744\n",
            "27     \t [0.42200042 1.         0.64746186]. \t  1.582561590437943 \t 3.4707194874995744\n",
            "28     \t [0.03306866 0.13243347 0.05564719]. \t  0.1855186566888771 \t 3.4707194874995744\n",
            "29     \t [0.96971871 0.07672444 0.98133329]. \t  0.23124893070651012 \t 3.4707194874995744\n",
            "30     \t [0.30290651 0.97158903 0.92321821]. \t  0.6880806521764364 \t 3.4707194874995744\n",
            "31     \t [0.99945639 0.05676921 0.31461287]. \t  0.27401534485114587 \t 3.4707194874995744\n",
            "32     \t [0.5306055  0.45617329 0.81801175]. \t  3.4386621001003372 \t 3.4707194874995744\n",
            "33     \t [0.03229415 0.29158942 0.82163014]. \t  2.039119504648129 \t 3.4707194874995744\n",
            "34     \t [0.96589827 0.36041154 0.9601664 ]. \t  1.8118392519197248 \t 3.4707194874995744\n",
            "35     \t [0.32391327 0.61274395 0.84473936]. \t  \u001b[92m3.7355918216805355\u001b[0m \t 3.7355918216805355\n",
            "36     \t [0.60884855 0.99491383 0.1453837 ]. \t  0.008269107149412248 \t 3.7355918216805355\n",
            "37     \t [0.43143981 0.94418417 0.04603016]. \t  0.0011315240739138322 \t 3.7355918216805355\n",
            "38     \t [0.41244428 0.67610823 0.82570922]. \t  3.2937657667375504 \t 3.7355918216805355\n",
            "39     \t [0.         0.         0.29726003]. \t  0.5646890520678145 \t 3.7355918216805355\n",
            "40     \t [0.33669788 0.5276953  0.84686191]. \t  \u001b[92m3.833003179099967\u001b[0m \t 3.833003179099967\n",
            "41     \t [3.5685209e-09 3.5685209e-09 3.5685209e-09]. \t  0.06797412158494298 \t 3.833003179099967\n",
            "42     \t [1.        1.        0.7769576]. \t  0.4418543466609254 \t 3.833003179099967\n",
            "43     \t [0.18820495 0.59353927 0.8377608 ]. \t  3.782486038836399 \t 3.833003179099967\n",
            "44     \t [0.99539394 0.99682915 0.28115061]. \t  0.02600526865258119 \t 3.833003179099967\n",
            "45     \t [0.42580809 0.5000026  0.85291908]. \t  3.7529362706469134 \t 3.833003179099967\n",
            "46     \t [1.         0.51920823 0.74480351]. \t  2.6543436237195603 \t 3.833003179099967\n",
            "47     \t [0.35753352 0.5578642  0.83702148]. \t  3.8329538923343085 \t 3.833003179099967\n",
            "48     \t [0.36576758 0.45631958 0.8434048 ]. \t  3.5356323922793984 \t 3.833003179099967\n",
            "49     \t [0.33856166 0.50347449 0.8105223 ]. \t  3.6235296216072577 \t 3.833003179099967\n",
            "50     \t [0.01657523 0.57324619 0.83093274]. \t  3.761095658188848 \t 3.833003179099967\n",
            "51     \t [0.40285001 0.57940036 0.81201469]. \t  3.667191294657641 \t 3.833003179099967\n",
            "52     \t [0.39128069 0.5605795  0.84977008]. \t  \u001b[92m3.8538058676901072\u001b[0m \t 3.8538058676901072\n",
            "53     \t [0.04442027 0.60469981 0.81101668]. \t  3.593356903986762 \t 3.8538058676901072\n",
            "54     \t [0.18556408 0.53434752 0.830309  ]. \t  3.7966763954038583 \t 3.8538058676901072\n",
            "55     \t [0.70206178 0.52137529 0.86241619]. \t  3.752249719241261 \t 3.8538058676901072\n",
            "56     \t [0.53494683 0.56221568 0.85274863]. \t  3.8309016287493485 \t 3.8538058676901072\n",
            "57     \t [0.05777955 0.58382537 0.85322627]. \t  3.801291535750422 \t 3.8538058676901072\n",
            "58     \t [0.25550419 0.57936826 0.85167673]. \t  3.8397173515088032 \t 3.8538058676901072\n",
            "59     \t [0.20482757 0.57186317 0.84931625]. \t  3.846052724547246 \t 3.8538058676901072\n",
            "60     \t [0.21829871 0.58297942 0.85185433]. \t  3.83122810254845 \t 3.8538058676901072\n",
            "61     \t [0.32335961 0.46771734 0.8712219 ]. \t  3.558949037611737 \t 3.8538058676901072\n",
            "62     \t [0.02757666 0.52222448 0.81151465]. \t  3.641904380349425 \t 3.8538058676901072\n",
            "63     \t [0.64156254 0.49336174 0.83958005]. \t  3.6752407850894975 \t 3.8538058676901072\n",
            "64     \t [0.67271374 0.52706591 0.8801307 ]. \t  3.7110615764052812 \t 3.8538058676901072\n",
            "65     \t [0.47763252 0.55838881 0.82876313]. \t  3.7798651733719906 \t 3.8538058676901072\n",
            "66     \t [0.83649652 0.58149471 0.8476442 ]. \t  3.6973725710740544 \t 3.8538058676901072\n",
            "67     \t [0.64183823 0.51706213 0.85752886]. \t  3.764600358064461 \t 3.8538058676901072\n",
            "68     \t [0.08876777 0.51862127 0.86520191]. \t  3.7704240035097456 \t 3.8538058676901072\n",
            "69     \t [0.79065165 0.51889655 0.85542334]. \t  3.724756410731149 \t 3.8538058676901072\n",
            "70     \t [0.22546267 0.55442445 0.85613898]. \t  \u001b[92m3.857489159924334\u001b[0m \t 3.857489159924334\n",
            "71     \t [0.33759377 0.51145948 0.86885455]. \t  3.7662046508992777 \t 3.857489159924334\n",
            "72     \t [0.29206334 0.52581095 0.83811972]. \t  3.8145029161418647 \t 3.857489159924334\n",
            "73     \t [0.69557986 0.49216688 0.86932345]. \t  3.6431237239577356 \t 3.857489159924334\n",
            "74     \t [0.06980431 0.49216997 0.88502865]. \t  3.5775454311893897 \t 3.857489159924334\n",
            "75     \t [0.37212223 0.55754695 0.87933418]. \t  3.7917278305402933 \t 3.857489159924334\n",
            "76     \t [0.22016001 0.55574819 0.82702092]. \t  3.7977476519773328 \t 3.857489159924334\n",
            "77     \t [0.72657067 0.52650333 0.87280757]. \t  3.727859214724866 \t 3.857489159924334\n",
            "78     \t [0.27478054 0.50272148 0.85538221]. \t  3.7650169577354022 \t 3.857489159924334\n",
            "79     \t [0.35311948 0.55108618 0.84034891]. \t  3.8434485109577414 \t 3.857489159924334\n",
            "80     \t [0.48443741 0.50563511 0.87304013]. \t  3.7215864801717737 \t 3.857489159924334\n",
            "81     \t [0.88004923 0.53452063 0.8566505 ]. \t  3.716737821189783 \t 3.857489159924334\n",
            "82     \t [0.03376617 0.5239087  0.8855107 ]. \t  3.6737053015177072 \t 3.857489159924334\n",
            "83     \t [0.87306313 0.481539   0.83260858]. \t  3.5320520099210153 \t 3.857489159924334\n",
            "84     \t [0.37188314 0.54192495 0.8490262 ]. \t  3.8518233829157116 \t 3.857489159924334\n",
            "85     \t [0.29101632 0.52363381 0.84969161]. \t  3.8273325580104 \t 3.857489159924334\n",
            "86     \t [0.77759498 0.51720356 0.84221524]. \t  3.7105063349628837 \t 3.857489159924334\n",
            "87     \t [0.29864428 0.5860713  0.82821163]. \t  3.7667552833851707 \t 3.857489159924334\n",
            "88     \t [0.29305678 0.55948981 0.83491252]. \t  3.829359519930548 \t 3.857489159924334\n",
            "89     \t [3.38949951e-04 5.47472563e-01 8.70097310e-01]. \t  3.778010378147218 \t 3.857489159924334\n",
            "90     \t [0.21999986 0.62116361 0.83894622]. \t  3.690704846217956 \t 3.857489159924334\n",
            "91     \t [0.01400424 0.57476674 0.85464394]. \t  3.802455840235262 \t 3.857489159924334\n",
            "92     \t [0.4004839  0.54648438 0.8629017 ]. \t  3.8451074678503447 \t 3.857489159924334\n",
            "93     \t [0.20703725 0.51726824 0.83327743]. \t  3.7776824092221433 \t 3.857489159924334\n",
            "94     \t [0.74878713 0.56541909 0.8405792 ]. \t  3.739455663736451 \t 3.857489159924334\n",
            "95     \t [0.17307012 0.51873591 0.81815473]. \t  3.708621088336112 \t 3.857489159924334\n",
            "96     \t [0.22421558 0.55470664 0.85323405]. \t  \u001b[92m3.8586113359794134\u001b[0m \t 3.8586113359794134\n",
            "97     \t [0.77094995 0.47871464 0.88901287]. \t  3.456325966472538 \t 3.8586113359794134\n",
            "98     \t [0.27189813 0.52778106 0.84657273]. \t  3.8329762409004537 \t 3.8586113359794134\n",
            "99     \t [0.17548091 0.56075291 0.84872894]. \t  3.8509448976488163 \t 3.8586113359794134\n",
            "100    \t [0.72817923 0.55094254 0.87119496]. \t  3.758845543013483 \t 3.8586113359794134\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejw6v-Ihuvf_",
        "outputId": "caba0da0-34dc-4c3a-f335-317dea867a63"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 9 \r\n",
        "\r\n",
        "np.random.seed(run_num_9)\r\n",
        "surrogate_winner_9 = dtStudentProcess(d_cov_func, nu = df)\r\n",
        "\r\n",
        "winner_9 = dGPGO_stp(surrogate_winner_9, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_9.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.67227856 0.4880784  0.82549517]. \t  3.595021899183128 \t 3.595021899183128\n",
            "init   \t [0.03144639 0.80804996 0.56561742]. \t  2.9633561694281085 \t 3.595021899183128\n",
            "init   \t [0.2976225  0.04669572 0.9906274 ]. \t  0.16382388103073592 \t 3.595021899183128\n",
            "init   \t [0.00682573 0.76979303 0.7467671 ]. \t  2.382987807172393 \t 3.595021899183128\n",
            "init   \t [0.37743894 0.49414745 0.92894839]. \t  3.1588932929069533 \t 3.595021899183128\n",
            "1      \t [0.98938349 0.97709544 0.1156812 ]. \t  0.0009087970643985222 \t 3.595021899183128\n",
            "2      \t [0.27707152 0.07483468 0.09716521]. \t  0.4019491424951247 \t 3.595021899183128\n",
            "3      \t [1. 1. 1.]. \t  0.31688362070415665 \t 3.595021899183128\n",
            "4      \t [0.00306396 0.62437279 0.00435907]. \t  0.006586660328686004 \t 3.595021899183128\n",
            "5      \t [0.95812618 0.05737158 0.50176599]. \t  0.10055044737273429 \t 3.595021899183128\n",
            "6      \t [0.43994218 0.99994131 0.66779018]. \t  1.3984021718922168 \t 3.595021899183128\n",
            "7      \t [9.04031131e-01 8.45612117e-05 1.81504516e-02]. \t  0.05738040571650537 \t 3.595021899183128\n",
            "8      \t [0.55579614 0.48982846 0.34059246]. \t  0.29046311203014147 \t 3.595021899183128\n",
            "9      \t [0.95824072 0.02293192 0.97345539]. \t  0.14560058477715682 \t 3.595021899183128\n",
            "10     \t [0.04885033 0.11290886 0.31546744]. \t  0.6881237744956575 \t 3.595021899183128\n",
            "11     \t [0.36543417 0.95348505 0.09961531]. \t  0.005019606578235735 \t 3.595021899183128\n",
            "12     \t [0.99713422 0.67401228 0.88225388]. \t  3.1504127864088844 \t 3.595021899183128\n",
            "13     \t [0.73088779 0.60958652 1.        ]. \t  2.0262981451904163 \t 3.595021899183128\n",
            "14     \t [0.85533164 1.         0.56978988]. \t  0.5107553950089447 \t 3.595021899183128\n",
            "15     \t [0.07934713 0.95674979 0.04699611]. \t  0.0013796279485808415 \t 3.595021899183128\n",
            "16     \t [0.4517051  0.02846213 0.62262196]. \t  0.16693993602703067 \t 3.595021899183128\n",
            "17     \t [0.0893526  0.26465763 0.7475595 ]. \t  1.539750521178547 \t 3.595021899183128\n",
            "18     \t [0.02761168 0.02024994 0.06572784]. \t  0.18976465805912612 \t 3.595021899183128\n",
            "19     \t [0.98262361 0.37134825 0.86126054]. \t  2.720170988597954 \t 3.595021899183128\n",
            "20     \t [0.89094914 0.34229008 0.00185421]. \t  0.032099552456433714 \t 3.595021899183128\n",
            "21     \t [0.3105857  0.64459312 0.68578602]. \t  2.4104562318731237 \t 3.595021899183128\n",
            "22     \t [0.63551485 0.2079334  0.97399455]. \t  0.7366848075214525 \t 3.595021899183128\n",
            "23     \t [0.20617143 0.98128049 0.88499043]. \t  0.7392900007728328 \t 3.595021899183128\n",
            "24     \t [0.82527897 0.60725692 0.00181924]. \t  0.00587456284729095 \t 3.595021899183128\n",
            "25     \t [0.00814788 0.17888887 0.97097049]. \t  0.6068856561304349 \t 3.595021899183128\n",
            "26     \t [0.9817136  0.69444576 0.68736046]. \t  1.3693685560297197 \t 3.595021899183128\n",
            "27     \t [0.07616803 0.49531363 0.35364339]. \t  0.3662869451925101 \t 3.595021899183128\n",
            "28     \t [0.39643553 0.58918424 0.00925343]. \t  0.014698739051808608 \t 3.595021899183128\n",
            "29     \t [0.02377096 0.99728578 0.91441815]. \t  0.5811199035584842 \t 3.595021899183128\n",
            "30     \t [0.03878991 0.9764706  0.3580824 ]. \t  0.8338018431687888 \t 3.595021899183128\n",
            "31     \t [0.65467797 0.19087507 0.01197099]. \t  0.10483649957817694 \t 3.595021899183128\n",
            "32     \t [0.7005942  0.97599935 0.8855754 ]. \t  0.7000599354063857 \t 3.595021899183128\n",
            "33     \t [0.76104519 0.03004852 0.16705516]. \t  0.4324067863462345 \t 3.595021899183128\n",
            "34     \t [0.99002948 0.44287165 0.99372542]. \t  1.825360207991876 \t 3.595021899183128\n",
            "35     \t [0.01321417 0.01205514 0.67819196]. \t  0.19718414829729608 \t 3.595021899183128\n",
            "36     \t [0.71420415 0.99981454 0.06704429]. \t  0.0007659243219025859 \t 3.595021899183128\n",
            "37     \t [0.07915425 0.98826755 0.70246951]. \t  1.5981798290186273 \t 3.595021899183128\n",
            "38     \t [0.99465537 0.95178442 0.81578822]. \t  0.7392483355731092 \t 3.595021899183128\n",
            "39     \t [0.06047674 0.72717623 0.96125576]. \t  2.1411497865270617 \t 3.595021899183128\n",
            "40     \t [0.69833177 0.74914301 0.75431285]. \t  2.0385840089052585 \t 3.595021899183128\n",
            "41     \t [0.70165351 0.36886407 0.62033164]. \t  0.9159270429639357 \t 3.595021899183128\n",
            "42     \t [1.38511951e-02 1.24771116e-09 3.14463614e-01]. \t  0.5593903396852037 \t 3.595021899183128\n",
            "43     \t [0.99691954 0.58022884 0.24276995]. \t  0.04177083664453346 \t 3.595021899183128\n",
            "44     \t [0.82251079 0.48865998 0.84050361]. \t  \u001b[92m3.6033429242521\u001b[0m \t 3.6033429242521\n",
            "45     \t [0.80961617 0.02467099 0.73991669]. \t  0.2890570297765183 \t 3.6033429242521\n",
            "46     \t [0.27484607 0.33027212 0.97121711]. \t  1.5240581210972322 \t 3.6033429242521\n",
            "47     \t [0.03745864 0.80695071 0.2121102 ]. \t  0.08923511373658981 \t 3.6033429242521\n",
            "48     \t [0.53585105 0.48954499 0.7893015 ]. \t  3.362363176433015 \t 3.6033429242521\n",
            "49     \t [0.99418659 0.51807427 0.84677492]. \t  \u001b[92m3.633363753941588\u001b[0m \t 3.633363753941588\n",
            "50     \t [0.00379817 0.00696278 0.98374188]. \t  0.11396331220371757 \t 3.633363753941588\n",
            "51     \t [0.01101941 0.58721975 0.9204813 ]. \t  3.3503740093068166 \t 3.633363753941588\n",
            "52     \t [0.92608074 0.53070152 0.8496284 ]. \t  \u001b[92m3.688848610527671\u001b[0m \t 3.688848610527671\n",
            "53     \t [0.78065183 0.49252168 0.85103778]. \t  3.646906910855403 \t 3.688848610527671\n",
            "54     \t [0.73060272 0.52138488 0.85553657]. \t  \u001b[92m3.7500033528043817\u001b[0m \t 3.7500033528043817\n",
            "55     \t [0.84839082 0.57044182 0.86363373]. \t  3.7214293118617854 \t 3.7500033528043817\n",
            "56     \t [0.86859572 0.61560732 0.83685212]. \t  3.5293642831300764 \t 3.7500033528043817\n",
            "57     \t [0.86017842 0.54918792 0.86979294]. \t  3.716290457329645 \t 3.7500033528043817\n",
            "58     \t [0.83846803 0.54873817 0.86003164]. \t  3.74093581021803 \t 3.7500033528043817\n",
            "59     \t [0.01631662 0.57790219 0.71440384]. \t  2.6722946494291877 \t 3.7500033528043817\n",
            "60     \t [0.         0.20057937 0.        ]. \t  0.07268988022082429 \t 3.7500033528043817\n",
            "61     \t [0.97596318 0.14692172 0.07268957]. \t  0.10532422750678015 \t 3.7500033528043817\n",
            "62     \t [0.73733606 0.46158864 0.88170206]. \t  3.4174400473346296 \t 3.7500033528043817\n",
            "63     \t [0.26960138 0.53425636 0.72107453]. \t  2.7087399417288562 \t 3.7500033528043817\n",
            "64     \t [0.99962111 0.53940564 0.86595862]. \t  3.6599746157281983 \t 3.7500033528043817\n",
            "65     \t [0.53634106 0.53484224 0.85777391]. \t  \u001b[92m3.82167905511284\u001b[0m \t 3.82167905511284\n",
            "66     \t [0.8456744  0.54789376 0.84823423]. \t  3.7315895790648286 \t 3.82167905511284\n",
            "67     \t [0.98556707 0.00652048 0.6733453 ]. \t  0.17784421382356919 \t 3.82167905511284\n",
            "68     \t [0.82317804 0.52621282 0.85626001]. \t  3.727221937931535 \t 3.82167905511284\n",
            "69     \t [0.01319056 0.56339167 0.99927057]. \t  2.072798057185837 \t 3.82167905511284\n",
            "70     \t [0.69016857 0.55072889 0.84718965]. \t  3.786033357274692 \t 3.82167905511284\n",
            "71     \t [0.38512808 0.58971382 0.87363044]. \t  3.7781670534153076 \t 3.82167905511284\n",
            "72     \t [0.51884054 0.49038766 0.88264145]. \t  3.6068860530580977 \t 3.82167905511284\n",
            "73     \t [0.63994591 0.52572522 0.85622262]. \t  3.7845057549643695 \t 3.82167905511284\n",
            "74     \t [0.52034103 0.57151528 0.86829055]. \t  3.809103054596018 \t 3.82167905511284\n",
            "75     \t [0.54748904 0.54724707 0.86776563]. \t  3.813526369110319 \t 3.82167905511284\n",
            "76     \t [0.65035174 0.53584263 0.86926211]. \t  3.7757412439850713 \t 3.82167905511284\n",
            "77     \t [0.761864   0.60707749 0.81926702]. \t  3.5059262058324276 \t 3.82167905511284\n",
            "78     \t [0.59307983 0.56762433 0.83158803]. \t  3.753408847688225 \t 3.82167905511284\n",
            "79     \t [0.48119318 0.61507861 0.8369923 ]. \t  3.6767471449875346 \t 3.82167905511284\n",
            "80     \t [0.54603668 0.52503657 0.86013333]. \t  3.801151203678421 \t 3.82167905511284\n",
            "81     \t [0.43375676 0.56220885 0.84114661]. \t  \u001b[92m3.8324921797194396\u001b[0m \t 3.8324921797194396\n",
            "82     \t [0.53372302 0.56418666 0.86998101]. \t  3.8086281660322703 \t 3.8324921797194396\n",
            "83     \t [0.46562985 0.50169946 0.85355381]. \t  3.7545340240483123 \t 3.8324921797194396\n",
            "84     \t [0.47644081 0.56495119 0.865056  ]. \t  3.83052406970943 \t 3.8324921797194396\n",
            "85     \t [0.26626138 0.59676413 0.80477505]. \t  3.596456532816174 \t 3.8324921797194396\n",
            "86     \t [0.43861487 0.54768619 0.82242076]. \t  3.756448409665236 \t 3.8324921797194396\n",
            "87     \t [0.5476961  0.5906623  0.82649442]. \t  3.6940922113962475 \t 3.8324921797194396\n",
            "88     \t [0.52100323 0.50690181 0.87585804]. \t  3.7085545832498936 \t 3.8324921797194396\n",
            "89     \t [0.44388344 0.59918222 0.85301462]. \t  3.7777135594326605 \t 3.8324921797194396\n",
            "90     \t [0.3709163  0.58426192 0.83261618]. \t  3.781977278754375 \t 3.8324921797194396\n",
            "91     \t [0.73179906 0.50108144 0.83621364]. \t  3.666124510208711 \t 3.8324921797194396\n",
            "92     \t [0.59749429 0.53133386 0.86802303]. \t  3.785476728876872 \t 3.8324921797194396\n",
            "93     \t [0.49406398 0.537905   0.87325873]. \t  3.7960341524704115 \t 3.8324921797194396\n",
            "94     \t [0.51577076 0.63176645 0.83718679]. \t  3.588149821155959 \t 3.8324921797194396\n",
            "95     \t [0.69473528 0.54182685 0.86904292]. \t  3.7705487407035676 \t 3.8324921797194396\n",
            "96     \t [0.5078156  0.49603028 0.81464199]. \t  3.602458578552163 \t 3.8324921797194396\n",
            "97     \t [0.50906743 0.51458873 0.82871526]. \t  3.7334195583488095 \t 3.8324921797194396\n",
            "98     \t [0.50123492 0.53245211 0.87155733]. \t  3.793656045987839 \t 3.8324921797194396\n",
            "99     \t [0.62150084 0.55450365 0.84758151]. \t  3.8069662352537357 \t 3.8324921797194396\n",
            "100    \t [0.52633921 0.5992915  0.82346448]. \t  3.6570888006234945 \t 3.8324921797194396\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N4xG5dbuuvig",
        "outputId": "342f7366-2c9c-40d2-9bf3-1f1f3f5e3d6b"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 10 \r\n",
        "\r\n",
        "np.random.seed(run_num_10)\r\n",
        "surrogate_winner_10 = dtStudentProcess(d_cov_func, nu = df)\r\n",
        "\r\n",
        "winner_10 = dGPGO_stp(surrogate_winner_10, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_10.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.65358959 0.11500694 0.95028286]. \t  0.42730315147591735 \t 1.1029187088185965\n",
            "init   \t [0.4821914  0.87247454 0.21233268]. \t  0.06161964400032635 \t 1.1029187088185965\n",
            "init   \t [0.04070962 0.39719446 0.2331322 ]. \t  0.33269334660262956 \t 1.1029187088185965\n",
            "init   \t [0.84174072 0.20708234 0.74246953]. \t  1.1029187088185965 \t 1.1029187088185965\n",
            "init   \t [0.39215413 0.18225652 0.74353941]. \t  0.9779763535009853 \t 1.1029187088185965\n",
            "1      \t [1. 1. 1.]. \t  0.31688362070415665 \t 1.1029187088185965\n",
            "2      \t [0.90319227 0.26571677 0.01561985]. \t  0.05090358170191965 \t 1.1029187088185965\n",
            "3      \t [0.03722426 0.76271424 0.89011521]. \t  \u001b[92m2.5039656647591935\u001b[0m \t 2.5039656647591935\n",
            "4      \t [0.28043343 0.98120805 0.90994271]. \t  0.675228445759718 \t 2.5039656647591935\n",
            "5      \t [0.97427312 0.75988935 0.38786172]. \t  0.1296218316152246 \t 2.5039656647591935\n",
            "6      \t [0.01006359 0.94218117 0.66418144]. \t  2.1650460243161502 \t 2.5039656647591935\n",
            "7      \t [0.05955692 0.03232917 0.89086014]. \t  0.27726435960887696 \t 2.5039656647591935\n",
            "8      \t [1.         0.58471237 1.        ]. \t  2.00281637283549 \t 2.5039656647591935\n",
            "9      \t [0.14799361 0.95085635 0.08267139]. \t  0.003812436404482828 \t 2.5039656647591935\n",
            "10     \t [0.01900153 0.4336611  0.75650204]. \t  \u001b[92m2.779884451526289\u001b[0m \t 2.779884451526289\n",
            "11     \t [0.18715994 0.66448891 0.61737097]. \t  2.341528147904633 \t 2.779884451526289\n",
            "12     \t [0.69088756 0.55025923 0.90199215]. \t  \u001b[92m3.575905515208266\u001b[0m \t 3.575905515208266\n",
            "13     \t [0.74202971 0.08377593 0.01658016]. \t  0.09881302872667641 \t 3.575905515208266\n",
            "14     \t [0.28858982 0.45586751 0.97114   ]. \t  2.3227980093549974 \t 3.575905515208266\n",
            "15     \t [0.00599313 0.04703605 0.00487161]. \t  0.08125937194708677 \t 3.575905515208266\n",
            "16     \t [0.72782311 0.70121707 1.        ]. \t  1.7296387125203623 \t 3.575905515208266\n",
            "17     \t [0.41602965 0.44038536 0.0310747 ]. \t  0.06553992345129989 \t 3.575905515208266\n",
            "18     \t [0.77056788 0.97935846 0.66953295]. \t  0.6618124704144871 \t 3.575905515208266\n",
            "19     \t [0.81355348 0.93523956 0.0123949 ]. \t  0.0001932796537245774 \t 3.575905515208266\n",
            "20     \t [0.66619668 0.50953193 0.61490918]. \t  1.1511919431307869 \t 3.575905515208266\n",
            "21     \t [0.02326068 0.03485314 0.45661049]. \t  0.23718164686133417 \t 3.575905515208266\n",
            "22     \t [0.9969284  0.69453589 0.82130777]. \t  2.8599259546734563 \t 3.575905515208266\n",
            "23     \t [0.99937745 0.0791505  0.01297497]. \t  0.04296944196860003 \t 3.575905515208266\n",
            "24     \t [0.0490729  0.68711373 0.11950792]. \t  0.02196371543625471 \t 3.575905515208266\n",
            "25     \t [0.9788148  0.7778885  0.02381647]. \t  0.0007618081160189001 \t 3.575905515208266\n",
            "26     \t [0.69854829 0.40254145 0.97636888]. \t  1.9235045263049502 \t 3.575905515208266\n",
            "27     \t [0.98061468 0.20419286 0.95497711]. \t  0.8022802014760235 \t 3.575905515208266\n",
            "28     \t [0.83876898 0.00639341 0.17641837]. \t  0.3561389211301272 \t 3.575905515208266\n",
            "29     \t [0.85390049 0.01359116 0.90186467]. \t  0.217041841771207 \t 3.575905515208266\n",
            "30     \t [0.96494241 0.96730593 0.18861496]. \t  0.005681745083847766 \t 3.575905515208266\n",
            "31     \t [0.49203297 0.7161544  0.77200495]. \t  2.6105903478671775 \t 3.575905515208266\n",
            "32     \t [0.28463033 0.02189491 0.07546717]. \t  0.2964898831066517 \t 3.575905515208266\n",
            "33     \t [0.97951326 0.47404241 0.80915087]. \t  3.3270724135641965 \t 3.575905515208266\n",
            "34     \t [0.89097003 0.57629738 0.84791671]. \t  \u001b[92m3.685838659322\u001b[0m \t 3.685838659322\n",
            "35     \t [0.01367145 0.70740724 0.7438066 ]. \t  2.674110665051354 \t 3.685838659322\n",
            "36     \t [0.26653694 0.98390955 0.65648239]. \t  1.9509172158964752 \t 3.685838659322\n",
            "37     \t [0.20886907 0.5532342  0.81446682]. \t  \u001b[92m3.7270362319163004\u001b[0m \t 3.7270362319163004\n",
            "38     \t [0.0565966  0.97172794 0.24456957]. \t  0.15124968134027175 \t 3.7270362319163004\n",
            "39     \t [0.88199855 0.51218836 0.86255566]. \t  3.6681829547874836 \t 3.7270362319163004\n",
            "40     \t [0.24833023 0.63375748 0.84677531]. \t  3.6460151124885813 \t 3.7270362319163004\n",
            "41     \t [0.9967867  0.0522863  0.37249638]. \t  0.21293440671871045 \t 3.7270362319163004\n",
            "42     \t [0.16908407 0.59335961 0.81161263]. \t  3.65440345997422 \t 3.7270362319163004\n",
            "43     \t [0.80495026 0.59262772 0.86239438]. \t  3.6932875973791397 \t 3.7270362319163004\n",
            "44     \t [0.34298704 0.51035153 0.83793247]. \t  \u001b[92m3.77600943056808\u001b[0m \t 3.77600943056808\n",
            "45     \t [0.25795824 0.50731565 0.78047098]. \t  3.373011246969173 \t 3.77600943056808\n",
            "46     \t [0.90183045 0.55692969 0.84075923]. \t  3.68603133628175 \t 3.77600943056808\n",
            "47     \t [0.1288282  0.52863163 0.79327494]. \t  3.5331095362215263 \t 3.77600943056808\n",
            "48     \t [0.19872213 0.58104464 0.8412288 ]. \t  \u001b[92m3.819974142134662\u001b[0m \t 3.819974142134662\n",
            "49     \t [0.77936683 0.54795363 0.86786004]. \t  3.7504009393159046 \t 3.819974142134662\n",
            "50     \t [0.20083102 0.61440726 0.8120944 ]. \t  3.59438416780639 \t 3.819974142134662\n",
            "51     \t [0.19506232 0.65806182 0.82389224]. \t  3.441431215988225 \t 3.819974142134662\n",
            "52     \t [0.16429061 0.55706134 0.85455015]. \t  \u001b[92m3.85120710363921\u001b[0m \t 3.85120710363921\n",
            "53     \t [0.7383335  0.55544046 0.83322361]. \t  3.7226194146557066 \t 3.85120710363921\n",
            "54     \t [0.02052922 0.42236747 0.91516309]. \t  2.86103704599664 \t 3.85120710363921\n",
            "55     \t [0.50503157 0.55702338 0.84453085]. \t  3.8296036594562413 \t 3.85120710363921\n",
            "56     \t [0.80842329 0.54296078 0.86158548]. \t  3.7485929175120427 \t 3.85120710363921\n",
            "57     \t [0.81267917 0.56050863 0.86061957]. \t  3.7476095016253903 \t 3.85120710363921\n",
            "58     \t [0.22849175 0.58823549 0.82336544]. \t  3.74002639102477 \t 3.85120710363921\n",
            "59     \t [1.55108188e-07 1.49522450e-07 1.32687177e-01]. \t  0.3366376023139406 \t 3.85120710363921\n",
            "60     \t [0.26006815 0.51027895 0.83349343]. \t  3.76332297158066 \t 3.85120710363921\n",
            "61     \t [0.63078441 0.54402771 0.86193813]. \t  3.804519921533221 \t 3.85120710363921\n",
            "62     \t [0.85273149 0.52947702 0.86201594]. \t  3.717015580320722 \t 3.85120710363921\n",
            "63     \t [0.73413328 0.51792597 0.83550942]. \t  3.706460126775405 \t 3.85120710363921\n",
            "64     \t [0.78969301 0.55096944 0.8394503 ]. \t  3.7303143571517072 \t 3.85120710363921\n",
            "65     \t [0.77041326 0.5911089  0.82671757]. \t  3.609644471823897 \t 3.85120710363921\n",
            "66     \t [0.26842931 0.55630397 0.80683255]. \t  3.6724419193520386 \t 3.85120710363921\n",
            "67     \t [0.4826058  0.57323359 0.84782112]. \t  3.825191875337553 \t 3.85120710363921\n",
            "68     \t [0.7142198  0.53577539 0.86604263]. \t  3.7661192765737628 \t 3.85120710363921\n",
            "69     \t [0.31550114 0.5718902  0.83286249]. \t  3.810185662781201 \t 3.85120710363921\n",
            "70     \t [0.34380908 0.47074622 0.85845649]. \t  3.614548698943591 \t 3.85120710363921\n",
            "71     \t [0.99655287 0.34642151 0.25690702]. \t  0.18208088231122452 \t 3.85120710363921\n",
            "72     \t [0.91382221 0.56771414 0.82239063]. \t  3.572465135038753 \t 3.85120710363921\n",
            "73     \t [0.29426697 0.51857286 0.84932154]. \t  3.815533686554268 \t 3.85120710363921\n",
            "74     \t [0.65525121 0.52757953 0.86530269]. \t  3.771966745356971 \t 3.85120710363921\n",
            "75     \t [0.51107703 0.54076896 0.8563325 ]. \t  3.8337782167331276 \t 3.85120710363921\n",
            "76     \t [0.27706283 0.46101927 0.86817811]. \t  3.5285985134877755 \t 3.85120710363921\n",
            "77     \t [0.65413186 0.59341022 0.84212915]. \t  3.7196140788513894 \t 3.85120710363921\n",
            "78     \t [0.22816211 0.54031523 0.83467993]. \t  3.8227263635427517 \t 3.85120710363921\n",
            "79     \t [0.80366462 0.55211776 0.86013143]. \t  3.754190368823166 \t 3.85120710363921\n",
            "80     \t [0.45988651 0.53790103 0.85602798]. \t  3.8393346822953824 \t 3.85120710363921\n",
            "81     \t [1.         0.53653021 0.82124707]. \t  3.5438482593815523 \t 3.85120710363921\n",
            "82     \t [0.14655941 0.55451612 0.83628592]. \t  3.82473523068591 \t 3.85120710363921\n",
            "83     \t [0.34161369 0.51324872 0.81190958]. \t  3.659139478219875 \t 3.85120710363921\n",
            "84     \t [0.37724368 0.54221204 0.82401875]. \t  3.77342375478337 \t 3.85120710363921\n",
            "85     \t [0.76045928 0.46408791 0.8753546 ]. \t  3.4594568312542293 \t 3.85120710363921\n",
            "86     \t [0.07774561 0.48790644 0.85771293]. \t  3.672752910550376 \t 3.85120710363921\n",
            "87     \t [0.26898846 0.53494849 0.85559289]. \t  3.8459457080361505 \t 3.85120710363921\n",
            "88     \t [0.81280635 0.55562584 0.85404488]. \t  3.750184752084448 \t 3.85120710363921\n",
            "89     \t [0.7694161  0.51068817 0.84164732]. \t  3.696608435882533 \t 3.85120710363921\n",
            "90     \t [0.73039071 0.53076794 0.83070059]. \t  3.707307213193356 \t 3.85120710363921\n",
            "91     \t [0.43252743 0.68975417 0.820368  ]. \t  3.1604136670402947 \t 3.85120710363921\n",
            "92     \t [0.22183105 0.54780515 0.83592155]. \t  3.831156351334088 \t 3.85120710363921\n",
            "93     \t [0.26959779 0.55923169 0.82937399]. \t  3.8084752061723006 \t 3.85120710363921\n",
            "94     \t [0.85191594 0.5817472  0.82728925]. \t  3.6037640313101686 \t 3.85120710363921\n",
            "95     \t [0.2310653  0.52352922 0.84983193]. \t  3.8242181860723816 \t 3.85120710363921\n",
            "96     \t [0.6045789  0.56689047 0.84562468]. \t  3.799466445079047 \t 3.85120710363921\n",
            "97     \t [0.86753092 0.51599201 0.86013455]. \t  3.6868426387854356 \t 3.85120710363921\n",
            "98     \t [0.31217516 0.51281912 0.84565018]. \t  3.797654993643094 \t 3.85120710363921\n",
            "99     \t [0.2648809  0.49716606 0.84212847]. \t  3.7399317003684067 \t 3.85120710363921\n",
            "100    \t [0.80791616 0.49410358 0.87448499]. \t  3.5984186761012067 \t 3.85120710363921\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pSj_CQIAuvk3",
        "outputId": "2ed1c17d-d279-4767-953e-a20e40d659f6"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 11 \r\n",
        "\r\n",
        "np.random.seed(run_num_11)\r\n",
        "surrogate_winner_11 = dtStudentProcess(d_cov_func, nu = df)\r\n",
        "\r\n",
        "winner_11 = dGPGO_stp(surrogate_winner_11, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_11.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.98103566 0.82124785 0.64589361]. \t  0.6570240879762975 \t 0.687459437576373\n",
            "init   \t [0.42368801 0.20231098 0.49190677]. \t  0.29901296656621285 \t 0.687459437576373\n",
            "init   \t [0.13855833 0.45252104 0.11373463]. \t  0.1378633538075966 \t 0.687459437576373\n",
            "init   \t [0.00292449 0.52342617 0.18997116]. \t  0.12299763462541902 \t 0.687459437576373\n",
            "init   \t [0.14171979 0.33586637 0.58369219]. \t  0.687459437576373 \t 0.687459437576373\n",
            "1      \t [0.27904639 0.86674937 0.96457795]. \t  \u001b[92m1.129791867918356\u001b[0m \t 1.129791867918356\n",
            "2      \t [0.95983675 0.98162579 0.03051357]. \t  0.00011698065361110391 \t 1.129791867918356\n",
            "3      \t [1. 1. 1.]. \t  0.31688362070415665 \t 1.129791867918356\n",
            "4      \t [0.10922994 0.17736885 0.99587964]. \t  0.4885993482312173 \t 1.129791867918356\n",
            "5      \t [0.9835545  0.11597814 0.06639528]. \t  0.09591588417042934 \t 1.129791867918356\n",
            "6      \t [0.90860347 0.41246048 0.99489581]. \t  \u001b[92m1.6790256561966166\u001b[0m \t 1.6790256561966166\n",
            "7      \t [0.86920171 0.03073615 0.95774403]. \t  0.1809847303126208 \t 1.6790256561966166\n",
            "8      \t [0.11211938 0.00257425 0.09146158]. \t  0.284703468653731 \t 1.6790256561966166\n",
            "9      \t [0.64093833 0.64105375 0.98512101]. \t  \u001b[92m2.219362654893817\u001b[0m \t 2.219362654893817\n",
            "10     \t [0.38756777 0.99331493 0.36600103]. \t  0.7078847538275916 \t 2.219362654893817\n",
            "11     \t [0.00147484 0.93964119 0.00205779]. \t  0.00038384840449957045 \t 2.219362654893817\n",
            "12     \t [0.35152473 0.56836539 0.99888287]. \t  2.108480475772055 \t 2.219362654893817\n",
            "13     \t [0.47612345 0.56847608 0.97023103]. \t  \u001b[92m2.6274690866408763\u001b[0m \t 2.6274690866408763\n",
            "14     \t [0.0176432  0.8816692  0.60416436]. \t  \u001b[92m2.878723219276305\u001b[0m \t 2.878723219276305\n",
            "15     \t [0.95844906 0.58047578 0.00365328]. \t  0.0051285598456790715 \t 2.878723219276305\n",
            "16     \t [0.0901376  0.99460714 0.43312952]. \t  1.6622225626871228 \t 2.878723219276305\n",
            "17     \t [4.94860658e-04 9.90237744e-01 8.69827454e-01]. \t  0.7172608470353898 \t 2.878723219276305\n",
            "18     \t [0.76357627 0.27981781 0.01913338]. \t  0.07577816334057556 \t 2.878723219276305\n",
            "19     \t [0.42566839 0.67101306 0.67172539]. \t  2.1651340067682194 \t 2.878723219276305\n",
            "20     \t [0.90210755 0.23105956 0.66583571]. \t  0.8005448474866883 \t 2.878723219276305\n",
            "21     \t [0.49070223 0.01127143 0.79477464]. \t  0.2810186724355408 \t 2.878723219276305\n",
            "22     \t [0.77297964 0.98123609 0.84180914]. \t  0.6729710249674772 \t 2.878723219276305\n",
            "23     \t [0.55085176 0.34337612 0.98409711]. \t  1.4606656708271628 \t 2.878723219276305\n",
            "24     \t [0.96359601 0.00417819 0.28344023]. \t  0.302489113019333 \t 2.878723219276305\n",
            "25     \t [0.42094852 0.60123462 0.00078723]. \t  0.011397655869659011 \t 2.878723219276305\n",
            "26     \t [0.52219985 0.01647008 0.13391255]. \t  0.4939509442020163 \t 2.878723219276305\n",
            "27     \t [0.0286652  0.05496112 0.85580736]. \t  0.3862513873672331 \t 2.878723219276305\n",
            "28     \t [0.05348474 0.55810911 0.93624623]. \t  \u001b[92m3.1630939372343234\u001b[0m \t 3.1630939372343234\n",
            "29     \t [0.14305882 0.29595424 0.00112201]. \t  0.07437507172082643 \t 3.1630939372343234\n",
            "30     \t [0.95881693 0.98526313 0.33823045]. \t  0.07471217984785376 \t 3.1630939372343234\n",
            "31     \t [0.98295066 0.3850905  0.31880443]. \t  0.15238811708853536 \t 3.1630939372343234\n",
            "32     \t [0.16073746 0.65970612 0.82825757]. \t  \u001b[92m3.4464206800839823\u001b[0m \t 3.4464206800839823\n",
            "33     \t [0.00959962 0.52806354 0.8762515 ]. \t  \u001b[92m3.7290166186316687\u001b[0m \t 3.7290166186316687\n",
            "34     \t [0.23733352 0.92142449 0.70207617]. \t  1.8674855209435428 \t 3.7290166186316687\n",
            "35     \t [0.01814484 0.61864911 0.78791629]. \t  3.3612275937144513 \t 3.7290166186316687\n",
            "36     \t [1.        0.7044096 1.       ]. \t  1.669520943975583 \t 3.7290166186316687\n",
            "37     \t [0.58834017 0.98023741 0.01225463]. \t  0.0002683514880368692 \t 3.7290166186316687\n",
            "38     \t [0.84999922 0.71827755 0.19869429]. \t  0.02183147118841281 \t 3.7290166186316687\n",
            "39     \t [0.3149974  0.53694132 0.8732597 ]. \t  \u001b[92m3.8070259040589045\u001b[0m \t 3.8070259040589045\n",
            "40     \t [0.33700154 0.44054228 0.79118152]. \t  3.1868279781218725 \t 3.8070259040589045\n",
            "41     \t [0.96955714 0.15218286 0.95842857]. \t  0.5336614519876771 \t 3.8070259040589045\n",
            "42     \t [0.00086149 0.6113122  0.81335031]. \t  3.570947683156863 \t 3.8070259040589045\n",
            "43     \t [0.62177277 0.51739244 0.86272347]. \t  3.7635450632533773 \t 3.8070259040589045\n",
            "44     \t [1.29196866e-08 9.75029688e-09 1.74546072e-08]. \t  0.06797413911335859 \t 3.8070259040589045\n",
            "45     \t [1.09159402e-08 1.04318877e-08 2.36291631e-01]. \t  0.5633555969360801 \t 3.8070259040589045\n",
            "46     \t [0.62169081 0.45884456 0.81046955]. \t  3.3886081580257006 \t 3.8070259040589045\n",
            "47     \t [0.34477519 0.61345743 0.83934127]. \t  3.7181734981927006 \t 3.8070259040589045\n",
            "48     \t [0.04921885 0.4917945  0.86038764]. \t  3.6780117534289762 \t 3.8070259040589045\n",
            "49     \t [0.54440859 0.56127732 0.84065639]. \t  \u001b[92m3.8084879412580155\u001b[0m \t 3.8084879412580155\n",
            "50     \t [0.2624437  0.55353598 0.85647263]. \t  \u001b[92m3.8594534110545906\u001b[0m \t 3.8594534110545906\n",
            "51     \t [0.00104149 0.36079523 0.89446598]. \t  2.512611324633963 \t 3.8594534110545906\n",
            "52     \t [0.9696223  0.5726719  0.78111681]. \t  3.119828001227031 \t 3.8594534110545906\n",
            "53     \t [0.82601095 0.54713529 0.84377482]. \t  3.7296297477315967 \t 3.8594534110545906\n",
            "54     \t [0.71493551 0.54939888 0.84814417]. \t  3.7795539581218742 \t 3.8594534110545906\n",
            "55     \t [0.9649036  0.00353229 0.80566085]. \t  0.25294226799284736 \t 3.8594534110545906\n",
            "56     \t [0.48681118 0.55606768 0.8667408 ]. \t  3.828722557691264 \t 3.8594534110545906\n",
            "57     \t [0.6744685  0.51855439 0.85078433]. \t  3.759709819195774 \t 3.8594534110545906\n",
            "58     \t [0.61148845 0.54199117 0.83251842]. \t  3.7629278628613383 \t 3.8594534110545906\n",
            "59     \t [0.72037297 0.51322862 0.82692655]. \t  3.6631383978003287 \t 3.8594534110545906\n",
            "60     \t [0.47470362 0.54180068 0.85623506]. \t  3.84083984213194 \t 3.8594534110545906\n",
            "61     \t [0.29856893 0.62033751 0.85603469]. \t  3.7139162808823816 \t 3.8594534110545906\n",
            "62     \t [0.76391245 0.56116636 0.81343306]. \t  3.579516541044418 \t 3.8594534110545906\n",
            "63     \t [0.17943106 0.63742331 0.81877806]. \t  3.531914664775532 \t 3.8594534110545906\n",
            "64     \t [0.12067787 0.40147466 0.90864087]. \t  2.7793061380109436 \t 3.8594534110545906\n",
            "65     \t [0.52513044 0.50031495 0.81739938]. \t  3.630014683290339 \t 3.8594534110545906\n",
            "66     \t [0.38311766 0.57716223 0.84585103]. \t  3.832003952745011 \t 3.8594534110545906\n",
            "67     \t [0.09943093 0.68736543 0.82491126]. \t  3.2378132907159856 \t 3.8594534110545906\n",
            "68     \t [0.64736464 0.53111796 0.84638289]. \t  3.784933867296838 \t 3.8594534110545906\n",
            "69     \t [0.44371265 0.5433961  0.8525024 ]. \t  3.847055794380256 \t 3.8594534110545906\n",
            "70     \t [0.65165326 0.52770733 0.84314647]. \t  3.772488899169741 \t 3.8594534110545906\n",
            "71     \t [1.         0.57924972 0.86541579]. \t  3.639209145852363 \t 3.8594534110545906\n",
            "72     \t [0.5072556  0.53115743 0.83558026]. \t  3.791848456837311 \t 3.8594534110545906\n",
            "73     \t [0.43461552 0.49182709 0.84004557]. \t  3.709362110445319 \t 3.8594534110545906\n",
            "74     \t [0.56657414 0.53444056 0.86483432]. \t  3.8046914754803844 \t 3.8594534110545906\n",
            "75     \t [0.40734199 0.59544685 0.84998257]. \t  3.792614729818915 \t 3.8594534110545906\n",
            "76     \t [0.58420817 0.56050585 0.84317701]. \t  3.8053560929981916 \t 3.8594534110545906\n",
            "77     \t [0.55062723 0.53603712 0.85250077]. \t  3.821366422099753 \t 3.8594534110545906\n",
            "78     \t [0.67645782 0.5317991  0.83047192]. \t  3.7256370670301706 \t 3.8594534110545906\n",
            "79     \t [0.44003    0.57018677 0.83855216]. \t  3.816175425336858 \t 3.8594534110545906\n",
            "80     \t [0.67186783 0.52315054 0.83950803]. \t  3.7495523170761773 \t 3.8594534110545906\n",
            "81     \t [0.44676363 0.57562435 0.8095429 ]. \t  3.643623802893298 \t 3.8594534110545906\n",
            "82     \t [0.65168806 0.52848867 0.84563876]. \t  3.7786760609834453 \t 3.8594534110545906\n",
            "83     \t [0.52011958 0.51860232 0.84298355]. \t  3.7868958512994353 \t 3.8594534110545906\n",
            "84     \t [0.63502276 0.54815335 0.83931801]. \t  3.783413761299897 \t 3.8594534110545906\n",
            "85     \t [0.58964396 0.55253521 0.83234367]. \t  3.7696231727331044 \t 3.8594534110545906\n",
            "86     \t [0.52897083 0.5259127  0.8532954 ]. \t  3.8105968629092066 \t 3.8594534110545906\n",
            "87     \t [0.33710517 0.47970261 0.79148268]. \t  3.3856572861482475 \t 3.8594534110545906\n",
            "88     \t [0.49878184 0.52266045 0.83726832]. \t  3.785214145158717 \t 3.8594534110545906\n",
            "89     \t [0.5025933  0.56793445 0.84964693]. \t  3.8302269038044425 \t 3.8594534110545906\n",
            "90     \t [0.54458152 0.52925417 0.81970658]. \t  3.7021875404380147 \t 3.8594534110545906\n",
            "91     \t [0.50664781 0.49179029 0.86479967]. \t  3.6930973414449495 \t 3.8594534110545906\n",
            "92     \t [0.4150006  0.54747824 0.8490258 ]. \t  3.8510374006048878 \t 3.8594534110545906\n",
            "93     \t [0.53726375 0.57444377 0.82529948]. \t  3.727141131073377 \t 3.8594534110545906\n",
            "94     \t [0.47537927 0.50245043 0.84721882]. \t  3.75434878018985 \t 3.8594534110545906\n",
            "95     \t [0.54248722 0.54449122 0.83002784]. \t  3.771768082802808 \t 3.8594534110545906\n",
            "96     \t [0.50551752 0.58369956 0.81638494]. \t  3.6609204875931796 \t 3.8594534110545906\n",
            "97     \t [0.58036601 0.50091355 0.86655901]. \t  3.7124117009599216 \t 3.8594534110545906\n",
            "98     \t [0.67789101 0.54895769 0.8439171 ]. \t  3.783090167778463 \t 3.8594534110545906\n",
            "99     \t [0.55257488 0.56932241 0.84752827]. \t  3.8141761406553476 \t 3.8594534110545906\n",
            "100    \t [0.59396662 0.50674501 0.86605811]. \t  3.731350508521097 \t 3.8594534110545906\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l98Nt7Tguvna",
        "outputId": "3829266d-fd2a-44bf-b673-9f4ff2f98c7e"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 12\r\n",
        "\r\n",
        "np.random.seed(run_num_12)\r\n",
        "surrogate_winner_12 = dtStudentProcess(d_cov_func, nu = df)\r\n",
        "\r\n",
        "winner_12 = dGPGO_stp(surrogate_winner_12, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_12.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.19151945 0.62210877 0.43772774]. \t  1.1006281843679786 \t 1.6482992955272024\n",
            "init   \t [0.78535858 0.77997581 0.27259261]. \t  0.07153771680480671 \t 1.6482992955272024\n",
            "init   \t [0.27646426 0.80187218 0.95813935]. \t  1.6482992955272024 \t 1.6482992955272024\n",
            "init   \t [0.87593263 0.35781727 0.50099513]. \t  0.2282556248207173 \t 1.6482992955272024\n",
            "init   \t [0.68346294 0.71270203 0.37025075]. \t  0.33032494760932407 \t 1.6482992955272024\n",
            "1      \t [0.1471569  0.029647   0.59389349]. \t  0.14183051318638495 \t 1.6482992955272024\n",
            "2      \t [1. 1. 1.]. \t  0.31688362070415665 \t 1.6482992955272024\n",
            "3      \t [0.00571508 0.95401436 0.6052526 ]. \t  \u001b[92m2.611993599963295\u001b[0m \t 2.611993599963295\n",
            "4      \t [0.13413814 0.98228353 0.15387565]. \t  0.021772348581531112 \t 2.611993599963295\n",
            "5      \t [0.03603937 0.06122445 0.02316075]. \t  0.11629681622021096 \t 2.611993599963295\n",
            "6      \t [0.00761147 0.89633798 0.99755295]. \t  0.7296905183544732 \t 2.611993599963295\n",
            "7      \t [0.96973702 0.00389665 0.18429294]. \t  0.24228960917345527 \t 2.611993599963295\n",
            "8      \t [0.69650759 0.12349388 0.98261972]. \t  0.3562779530885304 \t 2.611993599963295\n",
            "9      \t [0.03268502 0.19684067 0.90085476]. \t  1.03917682720538 \t 2.611993599963295\n",
            "10     \t [0.88302627 1.         0.59967214]. \t  0.4530788007661746 \t 2.611993599963295\n",
            "11     \t [0.89972444 0.34829979 0.00113489]. \t  0.030030660356288232 \t 2.611993599963295\n",
            "12     \t [0.99120566 0.65579508 0.91455073]. \t  \u001b[92m3.061519255743165\u001b[0m \t 3.061519255743165\n",
            "13     \t [0.99403226 0.09534562 0.75674346]. \t  0.5366207668344736 \t 3.061519255743165\n",
            "14     \t [0.89584401 0.59674704 0.98535625]. \t  2.2722443810107142 \t 3.061519255743165\n",
            "15     \t [0.02935112 0.70100715 0.8213863 ]. \t  \u001b[92m3.105400376495039\u001b[0m \t 3.105400376495039\n",
            "16     \t [0.6742932  0.04296573 0.00792506]. \t  0.09509632934328453 \t 3.105400376495039\n",
            "17     \t [0.24689106 0.50485043 0.90108533]. \t  \u001b[92m3.526259091369143\u001b[0m \t 3.526259091369143\n",
            "18     \t [0.01301595 0.75683712 0.01253624]. \t  0.0020042105364007873 \t 3.526259091369143\n",
            "19     \t [0.91040076 0.92481574 0.05170138]. \t  0.0003658446185011338 \t 3.526259091369143\n",
            "20     \t [1.         0.8130577  0.73680515]. \t  1.2111210783375246 \t 3.526259091369143\n",
            "21     \t [0.93769202 0.32982181 0.99029746]. \t  1.2702632473521085 \t 3.526259091369143\n",
            "22     \t [0.34407222 0.73285006 0.00742643]. \t  0.003227309162114178 \t 3.526259091369143\n",
            "23     \t [0.4761812  0.04736164 0.57840098]. \t  0.15632108940008127 \t 3.526259091369143\n",
            "24     \t [0.61376602 0.98479491 0.07769178]. \t  0.0014910490882393474 \t 3.526259091369143\n",
            "25     \t [0.27656462 0.09369575 0.90288524]. \t  0.46840456394523466 \t 3.526259091369143\n",
            "26     \t [0.55594269 0.99894095 0.75027313]. \t  0.7828765493819902 \t 3.526259091369143\n",
            "27     \t [0.02534579 0.36195496 0.68089662]. \t  1.570928401612856 \t 3.526259091369143\n",
            "28     \t [0.29436355 0.27296824 0.1593644 ]. \t  0.5443437301523788 \t 3.526259091369143\n",
            "29     \t [0.08656325 0.44065089 0.97538334]. \t  2.1585127598530582 \t 3.526259091369143\n",
            "30     \t [0.24497218 0.83134644 0.69037729]. \t  2.2853880490779437 \t 3.526259091369143\n",
            "31     \t [0.22535099 0.00969059 0.04648757]. \t  0.19403587992974966 \t 3.526259091369143\n",
            "32     \t [0.7257195  0.41426715 0.81555553]. \t  3.1099588007218344 \t 3.526259091369143\n",
            "33     \t [0.99389027 0.99914332 0.3695516 ]. \t  0.08791536542523454 \t 3.526259091369143\n",
            "34     \t [0.03025806 0.00732578 0.32462394]. \t  0.5702462055951378 \t 3.526259091369143\n",
            "35     \t [0.46121843 0.51240483 0.93956677]. \t  3.066495141042437 \t 3.526259091369143\n",
            "36     \t [0.52928354 0.54573398 0.74066771]. \t  2.8442325981960384 \t 3.526259091369143\n",
            "37     \t [0.42730413 0.38642465 0.81741835]. \t  2.943066601679501 \t 3.526259091369143\n",
            "38     \t [0.00550297 0.44096151 0.04518998]. \t  0.05381483342407054 \t 3.526259091369143\n",
            "39     \t [0.46448684 0.56547857 0.85832656]. \t  \u001b[92m3.8413443343406346\u001b[0m \t 3.8413443343406346\n",
            "40     \t [0.02401995 0.96170179 0.84719456]. \t  0.936001388856086 \t 3.8413443343406346\n",
            "41     \t [0.00943958 0.76599981 0.32252877]. \t  0.5251438070498804 \t 3.8413443343406346\n",
            "42     \t [0.58466043 0.50963044 0.02612565]. \t  0.03260333236092526 \t 3.8413443343406346\n",
            "43     \t [0.25862046 0.58769485 0.84060744]. \t  3.8078336899180316 \t 3.8413443343406346\n",
            "44     \t [0.71043884 0.72408631 0.89067892]. \t  2.807806993646862 \t 3.8413443343406346\n",
            "45     \t [0.42853442 0.57106047 0.81852894]. \t  3.7200807934545157 \t 3.8413443343406346\n",
            "46     \t [0.31823856 0.5760559  0.84729061]. \t  3.8411767444238416 \t 3.8413443343406346\n",
            "47     \t [0.30399611 0.59092191 0.8383368 ]. \t  3.7929016968743166 \t 3.8413443343406346\n",
            "48     \t [0.53688159 0.53465101 0.87456408]. \t  3.7795224760474344 \t 3.8413443343406346\n",
            "49     \t [0.35857963 0.53813046 0.85591931]. \t  \u001b[92m3.8494009811592407\u001b[0m \t 3.8494009811592407\n",
            "50     \t [0.42320828 0.67457814 0.79066521]. \t  3.076059402632968 \t 3.8494009811592407\n",
            "51     \t [0.28038204 0.56581616 0.85128424]. \t  \u001b[92m3.8568689666059823\u001b[0m \t 3.8568689666059823\n",
            "52     \t [0.97360337 0.5001345  0.86784386]. \t  3.581366682231876 \t 3.8568689666059823\n",
            "53     \t [0.13339033 0.57567694 0.8280431 ]. \t  3.7764310064904523 \t 3.8568689666059823\n",
            "54     \t [0.12604905 0.60567383 0.85456067]. \t  3.7587581112174977 \t 3.8568689666059823\n",
            "55     \t [0.99797936 0.5072598  0.12414391]. \t  0.03625419202104712 \t 3.8568689666059823\n",
            "56     \t [0.51213969 0.98563925 0.36225843]. \t  0.5354093683180847 \t 3.8568689666059823\n",
            "57     \t [0.98578307 0.52424487 0.8671148 ]. \t  3.643121229603144 \t 3.8568689666059823\n",
            "58     \t [0.41785364 0.56149216 0.77829244]. \t  3.366958203431307 \t 3.8568689666059823\n",
            "59     \t [0.99900374 0.52621658 0.71851516]. \t  2.2583311917023345 \t 3.8568689666059823\n",
            "60     \t [0.94127581 0.02279573 0.95299035]. \t  0.17178578813362333 \t 3.8568689666059823\n",
            "61     \t [0.32061442 0.59578658 0.83730642]. \t  3.775490964841466 \t 3.8568689666059823\n",
            "62     \t [0.944549   0.01940069 0.0303243 ]. \t  0.062405931694092066 \t 3.8568689666059823\n",
            "63     \t [0.67529336 0.54287012 0.88668649]. \t  3.6952117361752315 \t 3.8568689666059823\n",
            "64     \t [0.01172646 0.02098269 0.83819529]. \t  0.291462417776862 \t 3.8568689666059823\n",
            "65     \t [0.26661969 0.55933459 0.84181833]. \t  3.848589219350985 \t 3.8568689666059823\n",
            "66     \t [0.24636497 0.53261656 0.86074961]. \t  3.8348000093715897 \t 3.8568689666059823\n",
            "67     \t [0.82003407 0.5753829  0.83521423]. \t  3.6742301901114276 \t 3.8568689666059823\n",
            "68     \t [0.54336594 0.65059998 0.89874566]. \t  3.3561093848128807 \t 3.8568689666059823\n",
            "69     \t [0.75886728 0.50194001 0.85562063]. \t  3.6879477839642103 \t 3.8568689666059823\n",
            "70     \t [0.75032195 0.53674233 0.85084489]. \t  3.764936377037696 \t 3.8568689666059823\n",
            "71     \t [0.12764831 0.58570299 0.85499104]. \t  3.8135661345028007 \t 3.8568689666059823\n",
            "72     \t [0.28937295 0.57344448 0.84881213]. \t  3.847299070976 \t 3.8568689666059823\n",
            "73     \t [0.30476457 0.60162608 0.8316369 ]. \t  3.738811188724947 \t 3.8568689666059823\n",
            "74     \t [0.84350692 0.52418764 0.83796952]. \t  3.686326289897733 \t 3.8568689666059823\n",
            "75     \t [0.54119408 0.60390922 0.81667899]. \t  3.5911190972000497 \t 3.8568689666059823\n",
            "76     \t [0.01071901 0.54582504 0.89325608]. \t  3.6465121388071715 \t 3.8568689666059823\n",
            "77     \t [0.79594041 0.53593131 0.84979996]. \t  3.746866344334949 \t 3.8568689666059823\n",
            "78     \t [0.27377542 0.54576111 0.81608921]. \t  3.736952730508469 \t 3.8568689666059823\n",
            "79     \t [0.49796248 0.53765083 0.86171765]. \t  3.827299929004464 \t 3.8568689666059823\n",
            "80     \t [0.27907801 0.58442815 0.83938549]. \t  3.8117786061732435 \t 3.8568689666059823\n",
            "81     \t [0.72705501 0.53076754 0.86478544]. \t  3.7579494264038797 \t 3.8568689666059823\n",
            "82     \t [0.78099313 0.00136573 0.81943467]. \t  0.24888534667494644 \t 3.8568689666059823\n",
            "83     \t [0.77162591 0.53266581 0.84323364]. \t  3.7402375814280466 \t 3.8568689666059823\n",
            "84     \t [0.27176941 0.61157548 0.80671928]. \t  3.564433676761565 \t 3.8568689666059823\n",
            "85     \t [0.7875522  0.5488036  0.84269573]. \t  3.74159663437396 \t 3.8568689666059823\n",
            "86     \t [0.16604301 0.62698856 0.83159305]. \t  3.6380995621739225 \t 3.8568689666059823\n",
            "87     \t [0.1178865  0.60364099 0.85199622]. \t  3.7639429181639206 \t 3.8568689666059823\n",
            "88     \t [0.09834065 0.55090902 0.86701586]. \t  3.8170133349071964 \t 3.8568689666059823\n",
            "89     \t [0.27829981 0.59168148 0.82145049]. \t  3.7203444607518916 \t 3.8568689666059823\n",
            "90     \t [0.92740967 0.56251306 0.84403827]. \t  3.680187758055812 \t 3.8568689666059823\n",
            "91     \t [0.79428439 0.53924567 0.86022033]. \t  3.752313886611797 \t 3.8568689666059823\n",
            "92     \t [0.38344914 0.57962998 0.83664952]. \t  3.804461604065813 \t 3.8568689666059823\n",
            "93     \t [0.57132577 0.52258713 0.87100174]. \t  3.7644901563654303 \t 3.8568689666059823\n",
            "94     \t [0.35068597 0.54569319 0.84784923]. \t  3.854815009101398 \t 3.8568689666059823\n",
            "95     \t [0.36375687 0.52158185 0.83291233]. \t  3.787234760378316 \t 3.8568689666059823\n",
            "96     \t [0.63281646 0.51330002 0.86484501]. \t  3.7456455786700182 \t 3.8568689666059823\n",
            "97     \t [0.07031044 0.60950111 0.85478887]. \t  3.732828733144359 \t 3.8568689666059823\n",
            "98     \t [0.33498731 0.58292472 0.84328665]. \t  3.8213452994307167 \t 3.8568689666059823\n",
            "99     \t [0.19636871 0.61752957 0.82559573]. \t  3.6569440414146666 \t 3.8568689666059823\n",
            "100    \t [0.3940979  0.57422022 0.81937432]. \t  3.728187585145805 \t 3.8568689666059823\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Bpn-kmNuvqC",
        "outputId": "bc92598b-74e5-4aca-fc6f-53dd2b541343"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 13 \r\n",
        "\r\n",
        "np.random.seed(run_num_13)\r\n",
        "surrogate_winner_13 = dtStudentProcess(d_cov_func, nu = df)\r\n",
        "\r\n",
        "winner_13 = dGPGO_stp(surrogate_winner_13, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_13.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.66064431 0.41360065 0.66810256]. \t  1.5457680899590245 \t 2.6697919207500047\n",
            "init   \t [0.22994342 0.80767834 0.63681846]. \t  2.6697919207500047 \t 2.6697919207500047\n",
            "init   \t [0.17219385 0.26038587 0.91531999]. \t  1.4685599870430508 \t 2.6697919207500047\n",
            "init   \t [0.46281551 0.12019095 0.88088551]. \t  0.639297028201682 \t 2.6697919207500047\n",
            "init   \t [0.22621895 0.81144033 0.44587892]. \t  1.960293029067079 \t 2.6697919207500047\n",
            "1      \t [1. 1. 1.]. \t  0.31688362070415665 \t 2.6697919207500047\n",
            "2      \t [0.91418486 0.13498533 0.09072509]. \t  0.16031997849670143 \t 2.6697919207500047\n",
            "3      \t [0.04920409 0.08370037 0.06705598]. \t  0.21857769543557976 \t 2.6697919207500047\n",
            "4      \t [0.94262441 0.88509784 0.1558813 ]. \t  0.003859098506520812 \t 2.6697919207500047\n",
            "5      \t [0.02210984 0.92408402 0.84182838]. \t  1.198687679396682 \t 2.6697919207500047\n",
            "6      \t [0.48123301 0.99062908 0.95037097]. \t  0.5106032182213063 \t 2.6697919207500047\n",
            "7      \t [0.01638524 0.07150708 0.45513128]. \t  0.2535329679855875 \t 2.6697919207500047\n",
            "8      \t [3.74128618e-01 2.90520797e-04 6.79335015e-03]. \t  0.11392633904207171 \t 2.6697919207500047\n",
            "9      \t [0.95933324 0.17958198 0.89986477]. \t  0.9093096856520038 \t 2.6697919207500047\n",
            "10     \t [0.97362261 0.98423065 0.57740869]. \t  0.32120330668849173 \t 2.6697919207500047\n",
            "11     \t [0.8932185  0.60925949 1.        ]. \t  1.9983590598681282 \t 2.6697919207500047\n",
            "12     \t [0.10902588 0.56727294 0.75965328]. \t  \u001b[92m3.199449416933006\u001b[0m \t 3.199449416933006\n",
            "13     \t [0.37575655 0.66094399 0.94898749]. \t  2.7365801907231426 \t 3.199449416933006\n",
            "14     \t [0.24257297 0.53255076 0.43810854]. \t  0.7052130383385855 \t 3.199449416933006\n",
            "15     \t [0.08000187 0.93009615 0.12745989]. \t  0.012709563576291133 \t 3.199449416933006\n",
            "16     \t [0.9815211  0.02984986 0.57513473]. \t  0.10169848723601846 \t 3.199449416933006\n",
            "17     \t [0.61968117 0.95660484 0.5191739 ]. \t  1.2532106243239198 \t 3.199449416933006\n",
            "18     \t [0.00578526 0.72353919 0.33883358]. \t  0.5900694172067447 \t 3.199449416933006\n",
            "19     \t [0.97868508 0.59499381 0.55683648]. \t  0.47874657380409247 \t 3.199449416933006\n",
            "20     \t [0.65768556 0.47774082 0.00272863]. \t  0.02598023887983216 \t 3.199449416933006\n",
            "21     \t [0.28741398 0.97418176 0.02885626]. \t  0.0007295725322032098 \t 3.199449416933006\n",
            "22     \t [0.15681555 0.9714754  0.4517    ]. \t  1.9723125563048933 \t 3.199449416933006\n",
            "23     \t [0.03636463 0.68862757 0.91354292]. \t  2.9674484544015045 \t 3.199449416933006\n",
            "24     \t [0.7053026  0.74671579 0.81210926]. \t  2.4937453496576114 \t 3.199449416933006\n",
            "25     \t [0.70437367 0.4021351  1.        ]. \t  1.581382339409177 \t 3.199449416933006\n",
            "26     \t [0.98664644 0.21862734 0.01483398]. \t  0.04241427577940297 \t 3.199449416933006\n",
            "27     \t [0.00603379 0.27560719 0.83117413]. \t  1.8885259834385257 \t 3.199449416933006\n",
            "28     \t [0.88040149 0.00702259 0.07357619]. \t  0.13111022990975454 \t 3.199449416933006\n",
            "29     \t [0.22282629 0.74851791 0.8191419 ]. \t  2.7286210044973505 \t 3.199449416933006\n",
            "30     \t [5.01818452e-04 5.07514441e-01 9.62903786e-01]. \t  2.633819292834559 \t 3.199449416933006\n",
            "31     \t [0.7507947  0.92001736 0.99704114]. \t  0.620577463292687 \t 3.199449416933006\n",
            "32     \t [0.19817524 0.03404759 0.93123286]. \t  0.22931239669784867 \t 3.199449416933006\n",
            "33     \t [0.71872564 0.94425228 0.01912161]. \t  0.00028770718318120924 \t 3.199449416933006\n",
            "34     \t [0.99095656 0.48820892 0.82781602]. \t  \u001b[92m3.4891976174521333\u001b[0m \t 3.4891976174521333\n",
            "35     \t [1.         0.7494621  0.83876731]. \t  2.4692707778629877 \t 3.4891976174521333\n",
            "36     \t [1.         0.49643237 0.93222037]. \t  3.001624996064337 \t 3.4891976174521333\n",
            "37     \t [0.01930629 0.65373049 0.67122893]. \t  2.4286776348791137 \t 3.4891976174521333\n",
            "38     \t [0.31185165 0.70508817 0.02388579]. \t  0.005696202989178087 \t 3.4891976174521333\n",
            "39     \t [0.44855432 0.01810252 0.29544258]. \t  0.8691875649467243 \t 3.4891976174521333\n",
            "40     \t [0.00392926 0.37452846 0.02666966]. \t  0.06087186345745652 \t 3.4891976174521333\n",
            "41     \t [0.13313359 0.00718412 0.60811446]. \t  0.12745866282917478 \t 3.4891976174521333\n",
            "42     \t [0.91670578 0.51559066 0.8306563 ]. \t  \u001b[92m3.6113167664868584\u001b[0m \t 3.6113167664868584\n",
            "43     \t [0.79761722 1.         0.809871  ]. \t  0.5579565130924009 \t 3.6113167664868584\n",
            "44     \t [0.94249181 0.38073938 0.81170512]. \t  2.7707824255526363 \t 3.6113167664868584\n",
            "45     \t [6.02117100e-11 3.52420119e-11 4.99385745e-11]. \t  0.0679741166592386 \t 3.6113167664868584\n",
            "46     \t [0.43409753 0.96598791 0.28398903]. \t  0.22325574543686724 \t 3.6113167664868584\n",
            "47     \t [0.73781145 0.00921672 0.75693401]. \t  0.2627851256104905 \t 3.6113167664868584\n",
            "48     \t [0.96838512 0.66142807 0.00765085]. \t  0.0023498468654493433 \t 3.6113167664868584\n",
            "49     \t [0.93151032 0.55584353 0.83229934]. \t  \u001b[92m3.6378351383838448\u001b[0m \t 3.6378351383838448\n",
            "50     \t [0.16894869 0.61723918 0.96988865]. \t  2.5589916595649407 \t 3.6378351383838448\n",
            "51     \t [0.61940144 0.58289    0.85798227]. \t  \u001b[92m3.7811265055932632\u001b[0m \t 3.7811265055932632\n",
            "52     \t [1.05651539e-08 9.20761292e-09 1.46450352e-01]. \t  0.3740881766150131 \t 3.7811265055932632\n",
            "53     \t [0.46608174 0.61245006 0.81715549]. \t  3.588502829255023 \t 3.7811265055932632\n",
            "54     \t [0.39032261 0.55311514 0.82495477]. \t  3.7790686519060617 \t 3.7811265055932632\n",
            "55     \t [0.96940131 0.90752054 0.04276951]. \t  0.0002682684267714719 \t 3.7811265055932632\n",
            "56     \t [0.5521791  0.54760559 0.82416665]. \t  3.7388408025805924 \t 3.7811265055932632\n",
            "57     \t [0.32522081 0.60322408 0.80368081]. \t  3.5604619937724475 \t 3.7811265055932632\n",
            "58     \t [0.28309708 0.60345168 0.82136295]. \t  3.685506946975298 \t 3.7811265055932632\n",
            "59     \t [0.0329684  0.98787527 0.9805881 ]. \t  0.42872411477246064 \t 3.7811265055932632\n",
            "60     \t [0.47351221 0.5501445  0.83357589]. \t  \u001b[92m3.804300829664129\u001b[0m \t 3.804300829664129\n",
            "61     \t [0.01852168 0.04590208 0.86305604]. \t  0.3480235414756042 \t 3.804300829664129\n",
            "62     \t [0.61910366 0.60202556 0.82825821]. \t  3.6429097169036373 \t 3.804300829664129\n",
            "63     \t [0.5009653  0.54802504 0.84522531]. \t  \u001b[92m3.8322594161587755\u001b[0m \t 3.8322594161587755\n",
            "64     \t [0.49093436 0.51430466 0.8911607 ]. \t  3.6410751188815245 \t 3.8322594161587755\n",
            "65     \t [0.66762618 0.54672575 0.83558305]. \t  3.7595745910203706 \t 3.8322594161587755\n",
            "66     \t [0.64140373 0.56679931 0.82864728]. \t  3.723606404814642 \t 3.8322594161587755\n",
            "67     \t [0.54244184 0.50003732 0.86656665]. \t  3.7159640702349956 \t 3.8322594161587755\n",
            "68     \t [0.3933551  0.56174591 0.83989624]. \t  \u001b[92m3.8354278025962927\u001b[0m \t 3.8354278025962927\n",
            "69     \t [0.55858389 0.5317298  0.86318147]. \t  3.8056879354858735 \t 3.8354278025962927\n",
            "70     \t [0.7216318  0.50376923 0.87017953]. \t  3.677465432233884 \t 3.8354278025962927\n",
            "71     \t [0.53020834 0.56893848 0.84909403]. \t  3.8223465639584675 \t 3.8354278025962927\n",
            "72     \t [0.60935125 0.57532029 0.83848337]. \t  3.7653322346887674 \t 3.8354278025962927\n",
            "73     \t [0.3948404  0.59453636 0.82484983]. \t  3.715361122404502 \t 3.8354278025962927\n",
            "74     \t [0.58212183 0.58224386 0.88011211]. \t  3.7362875572513397 \t 3.8354278025962927\n",
            "75     \t [0.71549866 0.50605584 0.82322694]. \t  3.6274475387801277 \t 3.8354278025962927\n",
            "76     \t [0.49287464 0.54874964 0.87335229]. \t  3.805277930485082 \t 3.8354278025962927\n",
            "77     \t [0.63119619 0.5441101  0.88932923]. \t  3.6891436614998856 \t 3.8354278025962927\n",
            "78     \t [0.57258374 0.50623227 0.86573556]. \t  3.7347839153562594 \t 3.8354278025962927\n",
            "79     \t [0.40549186 0.49470802 0.78320412]. \t  3.3477281365548146 \t 3.8354278025962927\n",
            "80     \t [0.61900644 0.57567621 0.83019058]. \t  3.725318422414175 \t 3.8354278025962927\n",
            "81     \t [0.77188742 0.58862194 0.89803571]. \t  3.557402551457265 \t 3.8354278025962927\n",
            "82     \t [0.62949627 0.54667246 0.86157013]. \t  3.8067638531525354 \t 3.8354278025962927\n",
            "83     \t [0.49750687 0.53074294 0.86374964]. \t  3.8138436492107033 \t 3.8354278025962927\n",
            "84     \t [0.50801895 0.64857311 0.78415309]. \t  3.1182121220613324 \t 3.8354278025962927\n",
            "85     \t [0.4876164  0.55796725 0.85961496]. \t  \u001b[92m3.840611538877403\u001b[0m \t 3.840611538877403\n",
            "86     \t [0.46414927 0.53412258 0.85736876]. \t  3.833129579532729 \t 3.840611538877403\n",
            "87     \t [0.45458558 0.55732375 0.86320295]. \t  \u001b[92m3.840820160074265\u001b[0m \t 3.840820160074265\n",
            "88     \t [0.54662207 0.53843379 0.86470954]. \t  3.813917458741164 \t 3.840820160074265\n",
            "89     \t [0.50711214 0.60160627 0.87229572]. \t  3.7334036263483377 \t 3.840820160074265\n",
            "90     \t [0.61161438 0.53451934 0.86609557]. \t  3.7919848095310353 \t 3.840820160074265\n",
            "91     \t [0.00546279 0.76329021 0.00653021]. \t  0.001655595788681572 \t 3.840820160074265\n",
            "92     \t [0.76393116 0.43812402 0.84668039]. \t  3.3526424371430634 \t 3.840820160074265\n",
            "93     \t [0.61153397 0.54534242 0.85233782]. \t  3.8141240646128285 \t 3.840820160074265\n",
            "94     \t [0.31356744 0.53652796 0.82149678]. \t  3.761080771759825 \t 3.840820160074265\n",
            "95     \t [0.48710965 0.57423236 0.85031828]. \t  3.825903779680892 \t 3.840820160074265\n",
            "96     \t [0.55910482 0.55161996 0.84583673]. \t  3.8208566091319374 \t 3.840820160074265\n",
            "97     \t [0.5142556  0.55037793 0.8128646 ]. \t  3.673858770635034 \t 3.840820160074265\n",
            "98     \t [0.65011516 0.61256525 0.82329029]. \t  3.5594929984085377 \t 3.840820160074265\n",
            "99     \t [0.82383056 0.55609337 0.85564292]. \t  3.7465324829915856 \t 3.840820160074265\n",
            "100    \t [0.36384476 0.49283132 0.8482177 ]. \t  3.728783511220886 \t 3.840820160074265\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9NdFRXtPuvsP",
        "outputId": "eda8f2a0-3d7b-47ea-87b5-0dc533323da5"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 14 \r\n",
        "\r\n",
        "np.random.seed(run_num_14)\r\n",
        "surrogate_winner_14 = dtStudentProcess(d_cov_func, nu = df)\r\n",
        "\r\n",
        "winner_14 = dGPGO_stp(surrogate_winner_14, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_14.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.64755105 0.50714969 0.52834138]. \t  0.5963212059988954 \t 2.610000357863649\n",
            "init   \t [0.8962852  0.69999119 0.7142971 ]. \t  1.7197848290620104 \t 2.610000357863649\n",
            "init   \t [0.71733838 0.22281946 0.17515452]. \t  0.48166052848103497 \t 2.610000357863649\n",
            "init   \t [0.45684149 0.92873843 0.00988589]. \t  0.0004588015757462679 \t 2.610000357863649\n",
            "init   \t [0.08992219 0.85020027 0.48562106]. \t  2.610000357863649 \t 2.610000357863649\n",
            "1      \t [0.01802055 0.09120485 0.95771228]. \t  0.32586810305086356 \t 2.610000357863649\n",
            "2      \t [0.01891855 0.20464591 0.06840072]. \t  0.19574071602531773 \t 2.610000357863649\n",
            "3      \t [0.83752665 0.08536872 0.9061313 ]. \t  0.4216559227401855 \t 2.610000357863649\n",
            "4      \t [0.59520691 1.         1.        ]. \t  0.33035717964975087 \t 2.610000357863649\n",
            "5      \t [0.98759968 0.76622005 0.01354983]. \t  0.000719399745489602 \t 2.610000357863649\n",
            "6      \t [0.9941856  0.02190666 0.2793216 ]. \t  0.2815943490720258 \t 2.610000357863649\n",
            "7      \t [1. 1. 1.]. \t  0.31688362070415665 \t 2.610000357863649\n",
            "8      \t [0.08078959 0.63358833 0.88075537]. \t  \u001b[92m3.5629725223023776\u001b[0m \t 3.5629725223023776\n",
            "9      \t [0.07697431 0.95160856 0.90030099]. \t  0.8709936780770815 \t 3.5629725223023776\n",
            "10     \t [0.79888221 0.54492962 1.        ]. \t  2.041154519804912 \t 3.5629725223023776\n",
            "11     \t [0.11076469 0.6293393  0.12293801]. \t  0.03791219398507445 \t 3.5629725223023776\n",
            "12     \t [0.76848406 0.99096603 0.50788689]. \t  0.6752524510378968 \t 3.5629725223023776\n",
            "13     \t [0.49461843 0.01210847 0.49549672]. \t  0.2021305530067693 \t 3.5629725223023776\n",
            "14     \t [0.80116048 0.0240518  0.04793146]. \t  0.12361098927566984 \t 3.5629725223023776\n",
            "15     \t [0.03404035 0.31030654 0.45897457]. \t  0.3152238349204707 \t 3.5629725223023776\n",
            "16     \t [0.0726344  0.89759939 0.07077107]. \t  0.003184736964847079 \t 3.5629725223023776\n",
            "17     \t [0.33319617 0.02386789 0.01858048]. \t  0.14278722241996875 \t 3.5629725223023776\n",
            "18     \t [0.44383032 0.00542148 0.96280111]. \t  0.13649263674355352 \t 3.5629725223023776\n",
            "19     \t [0.27347816 0.49158933 0.96864059]. \t  2.5184453377147067 \t 3.5629725223023776\n",
            "20     \t [1.         0.69595469 1.        ]. \t  1.7068648546088678 \t 3.5629725223023776\n",
            "21     \t [0.00720762 0.5389815  0.98519908]. \t  2.308604195643187 \t 3.5629725223023776\n",
            "22     \t [0.41352095 0.91275938 0.67478759]. \t  1.7805488622529007 \t 3.5629725223023776\n",
            "23     \t [0.99058925 0.42346493 0.00201975]. \t  0.014851774470607197 \t 3.5629725223023776\n",
            "24     \t [0.37436753 0.80231701 0.98869201]. \t  1.3345021196260585 \t 3.5629725223023776\n",
            "25     \t [0.96290156 0.32856771 0.67456607]. \t  1.2469334526081441 \t 3.5629725223023776\n",
            "26     \t [0.03442208 0.00207141 0.00637148]. \t  0.08125030344706993 \t 3.5629725223023776\n",
            "27     \t [0.1165188  0.54230006 0.73626354]. \t  2.9171037715776698 \t 3.5629725223023776\n",
            "28     \t [0.69871654 0.54619983 0.04656949]. \t  0.02666998518107143 \t 3.5629725223023776\n",
            "29     \t [0.03949541 0.69152419 0.78996234]. \t  3.0256217850010505 \t 3.5629725223023776\n",
            "30     \t [0.9732867  0.51700296 0.40498806]. \t  0.1033906739119774 \t 3.5629725223023776\n",
            "31     \t [0.14584646 0.0382773  0.71480581]. \t  0.3006943987297136 \t 3.5629725223023776\n",
            "32     \t [0.19550795 0.95699894 0.3333358 ]. \t  0.6296314627280002 \t 3.5629725223023776\n",
            "33     \t [0.89846564 0.29703571 0.99084188]. \t  1.0802413033411005 \t 3.5629725223023776\n",
            "34     \t [0.93473176 0.92166157 0.01959159]. \t  0.0001641234933142684 \t 3.5629725223023776\n",
            "35     \t [0.14292429 0.06175253 0.25727748]. \t  0.8300223955729186 \t 3.5629725223023776\n",
            "36     \t [0.81932335 0.84522287 0.95183633]. \t  1.3333038942769813 \t 3.5629725223023776\n",
            "37     \t [0.94621015 0.96859764 0.75868709]. \t  0.5483592700259958 \t 3.5629725223023776\n",
            "38     \t [0.92087899 0.00326155 0.8024653 ]. \t  0.2537477990639085 \t 3.5629725223023776\n",
            "39     \t [0.02914675 0.97559128 0.70900661]. \t  1.579376188564309 \t 3.5629725223023776\n",
            "40     \t [0.16635323 0.70115698 0.72903961]. \t  2.663652143734816 \t 3.5629725223023776\n",
            "41     \t [0.01838238 0.77763667 0.43975725]. \t  1.8166954308419465 \t 3.5629725223023776\n",
            "42     \t [0.06485995 0.45776903 0.87787298]. \t  3.4325272348843647 \t 3.5629725223023776\n",
            "43     \t [0.14863403 0.30884732 0.99991979]. \t  1.0806252332487487 \t 3.5629725223023776\n",
            "44     \t [0.00081795 0.00902225 0.14625403]. \t  0.38190693370873513 \t 3.5629725223023776\n",
            "45     \t [0.43512531 0.44306827 0.85263501]. \t  3.4442327464702798 \t 3.5629725223023776\n",
            "46     \t [0.01895914 0.30512304 0.85105212]. \t  2.1590551087699463 \t 3.5629725223023776\n",
            "47     \t [0.6895558  0.88301604 0.18908781]. \t  0.02150848837581003 \t 3.5629725223023776\n",
            "48     \t [0.9648279  0.02045784 0.98315012]. \t  0.13010469517902026 \t 3.5629725223023776\n",
            "49     \t [0.67779545 0.55673727 0.83442384]. \t  \u001b[92m3.748887232097932\u001b[0m \t 3.748887232097932\n",
            "50     \t [0.99352118 0.59530759 0.87575168]. \t  3.580711531996167 \t 3.748887232097932\n",
            "51     \t [0.23583286 0.55180261 0.83685406]. \t  \u001b[92m3.8358976067261676\u001b[0m \t 3.8358976067261676\n",
            "52     \t [0.55332973 0.63475871 0.85247443]. \t  3.597451360308599 \t 3.8358976067261676\n",
            "53     \t [0.77506735 0.45643162 0.85989938]. \t  3.459303897065457 \t 3.8358976067261676\n",
            "54     \t [0.38986295 0.52899791 0.81242769]. \t  3.6863635906832815 \t 3.8358976067261676\n",
            "55     \t [0.47283169 0.53030118 0.86242395]. \t  3.819301468116767 \t 3.8358976067261676\n",
            "56     \t [0.95035872 0.51443861 0.88191804]. \t  3.57833066195847 \t 3.8358976067261676\n",
            "57     \t [0.00384158 0.99674942 0.21964385]. \t  0.08612619945926739 \t 3.8358976067261676\n",
            "58     \t [0.93751258 0.99748509 0.29136268]. \t  0.041163016913581214 \t 3.8358976067261676\n",
            "59     \t [0.69241133 0.51947754 0.82781224]. \t  3.689556310045849 \t 3.8358976067261676\n",
            "60     \t [0.62686541 0.5490926  0.85102081]. \t  3.810133669432999 \t 3.8358976067261676\n",
            "61     \t [0.47348823 0.47770581 0.82262193]. \t  3.58217579265568 \t 3.8358976067261676\n",
            "62     \t [0.55469801 0.54897682 0.85721654]. \t  3.829367215688516 \t 3.8358976067261676\n",
            "63     \t [0.42436138 0.51155118 0.83215289]. \t  3.754286375869853 \t 3.8358976067261676\n",
            "64     \t [0.4093    0.4935677 0.8634853]. \t  3.7139043415121367 \t 3.8358976067261676\n",
            "65     \t [0.44352735 0.53890849 0.83838019]. \t  3.8211354940833155 \t 3.8358976067261676\n",
            "66     \t [0.47594698 0.57926026 0.80768011]. \t  3.613066791665288 \t 3.8358976067261676\n",
            "67     \t [0.50985081 0.59039884 0.85008085]. \t  3.7871637161665688 \t 3.8358976067261676\n",
            "68     \t [1.         0.51060787 0.96333658]. \t  2.5725144633177437 \t 3.8358976067261676\n",
            "69     \t [0.01778112 0.69827601 0.00774369]. \t  0.0033931988764575763 \t 3.8358976067261676\n",
            "70     \t [0.72670452 0.51183269 0.85590195]. \t  3.7281004626422627 \t 3.8358976067261676\n",
            "71     \t [0.66172295 0.51623797 0.85313045]. \t  3.758740696412397 \t 3.8358976067261676\n",
            "72     \t [0.54855145 0.48633435 0.85382721]. \t  3.679914043773034 \t 3.8358976067261676\n",
            "73     \t [0.71813501 0.54698036 0.82659718]. \t  3.698393757272914 \t 3.8358976067261676\n",
            "74     \t [0.49582226 0.46084021 0.85624352]. \t  3.550488066591885 \t 3.8358976067261676\n",
            "75     \t [0.45499002 0.49863235 0.86222997]. \t  3.73341070124403 \t 3.8358976067261676\n",
            "76     \t [0.55460904 0.51367941 0.85805317]. \t  3.7749627491008635 \t 3.8358976067261676\n",
            "77     \t [0.00289678 0.68570015 0.79542665]. \t  3.0794041582919722 \t 3.8358976067261676\n",
            "78     \t [0.63528848 0.53250722 0.85929952]. \t  3.7946639023186814 \t 3.8358976067261676\n",
            "79     \t [0.29908807 0.60904563 0.81385849]. \t  3.6194410947146887 \t 3.8358976067261676\n",
            "80     \t [0.67190641 0.48012764 0.86211731]. \t  3.613921411152048 \t 3.8358976067261676\n",
            "81     \t [0.73293565 0.49221836 0.8616907 ]. \t  3.6526898174994726 \t 3.8358976067261676\n",
            "82     \t [0.59398161 0.58524417 0.81988881]. \t  3.651144257706845 \t 3.8358976067261676\n",
            "83     \t [0.32465679 0.58056328 0.81187539]. \t  3.678975299954757 \t 3.8358976067261676\n",
            "84     \t [0.71888547 0.53903106 0.8549037 ]. \t  3.779615016728407 \t 3.8358976067261676\n",
            "85     \t [0.5398206  0.54914242 0.83924532]. \t  3.809306179127745 \t 3.8358976067261676\n",
            "86     \t [0.57951616 0.4841699  0.86526221]. \t  3.6456932439340473 \t 3.8358976067261676\n",
            "87     \t [0.24523272 0.5593149  0.84844258]. \t  \u001b[92m3.857651959715417\u001b[0m \t 3.857651959715417\n",
            "88     \t [0.52196088 0.50321425 0.86575709]. \t  3.7331320285915486 \t 3.857651959715417\n",
            "89     \t [0.77746504 0.56241579 0.81711637]. \t  3.601426554661709 \t 3.857651959715417\n",
            "90     \t [0.57444671 0.55829592 0.8453776 ]. \t  3.814287356865882 \t 3.857651959715417\n",
            "91     \t [0.54693827 0.55445866 0.85204283]. \t  3.8310838991673926 \t 3.857651959715417\n",
            "92     \t [0.57744908 0.60598773 0.84236525]. \t  3.702880741191753 \t 3.857651959715417\n",
            "93     \t [0.70350415 0.53291046 0.84930604]. \t  3.774528291973705 \t 3.857651959715417\n",
            "94     \t [0.53823233 0.58285809 0.78559635]. \t  3.366631498563113 \t 3.857651959715417\n",
            "95     \t [0.5557804  0.53436276 0.86162748]. \t  3.812737769570616 \t 3.857651959715417\n",
            "96     \t [0.23059979 0.40212356 0.84755722]. \t  3.122911836542037 \t 3.857651959715417\n",
            "97     \t [0.3250367  0.52567258 0.8477069 ]. \t  3.8302764866161634 \t 3.857651959715417\n",
            "98     \t [0.53582346 0.50241906 0.8642232 ]. \t  3.732012815465721 \t 3.857651959715417\n",
            "99     \t [0.38554468 0.5818656  0.84982008]. \t  3.8283311787345804 \t 3.857651959715417\n",
            "100    \t [0.71333714 0.52520777 0.8523035 ]. \t  3.7623642516142835 \t 3.857651959715417\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d86panpOuvum",
        "outputId": "1570b30b-84ae-469f-ff27-2d9be17c5664"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 15 \r\n",
        "\r\n",
        "np.random.seed(run_num_15)\r\n",
        "surrogate_winner_15 = dtStudentProcess(d_cov_func, nu = df)\r\n",
        "\r\n",
        "winner_15 = dGPGO_stp(surrogate_winner_15, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_15.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.57788186 0.25550133 0.25252687]. \t  0.7217338409961529 \t 1.540625560354162\n",
            "init   \t [0.70990435 0.44755236 0.22694296]. \t  0.23199690080875204 \t 1.540625560354162\n",
            "init   \t [0.40170957 0.88225774 0.43721347]. \t  1.540625560354162 \t 1.540625560354162\n",
            "init   \t [0.87842518 0.78052775 0.53421429]. \t  0.5653855252190516 \t 1.540625560354162\n",
            "init   \t [0.01173301 0.26575648 0.3311941 ]. \t  0.5019682819961726 \t 1.540625560354162\n",
            "1      \t [0.20974591 0.27648732 0.98727858]. \t  1.018698490070848 \t 1.540625560354162\n",
            "2      \t [0.16066464 0.96759265 0.84404546]. \t  0.9163668542698835 \t 1.540625560354162\n",
            "3      \t [0.94092746 0.08463947 0.97697611]. \t  0.2593679988153737 \t 1.540625560354162\n",
            "4      \t [1. 1. 1.]. \t  0.31688362070415665 \t 1.540625560354162\n",
            "5      \t [0.85276908 0.96881867 0.09819948]. \t  0.0011523749539072918 \t 1.540625560354162\n",
            "6      \t [0.05164357 0.00265997 0.77165795]. \t  0.2513718942139017 \t 1.540625560354162\n",
            "7      \t [0.10920232 0.62316109 0.00506846]. \t  0.008194335621292969 \t 1.540625560354162\n",
            "8      \t [0.56172257 1.         1.        ]. \t  0.3311111126497283 \t 1.540625560354162\n",
            "9      \t [0.931463   0.05328064 0.13719876]. \t  0.22361958586436667 \t 1.540625560354162\n",
            "10     \t [0.08357687 0.94126522 0.20008695]. \t  0.06650017949240712 \t 1.540625560354162\n",
            "11     \t [0.50816107 0.00995969 0.01074378]. \t  0.11678871706697912 \t 1.540625560354162\n",
            "12     \t [0.2459057  0.55936004 0.73812819]. \t  \u001b[92m2.9475357591098432\u001b[0m \t 2.9475357591098432\n",
            "13     \t [0.95830805 0.61578726 0.97401772]. \t  2.41557022606133 \t 2.9475357591098432\n",
            "14     \t [0.38673482 0.94842317 0.01294306]. \t  0.0004801379504392351 \t 2.9475357591098432\n",
            "15     \t [0.65772228 0.40044763 0.97226444]. \t  1.973984492056607 \t 2.9475357591098432\n",
            "16     \t [0.001757   0.70481886 0.96174803]. \t  2.269105142911023 \t 2.9475357591098432\n",
            "17     \t [0.08984466 0.57897258 0.67888766]. \t  2.3605229877334346 \t 2.9475357591098432\n",
            "18     \t [0.94363121 0.01043971 0.62743442]. \t  0.13377771643498612 \t 2.9475357591098432\n",
            "19     \t [0.34266565 0.78251831 0.95864672]. \t  1.789874233604425 \t 2.9475357591098432\n",
            "20     \t [0.99725666 0.35063586 0.71554274]. \t  1.7538292285110586 \t 2.9475357591098432\n",
            "21     \t [0.36933094 0.01021661 0.43422371]. \t  0.39403444357759476 \t 2.9475357591098432\n",
            "22     \t [0.55882691 0.60971693 0.77090873]. \t  \u001b[92m3.1107224099971327\u001b[0m \t 3.1107224099971327\n",
            "23     \t [0.05949321 0.07046385 0.01903271]. \t  0.1155622765470507 \t 3.1107224099971327\n",
            "24     \t [0.50969082 0.46739241 0.64917545]. \t  1.5498571494962878 \t 3.1107224099971327\n",
            "25     \t [0.79473552 0.99849999 0.65397319]. \t  0.587084619127592 \t 3.1107224099971327\n",
            "26     \t [0.92715829 0.48610821 0.11104113]. \t  0.0485681499932278 \t 3.1107224099971327\n",
            "27     \t [0.39612528 0.01406251 0.99334736]. \t  0.11405283230892328 \t 3.1107224099971327\n",
            "28     \t [0.42352189 0.45224743 0.01010649]. \t  0.04432777219409647 \t 3.1107224099971327\n",
            "29     \t [0.78066219 0.76494805 1.        ]. \t  1.3992843326021225 \t 3.1107224099971327\n",
            "30     \t [0.21086186 0.62075004 0.2991946 ]. \t  0.28884208862492206 \t 3.1107224099971327\n",
            "31     \t [0.66186031 0.06313765 0.86652016]. \t  0.4077142676804051 \t 3.1107224099971327\n",
            "32     \t [0.46599704 0.89083971 0.73310085]. \t  1.4756794013598735 \t 3.1107224099971327\n",
            "33     \t [0.05021785 0.35729219 0.8054642 ]. \t  2.613657483587488 \t 3.1107224099971327\n",
            "34     \t [1.41711077e-09 1.41711077e-09 1.87686413e-01]. \t  0.4793888561732652 \t 3.1107224099971327\n",
            "35     \t [0.99324014 0.90459306 0.32879658]. \t  0.062461601001537366 \t 3.1107224099971327\n",
            "36     \t [0.03540792 0.15287337 0.97986877]. \t  0.46301171767286986 \t 3.1107224099971327\n",
            "37     \t [1.         0.65485181 0.86572385]. \t  \u001b[92m3.3128634941319066\u001b[0m \t 3.3128634941319066\n",
            "38     \t [0.34543626 0.98835711 0.31665284]. \t  0.4065020112731046 \t 3.3128634941319066\n",
            "39     \t [0.0010986  0.98346927 0.80200301]. \t  0.9343295341104214 \t 3.3128634941319066\n",
            "40     \t [0.71824447 0.68748875 0.01069232]. \t  0.0038095224554745375 \t 3.3128634941319066\n",
            "41     \t [0.68222264 0.26823495 0.03402658]. \t  0.11582733011344257 \t 3.3128634941319066\n",
            "42     \t [0.62254208 0.00388766 0.38948734]. \t  0.46595055698181503 \t 3.3128634941319066\n",
            "43     \t [0.01227984 0.54512913 0.94883516]. \t  2.948561458276758 \t 3.3128634941319066\n",
            "44     \t [0.99565316 1.         0.81439314]. \t  0.5017544475664806 \t 3.3128634941319066\n",
            "45     \t [0.00627153 0.95984887 0.01681627]. \t  0.0005427420766650457 \t 3.3128634941319066\n",
            "46     \t [2.02382691e-04 6.78918060e-01 2.91797787e-01]. \t  0.2778512363130019 \t 3.3128634941319066\n",
            "47     \t [1.         0.57630911 0.84948674]. \t  \u001b[92m3.6392023685047414\u001b[0m \t 3.6392023685047414\n",
            "48     \t [0.82695014 0.60930445 0.85285908]. \t  3.623480097492324 \t 3.6392023685047414\n",
            "49     \t [0.93796692 0.46676025 0.86991896]. \t  3.4375818856189575 \t 3.6392023685047414\n",
            "50     \t [0.79044314 0.58553513 0.83861119]. \t  \u001b[92m3.677935782880805\u001b[0m \t 3.677935782880805\n",
            "51     \t [0.34292394 0.56510166 0.8864544 ]. \t  \u001b[92m3.7486097034663355\u001b[0m \t 3.7486097034663355\n",
            "52     \t [0.00654787 0.47199986 0.84030616]. \t  3.574630815991303 \t 3.7486097034663355\n",
            "53     \t [0.61417832 0.87186511 0.24223091]. \t  0.07735575114599404 \t 3.7486097034663355\n",
            "54     \t [0.03293527 0.68675165 0.84079459]. \t  3.265488179764086 \t 3.7486097034663355\n",
            "55     \t [2.66289369e-01 5.04534238e-07 1.64342837e-01]. \t  0.6148013182387277 \t 3.7486097034663355\n",
            "56     \t [0.40244393 0.48507968 0.86158551]. \t  3.680716030185375 \t 3.7486097034663355\n",
            "57     \t [0.9904817  0.05243234 0.00876346]. \t  0.04051778234159188 \t 3.7486097034663355\n",
            "58     \t [0.        0.2264266 0.       ]. \t  0.0691539884350328 \t 3.7486097034663355\n",
            "59     \t [0.87986924 0.54125674 0.85994641]. \t  3.7214366105147274 \t 3.7486097034663355\n",
            "60     \t [0.99993497 0.49136119 0.86702354]. \t  3.538964301917269 \t 3.7486097034663355\n",
            "61     \t [0.57683818 0.60985049 0.85259389]. \t  3.708392576397823 \t 3.7486097034663355\n",
            "62     \t [0.98434355 0.79982782 0.01225668]. \t  0.00047251036949390866 \t 3.7486097034663355\n",
            "63     \t [0.4528402  0.57089002 0.88200899]. \t  \u001b[92m3.764203623450538\u001b[0m \t 3.764203623450538\n",
            "64     \t [0.44031504 0.56185783 0.88366733]. \t  3.7619431730305095 \t 3.764203623450538\n",
            "65     \t [0.60889868 0.55869063 0.86127719]. \t  \u001b[92m3.8126594299436474\u001b[0m \t 3.8126594299436474\n",
            "66     \t [0.58325996 0.66572896 0.87239943]. \t  3.381207352697191 \t 3.8126594299436474\n",
            "67     \t [0.47745868 0.63119767 0.83408043]. \t  3.5906695289507637 \t 3.8126594299436474\n",
            "68     \t [0.99995748 0.52985556 0.85321504]. \t  3.6577301784568395 \t 3.8126594299436474\n",
            "69     \t [0.57604521 0.55627738 0.88065003]. \t  3.7592334335510236 \t 3.8126594299436474\n",
            "70     \t [0.28206803 0.57884861 0.86831883]. \t  \u001b[92m3.82012793632404\u001b[0m \t 3.82012793632404\n",
            "71     \t [0.33137391 0.5801709  0.86863975]. \t  3.8168740328187787 \t 3.82012793632404\n",
            "72     \t [0.28373259 0.59039221 0.84195903]. \t  3.8042947264425244 \t 3.82012793632404\n",
            "73     \t [0.00751786 0.61005326 0.80743822]. \t  3.5387478839429685 \t 3.82012793632404\n",
            "74     \t [0.44218926 0.57631176 0.86999087]. \t  3.810257765813778 \t 3.82012793632404\n",
            "75     \t [0.39409165 0.60401894 0.85562624]. \t  3.7699628153706453 \t 3.82012793632404\n",
            "76     \t [0.17537865 0.55691692 0.86464543]. \t  \u001b[92m3.838745919057232\u001b[0m \t 3.838745919057232\n",
            "77     \t [0.30392242 0.48925901 0.87078563]. \t  3.6728872146176754 \t 3.838745919057232\n",
            "78     \t [0.86080084 0.55085818 0.86129106]. \t  3.731420986816447 \t 3.838745919057232\n",
            "79     \t [0.23832841 0.57589168 0.86554352]. \t  3.829910636501558 \t 3.838745919057232\n",
            "80     \t [0.31014565 0.53455313 0.87570275]. \t  3.7925654936197932 \t 3.838745919057232\n",
            "81     \t [0.99997427 0.54106741 0.83840462]. \t  3.6353745814407294 \t 3.838745919057232\n",
            "82     \t [0.47377976 0.57697821 0.86906916]. \t  3.807517992313729 \t 3.838745919057232\n",
            "83     \t [0.85495783 0.57244319 0.85399082]. \t  3.7171141148899953 \t 3.838745919057232\n",
            "84     \t [0.45507035 0.56394922 0.87012848]. \t  3.820999263349455 \t 3.838745919057232\n",
            "85     \t [0.31974334 0.57653497 0.85690171]. \t  \u001b[92m3.8435516290299256\u001b[0m \t 3.8435516290299256\n",
            "86     \t [0.27744147 0.59921727 0.85425577]. \t  3.793341732927461 \t 3.8435516290299256\n",
            "87     \t [0.20837319 0.53093511 0.8891527 ]. \t  3.6977527147688174 \t 3.8435516290299256\n",
            "88     \t [0.42257637 0.59731689 0.86090699]. \t  3.7846268249439383 \t 3.8435516290299256\n",
            "89     \t [0.34984011 0.49836727 0.88760338]. \t  3.619130840487774 \t 3.8435516290299256\n",
            "90     \t [0.43344998 0.55385404 0.87289323]. \t  3.816122818318026 \t 3.8435516290299256\n",
            "91     \t [0.99276438 0.49413934 0.87939112]. \t  3.505444657733684 \t 3.8435516290299256\n",
            "92     \t [0.44845211 0.51986015 0.8839514 ]. \t  3.711296716485591 \t 3.8435516290299256\n",
            "93     \t [0.35726071 0.58799823 0.8626906 ]. \t  3.8134848275330047 \t 3.8435516290299256\n",
            "94     \t [0.27352014 0.55734072 0.86847934]. \t  3.8370535202991354 \t 3.8435516290299256\n",
            "95     \t [0.48348345 0.55803697 0.86731349]. \t  3.827480776016929 \t 3.8435516290299256\n",
            "96     \t [0.6815507  0.56569152 0.87077797]. \t  3.769378862093523 \t 3.8435516290299256\n",
            "97     \t [0.42402851 0.59328826 0.85996761]. \t  3.796811389681121 \t 3.8435516290299256\n",
            "98     \t [0.43863465 0.52241751 0.889385  ]. \t  3.681447851507929 \t 3.8435516290299256\n",
            "99     \t [0.21335232 0.5228601  0.88586163]. \t  3.7039104418697013 \t 3.8435516290299256\n",
            "100    \t [0.73799488 0.56101413 0.86807599]. \t  3.762350921371347 \t 3.8435516290299256\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "any0xrgYuvxA",
        "outputId": "408c3079-0e4d-40bb-ca0b-8bfb4fc102dd"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 16 \r\n",
        "\r\n",
        "np.random.seed(run_num_16)\r\n",
        "surrogate_winner_16 = dtStudentProcess(d_cov_func, nu = df)\r\n",
        "\r\n",
        "winner_16 = dGPGO_stp(surrogate_winner_16, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_16.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.86394833 0.316366   0.67215078]. \t  1.1947633822631252 \t 3.8084053754826726\n",
            "init   \t [0.50791167 0.78166043 0.28368168]. \t  0.20052765717741963 \t 3.8084053754826726\n",
            "init   \t [0.23370878 0.56379969 0.87502436]. \t  3.8084053754826726 \t 3.8084053754826726\n",
            "init   \t [0.71894292 0.18213174 0.24380041]. \t  0.6532826728302027 \t 3.8084053754826726\n",
            "init   \t [0.08605673 0.53424539 0.83969965]. \t  3.8072565344663847 \t 3.8084053754826726\n",
            "1      \t [0.26431958 0.04731623 0.9519732 ]. \t  0.22754937654248866 \t 3.8084053754826726\n",
            "2      \t [1. 1. 1.]. \t  0.31688362070415665 \t 3.8084053754826726\n",
            "3      \t [0.05091113 0.23651463 0.05355928]. \t  0.16258223236492292 \t 3.8084053754826726\n",
            "4      \t [0.21504616 0.93428025 0.85726514]. \t  1.0997235365209492 \t 3.8084053754826726\n",
            "5      \t [0.99825509 0.726905   0.09313308]. \t  0.0033521205180266815 \t 3.8084053754826726\n",
            "6      \t [0.0549501  0.65287072 0.0655024 ]. \t  0.013808713124583845 \t 3.8084053754826726\n",
            "7      \t [0.83242009 0.60380638 0.99688529]. \t  2.07320267222489 \t 3.8084053754826726\n",
            "8      \t [0.00628562 0.62733695 0.98683809]. \t  2.216178855902818 \t 3.8084053754826726\n",
            "9      \t [0.12586454 0.00946537 0.50446347]. \t  0.16448361782141313 \t 3.8084053754826726\n",
            "10     \t [0.92273034 0.11962526 0.99238758]. \t  0.3098794296133343 \t 3.8084053754826726\n",
            "11     \t [0.92813146 0.0026293  0.06339311]. \t  0.09863145961844477 \t 3.8084053754826726\n",
            "12     \t [0.04907468 0.69450231 0.50797188]. \t  2.1273524048988834 \t 3.8084053754826726\n",
            "13     \t [0.01368174 0.34716502 0.73021516]. \t  1.9780065021531634 \t 3.8084053754826726\n",
            "14     \t [0.94290629 0.94417777 0.67240396]. \t  0.4765325878964518 \t 3.8084053754826726\n",
            "15     \t [0.09323927 0.93236702 0.01030606]. \t  0.0005406845968408741 \t 3.8084053754826726\n",
            "16     \t [0.98993929 0.60136671 0.94117146]. \t  2.9580496538051744 \t 3.8084053754826726\n",
            "17     \t [0.55921147 0.03153097 0.00851924]. \t  0.11183533180692776 \t 3.8084053754826726\n",
            "18     \t [0.40915321 0.5216491  0.00213467]. \t  0.023548974513400496 \t 3.8084053754826726\n",
            "19     \t [0.66509557 0.93174522 0.04635058]. \t  0.000725372313903174 \t 3.8084053754826726\n",
            "20     \t [0.6517925  0.03844004 0.53981801]. \t  0.13740736405317047 \t 3.8084053754826726\n",
            "21     \t [0.96988873 0.02017744 0.45997214]. \t  0.11297202180098427 \t 3.8084053754826726\n",
            "22     \t [0.26347995 0.39036012 0.98210609]. \t  1.783065064838238 \t 3.8084053754826726\n",
            "23     \t [0.28079357 0.52444499 0.56664572]. \t  1.2730547873497018 \t 3.8084053754826726\n",
            "24     \t [0.57640982 0.96145704 0.74151168]. \t  0.9378709839982033 \t 3.8084053754826726\n",
            "25     \t [0.94069121 0.41128493 0.10748557]. \t  0.07341886974713681 \t 3.8084053754826726\n",
            "26     \t [0.00127453 0.91562462 0.66262913]. \t  2.2807149391071984 \t 3.8084053754826726\n",
            "27     \t [0.05139585 0.05475333 0.06390342]. \t  0.2055090591947399 \t 3.8084053754826726\n",
            "28     \t [0.78889888 0.74417759 0.76252636]. \t  2.058426499976975 \t 3.8084053754826726\n",
            "29     \t [0.02875925 0.19737644 0.95258301]. \t  0.7937921659238248 \t 3.8084053754826726\n",
            "30     \t [0.26025853 0.71851858 0.74498734]. \t  2.6252239200731875 \t 3.8084053754826726\n",
            "31     \t [0.63647554 0.95992588 0.98730057]. \t  0.5052211871113155 \t 3.8084053754826726\n",
            "32     \t [0.99656962 0.06250794 0.78544588]. \t  0.4304526443944995 \t 3.8084053754826726\n",
            "33     \t [0.88428793 0.99489395 0.21610491]. \t  0.013872839258918844 \t 3.8084053754826726\n",
            "34     \t [0.04326643 0.9637775  0.24988684]. \t  0.16857260481012232 \t 3.8084053754826726\n",
            "35     \t [3.16291301e-10 1.21585101e-09 1.77912190e-09]. \t  0.06797411877066223 \t 3.8084053754826726\n",
            "36     \t [0.99697682 0.66469657 0.39449255]. \t  0.10359968291212561 \t 3.8084053754826726\n",
            "37     \t [0.4727824  0.58764517 0.88780295]. \t  3.701077735915996 \t 3.8084053754826726\n",
            "38     \t [0.97572205 0.42658892 0.91888753]. \t  2.7887441463490874 \t 3.8084053754826726\n",
            "39     \t [4.54750542e-04 5.75026673e-01 8.25805168e-01]. \t  3.730587199087372 \t 3.8084053754826726\n",
            "40     \t [3.09305756e-09 3.09305756e-09 1.55944056e-01]. \t  0.3996668618466441 \t 3.8084053754826726\n",
            "41     \t [0.32460879 0.74040192 0.97116065]. \t  1.9422430000829798 \t 3.8084053754826726\n",
            "42     \t [0.8050304  0.70797456 0.02115815]. \t  0.002897192581821706 \t 3.8084053754826726\n",
            "43     \t [0.42889994 0.53654338 0.83652596]. \t  \u001b[92m3.8154201210647054\u001b[0m \t 3.8154201210647054\n",
            "44     \t [0.03215124 0.04511637 0.75779709]. \t  0.3616868921164547 \t 3.8154201210647054\n",
            "45     \t [0.52994669 0.29804952 0.80380406]. \t  2.08576900688309 \t 3.8154201210647054\n",
            "46     \t [0.0030395  0.67615002 0.67794665]. \t  2.467444838313903 \t 3.8154201210647054\n",
            "47     \t [0.26680868 0.9759964  0.23734613]. \t  0.12267683511659878 \t 3.8154201210647054\n",
            "48     \t [0.626929   0.49918387 0.86802637]. \t  3.691913981337994 \t 3.8154201210647054\n",
            "49     \t [0.6172408  0.58593557 0.85175748]. \t  3.771979523377444 \t 3.8154201210647054\n",
            "50     \t [0.66448275 0.04339793 0.95718037]. \t  0.20951949721643165 \t 3.8154201210647054\n",
            "51     \t [0.72313204 0.49396261 0.85779664]. \t  3.668062673657507 \t 3.8154201210647054\n",
            "52     \t [0.04954193 0.96501284 0.9439065 ]. \t  0.6514583655031334 \t 3.8154201210647054\n",
            "53     \t [0.61650245 0.54587478 0.8733084 ]. \t  3.7800857119079225 \t 3.8154201210647054\n",
            "54     \t [0.46890392 0.5974413  0.85763065]. \t  3.7790497560306004 \t 3.8154201210647054\n",
            "55     \t [0.58697341 0.54226619 0.86439295]. \t  3.8097620060497723 \t 3.8154201210647054\n",
            "56     \t [0.99522848 0.94896874 0.08128374]. \t  0.00043056777856769965 \t 3.8154201210647054\n",
            "57     \t [0.60078521 0.57844765 0.81125437]. \t  3.599200931879733 \t 3.8154201210647054\n",
            "58     \t [0.57112626 0.50255991 0.89329905]. \t  3.5737990176139234 \t 3.8154201210647054\n",
            "59     \t [0.48961625 0.5333072  0.87688584]. \t  3.7742435625712547 \t 3.8154201210647054\n",
            "60     \t [0.60822789 0.56621434 0.86626385]. \t  3.800002957450101 \t 3.8154201210647054\n",
            "61     \t [0.11280494 0.53382273 0.80833142]. \t  3.660505063796747 \t 3.8154201210647054\n",
            "62     \t [0.36339591 0.52934102 0.86510883]. \t  \u001b[92m3.8213740996310737\u001b[0m \t 3.8213740996310737\n",
            "63     \t [0.69350227 0.54468258 0.86281393]. \t  3.7861721148077083 \t 3.8213740996310737\n",
            "64     \t [0.23824915 0.52794452 0.82840652]. \t  3.7837206503063765 \t 3.8213740996310737\n",
            "65     \t [0.31829415 0.51272439 0.83250722]. \t  3.767160535122776 \t 3.8213740996310737\n",
            "66     \t [0.60887028 0.53137488 0.87219757]. \t  3.7694796080215314 \t 3.8213740996310737\n",
            "67     \t [0.17575847 0.48211169 0.86915102]. \t  3.6334540379345066 \t 3.8213740996310737\n",
            "68     \t [0.07763718 0.52246634 0.85824089]. \t  3.791638228837192 \t 3.8213740996310737\n",
            "69     \t [0.20469496 0.5030403  0.86415897]. \t  3.7445368293521617 \t 3.8213740996310737\n",
            "70     \t [0.2379653  0.56620641 0.84659914]. \t  \u001b[92m3.8514974443651546\u001b[0m \t 3.8514974443651546\n",
            "71     \t [0.40508753 0.55721362 0.83898211]. \t  3.8335360137882253 \t 3.8514974443651546\n",
            "72     \t [0.74541246 0.56755333 0.8521057 ]. \t  3.763862039885836 \t 3.8514974443651546\n",
            "73     \t [0.37407005 0.5601508  0.84849655]. \t  \u001b[92m3.854526113876351\u001b[0m \t 3.854526113876351\n",
            "74     \t [0.61292099 0.55114444 0.85936081]. \t  3.8145778284344978 \t 3.854526113876351\n",
            "75     \t [0.40811892 0.54827912 0.85255292]. \t  3.854074670708504 \t 3.854526113876351\n",
            "76     \t [0.51987604 0.59818472 0.81653025]. \t  3.617217172507859 \t 3.854526113876351\n",
            "77     \t [0.54753937 0.53049124 0.81805975]. \t  3.692156841519896 \t 3.854526113876351\n",
            "78     \t [0.30294266 0.56295329 0.85096675]. \t  \u001b[92m3.858646102428297\u001b[0m \t 3.858646102428297\n",
            "79     \t [0.56725031 0.53279169 0.87519693]. \t  3.768454640764056 \t 3.858646102428297\n",
            "80     \t [0.60886771 0.53513967 0.87267954]. \t  3.773393657590956 \t 3.858646102428297\n",
            "81     \t [0.47879146 0.5393837  0.85916272]. \t  3.835651374508267 \t 3.858646102428297\n",
            "82     \t [0.45538391 0.54676818 0.85461998]. \t  3.8474941742319007 \t 3.858646102428297\n",
            "83     \t [0.57119451 0.57256996 0.83796059]. \t  3.779519530231469 \t 3.858646102428297\n",
            "84     \t [0.64277011 0.57051417 0.83393467]. \t  3.7440309596426333 \t 3.858646102428297\n",
            "85     \t [0.25211865 0.57421788 0.84404474]. \t  3.839577717998675 \t 3.858646102428297\n",
            "86     \t [0.54840136 0.50138618 0.88833775]. \t  3.610605745018495 \t 3.858646102428297\n",
            "87     \t [0.64699701 0.56286434 0.84521609]. \t  3.7899761644361982 \t 3.858646102428297\n",
            "88     \t [0.12271488 0.55326312 0.81863523]. \t  3.7421230450032543 \t 3.858646102428297\n",
            "89     \t [0.51196492 0.56897775 0.86388302]. \t  3.8230555095080074 \t 3.858646102428297\n",
            "90     \t [0.61187573 0.55581266 0.86093511]. \t  3.8131481382476506 \t 3.858646102428297\n",
            "91     \t [0.51397168 0.53773345 0.86013946]. \t  3.8269447788725786 \t 3.858646102428297\n",
            "92     \t [0.40161686 0.51766142 0.84997913]. \t  3.8096405055789813 \t 3.858646102428297\n",
            "93     \t [0.53873905 0.52940553 0.87416425]. \t  3.7722102083848332 \t 3.858646102428297\n",
            "94     \t [0.3475644  0.52874228 0.85787123]. \t  3.8340717987654926 \t 3.858646102428297\n",
            "95     \t [0.45366286 0.57287513 0.83893341]. \t  3.8110946504986583 \t 3.858646102428297\n",
            "96     \t [0.07892434 0.52016208 0.85099753]. \t  3.791424323918756 \t 3.858646102428297\n",
            "97     \t [0.61651752 0.5506332  0.87394807]. \t  3.7798615032497946 \t 3.858646102428297\n",
            "98     \t [0.60073954 0.60320022 0.81043778]. \t  3.5216392417512683 \t 3.858646102428297\n",
            "99     \t [0.01656921 0.65373082 0.83730304]. \t  3.4785760357885573 \t 3.858646102428297\n",
            "100    \t [0.20317663 0.56740956 0.82100957]. \t  3.7596395790296784 \t 3.858646102428297\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "reLyKt6Quvzx",
        "outputId": "2c0384d9-7c6b-4e03-9f9a-13ccdcd41eac"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 17 \r\n",
        "\r\n",
        "np.random.seed(run_num_17)\r\n",
        "surrogate_winner_17 = dtStudentProcess(d_cov_func, nu = df)\r\n",
        "\r\n",
        "winner_17 = dGPGO_stp(surrogate_winner_17, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_17.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.65182403 0.73051962 0.19336736]. \t  0.03562358530655929 \t 3.1179188940604616\n",
            "init   \t [0.51967724 0.67638327 0.80444487]. \t  3.1179188940604616 \t 3.1179188940604616\n",
            "init   \t [0.53351286 0.90803418 0.44348113]. \t  1.1984486017622078 \t 3.1179188940604616\n",
            "init   \t [0.25597221 0.27866502 0.04332186]. \t  0.1645724004847893 \t 3.1179188940604616\n",
            "init   \t [0.46897499 0.9416138  0.60040624]. \t  1.9192522196289552 \t 3.1179188940604616\n",
            "1      \t [1. 1. 1.]. \t  0.31688362070415665 \t 3.1179188940604616\n",
            "2      \t [0.03518277 0.02362337 0.94853962]. \t  0.18176382334954216 \t 3.1179188940604616\n",
            "3      \t [0.96988748 0.00781316 0.83764154]. \t  0.2534122068467659 \t 3.1179188940604616\n",
            "4      \t [0.03280843 0.75151925 0.98857466]. \t  1.627424858411729 \t 3.1179188940604616\n",
            "5      \t [0.86507371 0.09508591 0.16407237]. \t  0.3454177637065228 \t 3.1179188940604616\n",
            "6      \t [7.35666725e-04 9.24427221e-01 5.09677439e-01]. \t  2.6773978110061094 \t 3.1179188940604616\n",
            "7      \t [0.08473973 0.9960574  0.05740915]. \t  0.0016367306519927567 \t 3.1179188940604616\n",
            "8      \t [0.03163498 0.00280823 0.37177155]. \t  0.45158867811915787 \t 3.1179188940604616\n",
            "9      \t [0.99402546 0.57912873 0.71598764]. \t  2.145621985758046 \t 3.1179188940604616\n",
            "10     \t [0.06027878 0.59330142 0.56136326]. \t  1.727722821327885 \t 3.1179188940604616\n",
            "11     \t [0.78658505 0.46074145 0.99694793]. \t  1.8868171850718256 \t 3.1179188940604616\n",
            "12     \t [0.42688337 0.0075181  0.55115732]. \t  0.13110363209892228 \t 3.1179188940604616\n",
            "13     \t [0.94075589 0.99281096 0.45818671]. \t  0.2508550898125676 \t 3.1179188940604616\n",
            "14     \t [0.5588266 1.        1.       ]. \t  0.3311732092826975 \t 3.1179188940604616\n",
            "15     \t [0.40174973 0.31761709 0.98505159]. \t  1.2912756462632955 \t 3.1179188940604616\n",
            "16     \t [0.00593645 0.7584192  0.0719689 ]. \t  0.005847276159507908 \t 3.1179188940604616\n",
            "17     \t [0.07825719 0.04012036 0.06064785]. \t  0.20317891858680526 \t 3.1179188940604616\n",
            "18     \t [0.7830393  0.4811608  0.65122516]. \t  1.4152389583200338 \t 3.1179188940604616\n",
            "19     \t [0.230118   0.84413446 0.749127  ]. \t  1.9712454609123538 \t 3.1179188940604616\n",
            "20     \t [0.9107796  0.24775313 0.01336517]. \t  0.05047258758526262 \t 3.1179188940604616\n",
            "21     \t [0.53535079 0.0333876  0.04436634]. \t  0.1932042616549043 \t 3.1179188940604616\n",
            "22     \t [0.85583237 0.97666025 0.12250427]. \t  0.0020256331720579345 \t 3.1179188940604616\n",
            "23     \t [0.82589189 0.83407017 0.83138796]. \t  1.7135736633402339 \t 3.1179188940604616\n",
            "24     \t [0.96368261 0.85904114 0.02595699]. \t  0.0003210902930198221 \t 3.1179188940604616\n",
            "25     \t [0.02085639 0.16129505 0.66112705]. \t  0.5432394123184767 \t 3.1179188940604616\n",
            "26     \t [0.99168525 0.43242472 0.99650256]. \t  1.7355597698591239 \t 3.1179188940604616\n",
            "27     \t [0.6475363  0.63961729 1.        ]. \t  1.9694772375203708 \t 3.1179188940604616\n",
            "28     \t [3.75657316e-01 9.84983799e-01 3.31771064e-04]. \t  0.0002749436871536298 \t 3.1179188940604616\n",
            "29     \t [0.9719708  0.04698038 0.26521465]. \t  0.3198451902532169 \t 3.1179188940604616\n",
            "30     \t [0.99751118 0.00429221 0.99720207]. \t  0.09539020493459599 \t 3.1179188940604616\n",
            "31     \t [0.01272388 0.94894743 0.86920439]. \t  0.9639497431245474 \t 3.1179188940604616\n",
            "32     \t [0.89234347 0.01595942 0.00143224]. \t  0.047613428791274955 \t 3.1179188940604616\n",
            "33     \t [0.01632879 0.71800823 0.72104951]. \t  2.5560914837321738 \t 3.1179188940604616\n",
            "34     \t [0.03738209 0.42900844 0.86508514]. \t  \u001b[92m3.2777321109863586\u001b[0m \t 3.2777321109863586\n",
            "35     \t [0.3723719  0.51991138 0.81460577]. \t  \u001b[92m3.688679687458407\u001b[0m \t 3.688679687458407\n",
            "36     \t [0.19575322 0.95857707 0.36353481]. \t  0.9121785463069294 \t 3.688679687458407\n",
            "37     \t [0.3995437  0.35606843 0.5163474 ]. \t  0.43339109707458684 \t 3.688679687458407\n",
            "38     \t [1.         0.73508344 1.        ]. \t  1.5226309619341958 \t 3.688679687458407\n",
            "39     \t [1.21100390e-09 2.21094741e-09 3.51969220e-09]. \t  0.06797412096107647 \t 3.688679687458407\n",
            "40     \t [0.98601462 0.58194176 0.24351111]. \t  0.043245959414655685 \t 3.688679687458407\n",
            "41     \t [0.22314733 0.59704205 0.91022114]. \t  3.488779236938376 \t 3.688679687458407\n",
            "42     \t [0.02792626 0.44930695 0.01388729]. \t  0.034136225726136685 \t 3.688679687458407\n",
            "43     \t [0.19045079 0.4788869  0.88866287]. \t  3.50975156018112 \t 3.688679687458407\n",
            "44     \t [0.23727365 0.45830344 0.86128568]. \t  3.529674817497443 \t 3.688679687458407\n",
            "45     \t [0.04611878 0.49084796 0.943965  ]. \t  2.8915372723962447 \t 3.688679687458407\n",
            "46     \t [0.45932604 0.49564047 0.88159413]. \t  3.6424563582444147 \t 3.688679687458407\n",
            "47     \t [0.32204714 0.58790658 0.81511852]. \t  3.6864206958702015 \t 3.688679687458407\n",
            "48     \t [0.56701472 0.54463514 0.81644638]. \t  3.6843315709569606 \t 3.688679687458407\n",
            "49     \t [0.64731465 0.04067576 0.89259272]. \t  0.30051201140770023 \t 3.688679687458407\n",
            "50     \t [0.12984595 0.50374222 0.83087307]. \t  \u001b[92m3.719533046320669\u001b[0m \t 3.719533046320669\n",
            "51     \t [0.48410813 0.52205152 0.85475183]. \t  \u001b[92m3.8101572163121875\u001b[0m \t 3.8101572163121875\n",
            "52     \t [0.40735458 0.59441101 0.82696291]. \t  3.7241976701246506 \t 3.8101572163121875\n",
            "53     \t [0.34787634 0.55427514 0.85322777]. \t  \u001b[92m3.860191852273269\u001b[0m \t 3.860191852273269\n",
            "54     \t [0.35816368 0.58616056 0.85245892]. \t  3.8238267103915606 \t 3.860191852273269\n",
            "55     \t [0.47769567 0.55663274 0.80220902]. \t  3.5941632036651083 \t 3.860191852273269\n",
            "56     \t [0.43742069 0.60123874 0.76883587]. \t  3.18371010095346 \t 3.860191852273269\n",
            "57     \t [0.30919151 0.53846838 0.87936084]. \t  3.7798322004893423 \t 3.860191852273269\n",
            "58     \t [0.45087994 0.56994516 0.81091451]. \t  3.6618354703573037 \t 3.860191852273269\n",
            "59     \t [0.11083298 0.56372959 0.84484359]. \t  3.8339972152651187 \t 3.860191852273269\n",
            "60     \t [0.33389514 0.5503341  0.90112831]. \t  3.629045250900018 \t 3.860191852273269\n",
            "61     \t [0.47445134 0.50499015 0.91051459]. \t  3.428521241828867 \t 3.860191852273269\n",
            "62     \t [0.41777893 0.42767057 0.86161066]. \t  3.3153100947323666 \t 3.860191852273269\n",
            "63     \t [0.69844189 1.         0.70075009]. \t  0.6847894099010703 \t 3.860191852273269\n",
            "64     \t [0.37473371 0.66767014 0.87668444]. \t  3.3939129107915464 \t 3.860191852273269\n",
            "65     \t [0.10779229 0.54142401 0.84561894]. \t  3.8307028653271713 \t 3.860191852273269\n",
            "66     \t [0.00324288 0.60115893 0.80331363]. \t  3.5343664757086506 \t 3.860191852273269\n",
            "67     \t [0.2746941  0.51518321 0.79396739]. \t  3.5231624948224027 \t 3.860191852273269\n",
            "68     \t [0.43825159 0.58066111 0.88504217]. \t  3.7354287024717774 \t 3.860191852273269\n",
            "69     \t [0.2996234  0.56682393 0.84218604]. \t  3.844436868056313 \t 3.860191852273269\n",
            "70     \t [0.03043002 0.55365845 0.84809944]. \t  3.8189294254843715 \t 3.860191852273269\n",
            "71     \t [0.27601827 0.52254461 0.84527364]. \t  3.8211450582358273 \t 3.860191852273269\n",
            "72     \t [0.44086716 0.53285559 0.82433247]. \t  3.756995392562737 \t 3.860191852273269\n",
            "73     \t [0.35710011 0.4941582  0.88411209]. \t  3.6253332766717623 \t 3.860191852273269\n",
            "74     \t [0.4113314 0.4829666 0.8420043]. \t  3.6765633997004983 \t 3.860191852273269\n",
            "75     \t [0.4100149  0.52611147 0.85722278]. \t  3.8259259018596468 \t 3.860191852273269\n",
            "76     \t [0.         0.57645765 0.80393249]. \t  3.5897000108712285 \t 3.860191852273269\n",
            "77     \t [0.37580684 0.48172868 0.88040446]. \t  3.5896244847247907 \t 3.860191852273269\n",
            "78     \t [0.41503401 0.56548485 0.83300801]. \t  3.805973786376252 \t 3.860191852273269\n",
            "79     \t [0.3137087  0.52130506 0.83754458]. \t  3.803642756502287 \t 3.860191852273269\n",
            "80     \t [0.31848038 0.51930802 0.8240162 ]. \t  3.7468794918182335 \t 3.860191852273269\n",
            "81     \t [0.39448709 0.591067   0.80857548]. \t  3.617352129619014 \t 3.860191852273269\n",
            "82     \t [0.44703446 0.54262699 0.83033345]. \t  3.793607336526842 \t 3.860191852273269\n",
            "83     \t [0.38397995 0.5055439  0.8385855 ]. \t  3.7605392699853133 \t 3.860191852273269\n",
            "84     \t [0.19052227 0.47566572 0.87033083]. \t  3.597315717711275 \t 3.860191852273269\n",
            "85     \t [0.31799066 0.51357133 0.86969543]. \t  3.769591592301295 \t 3.860191852273269\n",
            "86     \t [0.39369222 0.47423042 0.86553033]. \t  3.615621806252677 \t 3.860191852273269\n",
            "87     \t [0.43563422 0.57535857 0.83388054]. \t  3.7929148829603063 \t 3.860191852273269\n",
            "88     \t [0.         0.62180194 0.80014916]. \t  3.440680559223994 \t 3.860191852273269\n",
            "89     \t [0.33190765 0.53229127 0.86178964]. \t  3.834705600893579 \t 3.860191852273269\n",
            "90     \t [0.45771974 0.54395104 0.84645334]. \t  3.840277440412452 \t 3.860191852273269\n",
            "91     \t [0.3437724  0.50654709 0.82879016]. \t  3.7343745866181526 \t 3.860191852273269\n",
            "92     \t [0.45972889 0.51521871 0.85440774]. \t  3.7970873218480996 \t 3.860191852273269\n",
            "93     \t [0.14313612 0.55365639 0.80958848]. \t  3.6868284750687006 \t 3.860191852273269\n",
            "94     \t [0.38489585 0.58842405 0.82403136]. \t  3.72902388500426 \t 3.860191852273269\n",
            "95     \t [0.35444967 0.59998491 0.84106067]. \t  3.7699916010224443 \t 3.860191852273269\n",
            "96     \t [0.41802285 0.52991404 0.85961161]. \t  3.829510770698673 \t 3.860191852273269\n",
            "97     \t [0.38988633 0.63819156 0.85532251]. \t  3.6166625797944385 \t 3.860191852273269\n",
            "98     \t [0.32721471 0.574637   0.819143  ]. \t  3.7366842376043263 \t 3.860191852273269\n",
            "99     \t [0.39385743 0.52159961 0.85888659]. \t  3.8156251771845855 \t 3.860191852273269\n",
            "100    \t [0.20888104 0.51241603 0.86051773]. \t  3.7846945125742497 \t 3.860191852273269\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cI3FtfYSuv2u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65beeb13-ef91-489c-fbdf-d675fa5f22bc"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 18 \r\n",
        "\r\n",
        "np.random.seed(run_num_18)\r\n",
        "surrogate_winner_18 = dtStudentProcess(d_cov_func, nu = df)\r\n",
        "\r\n",
        "winner_18 = dGPGO_stp(surrogate_winner_18, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_18.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.8734294  0.96854066 0.86919454]. \t  0.722531189286755 \t 1.1210522139432408\n",
            "init   \t [0.53085569 0.23272833 0.0113988 ]. \t  0.1133646794700679 \t 1.1210522139432408\n",
            "init   \t [0.43046882 0.40235136 0.52267467]. \t  0.5232646985053151 \t 1.1210522139432408\n",
            "init   \t [0.4783918  0.55535647 0.54338602]. \t  1.0390748854353227 \t 1.1210522139432408\n",
            "init   \t [0.76089558 0.71237457 0.6196821 ]. \t  1.1210522139432408 \t 1.1210522139432408\n",
            "1      \t [0.28982083 0.95201409 0.18404427]. \t  0.04220734680078784 \t 1.1210522139432408\n",
            "2      \t [0.00482474 0.64977529 0.94137026]. \t  \u001b[92m2.8634726533487465\u001b[0m \t 2.8634726533487465\n",
            "3      \t [0.99659594 0.17559011 0.91257217]. \t  0.8330954963238189 \t 2.8634726533487465\n",
            "4      \t [0.12883542 0.98258307 0.90180597]. \t  0.6910738837771888 \t 2.8634726533487465\n",
            "5      \t [0.1605449  0.05275826 0.97945737]. \t  0.19171137173584732 \t 2.8634726533487465\n",
            "6      \t [0.92481221 0.89538898 0.01033975]. \t  0.00018344766760242874 \t 2.8634726533487465\n",
            "7      \t [0.05463274 0.14825989 0.03109518]. \t  0.13809479076050474 \t 2.8634726533487465\n",
            "8      \t [0.56201017 0.69176506 0.99700729]. \t  1.8343031545933626 \t 2.8634726533487465\n",
            "9      \t [0.9993393  0.0086611  0.23114407]. \t  0.2595355554347633 \t 2.8634726533487465\n",
            "10     \t [0.02216781 0.53992861 0.2461829 ]. \t  0.17073871345961164 \t 2.8634726533487465\n",
            "11     \t [0.97876289 0.7415848  0.97574387]. \t  1.8011266309050586 \t 2.8634726533487465\n",
            "12     \t [0.06581014 0.37724521 0.99371781]. \t  1.5328034309170186 \t 2.8634726533487465\n",
            "13     \t [0.02481136 0.01824625 0.48540695]. \t  0.17319635963255886 \t 2.8634726533487465\n",
            "14     \t [0.10195839 0.9221158  0.5776728 ]. \t  \u001b[92m2.961030000956687\u001b[0m \t 2.961030000956687\n",
            "15     \t [0.81428584 0.00456472 0.98413431]. \t  0.11051245166557119 \t 2.961030000956687\n",
            "16     \t [0.99270414 0.11370485 0.05609963]. \t  0.0816213348710151 \t 2.961030000956687\n",
            "17     \t [0.58609932 0.01312132 0.12877797]. \t  0.43820205861790384 \t 2.961030000956687\n",
            "18     \t [0.03591706 0.95476209 0.07665491]. \t  0.0031251970398831212 \t 2.961030000956687\n",
            "19     \t [0.99313013 0.90439224 0.52918552]. \t  0.3178617233597224 \t 2.961030000956687\n",
            "20     \t [0.04888632 0.5413824  0.77782479]. \t  \u001b[92m3.3767736055039483\u001b[0m \t 3.3767736055039483\n",
            "21     \t [0.2791866 0.6440799 0.7555363]. \t  2.994725707076107 \t 3.3767736055039483\n",
            "22     \t [0.13346515 0.67388707 0.67026426]. \t  2.4966602731353253 \t 3.3767736055039483\n",
            "23     \t [0.66486867 0.03808225 0.69569699]. \t  0.2751556351337177 \t 3.3767736055039483\n",
            "24     \t [0.8838431  0.99999728 0.27516327]. \t  0.04068242720518498 \t 3.3767736055039483\n",
            "25     \t [0.69412041 0.32080107 0.984123  ]. \t  1.3088683503925223 \t 3.3767736055039483\n",
            "26     \t [0.98959452 0.66661142 0.00358503]. \t  0.0019268614911794597 \t 3.3767736055039483\n",
            "27     \t [0.49567004 0.91934328 0.70313349]. \t  1.3938268063291095 \t 3.3767736055039483\n",
            "28     \t [0.9950578  0.35345067 0.41676987]. \t  0.12583997146651915 \t 3.3767736055039483\n",
            "29     \t [1. 1. 1.]. \t  0.31688362070415665 \t 3.3767736055039483\n",
            "30     \t [0.39089679 0.49036206 0.98655399]. \t  2.204771871078166 \t 3.3767736055039483\n",
            "31     \t [0.05259668 0.93126404 0.42337443]. \t  1.7161401032011612 \t 3.3767736055039483\n",
            "32     \t [0.67447592 0.65975115 0.02035784]. \t  0.0065141861753424775 \t 3.3767736055039483\n",
            "33     \t [0.2104755  0.73152382 0.04076624]. \t  0.005417990297731637 \t 3.3767736055039483\n",
            "34     \t [0.79535334 0.19934445 0.24681059]. \t  0.5352921533918664 \t 3.3767736055039483\n",
            "35     \t [0.00135868 0.31449955 0.73629267]. \t  1.8079107731398107 \t 3.3767736055039483\n",
            "36     \t [0.00125862 0.920661   0.56938461]. \t  2.883075906271002 \t 3.3767736055039483\n",
            "37     \t [0.05924667 0.44521564 0.01146679]. \t  0.03591753099689977 \t 3.3767736055039483\n",
            "38     \t [0.66000013 0.54111575 0.85967381]. \t  \u001b[92m3.797156738595037\u001b[0m \t 3.797156738595037\n",
            "39     \t [0.56059883 0.49185922 0.85035456]. \t  3.701936439326789 \t 3.797156738595037\n",
            "40     \t [7.29754267e-10 6.63737287e-10 1.12717230e-01]. \t  0.2830839121474678 \t 3.797156738595037\n",
            "41     \t [0.00475997 0.05818068 0.82203684]. \t  0.42036219749393994 \t 3.797156738595037\n",
            "42     \t [0.94878568 0.51280735 0.81378961]. \t  3.4882754288413005 \t 3.797156738595037\n",
            "43     \t [0.7516846  0.48755015 0.86201195]. \t  3.6267809633884514 \t 3.797156738595037\n",
            "44     \t [0.9913189  1.         0.74047125]. \t  0.3855183561232732 \t 3.797156738595037\n",
            "45     \t [0.18053328 0.64239143 0.82985594]. \t  3.555383915082971 \t 3.797156738595037\n",
            "46     \t [1.         0.58315638 0.82429179]. \t  3.511664008149789 \t 3.797156738595037\n",
            "47     \t [0.07773591 0.98499179 0.72564396]. \t  1.4047097823436365 \t 3.797156738595037\n",
            "48     \t [7.81268101e-17 5.43319498e-17 9.97779522e-17]. \t  0.06797411659013242 \t 3.797156738595037\n",
            "49     \t [0.46867154 1.         0.91705426]. \t  0.5535482063069778 \t 3.797156738595037\n",
            "50     \t [0.01078733 0.64773997 0.6913461 ]. \t  2.5114617352786324 \t 3.797156738595037\n",
            "51     \t [0.41151202 0.22195127 0.86272256]. \t  1.382824808118837 \t 3.797156738595037\n",
            "52     \t [0.81301905 0.32704366 0.79254838]. \t  2.2615186800358944 \t 3.797156738595037\n",
            "53     \t [0.77933433 0.10770874 0.03763122]. \t  0.1238531442931908 \t 3.797156738595037\n",
            "54     \t [0.94738883 0.50342731 0.94658885]. \t  2.835859704214888 \t 3.797156738595037\n",
            "55     \t [0.23707204 0.73956288 0.85566292]. \t  2.845450619825491 \t 3.797156738595037\n",
            "56     \t [0.52224155 0.54916267 0.85959763]. \t  \u001b[92m3.834286853264193\u001b[0m \t 3.834286853264193\n",
            "57     \t [0.9668539  0.99275694 0.15084157]. \t  0.002251575197269827 \t 3.834286853264193\n",
            "58     \t [1.94803043e-01 3.72756259e-08 2.68774646e-01]. \t  0.7963978697305526 \t 3.834286853264193\n",
            "59     \t [0.01354801 0.75133907 0.00172035]. \t  0.0017366738488713061 \t 3.834286853264193\n",
            "60     \t [0.4306491  0.51631843 0.85531808]. \t  3.8028795722875812 \t 3.834286853264193\n",
            "61     \t [0.96813776 0.00330739 0.87305404]. \t  0.2190922320207996 \t 3.834286853264193\n",
            "62     \t [0.47318225 0.54032508 0.85544777]. \t  \u001b[92m3.8401035836106217\u001b[0m \t 3.8401035836106217\n",
            "63     \t [0.71211914 0.521973   0.85181149]. \t  3.756375590822211 \t 3.8401035836106217\n",
            "64     \t [0.5341542  0.58234816 0.83910432]. \t  3.7755174725843292 \t 3.8401035836106217\n",
            "65     \t [0.58304653 0.54341384 0.8529478 ]. \t  3.820588299212256 \t 3.8401035836106217\n",
            "66     \t [0.75134088 0.53914173 0.88936487]. \t  3.65232269438816 \t 3.8401035836106217\n",
            "67     \t [0.49378677 0.57691107 0.81503099]. \t  3.668966041856397 \t 3.8401035836106217\n",
            "68     \t [0.54793142 0.58249057 0.8447643 ]. \t  3.787423845061265 \t 3.8401035836106217\n",
            "69     \t [0.56991745 0.57042175 0.83258753]. \t  3.7613114640208174 \t 3.8401035836106217\n",
            "70     \t [1.         0.58804502 0.85922714]. \t  3.6230153301225885 \t 3.8401035836106217\n",
            "71     \t [0.88319232 0.4964446  0.84835701]. \t  3.622784450561135 \t 3.8401035836106217\n",
            "72     \t [0.84757355 0.55470088 0.84364837]. \t  3.7191675275927745 \t 3.8401035836106217\n",
            "73     \t [0.62051042 0.46815022 0.85551261]. \t  3.5731484043824056 \t 3.8401035836106217\n",
            "74     \t [0.47532893 0.55614824 0.85058163]. \t  \u001b[92m3.844161527392191\u001b[0m \t 3.844161527392191\n",
            "75     \t [0.91762603 0.50124187 0.85517118]. \t  3.6278916234837326 \t 3.844161527392191\n",
            "76     \t [0.21774654 0.60955027 0.83948732]. \t  3.738703500869203 \t 3.844161527392191\n",
            "77     \t [0.39069322 0.49608983 0.8277802 ]. \t  3.6918614241565475 \t 3.844161527392191\n",
            "78     \t [0.58473826 0.58076902 0.82461457]. \t  3.6954183205606905 \t 3.844161527392191\n",
            "79     \t [0.52813874 0.52911561 0.85691923]. \t  3.815301114641543 \t 3.844161527392191\n",
            "80     \t [0.71530314 0.5301175  0.85411365]. \t  3.7703124130303385 \t 3.844161527392191\n",
            "81     \t [0.40593191 0.63306646 0.83072545]. \t  3.5861339280499154 \t 3.844161527392191\n",
            "82     \t [0.68762857 0.94911861 0.00961222]. \t  0.0002385333562177538 \t 3.844161527392191\n",
            "83     \t [0.53770871 0.55149256 0.86668179]. \t  3.8197157360885354 \t 3.844161527392191\n",
            "84     \t [0.64498084 0.49968926 0.80807826]. \t  3.5295335451598078 \t 3.844161527392191\n",
            "85     \t [0.16723902 0.50262492 0.83839498]. \t  3.743474059348628 \t 3.844161527392191\n",
            "86     \t [0.40213781 0.52117517 0.85724806]. \t  3.815929917401372 \t 3.844161527392191\n",
            "87     \t [0.55429095 0.55505735 0.85219426]. \t  3.8293855690636063 \t 3.844161527392191\n",
            "88     \t [0.83096925 0.50734532 0.84451458]. \t  3.6717611294044152 \t 3.844161527392191\n",
            "89     \t [0.30151795 0.55279157 0.82862466]. \t  3.8057682294675206 \t 3.844161527392191\n",
            "90     \t [0.32300726 0.53379486 0.85113159]. \t  \u001b[92m3.845960866876595\u001b[0m \t 3.845960866876595\n",
            "91     \t [0.5781734  0.46761729 0.85779187]. \t  3.5756129890819883 \t 3.845960866876595\n",
            "92     \t [0.43876011 0.57805157 0.80639929]. \t  3.6162601749673318 \t 3.845960866876595\n",
            "93     \t [0.59798289 0.56606914 0.82124496]. \t  3.695089466657359 \t 3.845960866876595\n",
            "94     \t [0.5142109  0.53392271 0.85610723]. \t  3.825654001223622 \t 3.845960866876595\n",
            "95     \t [0.82594751 0.48529992 0.8663379 ]. \t  3.5830550600781974 \t 3.845960866876595\n",
            "96     \t [0.60520427 0.54406286 0.86384955]. \t  3.80787282749826 \t 3.845960866876595\n",
            "97     \t [0.59835922 0.49834412 0.85883516]. \t  3.715209127285943 \t 3.845960866876595\n",
            "98     \t [0.93319274 0.45946771 0.81203491]. \t  3.3013278088816103 \t 3.845960866876595\n",
            "99     \t [0.72123921 0.52513836 0.86089045]. \t  3.756417054213588 \t 3.845960866876595\n",
            "100    \t [0.23913699 0.53010755 0.83224022]. \t  3.802341092036368 \t 3.845960866876595\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9YT-CgKvuv4q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b92e1213-60f1-4bcb-9775-aa98597b3146"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 19 \r\n",
        "\r\n",
        "np.random.seed(run_num_19)\r\n",
        "surrogate_winner_19 = dtStudentProcess(d_cov_func, nu = df)\r\n",
        "\r\n",
        "winner_19 = dGPGO_stp(surrogate_winner_19, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_19.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.55944558 0.76149199 0.24453155]. \t  0.09433080659504385 \t 2.524990008735946\n",
            "init   \t [0.77168947 0.35447208 0.7966974 ]. \t  2.524990008735946 \t 2.524990008735946\n",
            "init   \t [0.34600002 0.44387444 0.30330359]. \t  0.3914194506273641 \t 2.524990008735946\n",
            "init   \t [0.44211392 0.57320936 0.06566402]. \t  0.036928944981920925 \t 2.524990008735946\n",
            "init   \t [0.02408889 0.82062263 0.36513765]. \t  0.9772047942100802 \t 2.524990008735946\n",
            "1      \t [1. 1. 1.]. \t  0.31688362070415665 \t 2.524990008735946\n",
            "2      \t [0.07273528 0.11679056 0.92142201]. \t  0.515611656190073 \t 2.524990008735946\n",
            "3      \t [0.92432215 0.00224485 0.20852539]. \t  0.31324730242838555 \t 2.524990008735946\n",
            "4      \t [0.31442429 0.70675657 0.98742714]. \t  1.9140879942091364 \t 2.524990008735946\n",
            "5      \t [0.63304028 0.03834706 0.92314675]. \t  0.25124688358467107 \t 2.524990008735946\n",
            "6      \t [0.9801444  0.31303689 0.98603756]. \t  1.2094522585519019 \t 2.524990008735946\n",
            "7      \t [0.96097387 0.68319896 0.13199964]. \t  0.009290414078297074 \t 2.524990008735946\n",
            "8      \t [0.11992636 0.02082448 0.08122865]. \t  0.26790366205121774 \t 2.524990008735946\n",
            "9      \t [0.01503963 0.9405733  0.8316103 ]. \t  1.106628875676408 \t 2.524990008735946\n",
            "10     \t [0.02581374 0.43761395 0.00587615]. \t  0.03239632935086902 \t 2.524990008735946\n",
            "11     \t [0.98559655 0.64864313 0.66939704]. \t  1.3240408889567048 \t 2.524990008735946\n",
            "12     \t [0.86890678 0.97256538 0.01290941]. \t  0.00011693650628731611 \t 2.524990008735946\n",
            "13     \t [0.5821969  0.97415855 0.99849111]. \t  0.41444940646966044 \t 2.524990008735946\n",
            "14     \t [0.84085026 0.67291347 0.99242075]. \t  1.9510558744255746 \t 2.524990008735946\n",
            "15     \t [0.0193673  0.94170574 0.10132114]. \t  0.006200678197065885 \t 2.524990008735946\n",
            "16     \t [0.83652111 0.14906396 0.01169926]. \t  0.07235033655995021 \t 2.524990008735946\n",
            "17     \t [0.05451781 0.49233413 0.72745882]. \t  \u001b[92m2.688012919442139\u001b[0m \t 2.688012919442139\n",
            "18     \t [0.03183465 0.0694183  0.60429075]. \t  0.19126620547412315 \t 2.688012919442139\n",
            "19     \t [0.38152827 0.56535409 0.82540396]. \t  \u001b[92m3.776180069196066\u001b[0m \t 3.776180069196066\n",
            "20     \t [0.30716295 0.95252026 0.58123291]. \t  2.516367915853483 \t 3.776180069196066\n",
            "21     \t [0.55080921 0.35513698 0.96248313]. \t  1.801413669136461 \t 3.776180069196066\n",
            "22     \t [0.58564117 0.76234776 0.80021586]. \t  2.36313823309259 \t 3.776180069196066\n",
            "23     \t [0.35153238 0.21461387 0.69722339]. \t  0.9493240214491123 \t 3.776180069196066\n",
            "24     \t [0.99281125 0.24400313 0.51316391]. \t  0.17497492859852792 \t 3.776180069196066\n",
            "25     \t [0.04295749 0.99960944 0.58061125]. \t  2.5137577042019283 \t 3.776180069196066\n",
            "26     \t [0.94132921 0.00206299 0.70221015]. \t  0.2012650778241426 \t 3.776180069196066\n",
            "27     \t [0.23792391 0.99325664 0.09348308]. \t  0.0043596975583593415 \t 3.776180069196066\n",
            "28     \t [5.07707710e-01 4.74275969e-04 1.56000585e-01]. \t  0.5682483292553047 \t 3.776180069196066\n",
            "29     \t [0.28533814 0.76904448 0.72577114]. \t  2.3375564962606177 \t 3.776180069196066\n",
            "30     \t [0.97202079 0.95648456 0.48976229]. \t  0.2753634396818416 \t 3.776180069196066\n",
            "31     \t [0.39703531 0.07693866 0.00784924]. \t  0.1303071683714754 \t 3.776180069196066\n",
            "32     \t [0.03005883 0.70215979 0.93284465]. \t  2.6645062732228655 \t 3.776180069196066\n",
            "33     \t [0.01541781 0.35569143 0.96837867]. \t  1.7137978663110727 \t 3.776180069196066\n",
            "34     \t [0.70268888 0.9937478  0.75455829]. \t  0.6332877237842449 \t 3.776180069196066\n",
            "35     \t [1.         0.67082205 1.        ]. \t  1.80776751374065 \t 3.776180069196066\n",
            "36     \t [0.56143848 0.50764033 0.67395606]. \t  1.891406764960775 \t 3.776180069196066\n",
            "37     \t [5.80523573e-10 5.80523573e-10 5.80523573e-10]. \t  0.06797411740268333 \t 3.776180069196066\n",
            "38     \t [0.97066417 0.52365105 0.91787616]. \t  3.279079751468473 \t 3.776180069196066\n",
            "39     \t [0.00706139 0.27882125 0.17766098]. \t  0.40952780403303873 \t 3.776180069196066\n",
            "40     \t [0.99682477 0.87685925 0.19019687]. \t  0.006039366913482054 \t 3.776180069196066\n",
            "41     \t [0.99767452 0.26448242 0.08286597]. \t  0.08857016661767272 \t 3.776180069196066\n",
            "42     \t [0.22063065 0.99724434 0.34866667]. \t  0.6940161614298773 \t 3.776180069196066\n",
            "43     \t [0.81912688 0.38625211 0.21646981]. \t  0.24605547460761643 \t 3.776180069196066\n",
            "44     \t [0.5870663  0.01858167 0.69272787]. \t  0.2300897236469444 \t 3.776180069196066\n",
            "45     \t [0.55620985 0.9387172  0.02934091]. \t  0.0005923322215505613 \t 3.776180069196066\n",
            "46     \t [1.         1.         0.78057906]. \t  0.4479962449530717 \t 3.776180069196066\n",
            "47     \t [0.02594108 0.54059643 0.760045  ]. \t  3.172373557865896 \t 3.776180069196066\n",
            "48     \t [0.18261068 0.63595588 0.85624392]. \t  3.634751065862042 \t 3.776180069196066\n",
            "49     \t [0.4080678  0.00529694 0.9864664 ]. \t  0.1108751489933052 \t 3.776180069196066\n",
            "50     \t [0.31474986 0.50114348 0.87610859]. \t  3.69944758802189 \t 3.776180069196066\n",
            "51     \t [0.98851943 0.44973191 0.86412823]. \t  3.33335589140297 \t 3.776180069196066\n",
            "52     \t [0.18727295 0.48655029 0.87381685]. \t  3.6372394008017377 \t 3.776180069196066\n",
            "53     \t [0.46559326 0.57329702 0.83902835]. \t  \u001b[92m3.8083486801272017\u001b[0m \t 3.8083486801272017\n",
            "54     \t [0.36020224 0.5523089  0.79994731]. \t  3.60523782686228 \t 3.8083486801272017\n",
            "55     \t [0.30463116 0.98682945 0.8291442 ]. \t  0.8195496337666259 \t 3.8083486801272017\n",
            "56     \t [0.00265529 0.86502449 0.59439221]. \t  2.9222374767605994 \t 3.8083486801272017\n",
            "57     \t [2.16256129e-10 7.67322101e-10 3.36446032e-01]. \t  0.5030968252179437 \t 3.8083486801272017\n",
            "58     \t [0.48955085 0.58503905 0.86789947]. \t  3.793231372061227 \t 3.8083486801272017\n",
            "59     \t [0.31286493 0.52340249 0.87928364]. \t  3.752305179635634 \t 3.8083486801272017\n",
            "60     \t [0.00146376 0.61194987 0.87810103]. \t  3.6474387923293086 \t 3.8083486801272017\n",
            "61     \t [0.31676435 0.64630988 0.8225264 ]. \t  3.494757170116166 \t 3.8083486801272017\n",
            "62     \t [0.03259821 0.52476707 0.80330554]. \t  3.58818183014887 \t 3.8083486801272017\n",
            "63     \t [0.29851046 0.56164327 0.8607276 ]. \t  \u001b[92m3.8543203246798887\u001b[0m \t 3.8543203246798887\n",
            "64     \t [0.26345614 0.45403693 0.83321725]. \t  3.503332877541431 \t 3.8543203246798887\n",
            "65     \t [0.04438122 0.64063633 0.85349447]. \t  3.583569540917485 \t 3.8543203246798887\n",
            "66     \t [0.16888576 0.53369223 0.83919218]. \t  3.8214293739688707 \t 3.8543203246798887\n",
            "67     \t [0.28336918 0.58773114 0.8403244 ]. \t  3.8068367059168793 \t 3.8543203246798887\n",
            "68     \t [0.35332968 0.58889943 0.80260742]. \t  3.584549977806783 \t 3.8543203246798887\n",
            "69     \t [0.24568236 0.59888027 0.81005908]. \t  3.630917587494511 \t 3.8543203246798887\n",
            "70     \t [0.51262041 0.49946426 0.87702637]. \t  3.67697735559856 \t 3.8543203246798887\n",
            "71     \t [0.20431318 0.58351669 0.88645951]. \t  3.721372261068683 \t 3.8543203246798887\n",
            "72     \t [1.         0.43844413 0.86687373]. \t  3.245876468260354 \t 3.8543203246798887\n",
            "73     \t [0.36063581 0.54424985 0.88223465]. \t  3.7700355262267764 \t 3.8543203246798887\n",
            "74     \t [0.1991344  0.59869499 0.82743227]. \t  3.7312637097623167 \t 3.8543203246798887\n",
            "75     \t [0.18384676 0.54192071 0.84121168]. \t  3.837175282898515 \t 3.8543203246798887\n",
            "76     \t [0.35529085 0.5268981  0.87112802]. \t  3.797790082869639 \t 3.8543203246798887\n",
            "77     \t [0.31219428 0.58081007 0.75630516]. \t  3.134497457011916 \t 3.8543203246798887\n",
            "78     \t [0.51452046 0.55925081 0.85612899]. \t  3.83733904735973 \t 3.8543203246798887\n",
            "79     \t [0.48554026 0.56647916 0.8719949 ]. \t  3.8087661576516343 \t 3.8543203246798887\n",
            "80     \t [0.00645401 0.53308614 0.87277279]. \t  3.7529313741186066 \t 3.8543203246798887\n",
            "81     \t [0.11343058 0.61301307 0.82667538]. \t  3.6690726312266913 \t 3.8543203246798887\n",
            "82     \t [0.00506178 0.54555405 0.86152499]. \t  3.8008455599814703 \t 3.8543203246798887\n",
            "83     \t [0.31143515 0.574661   0.85726006]. \t  3.8462367257087906 \t 3.8543203246798887\n",
            "84     \t [0.02420926 0.67872913 0.83647965]. \t  3.313525956731474 \t 3.8543203246798887\n",
            "85     \t [0.0036608  0.00743214 0.8163837 ]. \t  0.2634673477793573 \t 3.8543203246798887\n",
            "86     \t [0.4763943  0.54320192 0.83628873]. \t  3.8117551875014453 \t 3.8543203246798887\n",
            "87     \t [0.50990504 0.50798049 0.84600198]. \t  3.7654845659765326 \t 3.8543203246798887\n",
            "88     \t [0.93632077 0.0047476  0.06929368]. \t  0.1035088432719931 \t 3.8543203246798887\n",
            "89     \t [0.59752908 0.53549645 0.91078882]. \t  3.489613367991222 \t 3.8543203246798887\n",
            "90     \t [0.33256287 0.4706244  0.89438773]. \t  3.4300580755632772 \t 3.8543203246798887\n",
            "91     \t [0.46300738 0.60144201 0.95308326]. \t  2.8870560104649736 \t 3.8543203246798887\n",
            "92     \t [0.33332396 0.60171798 0.8334608 ]. \t  3.742806800399334 \t 3.8543203246798887\n",
            "93     \t [0.00878371 0.55641846 0.82292923]. \t  3.7340924393780437 \t 3.8543203246798887\n",
            "94     \t [0.15191099 0.52836326 0.86651492]. \t  3.802038545363783 \t 3.8543203246798887\n",
            "95     \t [0.09040439 0.58869752 0.77310347]. \t  3.3193398035283863 \t 3.8543203246798887\n",
            "96     \t [0.1440364  0.60995045 0.82537828]. \t  3.6796193193833204 \t 3.8543203246798887\n",
            "97     \t [0.07172067 0.57194263 0.85674237]. \t  3.82183907749139 \t 3.8543203246798887\n",
            "98     \t [0.32692265 0.63074226 0.84188908]. \t  3.6484987898276646 \t 3.8543203246798887\n",
            "99     \t [0.85279097 0.4987815  0.86090744]. \t  3.6396298613637272 \t 3.8543203246798887\n",
            "100    \t [0.22343574 0.51037291 0.86920446]. \t  3.75605745200819 \t 3.8543203246798887\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHz_Jg2_uv7E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d79f7ea-3974-449d-f94f-3f7fbd810071"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 20 \r\n",
        "\r\n",
        "np.random.seed(run_num_20)\r\n",
        "surrogate_winner_20 = dtStudentProcess(d_cov_func, nu = df)\r\n",
        "\r\n",
        "winner_20 = dGPGO_stp(surrogate_winner_20, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_20.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.57051729 0.56452876 0.48844183]. \t  0.675391399411646 \t 0.675391399411646\n",
            "init   \t [0.33647775 0.37586818 0.53203587]. \t  0.5331349538596052 \t 0.675391399411646\n",
            "init   \t [0.06810629 0.58452906 0.23789776]. \t  0.14747335095307315 \t 0.675391399411646\n",
            "init   \t [0.16075658 0.15211915 0.12706922]. \t  0.48089725804912437 \t 0.675391399411646\n",
            "init   \t [0.32744117 0.69415387 0.35896647]. \t  0.6289408047543804 \t 0.675391399411646\n",
            "1      \t [1. 1. 1.]. \t  0.31688362070415665 \t 0.675391399411646\n",
            "2      \t [0.91932094 0.0125958  0.17618825]. \t  0.2816999755052942 \t 0.675391399411646\n",
            "3      \t [0.837611   0.97862966 0.10314233]. \t  0.0013418779384681767 \t 0.675391399411646\n",
            "4      \t [0.06189466 0.96092926 0.90567924]. \t  \u001b[92m0.79804840905328\u001b[0m \t 0.79804840905328\n",
            "5      \t [0.59600245 0.11214229 0.96876455]. \t  0.3643251059670551 \t 0.79804840905328\n",
            "6      \t [0.48020344 1.         1.        ]. \t  0.3326558378814126 \t 0.79804840905328\n",
            "7      \t [0.95233614 0.11804843 0.92757127]. \t  0.49408805296979985 \t 0.79804840905328\n",
            "8      \t [0.12105741 0.02540749 0.95561791]. \t  0.1766161034792389 \t 0.79804840905328\n",
            "9      \t [1.         1.         0.55804557]. \t  0.2629465466609911 \t 0.79804840905328\n",
            "10     \t [0.16052631 0.97873968 0.2700848 ]. \t  0.2349097963687792 \t 0.79804840905328\n",
            "11     \t [1.         0.52608743 1.        ]. \t  \u001b[92m1.9757410708426362\u001b[0m \t 1.9757410708426362\n",
            "12     \t [0.97542554 0.50390474 0.13114004]. \t  0.04293659603319036 \t 1.9757410708426362\n",
            "13     \t [0.11705285 0.43160638 0.96433489]. \t  \u001b[92m2.283755194185346\u001b[0m \t 2.283755194185346\n",
            "14     \t [0.34058572 0.05288095 0.00942842]. \t  0.1302308050912332 \t 2.283755194185346\n",
            "15     \t [0.31050625 0.67549693 0.96280544]. \t  \u001b[92m2.4536256383635555\u001b[0m \t 2.4536256383635555\n",
            "16     \t [0.01032311 0.03192419 0.54224448]. \t  0.12258280209508052 \t 2.4536256383635555\n",
            "17     \t [0.54251771 0.52291485 1.        ]. \t  2.042242332896541 \t 2.4536256383635555\n",
            "18     \t [3.34486217e-01 9.24824784e-01 4.31684883e-04]. \t  0.0004216650332948359 \t 2.4536256383635555\n",
            "19     \t [0.31026769 0.69486733 0.78778293]. \t  \u001b[92m2.9810843881832554\u001b[0m \t 2.9810843881832554\n",
            "20     \t [0.57760943 1.         0.71315415]. \t  0.8619823622339559 \t 2.9810843881832554\n",
            "21     \t [0.9660779  0.59163665 0.68642229]. \t  1.7036873605234961 \t 2.9810843881832554\n",
            "22     \t [0.01843427 0.78659484 0.6859674 ]. \t  2.4456000776787 \t 2.9810843881832554\n",
            "23     \t [0.07678056 0.02439194 0.16335369]. \t  0.5138302019358658 \t 2.9810843881832554\n",
            "24     \t [0.97392158 0.04372239 0.05004288]. \t  0.07669897323348131 \t 2.9810843881832554\n",
            "25     \t [0.82711997 0.75555519 1.        ]. \t  1.4441405244190373 \t 2.9810843881832554\n",
            "26     \t [0.08808847 0.52203414 0.80624843]. \t  \u001b[92m3.6212343549990167\u001b[0m \t 3.6212343549990167\n",
            "27     \t [0.0250507  0.39743033 0.52111178]. \t  0.5591155331822365 \t 3.6212343549990167\n",
            "28     \t [0.95258531 0.00156086 0.58085808]. \t  0.08746576578361917 \t 3.6212343549990167\n",
            "29     \t [0.02008781 0.31388315 0.03005184]. \t  0.0870860874302206 \t 3.6212343549990167\n",
            "30     \t [0.90043588 0.31685158 0.75633304]. \t  1.9359044023066616 \t 3.6212343549990167\n",
            "31     \t [0.02663339 0.67092254 0.99706609]. \t  1.9049784236761094 \t 3.6212343549990167\n",
            "32     \t [0.96875164 0.30648242 0.35640757]. \t  0.19584357933848295 \t 3.6212343549990167\n",
            "33     \t [0.50346194 0.62784014 0.08144654]. \t  0.025964646252916387 \t 3.6212343549990167\n",
            "34     \t [3.48247387e-02 5.14208691e-09 5.12081631e-09]. \t  0.07315421665256301 \t 3.6212343549990167\n",
            "35     \t [0.01980084 0.9875931  0.12576637]. \t  0.010493609912051663 \t 3.6212343549990167\n",
            "36     \t [0.10008952 0.55552705 0.85208746]. \t  \u001b[92m3.8395641091254396\u001b[0m \t 3.8395641091254396\n",
            "37     \t [0.29662742 0.25099936 0.82873903]. \t  1.6867515873582348 \t 3.8395641091254396\n",
            "38     \t [0.17658642 0.70301814 0.77952661]. \t  2.915261185416365 \t 3.8395641091254396\n",
            "39     \t [0.42963223 0.01550954 0.57585782]. \t  0.12968645796349357 \t 3.8395641091254396\n",
            "40     \t [0.01948703 0.64825288 0.784739  ]. \t  3.217868873811952 \t 3.8395641091254396\n",
            "41     \t [0.9840872  0.81573241 0.02030661]. \t  0.0004461443039508049 \t 3.8395641091254396\n",
            "42     \t [0.05086383 0.37644611 0.84826717]. \t  2.8636937902309523 \t 3.8395641091254396\n",
            "43     \t [0.01429484 0.57219587 0.77073855]. \t  3.289557457012761 \t 3.8395641091254396\n",
            "44     \t [0.16674167 0.55249897 0.76620074]. \t  3.278681411639268 \t 3.8395641091254396\n",
            "45     \t [0.12505791 0.54925122 0.77493107]. \t  3.3685456881581386 \t 3.8395641091254396\n",
            "46     \t [0.52753496 0.5876544  0.83755802]. \t  3.7594404871624523 \t 3.8395641091254396\n",
            "47     \t [0.72415162 0.53745578 0.88343826]. \t  3.695778510381102 \t 3.8395641091254396\n",
            "48     \t [0.98035711 0.7990809  0.26933339]. \t  0.028527594597598446 \t 3.8395641091254396\n",
            "49     \t [0.73745298 0.50483253 0.8662743 ]. \t  3.6881075438313244 \t 3.8395641091254396\n",
            "50     \t [0.1245756  0.62179697 0.0030576 ]. \t  0.00822342949018662 \t 3.8395641091254396\n",
            "51     \t [0.5926078  0.54091719 0.85194665]. \t  3.816061114188427 \t 3.8395641091254396\n",
            "52     \t [0.53791715 0.51981083 0.85800328]. \t  3.79380410725762 \t 3.8395641091254396\n",
            "53     \t [0.58614856 0.48646192 0.86530568]. \t  3.655012992626349 \t 3.8395641091254396\n",
            "54     \t [0.47144903 0.52616729 0.84821269]. \t  3.8185126659583646 \t 3.8395641091254396\n",
            "55     \t [0.42166694 0.52979987 0.85846026]. \t  3.8303971199346862 \t 3.8395641091254396\n",
            "56     \t [0.55333232 0.59016663 0.82029902]. \t  3.6554647531012616 \t 3.8395641091254396\n",
            "57     \t [0.70839364 0.52490621 0.85928178]. \t  3.761655621068675 \t 3.8395641091254396\n",
            "58     \t [0.71959698 0.54772439 0.8500814 ]. \t  3.780581189031323 \t 3.8395641091254396\n",
            "59     \t [0.95014987 0.99992611 0.1033689 ]. \t  0.000745600462757961 \t 3.8395641091254396\n",
            "60     \t [0.5508306  0.55369257 0.85403999]. \t  3.8311149127733994 \t 3.8395641091254396\n",
            "61     \t [0.57174352 0.5204328  0.85835637]. \t  3.788197973320588 \t 3.8395641091254396\n",
            "62     \t [0.02298554 0.55670027 0.85201259]. \t  3.818378794620722 \t 3.8395641091254396\n",
            "63     \t [0.66950966 0.48286949 0.82951673]. \t  3.5929080968205342 \t 3.8395641091254396\n",
            "64     \t [0.47773293 0.60150924 0.83586537]. \t  3.7252490842922987 \t 3.8395641091254396\n",
            "65     \t [0.65017627 0.53302609 0.85747216]. \t  3.792821010581615 \t 3.8395641091254396\n",
            "66     \t [0.44800249 0.56581682 0.84644629]. \t  3.838288810084411 \t 3.8395641091254396\n",
            "67     \t [0.00250119 0.54938586 0.83376216]. \t  3.778270488866815 \t 3.8395641091254396\n",
            "68     \t [0.6242034  0.49012599 0.87319416]. \t  3.636585390992881 \t 3.8395641091254396\n",
            "69     \t [0.62719604 0.54674279 0.85013193]. \t  3.8086251464959893 \t 3.8395641091254396\n",
            "70     \t [0.63601041 0.50485591 0.87270396]. \t  3.69380909410327 \t 3.8395641091254396\n",
            "71     \t [0.28520209 0.53450032 0.84057208]. \t  3.8342755614293282 \t 3.8395641091254396\n",
            "72     \t [0.01679442 0.57202159 0.83946539]. \t  3.7900955807452967 \t 3.8395641091254396\n",
            "73     \t [0.06558108 0.60683762 0.81033467]. \t  3.5884923082851516 \t 3.8395641091254396\n",
            "74     \t [0.64661919 0.506795   0.88867612]. \t  3.609586599726966 \t 3.8395641091254396\n",
            "75     \t [0.4440182  0.57368259 0.84383353]. \t  3.824587338659878 \t 3.8395641091254396\n",
            "76     \t [0.61463955 0.59048068 0.81581243]. \t  3.599783832055868 \t 3.8395641091254396\n",
            "77     \t [0.65687974 0.52804064 0.84646316]. \t  3.777892701807322 \t 3.8395641091254396\n",
            "78     \t [0.61575855 0.51125289 0.77982513]. \t  3.286134794265065 \t 3.8395641091254396\n",
            "79     \t [0.39724439 0.52614165 0.77961192]. \t  3.3781449222564928 \t 3.8395641091254396\n",
            "80     \t [0.64061576 0.50679854 0.86853517]. \t  3.714036616948459 \t 3.8395641091254396\n",
            "81     \t [0.36261624 0.53944298 0.85557004]. \t  \u001b[92m3.8508525285324327\u001b[0m \t 3.8508525285324327\n",
            "82     \t [0.69574568 0.54163061 0.86022685]. \t  3.7866579932191367 \t 3.8508525285324327\n",
            "83     \t [0.49468962 0.55879425 0.83757814]. \t  3.812259076240134 \t 3.8508525285324327\n",
            "84     \t [0.42066488 0.5673608  0.84336154]. \t  3.8353487344685773 \t 3.8508525285324327\n",
            "85     \t [0.65639896 0.51385777 0.86383615]. \t  3.7435772652756247 \t 3.8508525285324327\n",
            "86     \t [0.65305    0.50590427 0.84317881]. \t  3.722570694753817 \t 3.8508525285324327\n",
            "87     \t [0.62472114 0.56286215 0.87836232]. \t  3.7578729537851396 \t 3.8508525285324327\n",
            "88     \t [0.63531811 0.55162671 0.89396748]. \t  3.658643948990682 \t 3.8508525285324327\n",
            "89     \t [0.66021717 0.59414762 0.8089862 ]. \t  3.514000998842091 \t 3.8508525285324327\n",
            "90     \t [0.65556563 0.48808298 0.85918603]. \t  3.6599380660193295 \t 3.8508525285324327\n",
            "91     \t [0.64251832 0.52235433 0.88862803]. \t  3.6558958512866475 \t 3.8508525285324327\n",
            "92     \t [0.72389798 0.53925152 0.83924185]. \t  3.751254041871751 \t 3.8508525285324327\n",
            "93     \t [0.68613198 0.54305781 0.84820908]. \t  3.787247657335995 \t 3.8508525285324327\n",
            "94     \t [0.61472396 0.57577012 0.83413152]. \t  3.7453917834057546 \t 3.8508525285324327\n",
            "95     \t [0.02436633 0.47415385 0.85450637]. \t  3.5945874183695095 \t 3.8508525285324327\n",
            "96     \t [0.01065354 0.70069819 0.7981116 ]. \t  2.9987210743234063 \t 3.8508525285324327\n",
            "97     \t [0.64693408 0.53360794 0.84871307]. \t  3.7915609605871934 \t 3.8508525285324327\n",
            "98     \t [0.27628675 0.55764251 0.83933395]. \t  3.8434123348293303 \t 3.8508525285324327\n",
            "99     \t [0.64243114 0.57366967 0.85158922]. \t  3.788672478148163 \t 3.8508525285324327\n",
            "100    \t [0.68241336 0.49358515 0.87002485]. \t  3.650012267687986 \t 3.8508525285324327\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUnhsKpCuv9o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f91b5bc-1ee9-4e99-cab9-bdb9a0d08b28"
      },
      "source": [
        "end_win = time.time()\r\n",
        "end_win\r\n",
        "\r\n",
        "time_win = end_win - start_win\r\n",
        "time_win"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13830.415667295456"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJG0SLpwuwAL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7550ecba-b4e1-40f0-cb70-d99e45cc2536"
      },
      "source": [
        "### Training regret minimization: run number = 1\r\n",
        "\r\n",
        "loser_output_1 = np.append(np.max(loser_1.GP.y[0:n_init]),loser_1.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_1 = np.append(np.max(winner_1.GP.y[0:n_init]),winner_1.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_1 = np.log(y_global_orig - loser_output_1)\r\n",
        "regret_winner_1 = np.log(y_global_orig - winner_output_1)\r\n",
        "\r\n",
        "train_regret_loser_1 = min_max_array(regret_loser_1)\r\n",
        "train_regret_winner_1 = min_max_array(regret_winner_1)\r\n",
        "\r\n",
        "min_train_regret_loser_1 = min(train_regret_loser_1)\r\n",
        "min_train_regret_winner_1 = min(train_regret_winner_1)\r\n",
        "\r\n",
        "min_train_regret_loser_1, min_train_regret_winner_1"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-3.282774727971788, -4.191404685927137)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lA9eZf0uwCx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b0cd009-038a-45ea-c7c8-bce4550c257c"
      },
      "source": [
        "### Training regret minimization: run number = 2\r\n",
        "\r\n",
        "loser_output_2 = np.append(np.max(loser_2.GP.y[0:n_init]),loser_2.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_2 = np.append(np.max(winner_2.GP.y[0:n_init]),winner_2.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_2 = np.log(y_global_orig - loser_output_2)\r\n",
        "regret_winner_2 = np.log(y_global_orig - winner_output_2)\r\n",
        "\r\n",
        "train_regret_loser_2 = min_max_array(regret_loser_2)\r\n",
        "train_regret_winner_2 = min_max_array(regret_winner_2)\r\n",
        "\r\n",
        "min_train_regret_loser_2 = min(train_regret_loser_2)\r\n",
        "min_train_regret_winner_2 = min(train_regret_winner_2)\r\n",
        "\r\n",
        "min_train_regret_loser_2, min_train_regret_winner_2"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-4.788559953253709, -5.059915749314656)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glrTGcpAuwFa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f9997ad-5ac7-4710-e223-18984c2e68d1"
      },
      "source": [
        "### Training regret minimization: run number = 3\r\n",
        "\r\n",
        "loser_output_3 = np.append(np.max(loser_3.GP.y[0:n_init]),loser_3.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_3 = np.append(np.max(winner_3.GP.y[0:n_init]),winner_3.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_3 = np.log(y_global_orig - loser_output_3)\r\n",
        "regret_winner_3 = np.log(y_global_orig - winner_output_3)\r\n",
        "\r\n",
        "train_regret_loser_3 = min_max_array(regret_loser_3)\r\n",
        "train_regret_winner_3 = min_max_array(regret_winner_3)\r\n",
        "\r\n",
        "min_train_regret_loser_3 = min(train_regret_loser_3)\r\n",
        "min_train_regret_winner_3 = min(train_regret_winner_3)\r\n",
        "\r\n",
        "min_train_regret_loser_3, min_train_regret_winner_3"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-2.6160962144747533, -4.47189402290943)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LaRwfbxeuwIR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1bea8d78-429f-4013-8a44-c2e7d73f22c6"
      },
      "source": [
        "### Training regret minimization: run number = 4\r\n",
        "\r\n",
        "loser_output_4 = np.append(np.max(loser_4.GP.y[0:n_init]),loser_4.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_4 = np.append(np.max(winner_4.GP.y[0:n_init]),winner_4.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_4 = np.log(y_global_orig - loser_output_4)\r\n",
        "regret_winner_4 = np.log(y_global_orig - winner_output_4)\r\n",
        "\r\n",
        "train_regret_loser_4 = min_max_array(regret_loser_4)\r\n",
        "train_regret_winner_4 = min_max_array(regret_winner_4)\r\n",
        "\r\n",
        "min_train_regret_loser_4 = min(train_regret_loser_4)\r\n",
        "min_train_regret_winner_4 = min(train_regret_winner_4)\r\n",
        "\r\n",
        "min_train_regret_loser_4, min_train_regret_winner_4"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-2.430281937963454, -4.182699704027455)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Nb5NkfyuwKp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e658278-3ffb-4583-fbb4-730cb5eec3a6"
      },
      "source": [
        "### Training regret minimization: run number = 5\r\n",
        "\r\n",
        "loser_output_5 = np.append(np.max(loser_5.GP.y[0:n_init]),loser_5.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_5 = np.append(np.max(winner_5.GP.y[0:n_init]),winner_5.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_5 = np.log(y_global_orig - loser_output_5)\r\n",
        "regret_winner_5 = np.log(y_global_orig - winner_output_5)\r\n",
        "\r\n",
        "train_regret_loser_5 = min_max_array(regret_loser_5)\r\n",
        "train_regret_winner_5 = min_max_array(regret_winner_5)\r\n",
        "\r\n",
        "min_train_regret_loser_5 = min(train_regret_loser_5)\r\n",
        "min_train_regret_winner_5 = min(train_regret_winner_5)\r\n",
        "\r\n",
        "min_train_regret_loser_5, min_train_regret_winner_5"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-3.7336139875344303, -6.207757561288645)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0Q-WfXbuwNg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e151e3b1-5a0c-48b9-d52d-5e6ec768db22"
      },
      "source": [
        "### Training regret minimization: run number = 6\r\n",
        "\r\n",
        "loser_output_6 = np.append(np.max(loser_6.GP.y[0:n_init]),loser_6.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_6 = np.append(np.max(winner_6.GP.y[0:n_init]),winner_6.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_6 = np.log(y_global_orig - loser_output_6)\r\n",
        "regret_winner_6 = np.log(y_global_orig - winner_output_6)\r\n",
        "\r\n",
        "train_regret_loser_6 = min_max_array(regret_loser_6)\r\n",
        "train_regret_winner_6 = min_max_array(regret_winner_6)\r\n",
        "\r\n",
        "min_train_regret_loser_6 = min(train_regret_loser_6)\r\n",
        "min_train_regret_winner_6 = min(train_regret_winner_6)\r\n",
        "\r\n",
        "min_train_regret_loser_6, min_train_regret_winner_6"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-2.652528982265508, -4.657550493565768)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqqS7VLcuwPy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "073daaed-933b-4be1-d2e8-853d1f6b4160"
      },
      "source": [
        "### Training regret minimization: run number = 7\r\n",
        "\r\n",
        "loser_output_7 = np.append(np.max(loser_7.GP.y[0:n_init]),loser_7.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_7 = np.append(np.max(winner_7.GP.y[0:n_init]),winner_7.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_7 = np.log(y_global_orig - loser_output_7)\r\n",
        "regret_winner_7 = np.log(y_global_orig - winner_output_7)\r\n",
        "\r\n",
        "train_regret_loser_7 = min_max_array(regret_loser_7)\r\n",
        "train_regret_winner_7 = min_max_array(regret_winner_7)\r\n",
        "\r\n",
        "min_train_regret_loser_7 = min(train_regret_loser_7)\r\n",
        "min_train_regret_winner_7 = min(train_regret_winner_7)\r\n",
        "\r\n",
        "min_train_regret_loser_7, min_train_regret_winner_7"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-3.769609928766803, -3.9986036249751225)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOi5iX8guwSS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "273c0f2f-d7ed-4876-ba51-2a304c0a43ef"
      },
      "source": [
        "### Training regret minimization: run number = 8\r\n",
        "\r\n",
        "loser_output_8 = np.append(np.max(loser_8.GP.y[0:n_init]),loser_8.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_8 = np.append(np.max(winner_8.GP.y[0:n_init]),winner_8.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_8 = np.log(y_global_orig - loser_output_8)\r\n",
        "regret_winner_8 = np.log(y_global_orig - winner_output_8)\r\n",
        "\r\n",
        "train_regret_loser_8 = min_max_array(regret_loser_8)\r\n",
        "train_regret_winner_8 = min_max_array(regret_winner_8)\r\n",
        "\r\n",
        "min_train_regret_loser_8 = min(train_regret_loser_8)\r\n",
        "min_train_regret_winner_8 = min(train_regret_winner_8)\r\n",
        "\r\n",
        "min_train_regret_loser_8, min_train_regret_winner_8"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-2.395386791871129, -5.480159673259908)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFVApeazuwU5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea6c5d36-7b66-485f-99f8-b3b4860df1a5"
      },
      "source": [
        "### Training regret minimization: run number = 9\r\n",
        "\r\n",
        "loser_output_9 = np.append(np.max(loser_9.GP.y[0:n_init]),loser_9.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_9 = np.append(np.max(winner_9.GP.y[0:n_init]),winner_9.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_9 = np.log(y_global_orig - loser_output_9)\r\n",
        "regret_winner_9 = np.log(y_global_orig - winner_output_9)\r\n",
        "\r\n",
        "train_regret_loser_9 = min_max_array(regret_loser_9)\r\n",
        "train_regret_winner_9 = min_max_array(regret_winner_9)\r\n",
        "\r\n",
        "min_train_regret_loser_9 = min(train_regret_loser_9)\r\n",
        "min_train_regret_winner_9 = min(train_regret_winner_9)\r\n",
        "\r\n",
        "min_train_regret_loser_9, min_train_regret_winner_9"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-2.2792796072553396, -3.4970096182173913)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g92jk9WJuwXb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cdb5ff40-4d28-4fda-9ed6-9b28f303e723"
      },
      "source": [
        "### Training regret minimization: run number = 10\r\n",
        "\r\n",
        "loser_output_10 = np.append(np.max(loser_10.GP.y[0:n_init]),loser_10.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_10 = np.append(np.max(winner_10.GP.y[0:n_init]),winner_10.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_10 = np.log(y_global_orig - loser_output_10)\r\n",
        "regret_winner_10 = np.log(y_global_orig - winner_output_10)\r\n",
        "\r\n",
        "train_regret_loser_10 = min_max_array(regret_loser_10)\r\n",
        "train_regret_winner_10 = min_max_array(regret_winner_10)\r\n",
        "\r\n",
        "min_train_regret_loser_10 = min(train_regret_loser_10)\r\n",
        "min_train_regret_winner_10 = min(train_regret_winner_10)\r\n",
        "\r\n",
        "min_train_regret_loser_10, min_train_regret_winner_10"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-4.737286933990644, -4.45908943541511)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmcF1x-NuwZz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93a96b7f-df20-4645-ff5f-9609cec37927"
      },
      "source": [
        "### Training regret minimization: run number = 11\r\n",
        "\r\n",
        "loser_output_11 = np.append(np.max(loser_11.GP.y[0:n_init]),loser_11.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_11 = np.append(np.max(winner_11.GP.y[0:n_init]),winner_11.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_11 = np.log(y_global_orig - loser_output_11)\r\n",
        "regret_winner_11 = np.log(y_global_orig - winner_output_11)\r\n",
        "\r\n",
        "train_regret_loser_11 = min_max_array(regret_loser_11)\r\n",
        "train_regret_winner_11 = min_max_array(regret_winner_11)\r\n",
        "\r\n",
        "min_train_regret_loser_11 = min(train_regret_loser_11)\r\n",
        "min_train_regret_winner_11 = min(train_regret_winner_11)\r\n",
        "\r\n",
        "min_train_regret_loser_11, min_train_regret_winner_11"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-3.097587462537361, -5.705807840703222)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8axVhb6uwcc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7401fcb-0d2a-41ca-bbe5-ceb3babc4d64"
      },
      "source": [
        "### Training regret minimization: run number = 12\r\n",
        "\r\n",
        "loser_output_12 = np.append(np.max(loser_12.GP.y[0:n_init]),loser_12.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_12 = np.append(np.max(winner_12.GP.y[0:n_init]),winner_12.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_12 = np.log(y_global_orig - loser_output_12)\r\n",
        "regret_winner_12 = np.log(y_global_orig - winner_output_12)\r\n",
        "\r\n",
        "train_regret_loser_12 = min_max_array(regret_loser_12)\r\n",
        "train_regret_winner_12 = min_max_array(regret_winner_12)\r\n",
        "\r\n",
        "min_train_regret_loser_12 = min(train_regret_loser_12)\r\n",
        "min_train_regret_winner_12 = min(train_regret_winner_12)\r\n",
        "\r\n",
        "min_train_regret_loser_12, min_train_regret_winner_12"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-3.1319697100500283, -5.1309346076857985)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzlrL8XFuwfB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02a3ab7e-1027-4573-f831-fb57b9486919"
      },
      "source": [
        "### Training regret minimization: run number = 13\r\n",
        "\r\n",
        "loser_output_13 = np.append(np.max(loser_13.GP.y[0:n_init]),loser_13.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_13 = np.append(np.max(winner_13.GP.y[0:n_init]),winner_13.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_13 = np.log(y_global_orig - loser_output_13)\r\n",
        "regret_winner_13 = np.log(y_global_orig - winner_output_13)\r\n",
        "\r\n",
        "train_regret_loser_13 = min_max_array(regret_loser_13)\r\n",
        "train_regret_winner_13 = min_max_array(regret_winner_13)\r\n",
        "\r\n",
        "min_train_regret_loser_13 = min(train_regret_loser_13)\r\n",
        "min_train_regret_winner_13 = min(train_regret_winner_13)\r\n",
        "\r\n",
        "min_train_regret_loser_13, min_train_regret_winner_13"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-2.566664591184776, -3.8185399517237033)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3uZlLJ1quwh6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd136ac2-4c8b-460c-df58-69429c2271ad"
      },
      "source": [
        "### Training regret minimization: run number = 14\r\n",
        "\r\n",
        "loser_output_14 = np.append(np.max(loser_14.GP.y[0:n_init]),loser_14.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_14 = np.append(np.max(winner_14.GP.y[0:n_init]),winner_14.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_14 = np.log(y_global_orig - loser_output_14)\r\n",
        "regret_winner_14 = np.log(y_global_orig - winner_output_14)\r\n",
        "\r\n",
        "train_regret_loser_14 = min_max_array(regret_loser_14)\r\n",
        "train_regret_winner_14 = min_max_array(regret_winner_14)\r\n",
        "\r\n",
        "min_train_regret_loser_14 = min(train_regret_loser_14)\r\n",
        "min_train_regret_winner_14 = min(train_regret_winner_14)\r\n",
        "\r\n",
        "min_train_regret_loser_14, min_train_regret_winner_14"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-2.6336265245450035, -5.273031703586746)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVBcHRiMuwjx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "248c853b-42ce-4eee-c783-dfffc353cd41"
      },
      "source": [
        "### Training regret minimization: run number = 15\r\n",
        "\r\n",
        "loser_output_15 = np.append(np.max(loser_15.GP.y[0:n_init]),loser_15.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_15 = np.append(np.max(winner_15.GP.y[0:n_init]),winner_15.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_15 = np.log(y_global_orig - loser_output_15)\r\n",
        "regret_winner_15 = np.log(y_global_orig - winner_output_15)\r\n",
        "\r\n",
        "train_regret_loser_15 = min_max_array(regret_loser_15)\r\n",
        "train_regret_winner_15 = min_max_array(regret_winner_15)\r\n",
        "\r\n",
        "min_train_regret_loser_15 = min(train_regret_loser_15)\r\n",
        "min_train_regret_winner_15 = min(train_regret_winner_15)\r\n",
        "\r\n",
        "min_train_regret_loser_15, min_train_regret_winner_15"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-2.4063246372361053, -3.9513684359144503)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8nZ_DrKuwnA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "983ca2e2-9c7a-4f7c-dc65-18478f9a2524"
      },
      "source": [
        "### Training regret minimization: run number = 16\r\n",
        "\r\n",
        "loser_output_16 = np.append(np.max(loser_16.GP.y[0:n_init]),loser_16.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_16 = np.append(np.max(winner_16.GP.y[0:n_init]),winner_16.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_16 = np.log(y_global_orig - loser_output_16)\r\n",
        "regret_winner_16 = np.log(y_global_orig - winner_output_16)\r\n",
        "\r\n",
        "train_regret_loser_16 = min_max_array(regret_loser_16)\r\n",
        "train_regret_winner_16 = min_max_array(regret_winner_16)\r\n",
        "\r\n",
        "min_train_regret_loser_16 = min(train_regret_loser_16)\r\n",
        "min_train_regret_winner_16 = min(train_regret_winner_16)\r\n",
        "\r\n",
        "min_train_regret_loser_16, min_train_regret_winner_16"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-3.8941740945751633, -5.488534595072893)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eg6qzzQFuwpU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4de89706-ec3c-47b9-b11a-0af09aa6a411"
      },
      "source": [
        "### Training regret minimization: run number = 17\r\n",
        "\r\n",
        "loser_output_17 = np.append(np.max(loser_17.GP.y[0:n_init]),loser_17.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_17 = np.append(np.max(winner_17.GP.y[0:n_init]),winner_17.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_17 = np.log(y_global_orig - loser_output_17)\r\n",
        "regret_winner_17 = np.log(y_global_orig - winner_output_17)\r\n",
        "\r\n",
        "train_regret_loser_17 = min_max_array(regret_loser_17)\r\n",
        "train_regret_winner_17 = min_max_array(regret_winner_17)\r\n",
        "\r\n",
        "min_train_regret_loser_17 = min(train_regret_loser_17)\r\n",
        "min_train_regret_winner_17 = min(train_regret_winner_17)\r\n",
        "\r\n",
        "min_train_regret_loser_17, min_train_regret_winner_17"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-2.016135056608469, -5.956812822546499)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrRtDLMFuwsB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73682133-4fc4-43b9-ba10-55eed977bf1f"
      },
      "source": [
        "### Training regret minimization: run number = 18\r\n",
        "\r\n",
        "loser_output_18 = np.append(np.max(loser_18.GP.y[0:n_init]),loser_18.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_18 = np.append(np.max(winner_18.GP.y[0:n_init]),winner_18.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_18 = np.log(y_global_orig - loser_output_18)\r\n",
        "regret_winner_18 = np.log(y_global_orig - winner_output_18)\r\n",
        "\r\n",
        "train_regret_loser_18 = min_max_array(regret_loser_18)\r\n",
        "train_regret_winner_18 = min_max_array(regret_winner_18)\r\n",
        "\r\n",
        "min_train_regret_loser_18 = min(train_regret_loser_18)\r\n",
        "min_train_regret_winner_18 = min(train_regret_winner_18)\r\n",
        "\r\n",
        "min_train_regret_loser_18, min_train_regret_winner_18"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-2.2364823015746342, -4.0852381642075395)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkmSu4CUuwuh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2261fb0-b66c-4b54-80d0-a2fde967d421"
      },
      "source": [
        "### Training regret minimization: run number = 19\r\n",
        "\r\n",
        "loser_output_19 = np.append(np.max(loser_19.GP.y[0:n_init]),loser_19.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_19 = np.append(np.max(winner_19.GP.y[0:n_init]),winner_19.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_19 = np.log(y_global_orig - loser_output_19)\r\n",
        "regret_winner_19 = np.log(y_global_orig - winner_output_19)\r\n",
        "\r\n",
        "train_regret_loser_19 = min_max_array(regret_loser_19)\r\n",
        "train_regret_winner_19 = min_max_array(regret_winner_19)\r\n",
        "\r\n",
        "min_train_regret_loser_19 = min(train_regret_loser_19)\r\n",
        "min_train_regret_winner_19 = min(train_regret_winner_19)\r\n",
        "\r\n",
        "min_train_regret_loser_19, min_train_regret_winner_19"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-1.9502120432018093, -4.77244448433792)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eymecjwkuwxb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f1358ea-21d2-4ba0-9ba7-df5fa39a70c3"
      },
      "source": [
        "### Training regret minimization: run number = 20\r\n",
        "\r\n",
        "loser_output_20 = np.append(np.max(loser_20.GP.y[0:n_init]),loser_20.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_20 = np.append(np.max(winner_20.GP.y[0:n_init]),winner_20.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_20 = np.log(y_global_orig - loser_output_20)\r\n",
        "regret_winner_20 = np.log(y_global_orig - winner_output_20)\r\n",
        "\r\n",
        "train_regret_loser_20 = min_max_array(regret_loser_20)\r\n",
        "train_regret_winner_20 = min_max_array(regret_winner_20)\r\n",
        "\r\n",
        "min_train_regret_loser_20 = min(train_regret_loser_20)\r\n",
        "min_train_regret_winner_20 = min(train_regret_winner_20)\r\n",
        "\r\n",
        "min_train_regret_loser_20, min_train_regret_winner_20"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-2.167359607010987, -4.428911012732209)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5P6kq2Kuw0d"
      },
      "source": [
        "# Iteration1 :\r\n",
        "\r\n",
        "slice1 = 0\r\n",
        "\r\n",
        "loser1 = [train_regret_loser_1[slice1],\r\n",
        "       train_regret_loser_2[slice1],\r\n",
        "       train_regret_loser_3[slice1],\r\n",
        "       train_regret_loser_4[slice1],\r\n",
        "       train_regret_loser_5[slice1],\r\n",
        "       train_regret_loser_6[slice1],\r\n",
        "       train_regret_loser_7[slice1],\r\n",
        "       train_regret_loser_8[slice1],\r\n",
        "       train_regret_loser_9[slice1],\r\n",
        "       train_regret_loser_10[slice1],\r\n",
        "       train_regret_loser_11[slice1],\r\n",
        "       train_regret_loser_12[slice1],\r\n",
        "       train_regret_loser_13[slice1],\r\n",
        "       train_regret_loser_14[slice1],\r\n",
        "       train_regret_loser_15[slice1],\r\n",
        "       train_regret_loser_16[slice1],\r\n",
        "       train_regret_loser_17[slice1],\r\n",
        "       train_regret_loser_18[slice1],\r\n",
        "       train_regret_loser_19[slice1],\r\n",
        "       train_regret_loser_20[slice1]]\r\n",
        "\r\n",
        "winner1 = [train_regret_winner_1[slice1],\r\n",
        "       train_regret_winner_2[slice1],\r\n",
        "       train_regret_winner_3[slice1],\r\n",
        "       train_regret_winner_4[slice1],\r\n",
        "       train_regret_winner_5[slice1],\r\n",
        "       train_regret_winner_6[slice1],\r\n",
        "       train_regret_winner_7[slice1],\r\n",
        "       train_regret_winner_8[slice1],\r\n",
        "       train_regret_winner_9[slice1],\r\n",
        "       train_regret_winner_10[slice1],\r\n",
        "       train_regret_winner_11[slice1],\r\n",
        "       train_regret_winner_12[slice1],\r\n",
        "       train_regret_winner_13[slice1],\r\n",
        "       train_regret_winner_14[slice1],\r\n",
        "       train_regret_winner_15[slice1],\r\n",
        "       train_regret_winner_16[slice1],\r\n",
        "       train_regret_winner_17[slice1],\r\n",
        "       train_regret_winner_18[slice1],\r\n",
        "       train_regret_winner_19[slice1],\r\n",
        "       train_regret_winner_20[slice1]]\r\n",
        "\r\n",
        "loser1_results = pd.DataFrame(loser1).sort_values(by=[0], ascending=False)\r\n",
        "winner1_results = pd.DataFrame(winner1).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser1 = np.asarray(loser1_results[4:5][0])[0]\r\n",
        "median_loser1 = np.asarray(loser1_results[9:10][0])[0]\r\n",
        "upper_loser1 = np.asarray(loser1_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner1 = np.asarray(winner1_results[4:5][0])[0]\r\n",
        "median_winner1 = np.asarray(winner1_results[9:10][0])[0]\r\n",
        "upper_winner1 = np.asarray(winner1_results[14:15][0])[0]"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXcnj2pNuw3X"
      },
      "source": [
        "# Iteration11 :\r\n",
        "\r\n",
        "slice11 = 10\r\n",
        "\r\n",
        "loser11 = [train_regret_loser_1[slice11],\r\n",
        "       train_regret_loser_2[slice11],\r\n",
        "       train_regret_loser_3[slice11],\r\n",
        "       train_regret_loser_4[slice11],\r\n",
        "       train_regret_loser_5[slice11],\r\n",
        "       train_regret_loser_6[slice11],\r\n",
        "       train_regret_loser_7[slice11],\r\n",
        "       train_regret_loser_8[slice11],\r\n",
        "       train_regret_loser_9[slice11],\r\n",
        "       train_regret_loser_10[slice11],\r\n",
        "       train_regret_loser_11[slice11],\r\n",
        "       train_regret_loser_12[slice11],\r\n",
        "       train_regret_loser_13[slice11],\r\n",
        "       train_regret_loser_14[slice11],\r\n",
        "       train_regret_loser_15[slice11],\r\n",
        "       train_regret_loser_16[slice11],\r\n",
        "       train_regret_loser_17[slice11],\r\n",
        "       train_regret_loser_18[slice11],\r\n",
        "       train_regret_loser_19[slice11],\r\n",
        "       train_regret_loser_20[slice11]]\r\n",
        "\r\n",
        "winner11 = [train_regret_winner_1[slice11],\r\n",
        "       train_regret_winner_2[slice11],\r\n",
        "       train_regret_winner_3[slice11],\r\n",
        "       train_regret_winner_4[slice11],\r\n",
        "       train_regret_winner_5[slice11],\r\n",
        "       train_regret_winner_6[slice11],\r\n",
        "       train_regret_winner_7[slice11],\r\n",
        "       train_regret_winner_8[slice11],\r\n",
        "       train_regret_winner_9[slice11],\r\n",
        "       train_regret_winner_10[slice11],\r\n",
        "       train_regret_winner_11[slice11],\r\n",
        "       train_regret_winner_12[slice11],\r\n",
        "       train_regret_winner_13[slice11],\r\n",
        "       train_regret_winner_14[slice11],\r\n",
        "       train_regret_winner_15[slice11],\r\n",
        "       train_regret_winner_16[slice11],\r\n",
        "       train_regret_winner_17[slice11],\r\n",
        "       train_regret_winner_18[slice11],\r\n",
        "       train_regret_winner_19[slice11],\r\n",
        "       train_regret_winner_20[slice11]]\r\n",
        "\r\n",
        "loser11_results = pd.DataFrame(loser11).sort_values(by=[0], ascending=False)\r\n",
        "winner11_results = pd.DataFrame(winner11).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser11 = np.asarray(loser11_results[4:5][0])[0]\r\n",
        "median_loser11 = np.asarray(loser11_results[9:10][0])[0]\r\n",
        "upper_loser11 = np.asarray(loser11_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner11 = np.asarray(winner11_results[4:5][0])[0]\r\n",
        "median_winner11 = np.asarray(winner11_results[9:10][0])[0]\r\n",
        "upper_winner11 = np.asarray(winner11_results[14:15][0])[0]"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxQa6BAVuw6L"
      },
      "source": [
        "# Iteration21 :\r\n",
        "\r\n",
        "slice21 = 20\r\n",
        "\r\n",
        "loser21 = [train_regret_loser_1[slice21],\r\n",
        "       train_regret_loser_2[slice21],\r\n",
        "       train_regret_loser_3[slice21],\r\n",
        "       train_regret_loser_4[slice21],\r\n",
        "       train_regret_loser_5[slice21],\r\n",
        "       train_regret_loser_6[slice21],\r\n",
        "       train_regret_loser_7[slice21],\r\n",
        "       train_regret_loser_8[slice21],\r\n",
        "       train_regret_loser_9[slice21],\r\n",
        "       train_regret_loser_10[slice21],\r\n",
        "       train_regret_loser_11[slice21],\r\n",
        "       train_regret_loser_12[slice21],\r\n",
        "       train_regret_loser_13[slice21],\r\n",
        "       train_regret_loser_14[slice21],\r\n",
        "       train_regret_loser_15[slice21],\r\n",
        "       train_regret_loser_16[slice21],\r\n",
        "       train_regret_loser_17[slice21],\r\n",
        "       train_regret_loser_18[slice21],\r\n",
        "       train_regret_loser_19[slice21],\r\n",
        "       train_regret_loser_20[slice21]]\r\n",
        "\r\n",
        "winner21 = [train_regret_winner_1[slice21],\r\n",
        "       train_regret_winner_2[slice21],\r\n",
        "       train_regret_winner_3[slice21],\r\n",
        "       train_regret_winner_4[slice21],\r\n",
        "       train_regret_winner_5[slice21],\r\n",
        "       train_regret_winner_6[slice21],\r\n",
        "       train_regret_winner_7[slice21],\r\n",
        "       train_regret_winner_8[slice21],\r\n",
        "       train_regret_winner_9[slice21],\r\n",
        "       train_regret_winner_10[slice21],\r\n",
        "       train_regret_winner_11[slice21],\r\n",
        "       train_regret_winner_12[slice21],\r\n",
        "       train_regret_winner_13[slice21],\r\n",
        "       train_regret_winner_14[slice21],\r\n",
        "       train_regret_winner_15[slice21],\r\n",
        "       train_regret_winner_16[slice21],\r\n",
        "       train_regret_winner_17[slice21],\r\n",
        "       train_regret_winner_18[slice21],\r\n",
        "       train_regret_winner_19[slice21],\r\n",
        "       train_regret_winner_20[slice21]]\r\n",
        "\r\n",
        "loser21_results = pd.DataFrame(loser21).sort_values(by=[0], ascending=False)\r\n",
        "winner21_results = pd.DataFrame(winner21).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser21 = np.asarray(loser21_results[4:5][0])[0]\r\n",
        "median_loser21 = np.asarray(loser21_results[9:10][0])[0]\r\n",
        "upper_loser21 = np.asarray(loser21_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner21 = np.asarray(winner21_results[4:5][0])[0]\r\n",
        "median_winner21 = np.asarray(winner21_results[9:10][0])[0]\r\n",
        "upper_winner21 = np.asarray(winner21_results[14:15][0])[0]"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wdJdIsWuw8r"
      },
      "source": [
        "# Iteration31 :\r\n",
        "\r\n",
        "slice31 = 30\r\n",
        "\r\n",
        "loser31 = [train_regret_loser_1[slice31],\r\n",
        "       train_regret_loser_2[slice31],\r\n",
        "       train_regret_loser_3[slice31],\r\n",
        "       train_regret_loser_4[slice31],\r\n",
        "       train_regret_loser_5[slice31],\r\n",
        "       train_regret_loser_6[slice31],\r\n",
        "       train_regret_loser_7[slice31],\r\n",
        "       train_regret_loser_8[slice31],\r\n",
        "       train_regret_loser_9[slice31],\r\n",
        "       train_regret_loser_10[slice31],\r\n",
        "       train_regret_loser_11[slice31],\r\n",
        "       train_regret_loser_12[slice31],\r\n",
        "       train_regret_loser_13[slice31],\r\n",
        "       train_regret_loser_14[slice31],\r\n",
        "       train_regret_loser_15[slice31],\r\n",
        "       train_regret_loser_16[slice31],\r\n",
        "       train_regret_loser_17[slice31],\r\n",
        "       train_regret_loser_18[slice31],\r\n",
        "       train_regret_loser_19[slice31],\r\n",
        "       train_regret_loser_20[slice31]]\r\n",
        "\r\n",
        "winner31 = [train_regret_winner_1[slice31],\r\n",
        "       train_regret_winner_2[slice31],\r\n",
        "       train_regret_winner_3[slice31],\r\n",
        "       train_regret_winner_4[slice31],\r\n",
        "       train_regret_winner_5[slice31],\r\n",
        "       train_regret_winner_6[slice31],\r\n",
        "       train_regret_winner_7[slice31],\r\n",
        "       train_regret_winner_8[slice31],\r\n",
        "       train_regret_winner_9[slice31],\r\n",
        "       train_regret_winner_10[slice31],\r\n",
        "       train_regret_winner_11[slice31],\r\n",
        "       train_regret_winner_12[slice31],\r\n",
        "       train_regret_winner_13[slice31],\r\n",
        "       train_regret_winner_14[slice31],\r\n",
        "       train_regret_winner_15[slice31],\r\n",
        "       train_regret_winner_16[slice31],\r\n",
        "       train_regret_winner_17[slice31],\r\n",
        "       train_regret_winner_18[slice31],\r\n",
        "       train_regret_winner_19[slice31],\r\n",
        "       train_regret_winner_20[slice31]]\r\n",
        "\r\n",
        "loser31_results = pd.DataFrame(loser31).sort_values(by=[0], ascending=False)\r\n",
        "winner31_results = pd.DataFrame(winner31).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser31 = np.asarray(loser31_results[4:5][0])[0]\r\n",
        "median_loser31 = np.asarray(loser31_results[9:10][0])[0]\r\n",
        "upper_loser31 = np.asarray(loser31_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner31 = np.asarray(winner31_results[4:5][0])[0]\r\n",
        "median_winner31 = np.asarray(winner31_results[9:10][0])[0]\r\n",
        "upper_winner31 = np.asarray(winner31_results[14:15][0])[0]"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vWZBDufuw_Z"
      },
      "source": [
        "# Iteration41 :\r\n",
        "\r\n",
        "slice41 = 40\r\n",
        "\r\n",
        "loser41 = [train_regret_loser_1[slice41],\r\n",
        "       train_regret_loser_2[slice41],\r\n",
        "       train_regret_loser_3[slice41],\r\n",
        "       train_regret_loser_4[slice41],\r\n",
        "       train_regret_loser_5[slice41],\r\n",
        "       train_regret_loser_6[slice41],\r\n",
        "       train_regret_loser_7[slice41],\r\n",
        "       train_regret_loser_8[slice41],\r\n",
        "       train_regret_loser_9[slice41],\r\n",
        "       train_regret_loser_10[slice41],\r\n",
        "       train_regret_loser_11[slice41],\r\n",
        "       train_regret_loser_12[slice41],\r\n",
        "       train_regret_loser_13[slice41],\r\n",
        "       train_regret_loser_14[slice41],\r\n",
        "       train_regret_loser_15[slice41],\r\n",
        "       train_regret_loser_16[slice41],\r\n",
        "       train_regret_loser_17[slice41],\r\n",
        "       train_regret_loser_18[slice41],\r\n",
        "       train_regret_loser_19[slice41],\r\n",
        "       train_regret_loser_20[slice41]]\r\n",
        "\r\n",
        "winner41 = [train_regret_winner_1[slice41],\r\n",
        "       train_regret_winner_2[slice41],\r\n",
        "       train_regret_winner_3[slice41],\r\n",
        "       train_regret_winner_4[slice41],\r\n",
        "       train_regret_winner_5[slice41],\r\n",
        "       train_regret_winner_6[slice41],\r\n",
        "       train_regret_winner_7[slice41],\r\n",
        "       train_regret_winner_8[slice41],\r\n",
        "       train_regret_winner_9[slice41],\r\n",
        "       train_regret_winner_10[slice41],\r\n",
        "       train_regret_winner_11[slice41],\r\n",
        "       train_regret_winner_12[slice41],\r\n",
        "       train_regret_winner_13[slice41],\r\n",
        "       train_regret_winner_14[slice41],\r\n",
        "       train_regret_winner_15[slice41],\r\n",
        "       train_regret_winner_16[slice41],\r\n",
        "       train_regret_winner_17[slice41],\r\n",
        "       train_regret_winner_18[slice41],\r\n",
        "       train_regret_winner_19[slice41],\r\n",
        "       train_regret_winner_20[slice41]]\r\n",
        "\r\n",
        "loser41_results = pd.DataFrame(loser41).sort_values(by=[0], ascending=False)\r\n",
        "winner41_results = pd.DataFrame(winner41).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser41 = np.asarray(loser41_results[4:5][0])[0]\r\n",
        "median_loser41 = np.asarray(loser41_results[9:10][0])[0]\r\n",
        "upper_loser41 = np.asarray(loser41_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner41 = np.asarray(winner41_results[4:5][0])[0]\r\n",
        "median_winner41 = np.asarray(winner41_results[9:10][0])[0]\r\n",
        "upper_winner41 = np.asarray(winner41_results[14:15][0])[0]"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwi_GzjiuxB6"
      },
      "source": [
        "# Iteration51 :\r\n",
        "\r\n",
        "slice51 = 50\r\n",
        "\r\n",
        "loser51 = [train_regret_loser_1[slice51],\r\n",
        "       train_regret_loser_2[slice51],\r\n",
        "       train_regret_loser_3[slice51],\r\n",
        "       train_regret_loser_4[slice51],\r\n",
        "       train_regret_loser_5[slice51],\r\n",
        "       train_regret_loser_6[slice51],\r\n",
        "       train_regret_loser_7[slice51],\r\n",
        "       train_regret_loser_8[slice51],\r\n",
        "       train_regret_loser_9[slice51],\r\n",
        "       train_regret_loser_10[slice51],\r\n",
        "       train_regret_loser_11[slice51],\r\n",
        "       train_regret_loser_12[slice51],\r\n",
        "       train_regret_loser_13[slice51],\r\n",
        "       train_regret_loser_14[slice51],\r\n",
        "       train_regret_loser_15[slice51],\r\n",
        "       train_regret_loser_16[slice51],\r\n",
        "       train_regret_loser_17[slice51],\r\n",
        "       train_regret_loser_18[slice51],\r\n",
        "       train_regret_loser_19[slice51],\r\n",
        "       train_regret_loser_20[slice51]]\r\n",
        "\r\n",
        "winner51 = [train_regret_winner_1[slice51],\r\n",
        "       train_regret_winner_2[slice51],\r\n",
        "       train_regret_winner_3[slice51],\r\n",
        "       train_regret_winner_4[slice51],\r\n",
        "       train_regret_winner_5[slice51],\r\n",
        "       train_regret_winner_6[slice51],\r\n",
        "       train_regret_winner_7[slice51],\r\n",
        "       train_regret_winner_8[slice51],\r\n",
        "       train_regret_winner_9[slice51],\r\n",
        "       train_regret_winner_10[slice51],\r\n",
        "       train_regret_winner_11[slice51],\r\n",
        "       train_regret_winner_12[slice51],\r\n",
        "       train_regret_winner_13[slice51],\r\n",
        "       train_regret_winner_14[slice51],\r\n",
        "       train_regret_winner_15[slice51],\r\n",
        "       train_regret_winner_16[slice51],\r\n",
        "       train_regret_winner_17[slice51],\r\n",
        "       train_regret_winner_18[slice51],\r\n",
        "       train_regret_winner_19[slice51],\r\n",
        "       train_regret_winner_20[slice51]]\r\n",
        "\r\n",
        "loser51_results = pd.DataFrame(loser51).sort_values(by=[0], ascending=False)\r\n",
        "winner51_results = pd.DataFrame(winner51).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser51 = np.asarray(loser51_results[4:5][0])[0]\r\n",
        "median_loser51 = np.asarray(loser51_results[9:10][0])[0]\r\n",
        "upper_loser51 = np.asarray(loser51_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner51 = np.asarray(winner51_results[4:5][0])[0]\r\n",
        "median_winner51 = np.asarray(winner51_results[9:10][0])[0]\r\n",
        "upper_winner51 = np.asarray(winner51_results[14:15][0])[0]"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wk9nlS6puxFZ"
      },
      "source": [
        "# Iteration61 :\r\n",
        "\r\n",
        "slice61 = 60\r\n",
        "\r\n",
        "loser61 = [train_regret_loser_1[slice61],\r\n",
        "       train_regret_loser_2[slice61],\r\n",
        "       train_regret_loser_3[slice61],\r\n",
        "       train_regret_loser_4[slice61],\r\n",
        "       train_regret_loser_5[slice61],\r\n",
        "       train_regret_loser_6[slice61],\r\n",
        "       train_regret_loser_7[slice61],\r\n",
        "       train_regret_loser_8[slice61],\r\n",
        "       train_regret_loser_9[slice61],\r\n",
        "       train_regret_loser_10[slice61],\r\n",
        "       train_regret_loser_11[slice61],\r\n",
        "       train_regret_loser_12[slice61],\r\n",
        "       train_regret_loser_13[slice61],\r\n",
        "       train_regret_loser_14[slice61],\r\n",
        "       train_regret_loser_15[slice61],\r\n",
        "       train_regret_loser_16[slice61],\r\n",
        "       train_regret_loser_17[slice61],\r\n",
        "       train_regret_loser_18[slice61],\r\n",
        "       train_regret_loser_19[slice61],\r\n",
        "       train_regret_loser_20[slice61]]\r\n",
        "\r\n",
        "winner61 = [train_regret_winner_1[slice61],\r\n",
        "       train_regret_winner_2[slice61],\r\n",
        "       train_regret_winner_3[slice61],\r\n",
        "       train_regret_winner_4[slice61],\r\n",
        "       train_regret_winner_5[slice61],\r\n",
        "       train_regret_winner_6[slice61],\r\n",
        "       train_regret_winner_7[slice61],\r\n",
        "       train_regret_winner_8[slice61],\r\n",
        "       train_regret_winner_9[slice61],\r\n",
        "       train_regret_winner_10[slice61],\r\n",
        "       train_regret_winner_11[slice61],\r\n",
        "       train_regret_winner_12[slice61],\r\n",
        "       train_regret_winner_13[slice61],\r\n",
        "       train_regret_winner_14[slice61],\r\n",
        "       train_regret_winner_15[slice61],\r\n",
        "       train_regret_winner_16[slice61],\r\n",
        "       train_regret_winner_17[slice61],\r\n",
        "       train_regret_winner_18[slice61],\r\n",
        "       train_regret_winner_19[slice61],\r\n",
        "       train_regret_winner_20[slice61]]\r\n",
        "\r\n",
        "loser61_results = pd.DataFrame(loser61).sort_values(by=[0], ascending=False)\r\n",
        "winner61_results = pd.DataFrame(winner61).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser61 = np.asarray(loser61_results[4:5][0])[0]\r\n",
        "median_loser61 = np.asarray(loser61_results[9:10][0])[0]\r\n",
        "upper_loser61 = np.asarray(loser61_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner61 = np.asarray(winner61_results[4:5][0])[0]\r\n",
        "median_winner61 = np.asarray(winner61_results[9:10][0])[0]\r\n",
        "upper_winner61 = np.asarray(winner61_results[14:15][0])[0]"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMus9vDpuxHN"
      },
      "source": [
        "# Iteration71 :\r\n",
        "\r\n",
        "slice71 = 70\r\n",
        "\r\n",
        "loser71 = [train_regret_loser_1[slice71],\r\n",
        "       train_regret_loser_2[slice71],\r\n",
        "       train_regret_loser_3[slice71],\r\n",
        "       train_regret_loser_4[slice71],\r\n",
        "       train_regret_loser_5[slice71],\r\n",
        "       train_regret_loser_6[slice71],\r\n",
        "       train_regret_loser_7[slice71],\r\n",
        "       train_regret_loser_8[slice71],\r\n",
        "       train_regret_loser_9[slice71],\r\n",
        "       train_regret_loser_10[slice71],\r\n",
        "       train_regret_loser_11[slice71],\r\n",
        "       train_regret_loser_12[slice71],\r\n",
        "       train_regret_loser_13[slice71],\r\n",
        "       train_regret_loser_14[slice71],\r\n",
        "       train_regret_loser_15[slice71],\r\n",
        "       train_regret_loser_16[slice71],\r\n",
        "       train_regret_loser_17[slice71],\r\n",
        "       train_regret_loser_18[slice71],\r\n",
        "       train_regret_loser_19[slice71],\r\n",
        "       train_regret_loser_20[slice71]]\r\n",
        "\r\n",
        "winner71 = [train_regret_winner_1[slice71],\r\n",
        "       train_regret_winner_2[slice71],\r\n",
        "       train_regret_winner_3[slice71],\r\n",
        "       train_regret_winner_4[slice71],\r\n",
        "       train_regret_winner_5[slice71],\r\n",
        "       train_regret_winner_6[slice71],\r\n",
        "       train_regret_winner_7[slice71],\r\n",
        "       train_regret_winner_8[slice71],\r\n",
        "       train_regret_winner_9[slice71],\r\n",
        "       train_regret_winner_10[slice71],\r\n",
        "       train_regret_winner_11[slice71],\r\n",
        "       train_regret_winner_12[slice71],\r\n",
        "       train_regret_winner_13[slice71],\r\n",
        "       train_regret_winner_14[slice71],\r\n",
        "       train_regret_winner_15[slice71],\r\n",
        "       train_regret_winner_16[slice71],\r\n",
        "       train_regret_winner_17[slice71],\r\n",
        "       train_regret_winner_18[slice71],\r\n",
        "       train_regret_winner_19[slice71],\r\n",
        "       train_regret_winner_20[slice71]]\r\n",
        "\r\n",
        "loser71_results = pd.DataFrame(loser71).sort_values(by=[0], ascending=False)\r\n",
        "winner71_results = pd.DataFrame(winner71).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser71 = np.asarray(loser71_results[4:5][0])[0]\r\n",
        "median_loser71 = np.asarray(loser71_results[9:10][0])[0]\r\n",
        "upper_loser71 = np.asarray(loser71_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner71 = np.asarray(winner71_results[4:5][0])[0]\r\n",
        "median_winner71 = np.asarray(winner71_results[9:10][0])[0]\r\n",
        "upper_winner71 = np.asarray(winner71_results[14:15][0])[0]"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6Mg0x9vuxL4"
      },
      "source": [
        "# Iteration81 :\r\n",
        "\r\n",
        "slice81 = 80\r\n",
        "\r\n",
        "loser81 = [train_regret_loser_1[slice81],\r\n",
        "       train_regret_loser_2[slice81],\r\n",
        "       train_regret_loser_3[slice81],\r\n",
        "       train_regret_loser_4[slice81],\r\n",
        "       train_regret_loser_5[slice81],\r\n",
        "       train_regret_loser_6[slice81],\r\n",
        "       train_regret_loser_7[slice81],\r\n",
        "       train_regret_loser_8[slice81],\r\n",
        "       train_regret_loser_9[slice81],\r\n",
        "       train_regret_loser_10[slice81],\r\n",
        "       train_regret_loser_11[slice81],\r\n",
        "       train_regret_loser_12[slice81],\r\n",
        "       train_regret_loser_13[slice81],\r\n",
        "       train_regret_loser_14[slice81],\r\n",
        "       train_regret_loser_15[slice81],\r\n",
        "       train_regret_loser_16[slice81],\r\n",
        "       train_regret_loser_17[slice81],\r\n",
        "       train_regret_loser_18[slice81],\r\n",
        "       train_regret_loser_19[slice81],\r\n",
        "       train_regret_loser_20[slice81]]\r\n",
        "\r\n",
        "winner81 = [train_regret_winner_1[slice81],\r\n",
        "       train_regret_winner_2[slice81],\r\n",
        "       train_regret_winner_3[slice81],\r\n",
        "       train_regret_winner_4[slice81],\r\n",
        "       train_regret_winner_5[slice81],\r\n",
        "       train_regret_winner_6[slice81],\r\n",
        "       train_regret_winner_7[slice81],\r\n",
        "       train_regret_winner_8[slice81],\r\n",
        "       train_regret_winner_9[slice81],\r\n",
        "       train_regret_winner_10[slice81],\r\n",
        "       train_regret_winner_11[slice81],\r\n",
        "       train_regret_winner_12[slice81],\r\n",
        "       train_regret_winner_13[slice81],\r\n",
        "       train_regret_winner_14[slice81],\r\n",
        "       train_regret_winner_15[slice81],\r\n",
        "       train_regret_winner_16[slice81],\r\n",
        "       train_regret_winner_17[slice81],\r\n",
        "       train_regret_winner_18[slice81],\r\n",
        "       train_regret_winner_19[slice81],\r\n",
        "       train_regret_winner_20[slice81]]\r\n",
        "\r\n",
        "loser81_results = pd.DataFrame(loser81).sort_values(by=[0], ascending=False)\r\n",
        "winner81_results = pd.DataFrame(winner81).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser81 = np.asarray(loser81_results[4:5][0])[0]\r\n",
        "median_loser81 = np.asarray(loser81_results[9:10][0])[0]\r\n",
        "upper_loser81 = np.asarray(loser81_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner81 = np.asarray(winner81_results[4:5][0])[0]\r\n",
        "median_winner81 = np.asarray(winner81_results[9:10][0])[0]\r\n",
        "upper_winner81 = np.asarray(winner81_results[14:15][0])[0]"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Gu9K8XlteV4"
      },
      "source": [
        "# Iteration91 :\r\n",
        "\r\n",
        "slice91 = 90\r\n",
        "\r\n",
        "loser91 = [train_regret_loser_1[slice91],\r\n",
        "       train_regret_loser_2[slice91],\r\n",
        "       train_regret_loser_3[slice91],\r\n",
        "       train_regret_loser_4[slice91],\r\n",
        "       train_regret_loser_5[slice91],\r\n",
        "       train_regret_loser_6[slice91],\r\n",
        "       train_regret_loser_7[slice91],\r\n",
        "       train_regret_loser_8[slice91],\r\n",
        "       train_regret_loser_9[slice91],\r\n",
        "       train_regret_loser_10[slice91],\r\n",
        "       train_regret_loser_11[slice91],\r\n",
        "       train_regret_loser_12[slice91],\r\n",
        "       train_regret_loser_13[slice91],\r\n",
        "       train_regret_loser_14[slice91],\r\n",
        "       train_regret_loser_15[slice91],\r\n",
        "       train_regret_loser_16[slice91],\r\n",
        "       train_regret_loser_17[slice91],\r\n",
        "       train_regret_loser_18[slice91],\r\n",
        "       train_regret_loser_19[slice91],\r\n",
        "       train_regret_loser_20[slice91]]\r\n",
        "\r\n",
        "winner91 = [train_regret_winner_1[slice91],\r\n",
        "       train_regret_winner_2[slice91],\r\n",
        "       train_regret_winner_3[slice91],\r\n",
        "       train_regret_winner_4[slice91],\r\n",
        "       train_regret_winner_5[slice91],\r\n",
        "       train_regret_winner_6[slice91],\r\n",
        "       train_regret_winner_7[slice91],\r\n",
        "       train_regret_winner_8[slice91],\r\n",
        "       train_regret_winner_9[slice91],\r\n",
        "       train_regret_winner_10[slice91],\r\n",
        "       train_regret_winner_11[slice91],\r\n",
        "       train_regret_winner_12[slice91],\r\n",
        "       train_regret_winner_13[slice91],\r\n",
        "       train_regret_winner_14[slice91],\r\n",
        "       train_regret_winner_15[slice91],\r\n",
        "       train_regret_winner_16[slice91],\r\n",
        "       train_regret_winner_17[slice91],\r\n",
        "       train_regret_winner_18[slice91],\r\n",
        "       train_regret_winner_19[slice91],\r\n",
        "       train_regret_winner_20[slice91]]\r\n",
        "\r\n",
        "loser91_results = pd.DataFrame(loser91).sort_values(by=[0], ascending=False)\r\n",
        "winner91_results = pd.DataFrame(winner91).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser91 = np.asarray(loser91_results[4:5][0])[0]\r\n",
        "median_loser91 = np.asarray(loser91_results[9:10][0])[0]\r\n",
        "upper_loser91 = np.asarray(loser91_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner91 = np.asarray(winner91_results[4:5][0])[0]\r\n",
        "median_winner91 = np.asarray(winner91_results[9:10][0])[0]\r\n",
        "upper_winner91 = np.asarray(winner91_results[14:15][0])[0]"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVVeft6steXv"
      },
      "source": [
        "# Iteration101 :\r\n",
        "\r\n",
        "slice101 = 100\r\n",
        "\r\n",
        "loser101 = [train_regret_loser_1[slice101],\r\n",
        "       train_regret_loser_2[slice101],\r\n",
        "       train_regret_loser_3[slice101],\r\n",
        "       train_regret_loser_4[slice101],\r\n",
        "       train_regret_loser_5[slice101],\r\n",
        "       train_regret_loser_6[slice101],\r\n",
        "       train_regret_loser_7[slice101],\r\n",
        "       train_regret_loser_8[slice101],\r\n",
        "       train_regret_loser_9[slice101],\r\n",
        "       train_regret_loser_10[slice101],\r\n",
        "       train_regret_loser_11[slice101],\r\n",
        "       train_regret_loser_12[slice101],\r\n",
        "       train_regret_loser_13[slice101],\r\n",
        "       train_regret_loser_14[slice101],\r\n",
        "       train_regret_loser_15[slice101],\r\n",
        "       train_regret_loser_16[slice101],\r\n",
        "       train_regret_loser_17[slice101],\r\n",
        "       train_regret_loser_18[slice101],\r\n",
        "       train_regret_loser_19[slice101],\r\n",
        "       train_regret_loser_20[slice101]]\r\n",
        "\r\n",
        "winner101 = [train_regret_winner_1[slice101],\r\n",
        "       train_regret_winner_2[slice101],\r\n",
        "       train_regret_winner_3[slice101],\r\n",
        "       train_regret_winner_4[slice101],\r\n",
        "       train_regret_winner_5[slice101],\r\n",
        "       train_regret_winner_6[slice101],\r\n",
        "       train_regret_winner_7[slice101],\r\n",
        "       train_regret_winner_8[slice101],\r\n",
        "       train_regret_winner_9[slice101],\r\n",
        "       train_regret_winner_10[slice101],\r\n",
        "       train_regret_winner_11[slice101],\r\n",
        "       train_regret_winner_12[slice101],\r\n",
        "       train_regret_winner_13[slice101],\r\n",
        "       train_regret_winner_14[slice101],\r\n",
        "       train_regret_winner_15[slice101],\r\n",
        "       train_regret_winner_16[slice101],\r\n",
        "       train_regret_winner_17[slice101],\r\n",
        "       train_regret_winner_18[slice101],\r\n",
        "       train_regret_winner_19[slice101],\r\n",
        "       train_regret_winner_20[slice101]]\r\n",
        "\r\n",
        "loser101_results = pd.DataFrame(loser101).sort_values(by=[0], ascending=False)\r\n",
        "winner101_results = pd.DataFrame(winner101).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser101 = np.asarray(loser101_results[4:5][0])[0]\r\n",
        "median_loser101 = np.asarray(loser101_results[9:10][0])[0]\r\n",
        "upper_loser101 = np.asarray(loser101_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner101 = np.asarray(winner101_results[4:5][0])[0]\r\n",
        "median_winner101 = np.asarray(winner101_results[9:10][0])[0]\r\n",
        "upper_winner101 = np.asarray(winner101_results[14:15][0])[0]"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "maqbtTXXtebT"
      },
      "source": [
        "# Iteration2 :\r\n",
        "\r\n",
        "slice2 = 1\r\n",
        "\r\n",
        "loser2 = [train_regret_loser_1[slice2],\r\n",
        "       train_regret_loser_2[slice2],\r\n",
        "       train_regret_loser_3[slice2],\r\n",
        "       train_regret_loser_4[slice2],\r\n",
        "       train_regret_loser_5[slice2],\r\n",
        "       train_regret_loser_6[slice2],\r\n",
        "       train_regret_loser_7[slice2],\r\n",
        "       train_regret_loser_8[slice2],\r\n",
        "       train_regret_loser_9[slice2],\r\n",
        "       train_regret_loser_10[slice2],\r\n",
        "       train_regret_loser_11[slice2],\r\n",
        "       train_regret_loser_12[slice2],\r\n",
        "       train_regret_loser_13[slice2],\r\n",
        "       train_regret_loser_14[slice2],\r\n",
        "       train_regret_loser_15[slice2],\r\n",
        "       train_regret_loser_16[slice2],\r\n",
        "       train_regret_loser_17[slice2],\r\n",
        "       train_regret_loser_18[slice2],\r\n",
        "       train_regret_loser_19[slice2],\r\n",
        "       train_regret_loser_20[slice2]]\r\n",
        "\r\n",
        "winner2 = [train_regret_winner_1[slice2],\r\n",
        "       train_regret_winner_2[slice2],\r\n",
        "       train_regret_winner_3[slice2],\r\n",
        "       train_regret_winner_4[slice2],\r\n",
        "       train_regret_winner_5[slice2],\r\n",
        "       train_regret_winner_6[slice2],\r\n",
        "       train_regret_winner_7[slice2],\r\n",
        "       train_regret_winner_8[slice2],\r\n",
        "       train_regret_winner_9[slice2],\r\n",
        "       train_regret_winner_10[slice2],\r\n",
        "       train_regret_winner_11[slice2],\r\n",
        "       train_regret_winner_12[slice2],\r\n",
        "       train_regret_winner_13[slice2],\r\n",
        "       train_regret_winner_14[slice2],\r\n",
        "       train_regret_winner_15[slice2],\r\n",
        "       train_regret_winner_16[slice2],\r\n",
        "       train_regret_winner_17[slice2],\r\n",
        "       train_regret_winner_18[slice2],\r\n",
        "       train_regret_winner_19[slice2],\r\n",
        "       train_regret_winner_20[slice2]]\r\n",
        "\r\n",
        "loser2_results = pd.DataFrame(loser2).sort_values(by=[0], ascending=False)\r\n",
        "winner2_results = pd.DataFrame(winner2).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser2 = np.asarray(loser2_results[4:5][0])[0]\r\n",
        "median_loser2 = np.asarray(loser2_results[9:10][0])[0]\r\n",
        "upper_loser2 = np.asarray(loser2_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner2 = np.asarray(winner2_results[4:5][0])[0]\r\n",
        "median_winner2 = np.asarray(winner2_results[9:10][0])[0]\r\n",
        "upper_winner2 = np.asarray(winner2_results[14:15][0])[0]"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWIBh0dQw-ZM"
      },
      "source": [
        "# Iteration12 :\r\n",
        "\r\n",
        "slice12 = 11\r\n",
        "\r\n",
        "loser12 = [train_regret_loser_1[slice12],\r\n",
        "       train_regret_loser_2[slice12],\r\n",
        "       train_regret_loser_3[slice12],\r\n",
        "       train_regret_loser_4[slice12],\r\n",
        "       train_regret_loser_5[slice12],\r\n",
        "       train_regret_loser_6[slice12],\r\n",
        "       train_regret_loser_7[slice12],\r\n",
        "       train_regret_loser_8[slice12],\r\n",
        "       train_regret_loser_9[slice12],\r\n",
        "       train_regret_loser_10[slice12],\r\n",
        "       train_regret_loser_11[slice12],\r\n",
        "       train_regret_loser_12[slice12],\r\n",
        "       train_regret_loser_13[slice12],\r\n",
        "       train_regret_loser_14[slice12],\r\n",
        "       train_regret_loser_15[slice12],\r\n",
        "       train_regret_loser_16[slice12],\r\n",
        "       train_regret_loser_17[slice12],\r\n",
        "       train_regret_loser_18[slice12],\r\n",
        "       train_regret_loser_19[slice12],\r\n",
        "       train_regret_loser_20[slice12]]\r\n",
        "\r\n",
        "winner12 = [train_regret_winner_1[slice12],\r\n",
        "       train_regret_winner_2[slice12],\r\n",
        "       train_regret_winner_3[slice12],\r\n",
        "       train_regret_winner_4[slice12],\r\n",
        "       train_regret_winner_5[slice12],\r\n",
        "       train_regret_winner_6[slice12],\r\n",
        "       train_regret_winner_7[slice12],\r\n",
        "       train_regret_winner_8[slice12],\r\n",
        "       train_regret_winner_9[slice12],\r\n",
        "       train_regret_winner_10[slice12],\r\n",
        "       train_regret_winner_11[slice12],\r\n",
        "       train_regret_winner_12[slice12],\r\n",
        "       train_regret_winner_13[slice12],\r\n",
        "       train_regret_winner_14[slice12],\r\n",
        "       train_regret_winner_15[slice12],\r\n",
        "       train_regret_winner_16[slice12],\r\n",
        "       train_regret_winner_17[slice12],\r\n",
        "       train_regret_winner_18[slice12],\r\n",
        "       train_regret_winner_19[slice12],\r\n",
        "       train_regret_winner_20[slice12]]\r\n",
        "\r\n",
        "loser12_results = pd.DataFrame(loser12).sort_values(by=[0], ascending=False)\r\n",
        "winner12_results = pd.DataFrame(winner12).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser12 = np.asarray(loser12_results[4:5][0])[0]\r\n",
        "median_loser12 = np.asarray(loser12_results[9:10][0])[0]\r\n",
        "upper_loser12 = np.asarray(loser12_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner12 = np.asarray(winner12_results[4:5][0])[0]\r\n",
        "median_winner12 = np.asarray(winner12_results[9:10][0])[0]\r\n",
        "upper_winner12 = np.asarray(winner12_results[14:15][0])[0]"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjIhsei3w-cV"
      },
      "source": [
        "# Iteration22 :\r\n",
        "\r\n",
        "slice22 = 21\r\n",
        "\r\n",
        "loser22 = [train_regret_loser_1[slice22],\r\n",
        "       train_regret_loser_2[slice22],\r\n",
        "       train_regret_loser_3[slice22],\r\n",
        "       train_regret_loser_4[slice22],\r\n",
        "       train_regret_loser_5[slice22],\r\n",
        "       train_regret_loser_6[slice22],\r\n",
        "       train_regret_loser_7[slice22],\r\n",
        "       train_regret_loser_8[slice22],\r\n",
        "       train_regret_loser_9[slice22],\r\n",
        "       train_regret_loser_10[slice22],\r\n",
        "       train_regret_loser_11[slice22],\r\n",
        "       train_regret_loser_12[slice22],\r\n",
        "       train_regret_loser_13[slice22],\r\n",
        "       train_regret_loser_14[slice22],\r\n",
        "       train_regret_loser_15[slice22],\r\n",
        "       train_regret_loser_16[slice22],\r\n",
        "       train_regret_loser_17[slice22],\r\n",
        "       train_regret_loser_18[slice22],\r\n",
        "       train_regret_loser_19[slice22],\r\n",
        "       train_regret_loser_20[slice22]]\r\n",
        "\r\n",
        "winner22 = [train_regret_winner_1[slice22],\r\n",
        "       train_regret_winner_2[slice22],\r\n",
        "       train_regret_winner_3[slice22],\r\n",
        "       train_regret_winner_4[slice22],\r\n",
        "       train_regret_winner_5[slice22],\r\n",
        "       train_regret_winner_6[slice22],\r\n",
        "       train_regret_winner_7[slice22],\r\n",
        "       train_regret_winner_8[slice22],\r\n",
        "       train_regret_winner_9[slice22],\r\n",
        "       train_regret_winner_10[slice22],\r\n",
        "       train_regret_winner_11[slice22],\r\n",
        "       train_regret_winner_12[slice22],\r\n",
        "       train_regret_winner_13[slice22],\r\n",
        "       train_regret_winner_14[slice22],\r\n",
        "       train_regret_winner_15[slice22],\r\n",
        "       train_regret_winner_16[slice22],\r\n",
        "       train_regret_winner_17[slice22],\r\n",
        "       train_regret_winner_18[slice22],\r\n",
        "       train_regret_winner_19[slice22],\r\n",
        "       train_regret_winner_20[slice22]]\r\n",
        "\r\n",
        "loser22_results = pd.DataFrame(loser22).sort_values(by=[0], ascending=False)\r\n",
        "winner22_results = pd.DataFrame(winner22).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser22 = np.asarray(loser22_results[4:5][0])[0]\r\n",
        "median_loser22 = np.asarray(loser22_results[9:10][0])[0]\r\n",
        "upper_loser22 = np.asarray(loser22_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner22 = np.asarray(winner22_results[4:5][0])[0]\r\n",
        "median_winner22 = np.asarray(winner22_results[9:10][0])[0]\r\n",
        "upper_winner22 = np.asarray(winner22_results[14:15][0])[0]"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ur9p9GkHw-fq"
      },
      "source": [
        "# Iteration32 :\r\n",
        "\r\n",
        "slice32 = 31\r\n",
        "\r\n",
        "loser32 = [train_regret_loser_1[slice32],\r\n",
        "       train_regret_loser_2[slice32],\r\n",
        "       train_regret_loser_3[slice32],\r\n",
        "       train_regret_loser_4[slice32],\r\n",
        "       train_regret_loser_5[slice32],\r\n",
        "       train_regret_loser_6[slice32],\r\n",
        "       train_regret_loser_7[slice32],\r\n",
        "       train_regret_loser_8[slice32],\r\n",
        "       train_regret_loser_9[slice32],\r\n",
        "       train_regret_loser_10[slice32],\r\n",
        "       train_regret_loser_11[slice32],\r\n",
        "       train_regret_loser_12[slice32],\r\n",
        "       train_regret_loser_13[slice32],\r\n",
        "       train_regret_loser_14[slice32],\r\n",
        "       train_regret_loser_15[slice32],\r\n",
        "       train_regret_loser_16[slice32],\r\n",
        "       train_regret_loser_17[slice32],\r\n",
        "       train_regret_loser_18[slice32],\r\n",
        "       train_regret_loser_19[slice32],\r\n",
        "       train_regret_loser_20[slice32]]\r\n",
        "\r\n",
        "winner32 = [train_regret_winner_1[slice32],\r\n",
        "       train_regret_winner_2[slice32],\r\n",
        "       train_regret_winner_3[slice32],\r\n",
        "       train_regret_winner_4[slice32],\r\n",
        "       train_regret_winner_5[slice32],\r\n",
        "       train_regret_winner_6[slice32],\r\n",
        "       train_regret_winner_7[slice32],\r\n",
        "       train_regret_winner_8[slice32],\r\n",
        "       train_regret_winner_9[slice32],\r\n",
        "       train_regret_winner_10[slice32],\r\n",
        "       train_regret_winner_11[slice32],\r\n",
        "       train_regret_winner_12[slice32],\r\n",
        "       train_regret_winner_13[slice32],\r\n",
        "       train_regret_winner_14[slice32],\r\n",
        "       train_regret_winner_15[slice32],\r\n",
        "       train_regret_winner_16[slice32],\r\n",
        "       train_regret_winner_17[slice32],\r\n",
        "       train_regret_winner_18[slice32],\r\n",
        "       train_regret_winner_19[slice32],\r\n",
        "       train_regret_winner_20[slice32]]\r\n",
        "\r\n",
        "loser32_results = pd.DataFrame(loser32).sort_values(by=[0], ascending=False)\r\n",
        "winner32_results = pd.DataFrame(winner32).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser32 = np.asarray(loser32_results[4:5][0])[0]\r\n",
        "median_loser32 = np.asarray(loser32_results[9:10][0])[0]\r\n",
        "upper_loser32 = np.asarray(loser32_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner32 = np.asarray(winner32_results[4:5][0])[0]\r\n",
        "median_winner32 = np.asarray(winner32_results[9:10][0])[0]\r\n",
        "upper_winner32 = np.asarray(winner32_results[14:15][0])[0]"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jd_rzHB8w-hy"
      },
      "source": [
        "# Iteration42 :\r\n",
        "\r\n",
        "slice42 = 41\r\n",
        "\r\n",
        "loser42 = [train_regret_loser_1[slice42],\r\n",
        "       train_regret_loser_2[slice42],\r\n",
        "       train_regret_loser_3[slice42],\r\n",
        "       train_regret_loser_4[slice42],\r\n",
        "       train_regret_loser_5[slice42],\r\n",
        "       train_regret_loser_6[slice42],\r\n",
        "       train_regret_loser_7[slice42],\r\n",
        "       train_regret_loser_8[slice42],\r\n",
        "       train_regret_loser_9[slice42],\r\n",
        "       train_regret_loser_10[slice42],\r\n",
        "       train_regret_loser_11[slice42],\r\n",
        "       train_regret_loser_12[slice42],\r\n",
        "       train_regret_loser_13[slice42],\r\n",
        "       train_regret_loser_14[slice42],\r\n",
        "       train_regret_loser_15[slice42],\r\n",
        "       train_regret_loser_16[slice42],\r\n",
        "       train_regret_loser_17[slice42],\r\n",
        "       train_regret_loser_18[slice42],\r\n",
        "       train_regret_loser_19[slice42],\r\n",
        "       train_regret_loser_20[slice42]]\r\n",
        "\r\n",
        "winner42 = [train_regret_winner_1[slice42],\r\n",
        "       train_regret_winner_2[slice42],\r\n",
        "       train_regret_winner_3[slice42],\r\n",
        "       train_regret_winner_4[slice42],\r\n",
        "       train_regret_winner_5[slice42],\r\n",
        "       train_regret_winner_6[slice42],\r\n",
        "       train_regret_winner_7[slice42],\r\n",
        "       train_regret_winner_8[slice42],\r\n",
        "       train_regret_winner_9[slice42],\r\n",
        "       train_regret_winner_10[slice42],\r\n",
        "       train_regret_winner_11[slice42],\r\n",
        "       train_regret_winner_12[slice42],\r\n",
        "       train_regret_winner_13[slice42],\r\n",
        "       train_regret_winner_14[slice42],\r\n",
        "       train_regret_winner_15[slice42],\r\n",
        "       train_regret_winner_16[slice42],\r\n",
        "       train_regret_winner_17[slice42],\r\n",
        "       train_regret_winner_18[slice42],\r\n",
        "       train_regret_winner_19[slice42],\r\n",
        "       train_regret_winner_20[slice42]]\r\n",
        "\r\n",
        "loser42_results = pd.DataFrame(loser42).sort_values(by=[0], ascending=False)\r\n",
        "winner42_results = pd.DataFrame(winner42).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser42 = np.asarray(loser42_results[4:5][0])[0]\r\n",
        "median_loser42 = np.asarray(loser42_results[9:10][0])[0]\r\n",
        "upper_loser42 = np.asarray(loser42_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner42 = np.asarray(winner42_results[4:5][0])[0]\r\n",
        "median_winner42 = np.asarray(winner42_results[9:10][0])[0]\r\n",
        "upper_winner42 = np.asarray(winner42_results[14:15][0])[0]"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKUUG6vAw-kz"
      },
      "source": [
        "# Iteration52 :\r\n",
        "\r\n",
        "slice52 = 51\r\n",
        "\r\n",
        "loser52 = [train_regret_loser_1[slice52],\r\n",
        "       train_regret_loser_2[slice52],\r\n",
        "       train_regret_loser_3[slice52],\r\n",
        "       train_regret_loser_4[slice52],\r\n",
        "       train_regret_loser_5[slice52],\r\n",
        "       train_regret_loser_6[slice52],\r\n",
        "       train_regret_loser_7[slice52],\r\n",
        "       train_regret_loser_8[slice52],\r\n",
        "       train_regret_loser_9[slice52],\r\n",
        "       train_regret_loser_10[slice52],\r\n",
        "       train_regret_loser_11[slice52],\r\n",
        "       train_regret_loser_12[slice52],\r\n",
        "       train_regret_loser_13[slice52],\r\n",
        "       train_regret_loser_14[slice52],\r\n",
        "       train_regret_loser_15[slice52],\r\n",
        "       train_regret_loser_16[slice52],\r\n",
        "       train_regret_loser_17[slice52],\r\n",
        "       train_regret_loser_18[slice52],\r\n",
        "       train_regret_loser_19[slice52],\r\n",
        "       train_regret_loser_20[slice52]]\r\n",
        "\r\n",
        "winner52 = [train_regret_winner_1[slice52],\r\n",
        "       train_regret_winner_2[slice52],\r\n",
        "       train_regret_winner_3[slice52],\r\n",
        "       train_regret_winner_4[slice52],\r\n",
        "       train_regret_winner_5[slice52],\r\n",
        "       train_regret_winner_6[slice52],\r\n",
        "       train_regret_winner_7[slice52],\r\n",
        "       train_regret_winner_8[slice52],\r\n",
        "       train_regret_winner_9[slice52],\r\n",
        "       train_regret_winner_10[slice52],\r\n",
        "       train_regret_winner_11[slice52],\r\n",
        "       train_regret_winner_12[slice52],\r\n",
        "       train_regret_winner_13[slice52],\r\n",
        "       train_regret_winner_14[slice52],\r\n",
        "       train_regret_winner_15[slice52],\r\n",
        "       train_regret_winner_16[slice52],\r\n",
        "       train_regret_winner_17[slice52],\r\n",
        "       train_regret_winner_18[slice52],\r\n",
        "       train_regret_winner_19[slice52],\r\n",
        "       train_regret_winner_20[slice52]]\r\n",
        "\r\n",
        "loser52_results = pd.DataFrame(loser52).sort_values(by=[0], ascending=False)\r\n",
        "winner52_results = pd.DataFrame(winner52).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser52 = np.asarray(loser52_results[4:5][0])[0]\r\n",
        "median_loser52 = np.asarray(loser52_results[9:10][0])[0]\r\n",
        "upper_loser52 = np.asarray(loser52_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner52 = np.asarray(winner52_results[4:5][0])[0]\r\n",
        "median_winner52 = np.asarray(winner52_results[9:10][0])[0]\r\n",
        "upper_winner52 = np.asarray(winner52_results[14:15][0])[0]"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-9cJNkew-nY"
      },
      "source": [
        "# Iteration62 :\r\n",
        "\r\n",
        "slice62 = 61\r\n",
        "\r\n",
        "loser62 = [train_regret_loser_1[slice62],\r\n",
        "       train_regret_loser_2[slice62],\r\n",
        "       train_regret_loser_3[slice62],\r\n",
        "       train_regret_loser_4[slice62],\r\n",
        "       train_regret_loser_5[slice62],\r\n",
        "       train_regret_loser_6[slice62],\r\n",
        "       train_regret_loser_7[slice62],\r\n",
        "       train_regret_loser_8[slice62],\r\n",
        "       train_regret_loser_9[slice62],\r\n",
        "       train_regret_loser_10[slice62],\r\n",
        "       train_regret_loser_11[slice62],\r\n",
        "       train_regret_loser_12[slice62],\r\n",
        "       train_regret_loser_13[slice62],\r\n",
        "       train_regret_loser_14[slice62],\r\n",
        "       train_regret_loser_15[slice62],\r\n",
        "       train_regret_loser_16[slice62],\r\n",
        "       train_regret_loser_17[slice62],\r\n",
        "       train_regret_loser_18[slice62],\r\n",
        "       train_regret_loser_19[slice62],\r\n",
        "       train_regret_loser_20[slice62]]\r\n",
        "\r\n",
        "winner62 = [train_regret_winner_1[slice62],\r\n",
        "       train_regret_winner_2[slice62],\r\n",
        "       train_regret_winner_3[slice62],\r\n",
        "       train_regret_winner_4[slice62],\r\n",
        "       train_regret_winner_5[slice62],\r\n",
        "       train_regret_winner_6[slice62],\r\n",
        "       train_regret_winner_7[slice62],\r\n",
        "       train_regret_winner_8[slice62],\r\n",
        "       train_regret_winner_9[slice62],\r\n",
        "       train_regret_winner_10[slice62],\r\n",
        "       train_regret_winner_11[slice62],\r\n",
        "       train_regret_winner_12[slice62],\r\n",
        "       train_regret_winner_13[slice62],\r\n",
        "       train_regret_winner_14[slice62],\r\n",
        "       train_regret_winner_15[slice62],\r\n",
        "       train_regret_winner_16[slice62],\r\n",
        "       train_regret_winner_17[slice62],\r\n",
        "       train_regret_winner_18[slice62],\r\n",
        "       train_regret_winner_19[slice62],\r\n",
        "       train_regret_winner_20[slice62]]\r\n",
        "\r\n",
        "loser62_results = pd.DataFrame(loser62).sort_values(by=[0], ascending=False)\r\n",
        "winner62_results = pd.DataFrame(winner62).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser62 = np.asarray(loser62_results[4:5][0])[0]\r\n",
        "median_loser62 = np.asarray(loser62_results[9:10][0])[0]\r\n",
        "upper_loser62 = np.asarray(loser62_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner62 = np.asarray(winner62_results[4:5][0])[0]\r\n",
        "median_winner62 = np.asarray(winner62_results[9:10][0])[0]\r\n",
        "upper_winner62 = np.asarray(winner62_results[14:15][0])[0]"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DZ8kHHmw-qK"
      },
      "source": [
        "# Iteration72 :\r\n",
        "\r\n",
        "slice72 = 71\r\n",
        "\r\n",
        "loser72 = [train_regret_loser_1[slice72],\r\n",
        "       train_regret_loser_2[slice72],\r\n",
        "       train_regret_loser_3[slice72],\r\n",
        "       train_regret_loser_4[slice72],\r\n",
        "       train_regret_loser_5[slice72],\r\n",
        "       train_regret_loser_6[slice72],\r\n",
        "       train_regret_loser_7[slice72],\r\n",
        "       train_regret_loser_8[slice72],\r\n",
        "       train_regret_loser_9[slice72],\r\n",
        "       train_regret_loser_10[slice72],\r\n",
        "       train_regret_loser_11[slice72],\r\n",
        "       train_regret_loser_12[slice72],\r\n",
        "       train_regret_loser_13[slice72],\r\n",
        "       train_regret_loser_14[slice72],\r\n",
        "       train_regret_loser_15[slice72],\r\n",
        "       train_regret_loser_16[slice72],\r\n",
        "       train_regret_loser_17[slice72],\r\n",
        "       train_regret_loser_18[slice72],\r\n",
        "       train_regret_loser_19[slice72],\r\n",
        "       train_regret_loser_20[slice72]]\r\n",
        "\r\n",
        "winner72 = [train_regret_winner_1[slice72],\r\n",
        "       train_regret_winner_2[slice72],\r\n",
        "       train_regret_winner_3[slice72],\r\n",
        "       train_regret_winner_4[slice72],\r\n",
        "       train_regret_winner_5[slice72],\r\n",
        "       train_regret_winner_6[slice72],\r\n",
        "       train_regret_winner_7[slice72],\r\n",
        "       train_regret_winner_8[slice72],\r\n",
        "       train_regret_winner_9[slice72],\r\n",
        "       train_regret_winner_10[slice72],\r\n",
        "       train_regret_winner_11[slice72],\r\n",
        "       train_regret_winner_12[slice72],\r\n",
        "       train_regret_winner_13[slice72],\r\n",
        "       train_regret_winner_14[slice72],\r\n",
        "       train_regret_winner_15[slice72],\r\n",
        "       train_regret_winner_16[slice72],\r\n",
        "       train_regret_winner_17[slice72],\r\n",
        "       train_regret_winner_18[slice72],\r\n",
        "       train_regret_winner_19[slice72],\r\n",
        "       train_regret_winner_20[slice72]]\r\n",
        "\r\n",
        "loser72_results = pd.DataFrame(loser72).sort_values(by=[0], ascending=False)\r\n",
        "winner72_results = pd.DataFrame(winner72).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser72 = np.asarray(loser72_results[4:5][0])[0]\r\n",
        "median_loser72 = np.asarray(loser72_results[9:10][0])[0]\r\n",
        "upper_loser72 = np.asarray(loser72_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner72 = np.asarray(winner72_results[4:5][0])[0]\r\n",
        "median_winner72 = np.asarray(winner72_results[9:10][0])[0]\r\n",
        "upper_winner72 = np.asarray(winner72_results[14:15][0])[0]"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBzBvnVyw-sz"
      },
      "source": [
        "# Iteration82 :\r\n",
        "\r\n",
        "slice82 = 81\r\n",
        "\r\n",
        "loser82 = [train_regret_loser_1[slice82],\r\n",
        "       train_regret_loser_2[slice82],\r\n",
        "       train_regret_loser_3[slice82],\r\n",
        "       train_regret_loser_4[slice82],\r\n",
        "       train_regret_loser_5[slice82],\r\n",
        "       train_regret_loser_6[slice82],\r\n",
        "       train_regret_loser_7[slice82],\r\n",
        "       train_regret_loser_8[slice82],\r\n",
        "       train_regret_loser_9[slice82],\r\n",
        "       train_regret_loser_10[slice82],\r\n",
        "       train_regret_loser_11[slice82],\r\n",
        "       train_regret_loser_12[slice82],\r\n",
        "       train_regret_loser_13[slice82],\r\n",
        "       train_regret_loser_14[slice82],\r\n",
        "       train_regret_loser_15[slice82],\r\n",
        "       train_regret_loser_16[slice82],\r\n",
        "       train_regret_loser_17[slice82],\r\n",
        "       train_regret_loser_18[slice82],\r\n",
        "       train_regret_loser_19[slice82],\r\n",
        "       train_regret_loser_20[slice82]]\r\n",
        "\r\n",
        "winner82 = [train_regret_winner_1[slice82],\r\n",
        "       train_regret_winner_2[slice82],\r\n",
        "       train_regret_winner_3[slice82],\r\n",
        "       train_regret_winner_4[slice82],\r\n",
        "       train_regret_winner_5[slice82],\r\n",
        "       train_regret_winner_6[slice82],\r\n",
        "       train_regret_winner_7[slice82],\r\n",
        "       train_regret_winner_8[slice82],\r\n",
        "       train_regret_winner_9[slice82],\r\n",
        "       train_regret_winner_10[slice82],\r\n",
        "       train_regret_winner_11[slice82],\r\n",
        "       train_regret_winner_12[slice82],\r\n",
        "       train_regret_winner_13[slice82],\r\n",
        "       train_regret_winner_14[slice82],\r\n",
        "       train_regret_winner_15[slice82],\r\n",
        "       train_regret_winner_16[slice82],\r\n",
        "       train_regret_winner_17[slice82],\r\n",
        "       train_regret_winner_18[slice82],\r\n",
        "       train_regret_winner_19[slice82],\r\n",
        "       train_regret_winner_20[slice82]]\r\n",
        "\r\n",
        "loser82_results = pd.DataFrame(loser82).sort_values(by=[0], ascending=False)\r\n",
        "winner82_results = pd.DataFrame(winner82).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser82 = np.asarray(loser82_results[4:5][0])[0]\r\n",
        "median_loser82 = np.asarray(loser82_results[9:10][0])[0]\r\n",
        "upper_loser82 = np.asarray(loser82_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner82 = np.asarray(winner82_results[4:5][0])[0]\r\n",
        "median_winner82 = np.asarray(winner82_results[9:10][0])[0]\r\n",
        "upper_winner82 = np.asarray(winner82_results[14:15][0])[0]"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Qj--n6Bw-y2"
      },
      "source": [
        "# Iteration92 :\r\n",
        "\r\n",
        "slice92 = 91\r\n",
        "\r\n",
        "loser92 = [train_regret_loser_1[slice92],\r\n",
        "       train_regret_loser_2[slice92],\r\n",
        "       train_regret_loser_3[slice92],\r\n",
        "       train_regret_loser_4[slice92],\r\n",
        "       train_regret_loser_5[slice92],\r\n",
        "       train_regret_loser_6[slice92],\r\n",
        "       train_regret_loser_7[slice92],\r\n",
        "       train_regret_loser_8[slice92],\r\n",
        "       train_regret_loser_9[slice92],\r\n",
        "       train_regret_loser_10[slice92],\r\n",
        "       train_regret_loser_11[slice92],\r\n",
        "       train_regret_loser_12[slice92],\r\n",
        "       train_regret_loser_13[slice92],\r\n",
        "       train_regret_loser_14[slice92],\r\n",
        "       train_regret_loser_15[slice92],\r\n",
        "       train_regret_loser_16[slice92],\r\n",
        "       train_regret_loser_17[slice92],\r\n",
        "       train_regret_loser_18[slice92],\r\n",
        "       train_regret_loser_19[slice92],\r\n",
        "       train_regret_loser_20[slice92]]\r\n",
        "\r\n",
        "winner92 = [train_regret_winner_1[slice92],\r\n",
        "       train_regret_winner_2[slice92],\r\n",
        "       train_regret_winner_3[slice92],\r\n",
        "       train_regret_winner_4[slice92],\r\n",
        "       train_regret_winner_5[slice92],\r\n",
        "       train_regret_winner_6[slice92],\r\n",
        "       train_regret_winner_7[slice92],\r\n",
        "       train_regret_winner_8[slice92],\r\n",
        "       train_regret_winner_9[slice92],\r\n",
        "       train_regret_winner_10[slice92],\r\n",
        "       train_regret_winner_11[slice92],\r\n",
        "       train_regret_winner_12[slice92],\r\n",
        "       train_regret_winner_13[slice92],\r\n",
        "       train_regret_winner_14[slice92],\r\n",
        "       train_regret_winner_15[slice92],\r\n",
        "       train_regret_winner_16[slice92],\r\n",
        "       train_regret_winner_17[slice92],\r\n",
        "       train_regret_winner_18[slice92],\r\n",
        "       train_regret_winner_19[slice92],\r\n",
        "       train_regret_winner_20[slice92]]\r\n",
        "\r\n",
        "loser92_results = pd.DataFrame(loser92).sort_values(by=[0], ascending=False)\r\n",
        "winner92_results = pd.DataFrame(winner92).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser92 = np.asarray(loser92_results[4:5][0])[0]\r\n",
        "median_loser92 = np.asarray(loser92_results[9:10][0])[0]\r\n",
        "upper_loser92 = np.asarray(loser92_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner92 = np.asarray(winner92_results[4:5][0])[0]\r\n",
        "median_winner92 = np.asarray(winner92_results[9:10][0])[0]\r\n",
        "upper_winner92 = np.asarray(winner92_results[14:15][0])[0]"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aaJ5KqLdw-0L"
      },
      "source": [
        "# Iteration3 :\r\n",
        "\r\n",
        "slice3 = 2\r\n",
        "\r\n",
        "loser3 = [train_regret_loser_1[slice3],\r\n",
        "       train_regret_loser_2[slice3],\r\n",
        "       train_regret_loser_3[slice3],\r\n",
        "       train_regret_loser_4[slice3],\r\n",
        "       train_regret_loser_5[slice3],\r\n",
        "       train_regret_loser_6[slice3],\r\n",
        "       train_regret_loser_7[slice3],\r\n",
        "       train_regret_loser_8[slice3],\r\n",
        "       train_regret_loser_9[slice3],\r\n",
        "       train_regret_loser_10[slice3],\r\n",
        "       train_regret_loser_11[slice3],\r\n",
        "       train_regret_loser_12[slice3],\r\n",
        "       train_regret_loser_13[slice3],\r\n",
        "       train_regret_loser_14[slice3],\r\n",
        "       train_regret_loser_15[slice3],\r\n",
        "       train_regret_loser_16[slice3],\r\n",
        "       train_regret_loser_17[slice3],\r\n",
        "       train_regret_loser_18[slice3],\r\n",
        "       train_regret_loser_19[slice3],\r\n",
        "       train_regret_loser_20[slice3]]\r\n",
        "\r\n",
        "winner3 = [train_regret_winner_1[slice3],\r\n",
        "       train_regret_winner_2[slice3],\r\n",
        "       train_regret_winner_3[slice3],\r\n",
        "       train_regret_winner_4[slice3],\r\n",
        "       train_regret_winner_5[slice3],\r\n",
        "       train_regret_winner_6[slice3],\r\n",
        "       train_regret_winner_7[slice3],\r\n",
        "       train_regret_winner_8[slice3],\r\n",
        "       train_regret_winner_9[slice3],\r\n",
        "       train_regret_winner_10[slice3],\r\n",
        "       train_regret_winner_11[slice3],\r\n",
        "       train_regret_winner_12[slice3],\r\n",
        "       train_regret_winner_13[slice3],\r\n",
        "       train_regret_winner_14[slice3],\r\n",
        "       train_regret_winner_15[slice3],\r\n",
        "       train_regret_winner_16[slice3],\r\n",
        "       train_regret_winner_17[slice3],\r\n",
        "       train_regret_winner_18[slice3],\r\n",
        "       train_regret_winner_19[slice3],\r\n",
        "       train_regret_winner_20[slice3]]\r\n",
        "\r\n",
        "loser3_results = pd.DataFrame(loser3).sort_values(by=[0], ascending=False)\r\n",
        "winner3_results = pd.DataFrame(winner3).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser3 = np.asarray(loser3_results[4:5][0])[0]\r\n",
        "median_loser3 = np.asarray(loser3_results[9:10][0])[0]\r\n",
        "upper_loser3 = np.asarray(loser3_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner3 = np.asarray(winner3_results[4:5][0])[0]\r\n",
        "median_winner3 = np.asarray(winner3_results[9:10][0])[0]\r\n",
        "upper_winner3 = np.asarray(winner3_results[14:15][0])[0]"
      ],
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkaoT1dyw-16"
      },
      "source": [
        "# Iteration13 :\r\n",
        "\r\n",
        "slice13 = 12\r\n",
        "\r\n",
        "loser13 = [train_regret_loser_1[slice13],\r\n",
        "       train_regret_loser_2[slice13],\r\n",
        "       train_regret_loser_3[slice13],\r\n",
        "       train_regret_loser_4[slice13],\r\n",
        "       train_regret_loser_5[slice13],\r\n",
        "       train_regret_loser_6[slice13],\r\n",
        "       train_regret_loser_7[slice13],\r\n",
        "       train_regret_loser_8[slice13],\r\n",
        "       train_regret_loser_9[slice13],\r\n",
        "       train_regret_loser_10[slice13],\r\n",
        "       train_regret_loser_11[slice13],\r\n",
        "       train_regret_loser_12[slice13],\r\n",
        "       train_regret_loser_13[slice13],\r\n",
        "       train_regret_loser_14[slice13],\r\n",
        "       train_regret_loser_15[slice13],\r\n",
        "       train_regret_loser_16[slice13],\r\n",
        "       train_regret_loser_17[slice13],\r\n",
        "       train_regret_loser_18[slice13],\r\n",
        "       train_regret_loser_19[slice13],\r\n",
        "       train_regret_loser_20[slice13]]\r\n",
        "\r\n",
        "winner13 = [train_regret_winner_1[slice13],\r\n",
        "       train_regret_winner_2[slice13],\r\n",
        "       train_regret_winner_3[slice13],\r\n",
        "       train_regret_winner_4[slice13],\r\n",
        "       train_regret_winner_5[slice13],\r\n",
        "       train_regret_winner_6[slice13],\r\n",
        "       train_regret_winner_7[slice13],\r\n",
        "       train_regret_winner_8[slice13],\r\n",
        "       train_regret_winner_9[slice13],\r\n",
        "       train_regret_winner_10[slice13],\r\n",
        "       train_regret_winner_11[slice13],\r\n",
        "       train_regret_winner_12[slice13],\r\n",
        "       train_regret_winner_13[slice13],\r\n",
        "       train_regret_winner_14[slice13],\r\n",
        "       train_regret_winner_15[slice13],\r\n",
        "       train_regret_winner_16[slice13],\r\n",
        "       train_regret_winner_17[slice13],\r\n",
        "       train_regret_winner_18[slice13],\r\n",
        "       train_regret_winner_19[slice13],\r\n",
        "       train_regret_winner_20[slice13]]\r\n",
        "\r\n",
        "loser13_results = pd.DataFrame(loser12).sort_values(by=[0], ascending=False)\r\n",
        "winner13_results = pd.DataFrame(winner12).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser13 = np.asarray(loser13_results[4:5][0])[0]\r\n",
        "median_loser13 = np.asarray(loser13_results[9:10][0])[0]\r\n",
        "upper_loser13 = np.asarray(loser13_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner13 = np.asarray(winner13_results[4:5][0])[0]\r\n",
        "median_winner13 = np.asarray(winner13_results[9:10][0])[0]\r\n",
        "upper_winner13 = np.asarray(winner13_results[14:15][0])[0]"
      ],
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZbVMrBaw-4B"
      },
      "source": [
        "# Iteration23 :\r\n",
        "\r\n",
        "slice23 = 22\r\n",
        "\r\n",
        "loser23 = [train_regret_loser_1[slice23],\r\n",
        "       train_regret_loser_2[slice23],\r\n",
        "       train_regret_loser_3[slice23],\r\n",
        "       train_regret_loser_4[slice23],\r\n",
        "       train_regret_loser_5[slice23],\r\n",
        "       train_regret_loser_6[slice23],\r\n",
        "       train_regret_loser_7[slice23],\r\n",
        "       train_regret_loser_8[slice23],\r\n",
        "       train_regret_loser_9[slice23],\r\n",
        "       train_regret_loser_10[slice23],\r\n",
        "       train_regret_loser_11[slice23],\r\n",
        "       train_regret_loser_12[slice23],\r\n",
        "       train_regret_loser_13[slice23],\r\n",
        "       train_regret_loser_14[slice23],\r\n",
        "       train_regret_loser_15[slice23],\r\n",
        "       train_regret_loser_16[slice23],\r\n",
        "       train_regret_loser_17[slice23],\r\n",
        "       train_regret_loser_18[slice23],\r\n",
        "       train_regret_loser_19[slice23],\r\n",
        "       train_regret_loser_20[slice23]]\r\n",
        "\r\n",
        "winner23 = [train_regret_winner_1[slice23],\r\n",
        "       train_regret_winner_2[slice23],\r\n",
        "       train_regret_winner_3[slice23],\r\n",
        "       train_regret_winner_4[slice23],\r\n",
        "       train_regret_winner_5[slice23],\r\n",
        "       train_regret_winner_6[slice23],\r\n",
        "       train_regret_winner_7[slice23],\r\n",
        "       train_regret_winner_8[slice23],\r\n",
        "       train_regret_winner_9[slice23],\r\n",
        "       train_regret_winner_10[slice23],\r\n",
        "       train_regret_winner_11[slice23],\r\n",
        "       train_regret_winner_12[slice23],\r\n",
        "       train_regret_winner_13[slice23],\r\n",
        "       train_regret_winner_14[slice23],\r\n",
        "       train_regret_winner_15[slice23],\r\n",
        "       train_regret_winner_16[slice23],\r\n",
        "       train_regret_winner_17[slice23],\r\n",
        "       train_regret_winner_18[slice23],\r\n",
        "       train_regret_winner_19[slice23],\r\n",
        "       train_regret_winner_20[slice23]]\r\n",
        "\r\n",
        "loser23_results = pd.DataFrame(loser23).sort_values(by=[0], ascending=False)\r\n",
        "winner23_results = pd.DataFrame(winner23).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser23 = np.asarray(loser23_results[4:5][0])[0]\r\n",
        "median_loser23 = np.asarray(loser23_results[9:10][0])[0]\r\n",
        "upper_loser23 = np.asarray(loser23_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner23 = np.asarray(winner23_results[4:5][0])[0]\r\n",
        "median_winner23 = np.asarray(winner23_results[9:10][0])[0]\r\n",
        "upper_winner23 = np.asarray(winner23_results[14:15][0])[0]"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUa6HRCUw-7G"
      },
      "source": [
        "# Iteration33 :\r\n",
        "\r\n",
        "slice33 = 32\r\n",
        "\r\n",
        "loser33 = [train_regret_loser_1[slice33],\r\n",
        "       train_regret_loser_2[slice33],\r\n",
        "       train_regret_loser_3[slice33],\r\n",
        "       train_regret_loser_4[slice33],\r\n",
        "       train_regret_loser_5[slice33],\r\n",
        "       train_regret_loser_6[slice33],\r\n",
        "       train_regret_loser_7[slice33],\r\n",
        "       train_regret_loser_8[slice33],\r\n",
        "       train_regret_loser_9[slice33],\r\n",
        "       train_regret_loser_10[slice33],\r\n",
        "       train_regret_loser_11[slice33],\r\n",
        "       train_regret_loser_12[slice33],\r\n",
        "       train_regret_loser_13[slice33],\r\n",
        "       train_regret_loser_14[slice33],\r\n",
        "       train_regret_loser_15[slice33],\r\n",
        "       train_regret_loser_16[slice33],\r\n",
        "       train_regret_loser_17[slice33],\r\n",
        "       train_regret_loser_18[slice33],\r\n",
        "       train_regret_loser_19[slice33],\r\n",
        "       train_regret_loser_20[slice33]]\r\n",
        "\r\n",
        "winner33 = [train_regret_winner_1[slice33],\r\n",
        "       train_regret_winner_2[slice33],\r\n",
        "       train_regret_winner_3[slice33],\r\n",
        "       train_regret_winner_4[slice33],\r\n",
        "       train_regret_winner_5[slice33],\r\n",
        "       train_regret_winner_6[slice33],\r\n",
        "       train_regret_winner_7[slice33],\r\n",
        "       train_regret_winner_8[slice33],\r\n",
        "       train_regret_winner_9[slice33],\r\n",
        "       train_regret_winner_10[slice33],\r\n",
        "       train_regret_winner_11[slice33],\r\n",
        "       train_regret_winner_12[slice33],\r\n",
        "       train_regret_winner_13[slice33],\r\n",
        "       train_regret_winner_14[slice33],\r\n",
        "       train_regret_winner_15[slice33],\r\n",
        "       train_regret_winner_16[slice33],\r\n",
        "       train_regret_winner_17[slice33],\r\n",
        "       train_regret_winner_18[slice33],\r\n",
        "       train_regret_winner_19[slice33],\r\n",
        "       train_regret_winner_20[slice33]]\r\n",
        "\r\n",
        "loser33_results = pd.DataFrame(loser33).sort_values(by=[0], ascending=False)\r\n",
        "winner33_results = pd.DataFrame(winner33).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser33 = np.asarray(loser33_results[4:5][0])[0]\r\n",
        "median_loser33 = np.asarray(loser33_results[9:10][0])[0]\r\n",
        "upper_loser33 = np.asarray(loser33_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner33 = np.asarray(winner33_results[4:5][0])[0]\r\n",
        "median_winner33 = np.asarray(winner33_results[9:10][0])[0]\r\n",
        "upper_winner33 = np.asarray(winner33_results[14:15][0])[0]"
      ],
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4EVxQwsxw-9n"
      },
      "source": [
        "# Iteration43 :\r\n",
        "\r\n",
        "slice43 = 42\r\n",
        "\r\n",
        "loser43 = [train_regret_loser_1[slice43],\r\n",
        "       train_regret_loser_2[slice43],\r\n",
        "       train_regret_loser_3[slice43],\r\n",
        "       train_regret_loser_4[slice43],\r\n",
        "       train_regret_loser_5[slice43],\r\n",
        "       train_regret_loser_6[slice43],\r\n",
        "       train_regret_loser_7[slice43],\r\n",
        "       train_regret_loser_8[slice43],\r\n",
        "       train_regret_loser_9[slice43],\r\n",
        "       train_regret_loser_10[slice43],\r\n",
        "       train_regret_loser_11[slice43],\r\n",
        "       train_regret_loser_12[slice43],\r\n",
        "       train_regret_loser_13[slice43],\r\n",
        "       train_regret_loser_14[slice43],\r\n",
        "       train_regret_loser_15[slice43],\r\n",
        "       train_regret_loser_16[slice43],\r\n",
        "       train_regret_loser_17[slice43],\r\n",
        "       train_regret_loser_18[slice43],\r\n",
        "       train_regret_loser_19[slice43],\r\n",
        "       train_regret_loser_20[slice43]]\r\n",
        "\r\n",
        "winner43 = [train_regret_winner_1[slice43],\r\n",
        "       train_regret_winner_2[slice43],\r\n",
        "       train_regret_winner_3[slice43],\r\n",
        "       train_regret_winner_4[slice43],\r\n",
        "       train_regret_winner_5[slice43],\r\n",
        "       train_regret_winner_6[slice43],\r\n",
        "       train_regret_winner_7[slice43],\r\n",
        "       train_regret_winner_8[slice43],\r\n",
        "       train_regret_winner_9[slice43],\r\n",
        "       train_regret_winner_10[slice43],\r\n",
        "       train_regret_winner_11[slice43],\r\n",
        "       train_regret_winner_12[slice43],\r\n",
        "       train_regret_winner_13[slice43],\r\n",
        "       train_regret_winner_14[slice43],\r\n",
        "       train_regret_winner_15[slice43],\r\n",
        "       train_regret_winner_16[slice43],\r\n",
        "       train_regret_winner_17[slice43],\r\n",
        "       train_regret_winner_18[slice43],\r\n",
        "       train_regret_winner_19[slice43],\r\n",
        "       train_regret_winner_20[slice43]]\r\n",
        "\r\n",
        "loser43_results = pd.DataFrame(loser43).sort_values(by=[0], ascending=False)\r\n",
        "winner43_results = pd.DataFrame(winner43).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser43 = np.asarray(loser43_results[4:5][0])[0]\r\n",
        "median_loser43 = np.asarray(loser43_results[9:10][0])[0]\r\n",
        "upper_loser43 = np.asarray(loser43_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner43 = np.asarray(winner43_results[4:5][0])[0]\r\n",
        "median_winner43 = np.asarray(winner43_results[9:10][0])[0]\r\n",
        "upper_winner43 = np.asarray(winner43_results[14:15][0])[0]"
      ],
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQtRyMv-w_AZ"
      },
      "source": [
        "# Iteration53 :\r\n",
        "\r\n",
        "slice53 = 52\r\n",
        "\r\n",
        "loser53 = [train_regret_loser_1[slice53],\r\n",
        "       train_regret_loser_2[slice53],\r\n",
        "       train_regret_loser_3[slice53],\r\n",
        "       train_regret_loser_4[slice53],\r\n",
        "       train_regret_loser_5[slice53],\r\n",
        "       train_regret_loser_6[slice53],\r\n",
        "       train_regret_loser_7[slice53],\r\n",
        "       train_regret_loser_8[slice53],\r\n",
        "       train_regret_loser_9[slice53],\r\n",
        "       train_regret_loser_10[slice53],\r\n",
        "       train_regret_loser_11[slice53],\r\n",
        "       train_regret_loser_12[slice53],\r\n",
        "       train_regret_loser_13[slice53],\r\n",
        "       train_regret_loser_14[slice53],\r\n",
        "       train_regret_loser_15[slice53],\r\n",
        "       train_regret_loser_16[slice53],\r\n",
        "       train_regret_loser_17[slice53],\r\n",
        "       train_regret_loser_18[slice53],\r\n",
        "       train_regret_loser_19[slice53],\r\n",
        "       train_regret_loser_20[slice53]]\r\n",
        "\r\n",
        "winner53 = [train_regret_winner_1[slice53],\r\n",
        "       train_regret_winner_2[slice53],\r\n",
        "       train_regret_winner_3[slice53],\r\n",
        "       train_regret_winner_4[slice53],\r\n",
        "       train_regret_winner_5[slice53],\r\n",
        "       train_regret_winner_6[slice53],\r\n",
        "       train_regret_winner_7[slice53],\r\n",
        "       train_regret_winner_8[slice53],\r\n",
        "       train_regret_winner_9[slice53],\r\n",
        "       train_regret_winner_10[slice53],\r\n",
        "       train_regret_winner_11[slice53],\r\n",
        "       train_regret_winner_12[slice53],\r\n",
        "       train_regret_winner_13[slice53],\r\n",
        "       train_regret_winner_14[slice53],\r\n",
        "       train_regret_winner_15[slice53],\r\n",
        "       train_regret_winner_16[slice53],\r\n",
        "       train_regret_winner_17[slice53],\r\n",
        "       train_regret_winner_18[slice53],\r\n",
        "       train_regret_winner_19[slice53],\r\n",
        "       train_regret_winner_20[slice53]]\r\n",
        "\r\n",
        "loser53_results = pd.DataFrame(loser53).sort_values(by=[0], ascending=False)\r\n",
        "winner53_results = pd.DataFrame(winner53).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser53 = np.asarray(loser53_results[4:5][0])[0]\r\n",
        "median_loser53 = np.asarray(loser53_results[9:10][0])[0]\r\n",
        "upper_loser53 = np.asarray(loser53_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner53 = np.asarray(winner53_results[4:5][0])[0]\r\n",
        "median_winner53 = np.asarray(winner53_results[9:10][0])[0]\r\n",
        "upper_winner53 = np.asarray(winner53_results[14:15][0])[0]"
      ],
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1CIMEXXYw_DR"
      },
      "source": [
        "# Iteration63 :\r\n",
        "\r\n",
        "slice63 = 62\r\n",
        "\r\n",
        "loser63 = [train_regret_loser_1[slice63],\r\n",
        "       train_regret_loser_2[slice63],\r\n",
        "       train_regret_loser_3[slice63],\r\n",
        "       train_regret_loser_4[slice63],\r\n",
        "       train_regret_loser_5[slice63],\r\n",
        "       train_regret_loser_6[slice63],\r\n",
        "       train_regret_loser_7[slice63],\r\n",
        "       train_regret_loser_8[slice63],\r\n",
        "       train_regret_loser_9[slice63],\r\n",
        "       train_regret_loser_10[slice63],\r\n",
        "       train_regret_loser_11[slice63],\r\n",
        "       train_regret_loser_12[slice63],\r\n",
        "       train_regret_loser_13[slice63],\r\n",
        "       train_regret_loser_14[slice63],\r\n",
        "       train_regret_loser_15[slice63],\r\n",
        "       train_regret_loser_16[slice63],\r\n",
        "       train_regret_loser_17[slice63],\r\n",
        "       train_regret_loser_18[slice63],\r\n",
        "       train_regret_loser_19[slice63],\r\n",
        "       train_regret_loser_20[slice63]]\r\n",
        "\r\n",
        "winner63 = [train_regret_winner_1[slice63],\r\n",
        "       train_regret_winner_2[slice63],\r\n",
        "       train_regret_winner_3[slice63],\r\n",
        "       train_regret_winner_4[slice63],\r\n",
        "       train_regret_winner_5[slice63],\r\n",
        "       train_regret_winner_6[slice63],\r\n",
        "       train_regret_winner_7[slice63],\r\n",
        "       train_regret_winner_8[slice63],\r\n",
        "       train_regret_winner_9[slice63],\r\n",
        "       train_regret_winner_10[slice63],\r\n",
        "       train_regret_winner_11[slice63],\r\n",
        "       train_regret_winner_12[slice63],\r\n",
        "       train_regret_winner_13[slice63],\r\n",
        "       train_regret_winner_14[slice63],\r\n",
        "       train_regret_winner_15[slice63],\r\n",
        "       train_regret_winner_16[slice63],\r\n",
        "       train_regret_winner_17[slice63],\r\n",
        "       train_regret_winner_18[slice63],\r\n",
        "       train_regret_winner_19[slice63],\r\n",
        "       train_regret_winner_20[slice63]]\r\n",
        "\r\n",
        "loser63_results = pd.DataFrame(loser63).sort_values(by=[0], ascending=False)\r\n",
        "winner63_results = pd.DataFrame(winner63).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser63 = np.asarray(loser63_results[4:5][0])[0]\r\n",
        "median_loser63 = np.asarray(loser63_results[9:10][0])[0]\r\n",
        "upper_loser63 = np.asarray(loser63_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner63 = np.asarray(winner63_results[4:5][0])[0]\r\n",
        "median_winner63 = np.asarray(winner63_results[9:10][0])[0]\r\n",
        "upper_winner63 = np.asarray(winner63_results[14:15][0])[0]"
      ],
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xg4GfwU_w_GF"
      },
      "source": [
        "# Iteration73 :\r\n",
        "\r\n",
        "slice73 = 72\r\n",
        "\r\n",
        "loser73 = [train_regret_loser_1[slice73],\r\n",
        "       train_regret_loser_2[slice73],\r\n",
        "       train_regret_loser_3[slice73],\r\n",
        "       train_regret_loser_4[slice73],\r\n",
        "       train_regret_loser_5[slice73],\r\n",
        "       train_regret_loser_6[slice73],\r\n",
        "       train_regret_loser_7[slice73],\r\n",
        "       train_regret_loser_8[slice73],\r\n",
        "       train_regret_loser_9[slice73],\r\n",
        "       train_regret_loser_10[slice73],\r\n",
        "       train_regret_loser_11[slice73],\r\n",
        "       train_regret_loser_12[slice73],\r\n",
        "       train_regret_loser_13[slice73],\r\n",
        "       train_regret_loser_14[slice73],\r\n",
        "       train_regret_loser_15[slice73],\r\n",
        "       train_regret_loser_16[slice73],\r\n",
        "       train_regret_loser_17[slice73],\r\n",
        "       train_regret_loser_18[slice73],\r\n",
        "       train_regret_loser_19[slice73],\r\n",
        "       train_regret_loser_20[slice73]]\r\n",
        "\r\n",
        "winner73 = [train_regret_winner_1[slice73],\r\n",
        "       train_regret_winner_2[slice73],\r\n",
        "       train_regret_winner_3[slice73],\r\n",
        "       train_regret_winner_4[slice73],\r\n",
        "       train_regret_winner_5[slice73],\r\n",
        "       train_regret_winner_6[slice73],\r\n",
        "       train_regret_winner_7[slice73],\r\n",
        "       train_regret_winner_8[slice73],\r\n",
        "       train_regret_winner_9[slice73],\r\n",
        "       train_regret_winner_10[slice73],\r\n",
        "       train_regret_winner_11[slice73],\r\n",
        "       train_regret_winner_12[slice73],\r\n",
        "       train_regret_winner_13[slice73],\r\n",
        "       train_regret_winner_14[slice73],\r\n",
        "       train_regret_winner_15[slice73],\r\n",
        "       train_regret_winner_16[slice73],\r\n",
        "       train_regret_winner_17[slice73],\r\n",
        "       train_regret_winner_18[slice73],\r\n",
        "       train_regret_winner_19[slice73],\r\n",
        "       train_regret_winner_20[slice73]]\r\n",
        "\r\n",
        "loser73_results = pd.DataFrame(loser73).sort_values(by=[0], ascending=False)\r\n",
        "winner73_results = pd.DataFrame(winner73).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser73 = np.asarray(loser73_results[4:5][0])[0]\r\n",
        "median_loser73 = np.asarray(loser73_results[9:10][0])[0]\r\n",
        "upper_loser73 = np.asarray(loser73_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner73 = np.asarray(winner73_results[4:5][0])[0]\r\n",
        "median_winner73 = np.asarray(winner73_results[9:10][0])[0]\r\n",
        "upper_winner73 = np.asarray(winner73_results[14:15][0])[0]"
      ],
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WmpcEV4w_I9"
      },
      "source": [
        "# Iteration83 :\r\n",
        "\r\n",
        "slice83 = 82\r\n",
        "\r\n",
        "loser83 = [train_regret_loser_1[slice83],\r\n",
        "       train_regret_loser_2[slice83],\r\n",
        "       train_regret_loser_3[slice83],\r\n",
        "       train_regret_loser_4[slice83],\r\n",
        "       train_regret_loser_5[slice83],\r\n",
        "       train_regret_loser_6[slice83],\r\n",
        "       train_regret_loser_7[slice83],\r\n",
        "       train_regret_loser_8[slice83],\r\n",
        "       train_regret_loser_9[slice83],\r\n",
        "       train_regret_loser_10[slice83],\r\n",
        "       train_regret_loser_11[slice83],\r\n",
        "       train_regret_loser_12[slice83],\r\n",
        "       train_regret_loser_13[slice83],\r\n",
        "       train_regret_loser_14[slice83],\r\n",
        "       train_regret_loser_15[slice83],\r\n",
        "       train_regret_loser_16[slice83],\r\n",
        "       train_regret_loser_17[slice83],\r\n",
        "       train_regret_loser_18[slice83],\r\n",
        "       train_regret_loser_19[slice83],\r\n",
        "       train_regret_loser_20[slice83]]\r\n",
        "\r\n",
        "winner83 = [train_regret_winner_1[slice83],\r\n",
        "       train_regret_winner_2[slice83],\r\n",
        "       train_regret_winner_3[slice83],\r\n",
        "       train_regret_winner_4[slice83],\r\n",
        "       train_regret_winner_5[slice83],\r\n",
        "       train_regret_winner_6[slice83],\r\n",
        "       train_regret_winner_7[slice83],\r\n",
        "       train_regret_winner_8[slice83],\r\n",
        "       train_regret_winner_9[slice83],\r\n",
        "       train_regret_winner_10[slice83],\r\n",
        "       train_regret_winner_11[slice83],\r\n",
        "       train_regret_winner_12[slice83],\r\n",
        "       train_regret_winner_13[slice83],\r\n",
        "       train_regret_winner_14[slice83],\r\n",
        "       train_regret_winner_15[slice83],\r\n",
        "       train_regret_winner_16[slice83],\r\n",
        "       train_regret_winner_17[slice83],\r\n",
        "       train_regret_winner_18[slice83],\r\n",
        "       train_regret_winner_19[slice83],\r\n",
        "       train_regret_winner_20[slice83]]\r\n",
        "\r\n",
        "loser83_results = pd.DataFrame(loser83).sort_values(by=[0], ascending=False)\r\n",
        "winner83_results = pd.DataFrame(winner83).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser83 = np.asarray(loser83_results[4:5][0])[0]\r\n",
        "median_loser83 = np.asarray(loser83_results[9:10][0])[0]\r\n",
        "upper_loser83 = np.asarray(loser83_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner83 = np.asarray(winner83_results[4:5][0])[0]\r\n",
        "median_winner83 = np.asarray(winner83_results[9:10][0])[0]\r\n",
        "upper_winner83 = np.asarray(winner83_results[14:15][0])[0]"
      ],
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGPwoBZyw_MF"
      },
      "source": [
        "# Iteration93 :\r\n",
        "\r\n",
        "slice93 = 92\r\n",
        "\r\n",
        "loser93 = [train_regret_loser_1[slice93],\r\n",
        "       train_regret_loser_2[slice93],\r\n",
        "       train_regret_loser_3[slice93],\r\n",
        "       train_regret_loser_4[slice93],\r\n",
        "       train_regret_loser_5[slice93],\r\n",
        "       train_regret_loser_6[slice93],\r\n",
        "       train_regret_loser_7[slice93],\r\n",
        "       train_regret_loser_8[slice93],\r\n",
        "       train_regret_loser_9[slice93],\r\n",
        "       train_regret_loser_10[slice93],\r\n",
        "       train_regret_loser_11[slice93],\r\n",
        "       train_regret_loser_12[slice93],\r\n",
        "       train_regret_loser_13[slice93],\r\n",
        "       train_regret_loser_14[slice93],\r\n",
        "       train_regret_loser_15[slice93],\r\n",
        "       train_regret_loser_16[slice93],\r\n",
        "       train_regret_loser_17[slice93],\r\n",
        "       train_regret_loser_18[slice93],\r\n",
        "       train_regret_loser_19[slice93],\r\n",
        "       train_regret_loser_20[slice93]]\r\n",
        "\r\n",
        "winner93 = [train_regret_winner_1[slice93],\r\n",
        "       train_regret_winner_2[slice93],\r\n",
        "       train_regret_winner_3[slice93],\r\n",
        "       train_regret_winner_4[slice93],\r\n",
        "       train_regret_winner_5[slice93],\r\n",
        "       train_regret_winner_6[slice93],\r\n",
        "       train_regret_winner_7[slice93],\r\n",
        "       train_regret_winner_8[slice93],\r\n",
        "       train_regret_winner_9[slice93],\r\n",
        "       train_regret_winner_10[slice93],\r\n",
        "       train_regret_winner_11[slice93],\r\n",
        "       train_regret_winner_12[slice93],\r\n",
        "       train_regret_winner_13[slice93],\r\n",
        "       train_regret_winner_14[slice93],\r\n",
        "       train_regret_winner_15[slice93],\r\n",
        "       train_regret_winner_16[slice93],\r\n",
        "       train_regret_winner_17[slice93],\r\n",
        "       train_regret_winner_18[slice93],\r\n",
        "       train_regret_winner_19[slice93],\r\n",
        "       train_regret_winner_20[slice93]]\r\n",
        "\r\n",
        "loser93_results = pd.DataFrame(loser93).sort_values(by=[0], ascending=False)\r\n",
        "winner93_results = pd.DataFrame(winner93).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser93 = np.asarray(loser93_results[4:5][0])[0]\r\n",
        "median_loser93 = np.asarray(loser93_results[9:10][0])[0]\r\n",
        "upper_loser93 = np.asarray(loser93_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner93 = np.asarray(winner93_results[4:5][0])[0]\r\n",
        "median_winner93 = np.asarray(winner93_results[9:10][0])[0]\r\n",
        "upper_winner93 = np.asarray(winner93_results[14:15][0])[0]"
      ],
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_ghdGzPw_Oq"
      },
      "source": [
        "# Iteration4 :\r\n",
        "\r\n",
        "slice4 = 3\r\n",
        "\r\n",
        "loser4 = [train_regret_loser_1[slice4],\r\n",
        "       train_regret_loser_2[slice4],\r\n",
        "       train_regret_loser_3[slice4],\r\n",
        "       train_regret_loser_4[slice4],\r\n",
        "       train_regret_loser_5[slice4],\r\n",
        "       train_regret_loser_6[slice4],\r\n",
        "       train_regret_loser_7[slice4],\r\n",
        "       train_regret_loser_8[slice4],\r\n",
        "       train_regret_loser_9[slice4],\r\n",
        "       train_regret_loser_10[slice4],\r\n",
        "       train_regret_loser_11[slice4],\r\n",
        "       train_regret_loser_12[slice4],\r\n",
        "       train_regret_loser_13[slice4],\r\n",
        "       train_regret_loser_14[slice4],\r\n",
        "       train_regret_loser_15[slice4],\r\n",
        "       train_regret_loser_16[slice4],\r\n",
        "       train_regret_loser_17[slice4],\r\n",
        "       train_regret_loser_18[slice4],\r\n",
        "       train_regret_loser_19[slice4],\r\n",
        "       train_regret_loser_20[slice4]]\r\n",
        "\r\n",
        "winner4 = [train_regret_winner_1[slice4],\r\n",
        "       train_regret_winner_2[slice4],\r\n",
        "       train_regret_winner_3[slice4],\r\n",
        "       train_regret_winner_4[slice4],\r\n",
        "       train_regret_winner_5[slice4],\r\n",
        "       train_regret_winner_6[slice4],\r\n",
        "       train_regret_winner_7[slice4],\r\n",
        "       train_regret_winner_8[slice4],\r\n",
        "       train_regret_winner_9[slice4],\r\n",
        "       train_regret_winner_10[slice4],\r\n",
        "       train_regret_winner_11[slice4],\r\n",
        "       train_regret_winner_12[slice4],\r\n",
        "       train_regret_winner_13[slice4],\r\n",
        "       train_regret_winner_14[slice4],\r\n",
        "       train_regret_winner_15[slice4],\r\n",
        "       train_regret_winner_16[slice4],\r\n",
        "       train_regret_winner_17[slice4],\r\n",
        "       train_regret_winner_18[slice4],\r\n",
        "       train_regret_winner_19[slice4],\r\n",
        "       train_regret_winner_20[slice4]]\r\n",
        "\r\n",
        "loser4_results = pd.DataFrame(loser4).sort_values(by=[0], ascending=False)\r\n",
        "winner4_results = pd.DataFrame(winner4).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser4 = np.asarray(loser4_results[4:5][0])[0]\r\n",
        "median_loser4 = np.asarray(loser4_results[9:10][0])[0]\r\n",
        "upper_loser4 = np.asarray(loser4_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner4 = np.asarray(winner4_results[4:5][0])[0]\r\n",
        "median_winner4 = np.asarray(winner4_results[9:10][0])[0]\r\n",
        "upper_winner4 = np.asarray(winner4_results[14:15][0])[0]"
      ],
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNvDt4QMw_Rz"
      },
      "source": [
        "# Iteration14 :\r\n",
        "\r\n",
        "slice14 = 13\r\n",
        "\r\n",
        "loser14 = [train_regret_loser_1[slice14],\r\n",
        "       train_regret_loser_2[slice14],\r\n",
        "       train_regret_loser_3[slice14],\r\n",
        "       train_regret_loser_4[slice14],\r\n",
        "       train_regret_loser_5[slice14],\r\n",
        "       train_regret_loser_6[slice14],\r\n",
        "       train_regret_loser_7[slice14],\r\n",
        "       train_regret_loser_8[slice14],\r\n",
        "       train_regret_loser_9[slice14],\r\n",
        "       train_regret_loser_10[slice14],\r\n",
        "       train_regret_loser_11[slice14],\r\n",
        "       train_regret_loser_12[slice14],\r\n",
        "       train_regret_loser_13[slice14],\r\n",
        "       train_regret_loser_14[slice14],\r\n",
        "       train_regret_loser_15[slice14],\r\n",
        "       train_regret_loser_16[slice14],\r\n",
        "       train_regret_loser_17[slice14],\r\n",
        "       train_regret_loser_18[slice14],\r\n",
        "       train_regret_loser_19[slice14],\r\n",
        "       train_regret_loser_20[slice14]]\r\n",
        "\r\n",
        "winner14 = [train_regret_winner_1[slice14],\r\n",
        "       train_regret_winner_2[slice14],\r\n",
        "       train_regret_winner_3[slice14],\r\n",
        "       train_regret_winner_4[slice14],\r\n",
        "       train_regret_winner_5[slice14],\r\n",
        "       train_regret_winner_6[slice14],\r\n",
        "       train_regret_winner_7[slice14],\r\n",
        "       train_regret_winner_8[slice14],\r\n",
        "       train_regret_winner_9[slice14],\r\n",
        "       train_regret_winner_10[slice14],\r\n",
        "       train_regret_winner_11[slice14],\r\n",
        "       train_regret_winner_12[slice14],\r\n",
        "       train_regret_winner_13[slice14],\r\n",
        "       train_regret_winner_14[slice14],\r\n",
        "       train_regret_winner_15[slice14],\r\n",
        "       train_regret_winner_16[slice14],\r\n",
        "       train_regret_winner_17[slice14],\r\n",
        "       train_regret_winner_18[slice14],\r\n",
        "       train_regret_winner_19[slice14],\r\n",
        "       train_regret_winner_20[slice14]]\r\n",
        "\r\n",
        "loser14_results = pd.DataFrame(loser14).sort_values(by=[0], ascending=False)\r\n",
        "winner14_results = pd.DataFrame(winner14).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser14 = np.asarray(loser14_results[4:5][0])[0]\r\n",
        "median_loser14 = np.asarray(loser14_results[9:10][0])[0]\r\n",
        "upper_loser14 = np.asarray(loser14_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner14 = np.asarray(winner14_results[4:5][0])[0]\r\n",
        "median_winner14 = np.asarray(winner14_results[9:10][0])[0]\r\n",
        "upper_winner14 = np.asarray(winner14_results[14:15][0])[0]"
      ],
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zx3kMQV0w_Ua"
      },
      "source": [
        "# Iteration24 :\r\n",
        "\r\n",
        "slice24 = 23\r\n",
        "\r\n",
        "loser24 = [train_regret_loser_1[slice24],\r\n",
        "       train_regret_loser_2[slice24],\r\n",
        "       train_regret_loser_3[slice24],\r\n",
        "       train_regret_loser_4[slice24],\r\n",
        "       train_regret_loser_5[slice24],\r\n",
        "       train_regret_loser_6[slice24],\r\n",
        "       train_regret_loser_7[slice24],\r\n",
        "       train_regret_loser_8[slice24],\r\n",
        "       train_regret_loser_9[slice24],\r\n",
        "       train_regret_loser_10[slice24],\r\n",
        "       train_regret_loser_11[slice24],\r\n",
        "       train_regret_loser_12[slice24],\r\n",
        "       train_regret_loser_13[slice24],\r\n",
        "       train_regret_loser_14[slice24],\r\n",
        "       train_regret_loser_15[slice24],\r\n",
        "       train_regret_loser_16[slice24],\r\n",
        "       train_regret_loser_17[slice24],\r\n",
        "       train_regret_loser_18[slice24],\r\n",
        "       train_regret_loser_19[slice24],\r\n",
        "       train_regret_loser_20[slice24]]\r\n",
        "\r\n",
        "winner24 = [train_regret_winner_1[slice24],\r\n",
        "       train_regret_winner_2[slice24],\r\n",
        "       train_regret_winner_3[slice24],\r\n",
        "       train_regret_winner_4[slice24],\r\n",
        "       train_regret_winner_5[slice24],\r\n",
        "       train_regret_winner_6[slice24],\r\n",
        "       train_regret_winner_7[slice24],\r\n",
        "       train_regret_winner_8[slice24],\r\n",
        "       train_regret_winner_9[slice24],\r\n",
        "       train_regret_winner_10[slice24],\r\n",
        "       train_regret_winner_11[slice24],\r\n",
        "       train_regret_winner_12[slice24],\r\n",
        "       train_regret_winner_13[slice24],\r\n",
        "       train_regret_winner_14[slice24],\r\n",
        "       train_regret_winner_15[slice24],\r\n",
        "       train_regret_winner_16[slice24],\r\n",
        "       train_regret_winner_17[slice24],\r\n",
        "       train_regret_winner_18[slice24],\r\n",
        "       train_regret_winner_19[slice24],\r\n",
        "       train_regret_winner_20[slice24]]\r\n",
        "\r\n",
        "loser24_results = pd.DataFrame(loser24).sort_values(by=[0], ascending=False)\r\n",
        "winner24_results = pd.DataFrame(winner24).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser24 = np.asarray(loser24_results[4:5][0])[0]\r\n",
        "median_loser24 = np.asarray(loser24_results[9:10][0])[0]\r\n",
        "upper_loser24 = np.asarray(loser24_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner24 = np.asarray(winner24_results[4:5][0])[0]\r\n",
        "median_winner24 = np.asarray(winner24_results[9:10][0])[0]\r\n",
        "upper_winner24 = np.asarray(winner24_results[14:15][0])[0]"
      ],
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4ljkyMfw_Xj"
      },
      "source": [
        "# Iteration34 :\r\n",
        "\r\n",
        "slice34 = 33\r\n",
        "\r\n",
        "loser34 = [train_regret_loser_1[slice34],\r\n",
        "       train_regret_loser_2[slice34],\r\n",
        "       train_regret_loser_3[slice34],\r\n",
        "       train_regret_loser_4[slice34],\r\n",
        "       train_regret_loser_5[slice34],\r\n",
        "       train_regret_loser_6[slice34],\r\n",
        "       train_regret_loser_7[slice34],\r\n",
        "       train_regret_loser_8[slice34],\r\n",
        "       train_regret_loser_9[slice34],\r\n",
        "       train_regret_loser_10[slice34],\r\n",
        "       train_regret_loser_11[slice34],\r\n",
        "       train_regret_loser_12[slice34],\r\n",
        "       train_regret_loser_13[slice34],\r\n",
        "       train_regret_loser_14[slice34],\r\n",
        "       train_regret_loser_15[slice34],\r\n",
        "       train_regret_loser_16[slice34],\r\n",
        "       train_regret_loser_17[slice34],\r\n",
        "       train_regret_loser_18[slice34],\r\n",
        "       train_regret_loser_19[slice34],\r\n",
        "       train_regret_loser_20[slice34]]\r\n",
        "\r\n",
        "winner34 = [train_regret_winner_1[slice34],\r\n",
        "       train_regret_winner_2[slice34],\r\n",
        "       train_regret_winner_3[slice34],\r\n",
        "       train_regret_winner_4[slice34],\r\n",
        "       train_regret_winner_5[slice34],\r\n",
        "       train_regret_winner_6[slice34],\r\n",
        "       train_regret_winner_7[slice34],\r\n",
        "       train_regret_winner_8[slice34],\r\n",
        "       train_regret_winner_9[slice34],\r\n",
        "       train_regret_winner_10[slice34],\r\n",
        "       train_regret_winner_11[slice34],\r\n",
        "       train_regret_winner_12[slice34],\r\n",
        "       train_regret_winner_13[slice34],\r\n",
        "       train_regret_winner_14[slice34],\r\n",
        "       train_regret_winner_15[slice34],\r\n",
        "       train_regret_winner_16[slice34],\r\n",
        "       train_regret_winner_17[slice34],\r\n",
        "       train_regret_winner_18[slice34],\r\n",
        "       train_regret_winner_19[slice34],\r\n",
        "       train_regret_winner_20[slice34]]\r\n",
        "\r\n",
        "loser34_results = pd.DataFrame(loser34).sort_values(by=[0], ascending=False)\r\n",
        "winner34_results = pd.DataFrame(winner34).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser34 = np.asarray(loser34_results[4:5][0])[0]\r\n",
        "median_loser34 = np.asarray(loser34_results[9:10][0])[0]\r\n",
        "upper_loser34 = np.asarray(loser34_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner34 = np.asarray(winner34_results[4:5][0])[0]\r\n",
        "median_winner34 = np.asarray(winner34_results[9:10][0])[0]\r\n",
        "upper_winner34 = np.asarray(winner34_results[14:15][0])[0]"
      ],
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGKj-NR8w_ae"
      },
      "source": [
        "# Iteration44 :\r\n",
        "\r\n",
        "slice44 = 43\r\n",
        "\r\n",
        "loser44 = [train_regret_loser_1[slice44],\r\n",
        "       train_regret_loser_2[slice44],\r\n",
        "       train_regret_loser_3[slice44],\r\n",
        "       train_regret_loser_4[slice44],\r\n",
        "       train_regret_loser_5[slice44],\r\n",
        "       train_regret_loser_6[slice44],\r\n",
        "       train_regret_loser_7[slice44],\r\n",
        "       train_regret_loser_8[slice44],\r\n",
        "       train_regret_loser_9[slice44],\r\n",
        "       train_regret_loser_10[slice44],\r\n",
        "       train_regret_loser_11[slice44],\r\n",
        "       train_regret_loser_12[slice44],\r\n",
        "       train_regret_loser_13[slice44],\r\n",
        "       train_regret_loser_14[slice44],\r\n",
        "       train_regret_loser_15[slice44],\r\n",
        "       train_regret_loser_16[slice44],\r\n",
        "       train_regret_loser_17[slice44],\r\n",
        "       train_regret_loser_18[slice44],\r\n",
        "       train_regret_loser_19[slice44],\r\n",
        "       train_regret_loser_20[slice44]]\r\n",
        "\r\n",
        "winner44 = [train_regret_winner_1[slice44],\r\n",
        "       train_regret_winner_2[slice44],\r\n",
        "       train_regret_winner_3[slice44],\r\n",
        "       train_regret_winner_4[slice44],\r\n",
        "       train_regret_winner_5[slice44],\r\n",
        "       train_regret_winner_6[slice44],\r\n",
        "       train_regret_winner_7[slice44],\r\n",
        "       train_regret_winner_8[slice44],\r\n",
        "       train_regret_winner_9[slice44],\r\n",
        "       train_regret_winner_10[slice44],\r\n",
        "       train_regret_winner_11[slice44],\r\n",
        "       train_regret_winner_12[slice44],\r\n",
        "       train_regret_winner_13[slice44],\r\n",
        "       train_regret_winner_14[slice44],\r\n",
        "       train_regret_winner_15[slice44],\r\n",
        "       train_regret_winner_16[slice44],\r\n",
        "       train_regret_winner_17[slice44],\r\n",
        "       train_regret_winner_18[slice44],\r\n",
        "       train_regret_winner_19[slice44],\r\n",
        "       train_regret_winner_20[slice44]]\r\n",
        "\r\n",
        "loser44_results = pd.DataFrame(loser44).sort_values(by=[0], ascending=False)\r\n",
        "winner44_results = pd.DataFrame(winner44).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser44 = np.asarray(loser44_results[4:5][0])[0]\r\n",
        "median_loser44 = np.asarray(loser44_results[9:10][0])[0]\r\n",
        "upper_loser44 = np.asarray(loser44_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner44 = np.asarray(winner44_results[4:5][0])[0]\r\n",
        "median_winner44 = np.asarray(winner44_results[9:10][0])[0]\r\n",
        "upper_winner44 = np.asarray(winner44_results[14:15][0])[0]"
      ],
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJzx094Fw_c_"
      },
      "source": [
        "# Iteration54 :\r\n",
        "\r\n",
        "slice54 = 53\r\n",
        "\r\n",
        "loser54 = [train_regret_loser_1[slice54],\r\n",
        "       train_regret_loser_2[slice54],\r\n",
        "       train_regret_loser_3[slice54],\r\n",
        "       train_regret_loser_4[slice54],\r\n",
        "       train_regret_loser_5[slice54],\r\n",
        "       train_regret_loser_6[slice54],\r\n",
        "       train_regret_loser_7[slice54],\r\n",
        "       train_regret_loser_8[slice54],\r\n",
        "       train_regret_loser_9[slice54],\r\n",
        "       train_regret_loser_10[slice54],\r\n",
        "       train_regret_loser_11[slice54],\r\n",
        "       train_regret_loser_12[slice54],\r\n",
        "       train_regret_loser_13[slice54],\r\n",
        "       train_regret_loser_14[slice54],\r\n",
        "       train_regret_loser_15[slice54],\r\n",
        "       train_regret_loser_16[slice54],\r\n",
        "       train_regret_loser_17[slice54],\r\n",
        "       train_regret_loser_18[slice54],\r\n",
        "       train_regret_loser_19[slice54],\r\n",
        "       train_regret_loser_20[slice54]]\r\n",
        "\r\n",
        "winner54 = [train_regret_winner_1[slice54],\r\n",
        "       train_regret_winner_2[slice54],\r\n",
        "       train_regret_winner_3[slice54],\r\n",
        "       train_regret_winner_4[slice54],\r\n",
        "       train_regret_winner_5[slice54],\r\n",
        "       train_regret_winner_6[slice54],\r\n",
        "       train_regret_winner_7[slice54],\r\n",
        "       train_regret_winner_8[slice54],\r\n",
        "       train_regret_winner_9[slice54],\r\n",
        "       train_regret_winner_10[slice54],\r\n",
        "       train_regret_winner_11[slice54],\r\n",
        "       train_regret_winner_12[slice54],\r\n",
        "       train_regret_winner_13[slice54],\r\n",
        "       train_regret_winner_14[slice54],\r\n",
        "       train_regret_winner_15[slice54],\r\n",
        "       train_regret_winner_16[slice54],\r\n",
        "       train_regret_winner_17[slice54],\r\n",
        "       train_regret_winner_18[slice54],\r\n",
        "       train_regret_winner_19[slice54],\r\n",
        "       train_regret_winner_20[slice54]]\r\n",
        "\r\n",
        "loser54_results = pd.DataFrame(loser54).sort_values(by=[0], ascending=False)\r\n",
        "winner54_results = pd.DataFrame(winner54).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser54 = np.asarray(loser54_results[4:5][0])[0]\r\n",
        "median_loser54 = np.asarray(loser54_results[9:10][0])[0]\r\n",
        "upper_loser54 = np.asarray(loser54_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner54 = np.asarray(winner54_results[4:5][0])[0]\r\n",
        "median_winner54 = np.asarray(winner54_results[9:10][0])[0]\r\n",
        "upper_winner54 = np.asarray(winner54_results[14:15][0])[0]"
      ],
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEVYMqj_w_fr"
      },
      "source": [
        "# Iteration64 :\r\n",
        "\r\n",
        "slice64 = 63\r\n",
        "\r\n",
        "loser64 = [train_regret_loser_1[slice64],\r\n",
        "       train_regret_loser_2[slice64],\r\n",
        "       train_regret_loser_3[slice64],\r\n",
        "       train_regret_loser_4[slice64],\r\n",
        "       train_regret_loser_5[slice64],\r\n",
        "       train_regret_loser_6[slice64],\r\n",
        "       train_regret_loser_7[slice64],\r\n",
        "       train_regret_loser_8[slice64],\r\n",
        "       train_regret_loser_9[slice64],\r\n",
        "       train_regret_loser_10[slice64],\r\n",
        "       train_regret_loser_11[slice64],\r\n",
        "       train_regret_loser_12[slice64],\r\n",
        "       train_regret_loser_13[slice64],\r\n",
        "       train_regret_loser_14[slice64],\r\n",
        "       train_regret_loser_15[slice64],\r\n",
        "       train_regret_loser_16[slice64],\r\n",
        "       train_regret_loser_17[slice64],\r\n",
        "       train_regret_loser_18[slice64],\r\n",
        "       train_regret_loser_19[slice64],\r\n",
        "       train_regret_loser_20[slice64]]\r\n",
        "\r\n",
        "winner64 = [train_regret_winner_1[slice64],\r\n",
        "       train_regret_winner_2[slice64],\r\n",
        "       train_regret_winner_3[slice64],\r\n",
        "       train_regret_winner_4[slice64],\r\n",
        "       train_regret_winner_5[slice64],\r\n",
        "       train_regret_winner_6[slice64],\r\n",
        "       train_regret_winner_7[slice64],\r\n",
        "       train_regret_winner_8[slice64],\r\n",
        "       train_regret_winner_9[slice64],\r\n",
        "       train_regret_winner_10[slice64],\r\n",
        "       train_regret_winner_11[slice64],\r\n",
        "       train_regret_winner_12[slice64],\r\n",
        "       train_regret_winner_13[slice64],\r\n",
        "       train_regret_winner_14[slice64],\r\n",
        "       train_regret_winner_15[slice64],\r\n",
        "       train_regret_winner_16[slice64],\r\n",
        "       train_regret_winner_17[slice64],\r\n",
        "       train_regret_winner_18[slice64],\r\n",
        "       train_regret_winner_19[slice64],\r\n",
        "       train_regret_winner_20[slice64]]\r\n",
        "\r\n",
        "loser64_results = pd.DataFrame(loser64).sort_values(by=[0], ascending=False)\r\n",
        "winner64_results = pd.DataFrame(winner64).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser64 = np.asarray(loser64_results[4:5][0])[0]\r\n",
        "median_loser64 = np.asarray(loser64_results[9:10][0])[0]\r\n",
        "upper_loser64 = np.asarray(loser64_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner64 = np.asarray(winner64_results[4:5][0])[0]\r\n",
        "median_winner64 = np.asarray(winner64_results[9:10][0])[0]\r\n",
        "upper_winner64 = np.asarray(winner64_results[14:15][0])[0]"
      ],
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvr3S_p0w_ii"
      },
      "source": [
        "# Iteration74 :\r\n",
        "\r\n",
        "slice74 = 73\r\n",
        "\r\n",
        "loser74 = [train_regret_loser_1[slice74],\r\n",
        "       train_regret_loser_2[slice74],\r\n",
        "       train_regret_loser_3[slice74],\r\n",
        "       train_regret_loser_4[slice74],\r\n",
        "       train_regret_loser_5[slice74],\r\n",
        "       train_regret_loser_6[slice74],\r\n",
        "       train_regret_loser_7[slice74],\r\n",
        "       train_regret_loser_8[slice74],\r\n",
        "       train_regret_loser_9[slice74],\r\n",
        "       train_regret_loser_10[slice74],\r\n",
        "       train_regret_loser_11[slice74],\r\n",
        "       train_regret_loser_12[slice74],\r\n",
        "       train_regret_loser_13[slice74],\r\n",
        "       train_regret_loser_14[slice74],\r\n",
        "       train_regret_loser_15[slice74],\r\n",
        "       train_regret_loser_16[slice74],\r\n",
        "       train_regret_loser_17[slice74],\r\n",
        "       train_regret_loser_18[slice74],\r\n",
        "       train_regret_loser_19[slice74],\r\n",
        "       train_regret_loser_20[slice74]]\r\n",
        "\r\n",
        "winner74 = [train_regret_winner_1[slice74],\r\n",
        "       train_regret_winner_2[slice74],\r\n",
        "       train_regret_winner_3[slice74],\r\n",
        "       train_regret_winner_4[slice74],\r\n",
        "       train_regret_winner_5[slice74],\r\n",
        "       train_regret_winner_6[slice74],\r\n",
        "       train_regret_winner_7[slice74],\r\n",
        "       train_regret_winner_8[slice74],\r\n",
        "       train_regret_winner_9[slice74],\r\n",
        "       train_regret_winner_10[slice74],\r\n",
        "       train_regret_winner_11[slice74],\r\n",
        "       train_regret_winner_12[slice74],\r\n",
        "       train_regret_winner_13[slice74],\r\n",
        "       train_regret_winner_14[slice74],\r\n",
        "       train_regret_winner_15[slice74],\r\n",
        "       train_regret_winner_16[slice74],\r\n",
        "       train_regret_winner_17[slice74],\r\n",
        "       train_regret_winner_18[slice74],\r\n",
        "       train_regret_winner_19[slice74],\r\n",
        "       train_regret_winner_20[slice74]]\r\n",
        "\r\n",
        "loser74_results = pd.DataFrame(loser74).sort_values(by=[0], ascending=False)\r\n",
        "winner74_results = pd.DataFrame(winner74).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser74 = np.asarray(loser74_results[4:5][0])[0]\r\n",
        "median_loser74 = np.asarray(loser74_results[9:10][0])[0]\r\n",
        "upper_loser74 = np.asarray(loser74_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner74 = np.asarray(winner74_results[4:5][0])[0]\r\n",
        "median_winner74 = np.asarray(winner74_results[9:10][0])[0]\r\n",
        "upper_winner74 = np.asarray(winner74_results[14:15][0])[0]"
      ],
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEAvUVK7w_lO"
      },
      "source": [
        "# Iteration84 :\r\n",
        "\r\n",
        "slice84 = 83\r\n",
        "\r\n",
        "loser84 = [train_regret_loser_1[slice84],\r\n",
        "       train_regret_loser_2[slice84],\r\n",
        "       train_regret_loser_3[slice84],\r\n",
        "       train_regret_loser_4[slice84],\r\n",
        "       train_regret_loser_5[slice84],\r\n",
        "       train_regret_loser_6[slice84],\r\n",
        "       train_regret_loser_7[slice84],\r\n",
        "       train_regret_loser_8[slice84],\r\n",
        "       train_regret_loser_9[slice84],\r\n",
        "       train_regret_loser_10[slice84],\r\n",
        "       train_regret_loser_11[slice84],\r\n",
        "       train_regret_loser_12[slice84],\r\n",
        "       train_regret_loser_13[slice84],\r\n",
        "       train_regret_loser_14[slice84],\r\n",
        "       train_regret_loser_15[slice84],\r\n",
        "       train_regret_loser_16[slice84],\r\n",
        "       train_regret_loser_17[slice84],\r\n",
        "       train_regret_loser_18[slice84],\r\n",
        "       train_regret_loser_19[slice84],\r\n",
        "       train_regret_loser_20[slice84]]\r\n",
        "\r\n",
        "winner84 = [train_regret_winner_1[slice84],\r\n",
        "       train_regret_winner_2[slice84],\r\n",
        "       train_regret_winner_3[slice84],\r\n",
        "       train_regret_winner_4[slice84],\r\n",
        "       train_regret_winner_5[slice84],\r\n",
        "       train_regret_winner_6[slice84],\r\n",
        "       train_regret_winner_7[slice84],\r\n",
        "       train_regret_winner_8[slice84],\r\n",
        "       train_regret_winner_9[slice84],\r\n",
        "       train_regret_winner_10[slice84],\r\n",
        "       train_regret_winner_11[slice84],\r\n",
        "       train_regret_winner_12[slice84],\r\n",
        "       train_regret_winner_13[slice84],\r\n",
        "       train_regret_winner_14[slice84],\r\n",
        "       train_regret_winner_15[slice84],\r\n",
        "       train_regret_winner_16[slice84],\r\n",
        "       train_regret_winner_17[slice84],\r\n",
        "       train_regret_winner_18[slice84],\r\n",
        "       train_regret_winner_19[slice84],\r\n",
        "       train_regret_winner_20[slice84]]\r\n",
        "\r\n",
        "loser84_results = pd.DataFrame(loser84).sort_values(by=[0], ascending=False)\r\n",
        "winner84_results = pd.DataFrame(winner84).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser84 = np.asarray(loser84_results[4:5][0])[0]\r\n",
        "median_loser84 = np.asarray(loser84_results[9:10][0])[0]\r\n",
        "upper_loser84 = np.asarray(loser84_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner84 = np.asarray(winner84_results[4:5][0])[0]\r\n",
        "median_winner84 = np.asarray(winner84_results[9:10][0])[0]\r\n",
        "upper_winner84 = np.asarray(winner84_results[14:15][0])[0]"
      ],
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9TMZzhdGw_oC"
      },
      "source": [
        "# Iteration94 :\r\n",
        "\r\n",
        "slice94 = 93\r\n",
        "\r\n",
        "loser94 = [train_regret_loser_1[slice94],\r\n",
        "       train_regret_loser_2[slice94],\r\n",
        "       train_regret_loser_3[slice94],\r\n",
        "       train_regret_loser_4[slice94],\r\n",
        "       train_regret_loser_5[slice94],\r\n",
        "       train_regret_loser_6[slice94],\r\n",
        "       train_regret_loser_7[slice94],\r\n",
        "       train_regret_loser_8[slice94],\r\n",
        "       train_regret_loser_9[slice94],\r\n",
        "       train_regret_loser_10[slice94],\r\n",
        "       train_regret_loser_11[slice94],\r\n",
        "       train_regret_loser_12[slice94],\r\n",
        "       train_regret_loser_13[slice94],\r\n",
        "       train_regret_loser_14[slice94],\r\n",
        "       train_regret_loser_15[slice94],\r\n",
        "       train_regret_loser_16[slice94],\r\n",
        "       train_regret_loser_17[slice94],\r\n",
        "       train_regret_loser_18[slice94],\r\n",
        "       train_regret_loser_19[slice94],\r\n",
        "       train_regret_loser_20[slice94]]\r\n",
        "\r\n",
        "winner94 = [train_regret_winner_1[slice94],\r\n",
        "       train_regret_winner_2[slice94],\r\n",
        "       train_regret_winner_3[slice94],\r\n",
        "       train_regret_winner_4[slice94],\r\n",
        "       train_regret_winner_5[slice94],\r\n",
        "       train_regret_winner_6[slice94],\r\n",
        "       train_regret_winner_7[slice94],\r\n",
        "       train_regret_winner_8[slice94],\r\n",
        "       train_regret_winner_9[slice94],\r\n",
        "       train_regret_winner_10[slice94],\r\n",
        "       train_regret_winner_11[slice94],\r\n",
        "       train_regret_winner_12[slice94],\r\n",
        "       train_regret_winner_13[slice94],\r\n",
        "       train_regret_winner_14[slice94],\r\n",
        "       train_regret_winner_15[slice94],\r\n",
        "       train_regret_winner_16[slice94],\r\n",
        "       train_regret_winner_17[slice94],\r\n",
        "       train_regret_winner_18[slice94],\r\n",
        "       train_regret_winner_19[slice94],\r\n",
        "       train_regret_winner_20[slice94]]\r\n",
        "\r\n",
        "loser94_results = pd.DataFrame(loser94).sort_values(by=[0], ascending=False)\r\n",
        "winner94_results = pd.DataFrame(winner94).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser94 = np.asarray(loser94_results[4:5][0])[0]\r\n",
        "median_loser94 = np.asarray(loser94_results[9:10][0])[0]\r\n",
        "upper_loser94 = np.asarray(loser94_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner94 = np.asarray(winner94_results[4:5][0])[0]\r\n",
        "median_winner94 = np.asarray(winner94_results[9:10][0])[0]\r\n",
        "upper_winner94 = np.asarray(winner94_results[14:15][0])[0]"
      ],
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMxHQOgaw_qu"
      },
      "source": [
        "# Iteration5 :\r\n",
        "\r\n",
        "slice5 = 4\r\n",
        "\r\n",
        "loser5 = [train_regret_loser_1[slice5],\r\n",
        "       train_regret_loser_2[slice5],\r\n",
        "       train_regret_loser_3[slice5],\r\n",
        "       train_regret_loser_4[slice5],\r\n",
        "       train_regret_loser_5[slice5],\r\n",
        "       train_regret_loser_6[slice5],\r\n",
        "       train_regret_loser_7[slice5],\r\n",
        "       train_regret_loser_8[slice5],\r\n",
        "       train_regret_loser_9[slice5],\r\n",
        "       train_regret_loser_10[slice5],\r\n",
        "       train_regret_loser_11[slice5],\r\n",
        "       train_regret_loser_12[slice5],\r\n",
        "       train_regret_loser_13[slice5],\r\n",
        "       train_regret_loser_14[slice5],\r\n",
        "       train_regret_loser_15[slice5],\r\n",
        "       train_regret_loser_16[slice5],\r\n",
        "       train_regret_loser_17[slice5],\r\n",
        "       train_regret_loser_18[slice5],\r\n",
        "       train_regret_loser_19[slice5],\r\n",
        "       train_regret_loser_20[slice5]]\r\n",
        "\r\n",
        "winner5 = [train_regret_winner_1[slice5],\r\n",
        "       train_regret_winner_2[slice5],\r\n",
        "       train_regret_winner_3[slice5],\r\n",
        "       train_regret_winner_4[slice5],\r\n",
        "       train_regret_winner_5[slice5],\r\n",
        "       train_regret_winner_6[slice5],\r\n",
        "       train_regret_winner_7[slice5],\r\n",
        "       train_regret_winner_8[slice5],\r\n",
        "       train_regret_winner_9[slice5],\r\n",
        "       train_regret_winner_10[slice5],\r\n",
        "       train_regret_winner_11[slice5],\r\n",
        "       train_regret_winner_12[slice5],\r\n",
        "       train_regret_winner_13[slice5],\r\n",
        "       train_regret_winner_14[slice5],\r\n",
        "       train_regret_winner_15[slice5],\r\n",
        "       train_regret_winner_16[slice5],\r\n",
        "       train_regret_winner_17[slice5],\r\n",
        "       train_regret_winner_18[slice5],\r\n",
        "       train_regret_winner_19[slice5],\r\n",
        "       train_regret_winner_20[slice5]]\r\n",
        "\r\n",
        "loser5_results = pd.DataFrame(loser5).sort_values(by=[0], ascending=False)\r\n",
        "winner5_results = pd.DataFrame(winner5).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser5 = np.asarray(loser5_results[4:5][0])[0]\r\n",
        "median_loser5 = np.asarray(loser5_results[9:10][0])[0]\r\n",
        "upper_loser5 = np.asarray(loser5_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner5 = np.asarray(winner5_results[4:5][0])[0]\r\n",
        "median_winner5 = np.asarray(winner5_results[9:10][0])[0]\r\n",
        "upper_winner5 = np.asarray(winner5_results[14:15][0])[0]"
      ],
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPVFQTfNw_wF"
      },
      "source": [
        "# Iteration15 :\r\n",
        "\r\n",
        "slice15 = 14\r\n",
        "\r\n",
        "loser15 = [train_regret_loser_1[slice15],\r\n",
        "       train_regret_loser_2[slice15],\r\n",
        "       train_regret_loser_3[slice15],\r\n",
        "       train_regret_loser_4[slice15],\r\n",
        "       train_regret_loser_5[slice15],\r\n",
        "       train_regret_loser_6[slice15],\r\n",
        "       train_regret_loser_7[slice15],\r\n",
        "       train_regret_loser_8[slice15],\r\n",
        "       train_regret_loser_9[slice15],\r\n",
        "       train_regret_loser_10[slice15],\r\n",
        "       train_regret_loser_11[slice15],\r\n",
        "       train_regret_loser_12[slice15],\r\n",
        "       train_regret_loser_13[slice15],\r\n",
        "       train_regret_loser_14[slice15],\r\n",
        "       train_regret_loser_15[slice15],\r\n",
        "       train_regret_loser_16[slice15],\r\n",
        "       train_regret_loser_17[slice15],\r\n",
        "       train_regret_loser_18[slice15],\r\n",
        "       train_regret_loser_19[slice15],\r\n",
        "       train_regret_loser_20[slice15]]\r\n",
        "\r\n",
        "winner15 = [train_regret_winner_1[slice15],\r\n",
        "       train_regret_winner_2[slice15],\r\n",
        "       train_regret_winner_3[slice15],\r\n",
        "       train_regret_winner_4[slice15],\r\n",
        "       train_regret_winner_5[slice15],\r\n",
        "       train_regret_winner_6[slice15],\r\n",
        "       train_regret_winner_7[slice15],\r\n",
        "       train_regret_winner_8[slice15],\r\n",
        "       train_regret_winner_9[slice15],\r\n",
        "       train_regret_winner_10[slice15],\r\n",
        "       train_regret_winner_11[slice15],\r\n",
        "       train_regret_winner_12[slice15],\r\n",
        "       train_regret_winner_13[slice15],\r\n",
        "       train_regret_winner_14[slice15],\r\n",
        "       train_regret_winner_15[slice15],\r\n",
        "       train_regret_winner_16[slice15],\r\n",
        "       train_regret_winner_17[slice15],\r\n",
        "       train_regret_winner_18[slice15],\r\n",
        "       train_regret_winner_19[slice15],\r\n",
        "       train_regret_winner_20[slice15]]\r\n",
        "\r\n",
        "loser15_results = pd.DataFrame(loser15).sort_values(by=[0], ascending=False)\r\n",
        "winner15_results = pd.DataFrame(winner15).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser15 = np.asarray(loser15_results[4:5][0])[0]\r\n",
        "median_loser15 = np.asarray(loser15_results[9:10][0])[0]\r\n",
        "upper_loser15 = np.asarray(loser15_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner15 = np.asarray(winner15_results[4:5][0])[0]\r\n",
        "median_winner15 = np.asarray(winner15_results[9:10][0])[0]\r\n",
        "upper_winner15 = np.asarray(winner15_results[14:15][0])[0]"
      ],
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADBmBA3ew_zK"
      },
      "source": [
        "# Iteration25 :\r\n",
        "\r\n",
        "slice25 = 24\r\n",
        "\r\n",
        "loser25 = [train_regret_loser_1[slice25],\r\n",
        "       train_regret_loser_2[slice25],\r\n",
        "       train_regret_loser_3[slice25],\r\n",
        "       train_regret_loser_4[slice25],\r\n",
        "       train_regret_loser_5[slice25],\r\n",
        "       train_regret_loser_6[slice25],\r\n",
        "       train_regret_loser_7[slice25],\r\n",
        "       train_regret_loser_8[slice25],\r\n",
        "       train_regret_loser_9[slice25],\r\n",
        "       train_regret_loser_10[slice25],\r\n",
        "       train_regret_loser_11[slice25],\r\n",
        "       train_regret_loser_12[slice25],\r\n",
        "       train_regret_loser_13[slice25],\r\n",
        "       train_regret_loser_14[slice25],\r\n",
        "       train_regret_loser_15[slice25],\r\n",
        "       train_regret_loser_16[slice25],\r\n",
        "       train_regret_loser_17[slice25],\r\n",
        "       train_regret_loser_18[slice25],\r\n",
        "       train_regret_loser_19[slice25],\r\n",
        "       train_regret_loser_20[slice25]]\r\n",
        "\r\n",
        "winner25 = [train_regret_winner_1[slice25],\r\n",
        "       train_regret_winner_2[slice25],\r\n",
        "       train_regret_winner_3[slice25],\r\n",
        "       train_regret_winner_4[slice25],\r\n",
        "       train_regret_winner_5[slice25],\r\n",
        "       train_regret_winner_6[slice25],\r\n",
        "       train_regret_winner_7[slice25],\r\n",
        "       train_regret_winner_8[slice25],\r\n",
        "       train_regret_winner_9[slice25],\r\n",
        "       train_regret_winner_10[slice25],\r\n",
        "       train_regret_winner_11[slice25],\r\n",
        "       train_regret_winner_12[slice25],\r\n",
        "       train_regret_winner_13[slice25],\r\n",
        "       train_regret_winner_14[slice25],\r\n",
        "       train_regret_winner_15[slice25],\r\n",
        "       train_regret_winner_16[slice25],\r\n",
        "       train_regret_winner_17[slice25],\r\n",
        "       train_regret_winner_18[slice25],\r\n",
        "       train_regret_winner_19[slice25],\r\n",
        "       train_regret_winner_20[slice25]]\r\n",
        "\r\n",
        "loser25_results = pd.DataFrame(loser25).sort_values(by=[0], ascending=False)\r\n",
        "winner25_results = pd.DataFrame(winner25).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser25 = np.asarray(loser25_results[4:5][0])[0]\r\n",
        "median_loser25 = np.asarray(loser25_results[9:10][0])[0]\r\n",
        "upper_loser25 = np.asarray(loser25_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner25 = np.asarray(winner25_results[4:5][0])[0]\r\n",
        "median_winner25 = np.asarray(winner25_results[9:10][0])[0]\r\n",
        "upper_winner25 = np.asarray(winner25_results[14:15][0])[0]"
      ],
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUk9UtSTw_2N"
      },
      "source": [
        "# Iteration35 :\r\n",
        "\r\n",
        "slice35 = 34\r\n",
        "\r\n",
        "loser35 = [train_regret_loser_1[slice35],\r\n",
        "       train_regret_loser_2[slice35],\r\n",
        "       train_regret_loser_3[slice35],\r\n",
        "       train_regret_loser_4[slice35],\r\n",
        "       train_regret_loser_5[slice35],\r\n",
        "       train_regret_loser_6[slice35],\r\n",
        "       train_regret_loser_7[slice35],\r\n",
        "       train_regret_loser_8[slice35],\r\n",
        "       train_regret_loser_9[slice35],\r\n",
        "       train_regret_loser_10[slice35],\r\n",
        "       train_regret_loser_11[slice35],\r\n",
        "       train_regret_loser_12[slice35],\r\n",
        "       train_regret_loser_13[slice35],\r\n",
        "       train_regret_loser_14[slice35],\r\n",
        "       train_regret_loser_15[slice35],\r\n",
        "       train_regret_loser_16[slice35],\r\n",
        "       train_regret_loser_17[slice35],\r\n",
        "       train_regret_loser_18[slice35],\r\n",
        "       train_regret_loser_19[slice35],\r\n",
        "       train_regret_loser_20[slice35]]\r\n",
        "\r\n",
        "winner35 = [train_regret_winner_1[slice35],\r\n",
        "       train_regret_winner_2[slice35],\r\n",
        "       train_regret_winner_3[slice35],\r\n",
        "       train_regret_winner_4[slice35],\r\n",
        "       train_regret_winner_5[slice35],\r\n",
        "       train_regret_winner_6[slice35],\r\n",
        "       train_regret_winner_7[slice35],\r\n",
        "       train_regret_winner_8[slice35],\r\n",
        "       train_regret_winner_9[slice35],\r\n",
        "       train_regret_winner_10[slice35],\r\n",
        "       train_regret_winner_11[slice35],\r\n",
        "       train_regret_winner_12[slice35],\r\n",
        "       train_regret_winner_13[slice35],\r\n",
        "       train_regret_winner_14[slice35],\r\n",
        "       train_regret_winner_15[slice35],\r\n",
        "       train_regret_winner_16[slice35],\r\n",
        "       train_regret_winner_17[slice35],\r\n",
        "       train_regret_winner_18[slice35],\r\n",
        "       train_regret_winner_19[slice35],\r\n",
        "       train_regret_winner_20[slice35]]\r\n",
        "\r\n",
        "loser35_results = pd.DataFrame(loser35).sort_values(by=[0], ascending=False)\r\n",
        "winner35_results = pd.DataFrame(winner35).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser35 = np.asarray(loser35_results[4:5][0])[0]\r\n",
        "median_loser35 = np.asarray(loser35_results[9:10][0])[0]\r\n",
        "upper_loser35 = np.asarray(loser35_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner35 = np.asarray(winner35_results[4:5][0])[0]\r\n",
        "median_winner35 = np.asarray(winner35_results[9:10][0])[0]\r\n",
        "upper_winner35 = np.asarray(winner35_results[14:15][0])[0]"
      ],
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xX3VgqD_w_4z"
      },
      "source": [
        "# Iteration45 :\r\n",
        "\r\n",
        "slice45 = 44\r\n",
        "\r\n",
        "loser45 = [train_regret_loser_1[slice45],\r\n",
        "       train_regret_loser_2[slice45],\r\n",
        "       train_regret_loser_3[slice45],\r\n",
        "       train_regret_loser_4[slice45],\r\n",
        "       train_regret_loser_5[slice45],\r\n",
        "       train_regret_loser_6[slice45],\r\n",
        "       train_regret_loser_7[slice45],\r\n",
        "       train_regret_loser_8[slice45],\r\n",
        "       train_regret_loser_9[slice45],\r\n",
        "       train_regret_loser_10[slice45],\r\n",
        "       train_regret_loser_11[slice45],\r\n",
        "       train_regret_loser_12[slice45],\r\n",
        "       train_regret_loser_13[slice45],\r\n",
        "       train_regret_loser_14[slice45],\r\n",
        "       train_regret_loser_15[slice45],\r\n",
        "       train_regret_loser_16[slice45],\r\n",
        "       train_regret_loser_17[slice45],\r\n",
        "       train_regret_loser_18[slice45],\r\n",
        "       train_regret_loser_19[slice45],\r\n",
        "       train_regret_loser_20[slice45]]\r\n",
        "\r\n",
        "winner45 = [train_regret_winner_1[slice45],\r\n",
        "       train_regret_winner_2[slice45],\r\n",
        "       train_regret_winner_3[slice45],\r\n",
        "       train_regret_winner_4[slice45],\r\n",
        "       train_regret_winner_5[slice45],\r\n",
        "       train_regret_winner_6[slice45],\r\n",
        "       train_regret_winner_7[slice45],\r\n",
        "       train_regret_winner_8[slice45],\r\n",
        "       train_regret_winner_9[slice45],\r\n",
        "       train_regret_winner_10[slice45],\r\n",
        "       train_regret_winner_11[slice45],\r\n",
        "       train_regret_winner_12[slice45],\r\n",
        "       train_regret_winner_13[slice45],\r\n",
        "       train_regret_winner_14[slice45],\r\n",
        "       train_regret_winner_15[slice45],\r\n",
        "       train_regret_winner_16[slice45],\r\n",
        "       train_regret_winner_17[slice45],\r\n",
        "       train_regret_winner_18[slice45],\r\n",
        "       train_regret_winner_19[slice45],\r\n",
        "       train_regret_winner_20[slice45]]\r\n",
        "\r\n",
        "loser45_results = pd.DataFrame(loser45).sort_values(by=[0], ascending=False)\r\n",
        "winner45_results = pd.DataFrame(winner45).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser45 = np.asarray(loser45_results[4:5][0])[0]\r\n",
        "median_loser45 = np.asarray(loser45_results[9:10][0])[0]\r\n",
        "upper_loser45 = np.asarray(loser45_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner45 = np.asarray(winner45_results[4:5][0])[0]\r\n",
        "median_winner45 = np.asarray(winner45_results[9:10][0])[0]\r\n",
        "upper_winner45 = np.asarray(winner45_results[14:15][0])[0]"
      ],
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o35W-AOzw_8F"
      },
      "source": [
        "# Iteration55 :\r\n",
        "\r\n",
        "slice55 = 54\r\n",
        "\r\n",
        "loser55 = [train_regret_loser_1[slice55],\r\n",
        "       train_regret_loser_2[slice55],\r\n",
        "       train_regret_loser_3[slice55],\r\n",
        "       train_regret_loser_4[slice55],\r\n",
        "       train_regret_loser_5[slice55],\r\n",
        "       train_regret_loser_6[slice55],\r\n",
        "       train_regret_loser_7[slice55],\r\n",
        "       train_regret_loser_8[slice55],\r\n",
        "       train_regret_loser_9[slice55],\r\n",
        "       train_regret_loser_10[slice55],\r\n",
        "       train_regret_loser_11[slice55],\r\n",
        "       train_regret_loser_12[slice55],\r\n",
        "       train_regret_loser_13[slice55],\r\n",
        "       train_regret_loser_14[slice55],\r\n",
        "       train_regret_loser_15[slice55],\r\n",
        "       train_regret_loser_16[slice55],\r\n",
        "       train_regret_loser_17[slice55],\r\n",
        "       train_regret_loser_18[slice55],\r\n",
        "       train_regret_loser_19[slice55],\r\n",
        "       train_regret_loser_20[slice55]]\r\n",
        "\r\n",
        "winner55 = [train_regret_winner_1[slice55],\r\n",
        "       train_regret_winner_2[slice55],\r\n",
        "       train_regret_winner_3[slice55],\r\n",
        "       train_regret_winner_4[slice55],\r\n",
        "       train_regret_winner_5[slice55],\r\n",
        "       train_regret_winner_6[slice55],\r\n",
        "       train_regret_winner_7[slice55],\r\n",
        "       train_regret_winner_8[slice55],\r\n",
        "       train_regret_winner_9[slice55],\r\n",
        "       train_regret_winner_10[slice55],\r\n",
        "       train_regret_winner_11[slice55],\r\n",
        "       train_regret_winner_12[slice55],\r\n",
        "       train_regret_winner_13[slice55],\r\n",
        "       train_regret_winner_14[slice55],\r\n",
        "       train_regret_winner_15[slice55],\r\n",
        "       train_regret_winner_16[slice55],\r\n",
        "       train_regret_winner_17[slice55],\r\n",
        "       train_regret_winner_18[slice55],\r\n",
        "       train_regret_winner_19[slice55],\r\n",
        "       train_regret_winner_20[slice55]]\r\n",
        "\r\n",
        "loser55_results = pd.DataFrame(loser55).sort_values(by=[0], ascending=False)\r\n",
        "winner55_results = pd.DataFrame(winner55).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser55 = np.asarray(loser55_results[4:5][0])[0]\r\n",
        "median_loser55 = np.asarray(loser55_results[9:10][0])[0]\r\n",
        "upper_loser55 = np.asarray(loser55_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner55 = np.asarray(winner55_results[4:5][0])[0]\r\n",
        "median_winner55 = np.asarray(winner55_results[9:10][0])[0]\r\n",
        "upper_winner55 = np.asarray(winner55_results[14:15][0])[0]"
      ],
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUAjKXWdw__J"
      },
      "source": [
        "# Iteration65 :\r\n",
        "\r\n",
        "slice65 = 64\r\n",
        "\r\n",
        "loser65 = [train_regret_loser_1[slice65],\r\n",
        "       train_regret_loser_2[slice65],\r\n",
        "       train_regret_loser_3[slice65],\r\n",
        "       train_regret_loser_4[slice65],\r\n",
        "       train_regret_loser_5[slice65],\r\n",
        "       train_regret_loser_6[slice65],\r\n",
        "       train_regret_loser_7[slice65],\r\n",
        "       train_regret_loser_8[slice65],\r\n",
        "       train_regret_loser_9[slice65],\r\n",
        "       train_regret_loser_10[slice65],\r\n",
        "       train_regret_loser_11[slice65],\r\n",
        "       train_regret_loser_12[slice65],\r\n",
        "       train_regret_loser_13[slice65],\r\n",
        "       train_regret_loser_14[slice65],\r\n",
        "       train_regret_loser_15[slice65],\r\n",
        "       train_regret_loser_16[slice65],\r\n",
        "       train_regret_loser_17[slice65],\r\n",
        "       train_regret_loser_18[slice65],\r\n",
        "       train_regret_loser_19[slice65],\r\n",
        "       train_regret_loser_20[slice65]]\r\n",
        "\r\n",
        "winner65 = [train_regret_winner_1[slice65],\r\n",
        "       train_regret_winner_2[slice65],\r\n",
        "       train_regret_winner_3[slice65],\r\n",
        "       train_regret_winner_4[slice65],\r\n",
        "       train_regret_winner_5[slice65],\r\n",
        "       train_regret_winner_6[slice65],\r\n",
        "       train_regret_winner_7[slice65],\r\n",
        "       train_regret_winner_8[slice65],\r\n",
        "       train_regret_winner_9[slice65],\r\n",
        "       train_regret_winner_10[slice65],\r\n",
        "       train_regret_winner_11[slice65],\r\n",
        "       train_regret_winner_12[slice65],\r\n",
        "       train_regret_winner_13[slice65],\r\n",
        "       train_regret_winner_14[slice65],\r\n",
        "       train_regret_winner_15[slice65],\r\n",
        "       train_regret_winner_16[slice65],\r\n",
        "       train_regret_winner_17[slice65],\r\n",
        "       train_regret_winner_18[slice65],\r\n",
        "       train_regret_winner_19[slice65],\r\n",
        "       train_regret_winner_20[slice65]]\r\n",
        "\r\n",
        "loser65_results = pd.DataFrame(loser65).sort_values(by=[0], ascending=False)\r\n",
        "winner65_results = pd.DataFrame(winner65).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser65 = np.asarray(loser65_results[4:5][0])[0]\r\n",
        "median_loser65 = np.asarray(loser65_results[9:10][0])[0]\r\n",
        "upper_loser65 = np.asarray(loser65_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner65 = np.asarray(winner65_results[4:5][0])[0]\r\n",
        "median_winner65 = np.asarray(winner65_results[9:10][0])[0]\r\n",
        "upper_winner65 = np.asarray(winner65_results[14:15][0])[0]"
      ],
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D00FcmZwxAB9"
      },
      "source": [
        "# Iteration75 :\r\n",
        "\r\n",
        "slice75 = 74\r\n",
        "\r\n",
        "loser75 = [train_regret_loser_1[slice75],\r\n",
        "       train_regret_loser_2[slice75],\r\n",
        "       train_regret_loser_3[slice75],\r\n",
        "       train_regret_loser_4[slice75],\r\n",
        "       train_regret_loser_5[slice75],\r\n",
        "       train_regret_loser_6[slice75],\r\n",
        "       train_regret_loser_7[slice75],\r\n",
        "       train_regret_loser_8[slice75],\r\n",
        "       train_regret_loser_9[slice75],\r\n",
        "       train_regret_loser_10[slice75],\r\n",
        "       train_regret_loser_11[slice75],\r\n",
        "       train_regret_loser_12[slice75],\r\n",
        "       train_regret_loser_13[slice75],\r\n",
        "       train_regret_loser_14[slice75],\r\n",
        "       train_regret_loser_15[slice75],\r\n",
        "       train_regret_loser_16[slice75],\r\n",
        "       train_regret_loser_17[slice75],\r\n",
        "       train_regret_loser_18[slice75],\r\n",
        "       train_regret_loser_19[slice75],\r\n",
        "       train_regret_loser_20[slice75]]\r\n",
        "\r\n",
        "winner75 = [train_regret_winner_1[slice75],\r\n",
        "       train_regret_winner_2[slice75],\r\n",
        "       train_regret_winner_3[slice75],\r\n",
        "       train_regret_winner_4[slice75],\r\n",
        "       train_regret_winner_5[slice75],\r\n",
        "       train_regret_winner_6[slice75],\r\n",
        "       train_regret_winner_7[slice75],\r\n",
        "       train_regret_winner_8[slice75],\r\n",
        "       train_regret_winner_9[slice75],\r\n",
        "       train_regret_winner_10[slice75],\r\n",
        "       train_regret_winner_11[slice75],\r\n",
        "       train_regret_winner_12[slice75],\r\n",
        "       train_regret_winner_13[slice75],\r\n",
        "       train_regret_winner_14[slice75],\r\n",
        "       train_regret_winner_15[slice75],\r\n",
        "       train_regret_winner_16[slice75],\r\n",
        "       train_regret_winner_17[slice75],\r\n",
        "       train_regret_winner_18[slice75],\r\n",
        "       train_regret_winner_19[slice75],\r\n",
        "       train_regret_winner_20[slice75]]\r\n",
        "\r\n",
        "loser75_results = pd.DataFrame(loser75).sort_values(by=[0], ascending=False)\r\n",
        "winner75_results = pd.DataFrame(winner75).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser75 = np.asarray(loser75_results[4:5][0])[0]\r\n",
        "median_loser75 = np.asarray(loser75_results[9:10][0])[0]\r\n",
        "upper_loser75 = np.asarray(loser75_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner75 = np.asarray(winner75_results[4:5][0])[0]\r\n",
        "median_winner75 = np.asarray(winner75_results[9:10][0])[0]\r\n",
        "upper_winner75 = np.asarray(winner75_results[14:15][0])[0]"
      ],
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvnzmprxxAFB"
      },
      "source": [
        "# Iteration85 :\r\n",
        "\r\n",
        "slice85 = 84\r\n",
        "\r\n",
        "loser85 = [train_regret_loser_1[slice85],\r\n",
        "       train_regret_loser_2[slice85],\r\n",
        "       train_regret_loser_3[slice85],\r\n",
        "       train_regret_loser_4[slice85],\r\n",
        "       train_regret_loser_5[slice85],\r\n",
        "       train_regret_loser_6[slice85],\r\n",
        "       train_regret_loser_7[slice85],\r\n",
        "       train_regret_loser_8[slice85],\r\n",
        "       train_regret_loser_9[slice85],\r\n",
        "       train_regret_loser_10[slice85],\r\n",
        "       train_regret_loser_11[slice85],\r\n",
        "       train_regret_loser_12[slice85],\r\n",
        "       train_regret_loser_13[slice85],\r\n",
        "       train_regret_loser_14[slice85],\r\n",
        "       train_regret_loser_15[slice85],\r\n",
        "       train_regret_loser_16[slice85],\r\n",
        "       train_regret_loser_17[slice85],\r\n",
        "       train_regret_loser_18[slice85],\r\n",
        "       train_regret_loser_19[slice85],\r\n",
        "       train_regret_loser_20[slice85]]\r\n",
        "\r\n",
        "winner85 = [train_regret_winner_1[slice85],\r\n",
        "       train_regret_winner_2[slice85],\r\n",
        "       train_regret_winner_3[slice85],\r\n",
        "       train_regret_winner_4[slice85],\r\n",
        "       train_regret_winner_5[slice85],\r\n",
        "       train_regret_winner_6[slice85],\r\n",
        "       train_regret_winner_7[slice85],\r\n",
        "       train_regret_winner_8[slice85],\r\n",
        "       train_regret_winner_9[slice85],\r\n",
        "       train_regret_winner_10[slice85],\r\n",
        "       train_regret_winner_11[slice85],\r\n",
        "       train_regret_winner_12[slice85],\r\n",
        "       train_regret_winner_13[slice85],\r\n",
        "       train_regret_winner_14[slice85],\r\n",
        "       train_regret_winner_15[slice85],\r\n",
        "       train_regret_winner_16[slice85],\r\n",
        "       train_regret_winner_17[slice85],\r\n",
        "       train_regret_winner_18[slice85],\r\n",
        "       train_regret_winner_19[slice85],\r\n",
        "       train_regret_winner_20[slice85]]\r\n",
        "\r\n",
        "loser85_results = pd.DataFrame(loser85).sort_values(by=[0], ascending=False)\r\n",
        "winner85_results = pd.DataFrame(winner85).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser85 = np.asarray(loser85_results[4:5][0])[0]\r\n",
        "median_loser85 = np.asarray(loser85_results[9:10][0])[0]\r\n",
        "upper_loser85 = np.asarray(loser85_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner85 = np.asarray(winner85_results[4:5][0])[0]\r\n",
        "median_winner85 = np.asarray(winner85_results[9:10][0])[0]\r\n",
        "upper_winner85 = np.asarray(winner85_results[14:15][0])[0]"
      ],
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztvJhrN6xAH1"
      },
      "source": [
        "# Iteration95 :\r\n",
        "\r\n",
        "slice95 = 94\r\n",
        "\r\n",
        "loser95 = [train_regret_loser_1[slice95],\r\n",
        "       train_regret_loser_2[slice95],\r\n",
        "       train_regret_loser_3[slice95],\r\n",
        "       train_regret_loser_4[slice95],\r\n",
        "       train_regret_loser_5[slice95],\r\n",
        "       train_regret_loser_6[slice95],\r\n",
        "       train_regret_loser_7[slice95],\r\n",
        "       train_regret_loser_8[slice95],\r\n",
        "       train_regret_loser_9[slice95],\r\n",
        "       train_regret_loser_10[slice95],\r\n",
        "       train_regret_loser_11[slice95],\r\n",
        "       train_regret_loser_12[slice95],\r\n",
        "       train_regret_loser_13[slice95],\r\n",
        "       train_regret_loser_14[slice95],\r\n",
        "       train_regret_loser_15[slice95],\r\n",
        "       train_regret_loser_16[slice95],\r\n",
        "       train_regret_loser_17[slice95],\r\n",
        "       train_regret_loser_18[slice95],\r\n",
        "       train_regret_loser_19[slice95],\r\n",
        "       train_regret_loser_20[slice95]]\r\n",
        "\r\n",
        "winner95 = [train_regret_winner_1[slice95],\r\n",
        "       train_regret_winner_2[slice95],\r\n",
        "       train_regret_winner_3[slice95],\r\n",
        "       train_regret_winner_4[slice95],\r\n",
        "       train_regret_winner_5[slice95],\r\n",
        "       train_regret_winner_6[slice95],\r\n",
        "       train_regret_winner_7[slice95],\r\n",
        "       train_regret_winner_8[slice95],\r\n",
        "       train_regret_winner_9[slice95],\r\n",
        "       train_regret_winner_10[slice95],\r\n",
        "       train_regret_winner_11[slice95],\r\n",
        "       train_regret_winner_12[slice95],\r\n",
        "       train_regret_winner_13[slice95],\r\n",
        "       train_regret_winner_14[slice95],\r\n",
        "       train_regret_winner_15[slice95],\r\n",
        "       train_regret_winner_16[slice95],\r\n",
        "       train_regret_winner_17[slice95],\r\n",
        "       train_regret_winner_18[slice95],\r\n",
        "       train_regret_winner_19[slice95],\r\n",
        "       train_regret_winner_20[slice95]]\r\n",
        "\r\n",
        "loser95_results = pd.DataFrame(loser95).sort_values(by=[0], ascending=False)\r\n",
        "winner95_results = pd.DataFrame(winner95).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser95 = np.asarray(loser95_results[4:5][0])[0]\r\n",
        "median_loser95 = np.asarray(loser95_results[9:10][0])[0]\r\n",
        "upper_loser95 = np.asarray(loser95_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner95 = np.asarray(winner95_results[4:5][0])[0]\r\n",
        "median_winner95 = np.asarray(winner95_results[9:10][0])[0]\r\n",
        "upper_winner95 = np.asarray(winner95_results[14:15][0])[0]"
      ],
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SK-4nr-OxALA"
      },
      "source": [
        "# Iteration6 :\r\n",
        "\r\n",
        "slice6 = 5\r\n",
        "\r\n",
        "loser6 = [train_regret_loser_1[slice6],\r\n",
        "       train_regret_loser_2[slice6],\r\n",
        "       train_regret_loser_3[slice6],\r\n",
        "       train_regret_loser_4[slice6],\r\n",
        "       train_regret_loser_5[slice6],\r\n",
        "       train_regret_loser_6[slice6],\r\n",
        "       train_regret_loser_7[slice6],\r\n",
        "       train_regret_loser_8[slice6],\r\n",
        "       train_regret_loser_9[slice6],\r\n",
        "       train_regret_loser_10[slice6],\r\n",
        "       train_regret_loser_11[slice6],\r\n",
        "       train_regret_loser_12[slice6],\r\n",
        "       train_regret_loser_13[slice6],\r\n",
        "       train_regret_loser_14[slice6],\r\n",
        "       train_regret_loser_15[slice6],\r\n",
        "       train_regret_loser_16[slice6],\r\n",
        "       train_regret_loser_17[slice6],\r\n",
        "       train_regret_loser_18[slice6],\r\n",
        "       train_regret_loser_19[slice6],\r\n",
        "       train_regret_loser_20[slice6]]\r\n",
        "\r\n",
        "winner6 = [train_regret_winner_1[slice6],\r\n",
        "       train_regret_winner_2[slice6],\r\n",
        "       train_regret_winner_3[slice6],\r\n",
        "       train_regret_winner_4[slice6],\r\n",
        "       train_regret_winner_5[slice6],\r\n",
        "       train_regret_winner_6[slice6],\r\n",
        "       train_regret_winner_7[slice6],\r\n",
        "       train_regret_winner_8[slice6],\r\n",
        "       train_regret_winner_9[slice6],\r\n",
        "       train_regret_winner_10[slice6],\r\n",
        "       train_regret_winner_11[slice6],\r\n",
        "       train_regret_winner_12[slice6],\r\n",
        "       train_regret_winner_13[slice6],\r\n",
        "       train_regret_winner_14[slice6],\r\n",
        "       train_regret_winner_15[slice6],\r\n",
        "       train_regret_winner_16[slice6],\r\n",
        "       train_regret_winner_17[slice6],\r\n",
        "       train_regret_winner_18[slice6],\r\n",
        "       train_regret_winner_19[slice6],\r\n",
        "       train_regret_winner_20[slice6]]\r\n",
        "\r\n",
        "loser6_results = pd.DataFrame(loser6).sort_values(by=[0], ascending=False)\r\n",
        "winner6_results = pd.DataFrame(winner6).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser6 = np.asarray(loser6_results[4:5][0])[0]\r\n",
        "median_loser6 = np.asarray(loser6_results[9:10][0])[0]\r\n",
        "upper_loser6 = np.asarray(loser6_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner6 = np.asarray(winner6_results[4:5][0])[0]\r\n",
        "median_winner6 = np.asarray(winner6_results[9:10][0])[0]\r\n",
        "upper_winner6 = np.asarray(winner6_results[14:15][0])[0]"
      ],
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSwfNX5ExANt"
      },
      "source": [
        "# Iteration16 :\r\n",
        "\r\n",
        "slice16 = 15\r\n",
        "\r\n",
        "loser16 = [train_regret_loser_1[slice16],\r\n",
        "       train_regret_loser_2[slice16],\r\n",
        "       train_regret_loser_3[slice16],\r\n",
        "       train_regret_loser_4[slice16],\r\n",
        "       train_regret_loser_5[slice16],\r\n",
        "       train_regret_loser_6[slice16],\r\n",
        "       train_regret_loser_7[slice16],\r\n",
        "       train_regret_loser_8[slice16],\r\n",
        "       train_regret_loser_9[slice16],\r\n",
        "       train_regret_loser_10[slice16],\r\n",
        "       train_regret_loser_11[slice16],\r\n",
        "       train_regret_loser_12[slice16],\r\n",
        "       train_regret_loser_13[slice16],\r\n",
        "       train_regret_loser_14[slice16],\r\n",
        "       train_regret_loser_15[slice16],\r\n",
        "       train_regret_loser_16[slice16],\r\n",
        "       train_regret_loser_17[slice16],\r\n",
        "       train_regret_loser_18[slice16],\r\n",
        "       train_regret_loser_19[slice16],\r\n",
        "       train_regret_loser_20[slice16]]\r\n",
        "\r\n",
        "winner16 = [train_regret_winner_1[slice16],\r\n",
        "       train_regret_winner_2[slice16],\r\n",
        "       train_regret_winner_3[slice16],\r\n",
        "       train_regret_winner_4[slice16],\r\n",
        "       train_regret_winner_5[slice16],\r\n",
        "       train_regret_winner_6[slice16],\r\n",
        "       train_regret_winner_7[slice16],\r\n",
        "       train_regret_winner_8[slice16],\r\n",
        "       train_regret_winner_9[slice16],\r\n",
        "       train_regret_winner_10[slice16],\r\n",
        "       train_regret_winner_11[slice16],\r\n",
        "       train_regret_winner_12[slice16],\r\n",
        "       train_regret_winner_13[slice16],\r\n",
        "       train_regret_winner_14[slice16],\r\n",
        "       train_regret_winner_15[slice16],\r\n",
        "       train_regret_winner_16[slice16],\r\n",
        "       train_regret_winner_17[slice16],\r\n",
        "       train_regret_winner_18[slice16],\r\n",
        "       train_regret_winner_19[slice16],\r\n",
        "       train_regret_winner_20[slice16]]\r\n",
        "\r\n",
        "loser16_results = pd.DataFrame(loser16).sort_values(by=[0], ascending=False)\r\n",
        "winner16_results = pd.DataFrame(winner16).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser16 = np.asarray(loser16_results[4:5][0])[0]\r\n",
        "median_loser16 = np.asarray(loser16_results[9:10][0])[0]\r\n",
        "upper_loser16 = np.asarray(loser16_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner16 = np.asarray(winner16_results[4:5][0])[0]\r\n",
        "median_winner16 = np.asarray(winner16_results[9:10][0])[0]\r\n",
        "upper_winner16 = np.asarray(winner16_results[14:15][0])[0]"
      ],
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOB3hOLIxAQy"
      },
      "source": [
        "# Iteration26 :\r\n",
        "\r\n",
        "slice26 = 25\r\n",
        "\r\n",
        "loser26 = [train_regret_loser_1[slice26],\r\n",
        "       train_regret_loser_2[slice26],\r\n",
        "       train_regret_loser_3[slice26],\r\n",
        "       train_regret_loser_4[slice26],\r\n",
        "       train_regret_loser_5[slice26],\r\n",
        "       train_regret_loser_6[slice26],\r\n",
        "       train_regret_loser_7[slice26],\r\n",
        "       train_regret_loser_8[slice26],\r\n",
        "       train_regret_loser_9[slice26],\r\n",
        "       train_regret_loser_10[slice26],\r\n",
        "       train_regret_loser_11[slice26],\r\n",
        "       train_regret_loser_12[slice26],\r\n",
        "       train_regret_loser_13[slice26],\r\n",
        "       train_regret_loser_14[slice26],\r\n",
        "       train_regret_loser_15[slice26],\r\n",
        "       train_regret_loser_16[slice26],\r\n",
        "       train_regret_loser_17[slice26],\r\n",
        "       train_regret_loser_18[slice26],\r\n",
        "       train_regret_loser_19[slice26],\r\n",
        "       train_regret_loser_20[slice26]]\r\n",
        "\r\n",
        "winner26 = [train_regret_winner_1[slice26],\r\n",
        "       train_regret_winner_2[slice26],\r\n",
        "       train_regret_winner_3[slice26],\r\n",
        "       train_regret_winner_4[slice26],\r\n",
        "       train_regret_winner_5[slice26],\r\n",
        "       train_regret_winner_6[slice26],\r\n",
        "       train_regret_winner_7[slice26],\r\n",
        "       train_regret_winner_8[slice26],\r\n",
        "       train_regret_winner_9[slice26],\r\n",
        "       train_regret_winner_10[slice26],\r\n",
        "       train_regret_winner_11[slice26],\r\n",
        "       train_regret_winner_12[slice26],\r\n",
        "       train_regret_winner_13[slice26],\r\n",
        "       train_regret_winner_14[slice26],\r\n",
        "       train_regret_winner_15[slice26],\r\n",
        "       train_regret_winner_16[slice26],\r\n",
        "       train_regret_winner_17[slice26],\r\n",
        "       train_regret_winner_18[slice26],\r\n",
        "       train_regret_winner_19[slice26],\r\n",
        "       train_regret_winner_20[slice26]]\r\n",
        "\r\n",
        "loser26_results = pd.DataFrame(loser26).sort_values(by=[0], ascending=False)\r\n",
        "winner26_results = pd.DataFrame(winner26).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser26 = np.asarray(loser26_results[4:5][0])[0]\r\n",
        "median_loser26 = np.asarray(loser26_results[9:10][0])[0]\r\n",
        "upper_loser26 = np.asarray(loser26_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner26 = np.asarray(winner26_results[4:5][0])[0]\r\n",
        "median_winner26 = np.asarray(winner26_results[9:10][0])[0]\r\n",
        "upper_winner26 = np.asarray(winner26_results[14:15][0])[0]"
      ],
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-0hQMtOxATh"
      },
      "source": [
        "# Iteration36 :\r\n",
        "\r\n",
        "slice36 = 35\r\n",
        "\r\n",
        "loser36 = [train_regret_loser_1[slice36],\r\n",
        "       train_regret_loser_2[slice36],\r\n",
        "       train_regret_loser_3[slice36],\r\n",
        "       train_regret_loser_4[slice36],\r\n",
        "       train_regret_loser_5[slice36],\r\n",
        "       train_regret_loser_6[slice36],\r\n",
        "       train_regret_loser_7[slice36],\r\n",
        "       train_regret_loser_8[slice36],\r\n",
        "       train_regret_loser_9[slice36],\r\n",
        "       train_regret_loser_10[slice36],\r\n",
        "       train_regret_loser_11[slice36],\r\n",
        "       train_regret_loser_12[slice36],\r\n",
        "       train_regret_loser_13[slice36],\r\n",
        "       train_regret_loser_14[slice36],\r\n",
        "       train_regret_loser_15[slice36],\r\n",
        "       train_regret_loser_16[slice36],\r\n",
        "       train_regret_loser_17[slice36],\r\n",
        "       train_regret_loser_18[slice36],\r\n",
        "       train_regret_loser_19[slice36],\r\n",
        "       train_regret_loser_20[slice36]]\r\n",
        "\r\n",
        "winner36 = [train_regret_winner_1[slice36],\r\n",
        "       train_regret_winner_2[slice36],\r\n",
        "       train_regret_winner_3[slice36],\r\n",
        "       train_regret_winner_4[slice36],\r\n",
        "       train_regret_winner_5[slice36],\r\n",
        "       train_regret_winner_6[slice36],\r\n",
        "       train_regret_winner_7[slice36],\r\n",
        "       train_regret_winner_8[slice36],\r\n",
        "       train_regret_winner_9[slice36],\r\n",
        "       train_regret_winner_10[slice36],\r\n",
        "       train_regret_winner_11[slice36],\r\n",
        "       train_regret_winner_12[slice36],\r\n",
        "       train_regret_winner_13[slice36],\r\n",
        "       train_regret_winner_14[slice36],\r\n",
        "       train_regret_winner_15[slice36],\r\n",
        "       train_regret_winner_16[slice36],\r\n",
        "       train_regret_winner_17[slice36],\r\n",
        "       train_regret_winner_18[slice36],\r\n",
        "       train_regret_winner_19[slice36],\r\n",
        "       train_regret_winner_20[slice36]]\r\n",
        "\r\n",
        "loser36_results = pd.DataFrame(loser36).sort_values(by=[0], ascending=False)\r\n",
        "winner36_results = pd.DataFrame(winner36).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser36 = np.asarray(loser36_results[4:5][0])[0]\r\n",
        "median_loser36 = np.asarray(loser36_results[9:10][0])[0]\r\n",
        "upper_loser36 = np.asarray(loser36_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner36 = np.asarray(winner36_results[4:5][0])[0]\r\n",
        "median_winner36 = np.asarray(winner36_results[9:10][0])[0]\r\n",
        "upper_winner36 = np.asarray(winner36_results[14:15][0])[0]"
      ],
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSqBLqTgxAWs"
      },
      "source": [
        "# Iteration46 :\r\n",
        "\r\n",
        "slice46 = 45\r\n",
        "\r\n",
        "loser46 = [train_regret_loser_1[slice46],\r\n",
        "       train_regret_loser_2[slice46],\r\n",
        "       train_regret_loser_3[slice46],\r\n",
        "       train_regret_loser_4[slice46],\r\n",
        "       train_regret_loser_5[slice46],\r\n",
        "       train_regret_loser_6[slice46],\r\n",
        "       train_regret_loser_7[slice46],\r\n",
        "       train_regret_loser_8[slice46],\r\n",
        "       train_regret_loser_9[slice46],\r\n",
        "       train_regret_loser_10[slice46],\r\n",
        "       train_regret_loser_11[slice46],\r\n",
        "       train_regret_loser_12[slice46],\r\n",
        "       train_regret_loser_13[slice46],\r\n",
        "       train_regret_loser_14[slice46],\r\n",
        "       train_regret_loser_15[slice46],\r\n",
        "       train_regret_loser_16[slice46],\r\n",
        "       train_regret_loser_17[slice46],\r\n",
        "       train_regret_loser_18[slice46],\r\n",
        "       train_regret_loser_19[slice46],\r\n",
        "       train_regret_loser_20[slice46]]\r\n",
        "\r\n",
        "winner46 = [train_regret_winner_1[slice46],\r\n",
        "       train_regret_winner_2[slice46],\r\n",
        "       train_regret_winner_3[slice46],\r\n",
        "       train_regret_winner_4[slice46],\r\n",
        "       train_regret_winner_5[slice46],\r\n",
        "       train_regret_winner_6[slice46],\r\n",
        "       train_regret_winner_7[slice46],\r\n",
        "       train_regret_winner_8[slice46],\r\n",
        "       train_regret_winner_9[slice46],\r\n",
        "       train_regret_winner_10[slice46],\r\n",
        "       train_regret_winner_11[slice46],\r\n",
        "       train_regret_winner_12[slice46],\r\n",
        "       train_regret_winner_13[slice46],\r\n",
        "       train_regret_winner_14[slice46],\r\n",
        "       train_regret_winner_15[slice46],\r\n",
        "       train_regret_winner_16[slice46],\r\n",
        "       train_regret_winner_17[slice46],\r\n",
        "       train_regret_winner_18[slice46],\r\n",
        "       train_regret_winner_19[slice46],\r\n",
        "       train_regret_winner_20[slice46]]\r\n",
        "\r\n",
        "loser46_results = pd.DataFrame(loser46).sort_values(by=[0], ascending=False)\r\n",
        "winner46_results = pd.DataFrame(winner46).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser46 = np.asarray(loser46_results[4:5][0])[0]\r\n",
        "median_loser46 = np.asarray(loser46_results[9:10][0])[0]\r\n",
        "upper_loser46 = np.asarray(loser46_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner46 = np.asarray(winner46_results[4:5][0])[0]\r\n",
        "median_winner46 = np.asarray(winner46_results[9:10][0])[0]\r\n",
        "upper_winner46 = np.asarray(winner46_results[14:15][0])[0]"
      ],
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFwO80w6xAZZ"
      },
      "source": [
        "# Iteration56 :\r\n",
        "\r\n",
        "slice56 = 55\r\n",
        "\r\n",
        "loser56 = [train_regret_loser_1[slice56],\r\n",
        "       train_regret_loser_2[slice56],\r\n",
        "       train_regret_loser_3[slice56],\r\n",
        "       train_regret_loser_4[slice56],\r\n",
        "       train_regret_loser_5[slice56],\r\n",
        "       train_regret_loser_6[slice56],\r\n",
        "       train_regret_loser_7[slice56],\r\n",
        "       train_regret_loser_8[slice56],\r\n",
        "       train_regret_loser_9[slice56],\r\n",
        "       train_regret_loser_10[slice56],\r\n",
        "       train_regret_loser_11[slice56],\r\n",
        "       train_regret_loser_12[slice56],\r\n",
        "       train_regret_loser_13[slice56],\r\n",
        "       train_regret_loser_14[slice56],\r\n",
        "       train_regret_loser_15[slice56],\r\n",
        "       train_regret_loser_16[slice56],\r\n",
        "       train_regret_loser_17[slice56],\r\n",
        "       train_regret_loser_18[slice56],\r\n",
        "       train_regret_loser_19[slice56],\r\n",
        "       train_regret_loser_20[slice56]]\r\n",
        "\r\n",
        "winner56 = [train_regret_winner_1[slice56],\r\n",
        "       train_regret_winner_2[slice56],\r\n",
        "       train_regret_winner_3[slice56],\r\n",
        "       train_regret_winner_4[slice56],\r\n",
        "       train_regret_winner_5[slice56],\r\n",
        "       train_regret_winner_6[slice56],\r\n",
        "       train_regret_winner_7[slice56],\r\n",
        "       train_regret_winner_8[slice56],\r\n",
        "       train_regret_winner_9[slice56],\r\n",
        "       train_regret_winner_10[slice56],\r\n",
        "       train_regret_winner_11[slice56],\r\n",
        "       train_regret_winner_12[slice56],\r\n",
        "       train_regret_winner_13[slice56],\r\n",
        "       train_regret_winner_14[slice56],\r\n",
        "       train_regret_winner_15[slice56],\r\n",
        "       train_regret_winner_16[slice56],\r\n",
        "       train_regret_winner_17[slice56],\r\n",
        "       train_regret_winner_18[slice56],\r\n",
        "       train_regret_winner_19[slice56],\r\n",
        "       train_regret_winner_20[slice56]]\r\n",
        "\r\n",
        "loser56_results = pd.DataFrame(loser56).sort_values(by=[0], ascending=False)\r\n",
        "winner56_results = pd.DataFrame(winner56).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser56 = np.asarray(loser56_results[4:5][0])[0]\r\n",
        "median_loser56 = np.asarray(loser56_results[9:10][0])[0]\r\n",
        "upper_loser56 = np.asarray(loser56_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner56 = np.asarray(winner56_results[4:5][0])[0]\r\n",
        "median_winner56 = np.asarray(winner56_results[9:10][0])[0]\r\n",
        "upper_winner56 = np.asarray(winner56_results[14:15][0])[0]"
      ],
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfVZBDP8xAcC"
      },
      "source": [
        "# Iteration66 :\r\n",
        "\r\n",
        "slice66 = 65\r\n",
        "\r\n",
        "loser66 = [train_regret_loser_1[slice66],\r\n",
        "       train_regret_loser_2[slice66],\r\n",
        "       train_regret_loser_3[slice66],\r\n",
        "       train_regret_loser_4[slice66],\r\n",
        "       train_regret_loser_5[slice66],\r\n",
        "       train_regret_loser_6[slice66],\r\n",
        "       train_regret_loser_7[slice66],\r\n",
        "       train_regret_loser_8[slice66],\r\n",
        "       train_regret_loser_9[slice66],\r\n",
        "       train_regret_loser_10[slice66],\r\n",
        "       train_regret_loser_11[slice66],\r\n",
        "       train_regret_loser_12[slice66],\r\n",
        "       train_regret_loser_13[slice66],\r\n",
        "       train_regret_loser_14[slice66],\r\n",
        "       train_regret_loser_15[slice66],\r\n",
        "       train_regret_loser_16[slice66],\r\n",
        "       train_regret_loser_17[slice66],\r\n",
        "       train_regret_loser_18[slice66],\r\n",
        "       train_regret_loser_19[slice66],\r\n",
        "       train_regret_loser_20[slice66]]\r\n",
        "\r\n",
        "winner66 = [train_regret_winner_1[slice66],\r\n",
        "       train_regret_winner_2[slice66],\r\n",
        "       train_regret_winner_3[slice66],\r\n",
        "       train_regret_winner_4[slice66],\r\n",
        "       train_regret_winner_5[slice66],\r\n",
        "       train_regret_winner_6[slice66],\r\n",
        "       train_regret_winner_7[slice66],\r\n",
        "       train_regret_winner_8[slice66],\r\n",
        "       train_regret_winner_9[slice66],\r\n",
        "       train_regret_winner_10[slice66],\r\n",
        "       train_regret_winner_11[slice66],\r\n",
        "       train_regret_winner_12[slice66],\r\n",
        "       train_regret_winner_13[slice66],\r\n",
        "       train_regret_winner_14[slice66],\r\n",
        "       train_regret_winner_15[slice66],\r\n",
        "       train_regret_winner_16[slice66],\r\n",
        "       train_regret_winner_17[slice66],\r\n",
        "       train_regret_winner_18[slice66],\r\n",
        "       train_regret_winner_19[slice66],\r\n",
        "       train_regret_winner_20[slice66]]\r\n",
        "\r\n",
        "loser66_results = pd.DataFrame(loser66).sort_values(by=[0], ascending=False)\r\n",
        "winner66_results = pd.DataFrame(winner66).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser66 = np.asarray(loser66_results[4:5][0])[0]\r\n",
        "median_loser66 = np.asarray(loser66_results[9:10][0])[0]\r\n",
        "upper_loser66 = np.asarray(loser66_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner66 = np.asarray(winner66_results[4:5][0])[0]\r\n",
        "median_winner66 = np.asarray(winner66_results[9:10][0])[0]\r\n",
        "upper_winner66 = np.asarray(winner66_results[14:15][0])[0]"
      ],
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KnSYoG7bxAfL"
      },
      "source": [
        "# Iteration76 :\r\n",
        "\r\n",
        "slice76 = 75\r\n",
        "\r\n",
        "loser76 = [train_regret_loser_1[slice76],\r\n",
        "       train_regret_loser_2[slice76],\r\n",
        "       train_regret_loser_3[slice76],\r\n",
        "       train_regret_loser_4[slice76],\r\n",
        "       train_regret_loser_5[slice76],\r\n",
        "       train_regret_loser_6[slice76],\r\n",
        "       train_regret_loser_7[slice76],\r\n",
        "       train_regret_loser_8[slice76],\r\n",
        "       train_regret_loser_9[slice76],\r\n",
        "       train_regret_loser_10[slice76],\r\n",
        "       train_regret_loser_11[slice76],\r\n",
        "       train_regret_loser_12[slice76],\r\n",
        "       train_regret_loser_13[slice76],\r\n",
        "       train_regret_loser_14[slice76],\r\n",
        "       train_regret_loser_15[slice76],\r\n",
        "       train_regret_loser_16[slice76],\r\n",
        "       train_regret_loser_17[slice76],\r\n",
        "       train_regret_loser_18[slice76],\r\n",
        "       train_regret_loser_19[slice76],\r\n",
        "       train_regret_loser_20[slice76]]\r\n",
        "\r\n",
        "winner76 = [train_regret_winner_1[slice76],\r\n",
        "       train_regret_winner_2[slice76],\r\n",
        "       train_regret_winner_3[slice76],\r\n",
        "       train_regret_winner_4[slice76],\r\n",
        "       train_regret_winner_5[slice76],\r\n",
        "       train_regret_winner_6[slice76],\r\n",
        "       train_regret_winner_7[slice76],\r\n",
        "       train_regret_winner_8[slice76],\r\n",
        "       train_regret_winner_9[slice76],\r\n",
        "       train_regret_winner_10[slice76],\r\n",
        "       train_regret_winner_11[slice76],\r\n",
        "       train_regret_winner_12[slice76],\r\n",
        "       train_regret_winner_13[slice76],\r\n",
        "       train_regret_winner_14[slice76],\r\n",
        "       train_regret_winner_15[slice76],\r\n",
        "       train_regret_winner_16[slice76],\r\n",
        "       train_regret_winner_17[slice76],\r\n",
        "       train_regret_winner_18[slice76],\r\n",
        "       train_regret_winner_19[slice76],\r\n",
        "       train_regret_winner_20[slice76]]\r\n",
        "\r\n",
        "loser76_results = pd.DataFrame(loser76).sort_values(by=[0], ascending=False)\r\n",
        "winner76_results = pd.DataFrame(winner76).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser76 = np.asarray(loser76_results[4:5][0])[0]\r\n",
        "median_loser76 = np.asarray(loser76_results[9:10][0])[0]\r\n",
        "upper_loser76 = np.asarray(loser76_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner76 = np.asarray(winner76_results[4:5][0])[0]\r\n",
        "median_winner76 = np.asarray(winner76_results[9:10][0])[0]\r\n",
        "upper_winner76 = np.asarray(winner76_results[14:15][0])[0]"
      ],
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ze_Oy9mxAiO"
      },
      "source": [
        "# Iteration86 :\r\n",
        "\r\n",
        "slice86 = 85\r\n",
        "\r\n",
        "loser86 = [train_regret_loser_1[slice86],\r\n",
        "       train_regret_loser_2[slice86],\r\n",
        "       train_regret_loser_3[slice86],\r\n",
        "       train_regret_loser_4[slice86],\r\n",
        "       train_regret_loser_5[slice86],\r\n",
        "       train_regret_loser_6[slice86],\r\n",
        "       train_regret_loser_7[slice86],\r\n",
        "       train_regret_loser_8[slice86],\r\n",
        "       train_regret_loser_9[slice86],\r\n",
        "       train_regret_loser_10[slice86],\r\n",
        "       train_regret_loser_11[slice86],\r\n",
        "       train_regret_loser_12[slice86],\r\n",
        "       train_regret_loser_13[slice86],\r\n",
        "       train_regret_loser_14[slice86],\r\n",
        "       train_regret_loser_15[slice86],\r\n",
        "       train_regret_loser_16[slice86],\r\n",
        "       train_regret_loser_17[slice86],\r\n",
        "       train_regret_loser_18[slice86],\r\n",
        "       train_regret_loser_19[slice86],\r\n",
        "       train_regret_loser_20[slice86]]\r\n",
        "\r\n",
        "winner86 = [train_regret_winner_1[slice86],\r\n",
        "       train_regret_winner_2[slice86],\r\n",
        "       train_regret_winner_3[slice86],\r\n",
        "       train_regret_winner_4[slice86],\r\n",
        "       train_regret_winner_5[slice86],\r\n",
        "       train_regret_winner_6[slice86],\r\n",
        "       train_regret_winner_7[slice86],\r\n",
        "       train_regret_winner_8[slice86],\r\n",
        "       train_regret_winner_9[slice86],\r\n",
        "       train_regret_winner_10[slice86],\r\n",
        "       train_regret_winner_11[slice86],\r\n",
        "       train_regret_winner_12[slice86],\r\n",
        "       train_regret_winner_13[slice86],\r\n",
        "       train_regret_winner_14[slice86],\r\n",
        "       train_regret_winner_15[slice86],\r\n",
        "       train_regret_winner_16[slice86],\r\n",
        "       train_regret_winner_17[slice86],\r\n",
        "       train_regret_winner_18[slice86],\r\n",
        "       train_regret_winner_19[slice86],\r\n",
        "       train_regret_winner_20[slice86]]\r\n",
        "\r\n",
        "loser86_results = pd.DataFrame(loser86).sort_values(by=[0], ascending=False)\r\n",
        "winner86_results = pd.DataFrame(winner86).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser86 = np.asarray(loser86_results[4:5][0])[0]\r\n",
        "median_loser86 = np.asarray(loser86_results[9:10][0])[0]\r\n",
        "upper_loser86 = np.asarray(loser86_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner86 = np.asarray(winner86_results[4:5][0])[0]\r\n",
        "median_winner86 = np.asarray(winner86_results[9:10][0])[0]\r\n",
        "upper_winner86 = np.asarray(winner86_results[14:15][0])[0]"
      ],
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glsE9_RPxAlS"
      },
      "source": [
        "# Iteration96 :\r\n",
        "\r\n",
        "slice96 = 95\r\n",
        "\r\n",
        "loser96 = [train_regret_loser_1[slice96],\r\n",
        "       train_regret_loser_2[slice96],\r\n",
        "       train_regret_loser_3[slice96],\r\n",
        "       train_regret_loser_4[slice96],\r\n",
        "       train_regret_loser_5[slice96],\r\n",
        "       train_regret_loser_6[slice96],\r\n",
        "       train_regret_loser_7[slice96],\r\n",
        "       train_regret_loser_8[slice96],\r\n",
        "       train_regret_loser_9[slice96],\r\n",
        "       train_regret_loser_10[slice96],\r\n",
        "       train_regret_loser_11[slice96],\r\n",
        "       train_regret_loser_12[slice96],\r\n",
        "       train_regret_loser_13[slice96],\r\n",
        "       train_regret_loser_14[slice96],\r\n",
        "       train_regret_loser_15[slice96],\r\n",
        "       train_regret_loser_16[slice96],\r\n",
        "       train_regret_loser_17[slice96],\r\n",
        "       train_regret_loser_18[slice96],\r\n",
        "       train_regret_loser_19[slice96],\r\n",
        "       train_regret_loser_20[slice96]]\r\n",
        "\r\n",
        "winner96 = [train_regret_winner_1[slice96],\r\n",
        "       train_regret_winner_2[slice96],\r\n",
        "       train_regret_winner_3[slice96],\r\n",
        "       train_regret_winner_4[slice96],\r\n",
        "       train_regret_winner_5[slice96],\r\n",
        "       train_regret_winner_6[slice96],\r\n",
        "       train_regret_winner_7[slice96],\r\n",
        "       train_regret_winner_8[slice96],\r\n",
        "       train_regret_winner_9[slice96],\r\n",
        "       train_regret_winner_10[slice96],\r\n",
        "       train_regret_winner_11[slice96],\r\n",
        "       train_regret_winner_12[slice96],\r\n",
        "       train_regret_winner_13[slice96],\r\n",
        "       train_regret_winner_14[slice96],\r\n",
        "       train_regret_winner_15[slice96],\r\n",
        "       train_regret_winner_16[slice96],\r\n",
        "       train_regret_winner_17[slice96],\r\n",
        "       train_regret_winner_18[slice96],\r\n",
        "       train_regret_winner_19[slice96],\r\n",
        "       train_regret_winner_20[slice96]]\r\n",
        "\r\n",
        "loser96_results = pd.DataFrame(loser96).sort_values(by=[0], ascending=False)\r\n",
        "winner96_results = pd.DataFrame(winner96).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser96 = np.asarray(loser96_results[4:5][0])[0]\r\n",
        "median_loser96 = np.asarray(loser96_results[9:10][0])[0]\r\n",
        "upper_loser96 = np.asarray(loser96_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner96 = np.asarray(winner96_results[4:5][0])[0]\r\n",
        "median_winner96 = np.asarray(winner96_results[9:10][0])[0]\r\n",
        "upper_winner96 = np.asarray(winner96_results[14:15][0])[0]"
      ],
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IhsEfl1RxAoA"
      },
      "source": [
        "# Iteration7 :\r\n",
        "\r\n",
        "slice7 = 6\r\n",
        "\r\n",
        "loser7 = [train_regret_loser_1[slice7],\r\n",
        "       train_regret_loser_2[slice7],\r\n",
        "       train_regret_loser_3[slice7],\r\n",
        "       train_regret_loser_4[slice7],\r\n",
        "       train_regret_loser_5[slice7],\r\n",
        "       train_regret_loser_6[slice7],\r\n",
        "       train_regret_loser_7[slice7],\r\n",
        "       train_regret_loser_8[slice7],\r\n",
        "       train_regret_loser_9[slice7],\r\n",
        "       train_regret_loser_10[slice7],\r\n",
        "       train_regret_loser_11[slice7],\r\n",
        "       train_regret_loser_12[slice7],\r\n",
        "       train_regret_loser_13[slice7],\r\n",
        "       train_regret_loser_14[slice7],\r\n",
        "       train_regret_loser_15[slice7],\r\n",
        "       train_regret_loser_16[slice7],\r\n",
        "       train_regret_loser_17[slice7],\r\n",
        "       train_regret_loser_18[slice7],\r\n",
        "       train_regret_loser_19[slice7],\r\n",
        "       train_regret_loser_20[slice7]]\r\n",
        "\r\n",
        "winner7 = [train_regret_winner_1[slice7],\r\n",
        "       train_regret_winner_2[slice7],\r\n",
        "       train_regret_winner_3[slice7],\r\n",
        "       train_regret_winner_4[slice7],\r\n",
        "       train_regret_winner_5[slice7],\r\n",
        "       train_regret_winner_6[slice7],\r\n",
        "       train_regret_winner_7[slice7],\r\n",
        "       train_regret_winner_8[slice7],\r\n",
        "       train_regret_winner_9[slice7],\r\n",
        "       train_regret_winner_10[slice7],\r\n",
        "       train_regret_winner_11[slice7],\r\n",
        "       train_regret_winner_12[slice7],\r\n",
        "       train_regret_winner_13[slice7],\r\n",
        "       train_regret_winner_14[slice7],\r\n",
        "       train_regret_winner_15[slice7],\r\n",
        "       train_regret_winner_16[slice7],\r\n",
        "       train_regret_winner_17[slice7],\r\n",
        "       train_regret_winner_18[slice7],\r\n",
        "       train_regret_winner_19[slice7],\r\n",
        "       train_regret_winner_20[slice7]]\r\n",
        "\r\n",
        "loser7_results = pd.DataFrame(loser7).sort_values(by=[0], ascending=False)\r\n",
        "winner7_results = pd.DataFrame(winner7).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser7 = np.asarray(loser7_results[4:5][0])[0]\r\n",
        "median_loser7 = np.asarray(loser7_results[9:10][0])[0]\r\n",
        "upper_loser7 = np.asarray(loser7_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner7 = np.asarray(winner7_results[4:5][0])[0]\r\n",
        "median_winner7 = np.asarray(winner7_results[9:10][0])[0]\r\n",
        "upper_winner7 = np.asarray(winner7_results[14:15][0])[0]"
      ],
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUCMheLKxArL"
      },
      "source": [
        "# Iteration17 :\r\n",
        "\r\n",
        "slice17 = 16\r\n",
        "\r\n",
        "loser17 = [train_regret_loser_1[slice17],\r\n",
        "       train_regret_loser_2[slice17],\r\n",
        "       train_regret_loser_3[slice17],\r\n",
        "       train_regret_loser_4[slice17],\r\n",
        "       train_regret_loser_5[slice17],\r\n",
        "       train_regret_loser_6[slice17],\r\n",
        "       train_regret_loser_7[slice17],\r\n",
        "       train_regret_loser_8[slice17],\r\n",
        "       train_regret_loser_9[slice17],\r\n",
        "       train_regret_loser_10[slice17],\r\n",
        "       train_regret_loser_11[slice17],\r\n",
        "       train_regret_loser_12[slice17],\r\n",
        "       train_regret_loser_13[slice17],\r\n",
        "       train_regret_loser_14[slice17],\r\n",
        "       train_regret_loser_15[slice17],\r\n",
        "       train_regret_loser_16[slice17],\r\n",
        "       train_regret_loser_17[slice17],\r\n",
        "       train_regret_loser_18[slice17],\r\n",
        "       train_regret_loser_19[slice17],\r\n",
        "       train_regret_loser_20[slice17]]\r\n",
        "\r\n",
        "winner17 = [train_regret_winner_1[slice17],\r\n",
        "       train_regret_winner_2[slice17],\r\n",
        "       train_regret_winner_3[slice17],\r\n",
        "       train_regret_winner_4[slice17],\r\n",
        "       train_regret_winner_5[slice17],\r\n",
        "       train_regret_winner_6[slice17],\r\n",
        "       train_regret_winner_7[slice17],\r\n",
        "       train_regret_winner_8[slice17],\r\n",
        "       train_regret_winner_9[slice17],\r\n",
        "       train_regret_winner_10[slice17],\r\n",
        "       train_regret_winner_11[slice17],\r\n",
        "       train_regret_winner_12[slice17],\r\n",
        "       train_regret_winner_13[slice17],\r\n",
        "       train_regret_winner_14[slice17],\r\n",
        "       train_regret_winner_15[slice17],\r\n",
        "       train_regret_winner_16[slice17],\r\n",
        "       train_regret_winner_17[slice17],\r\n",
        "       train_regret_winner_18[slice17],\r\n",
        "       train_regret_winner_19[slice17],\r\n",
        "       train_regret_winner_20[slice17]]\r\n",
        "\r\n",
        "loser17_results = pd.DataFrame(loser17).sort_values(by=[0], ascending=False)\r\n",
        "winner17_results = pd.DataFrame(winner17).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser17 = np.asarray(loser17_results[4:5][0])[0]\r\n",
        "median_loser17 = np.asarray(loser17_results[9:10][0])[0]\r\n",
        "upper_loser17 = np.asarray(loser17_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner17 = np.asarray(winner17_results[4:5][0])[0]\r\n",
        "median_winner17 = np.asarray(winner17_results[9:10][0])[0]\r\n",
        "upper_winner17 = np.asarray(winner17_results[14:15][0])[0]"
      ],
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYnBmq7_xAuB"
      },
      "source": [
        "# Iteration27 :\r\n",
        "\r\n",
        "slice27 = 26\r\n",
        "\r\n",
        "loser27 = [train_regret_loser_1[slice27],\r\n",
        "       train_regret_loser_2[slice27],\r\n",
        "       train_regret_loser_3[slice27],\r\n",
        "       train_regret_loser_4[slice27],\r\n",
        "       train_regret_loser_5[slice27],\r\n",
        "       train_regret_loser_6[slice27],\r\n",
        "       train_regret_loser_7[slice27],\r\n",
        "       train_regret_loser_8[slice27],\r\n",
        "       train_regret_loser_9[slice27],\r\n",
        "       train_regret_loser_10[slice27],\r\n",
        "       train_regret_loser_11[slice27],\r\n",
        "       train_regret_loser_12[slice27],\r\n",
        "       train_regret_loser_13[slice27],\r\n",
        "       train_regret_loser_14[slice27],\r\n",
        "       train_regret_loser_15[slice27],\r\n",
        "       train_regret_loser_16[slice27],\r\n",
        "       train_regret_loser_17[slice27],\r\n",
        "       train_regret_loser_18[slice27],\r\n",
        "       train_regret_loser_19[slice27],\r\n",
        "       train_regret_loser_20[slice27]]\r\n",
        "\r\n",
        "winner27 = [train_regret_winner_1[slice27],\r\n",
        "       train_regret_winner_2[slice27],\r\n",
        "       train_regret_winner_3[slice27],\r\n",
        "       train_regret_winner_4[slice27],\r\n",
        "       train_regret_winner_5[slice27],\r\n",
        "       train_regret_winner_6[slice27],\r\n",
        "       train_regret_winner_7[slice27],\r\n",
        "       train_regret_winner_8[slice27],\r\n",
        "       train_regret_winner_9[slice27],\r\n",
        "       train_regret_winner_10[slice27],\r\n",
        "       train_regret_winner_11[slice27],\r\n",
        "       train_regret_winner_12[slice27],\r\n",
        "       train_regret_winner_13[slice27],\r\n",
        "       train_regret_winner_14[slice27],\r\n",
        "       train_regret_winner_15[slice27],\r\n",
        "       train_regret_winner_16[slice27],\r\n",
        "       train_regret_winner_17[slice27],\r\n",
        "       train_regret_winner_18[slice27],\r\n",
        "       train_regret_winner_19[slice27],\r\n",
        "       train_regret_winner_20[slice27]]\r\n",
        "\r\n",
        "loser27_results = pd.DataFrame(loser27).sort_values(by=[0], ascending=False)\r\n",
        "winner27_results = pd.DataFrame(winner27).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser27 = np.asarray(loser27_results[4:5][0])[0]\r\n",
        "median_loser27 = np.asarray(loser27_results[9:10][0])[0]\r\n",
        "upper_loser27 = np.asarray(loser27_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner27 = np.asarray(winner27_results[4:5][0])[0]\r\n",
        "median_winner27 = np.asarray(winner27_results[9:10][0])[0]\r\n",
        "upper_winner27 = np.asarray(winner27_results[14:15][0])[0]"
      ],
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ChA8KOUxAw6"
      },
      "source": [
        "# Iteration37 :\r\n",
        "\r\n",
        "slice37 = 36\r\n",
        "\r\n",
        "loser37 = [train_regret_loser_1[slice37],\r\n",
        "       train_regret_loser_2[slice37],\r\n",
        "       train_regret_loser_3[slice37],\r\n",
        "       train_regret_loser_4[slice37],\r\n",
        "       train_regret_loser_5[slice37],\r\n",
        "       train_regret_loser_6[slice37],\r\n",
        "       train_regret_loser_7[slice37],\r\n",
        "       train_regret_loser_8[slice37],\r\n",
        "       train_regret_loser_9[slice37],\r\n",
        "       train_regret_loser_10[slice37],\r\n",
        "       train_regret_loser_11[slice37],\r\n",
        "       train_regret_loser_12[slice37],\r\n",
        "       train_regret_loser_13[slice37],\r\n",
        "       train_regret_loser_14[slice37],\r\n",
        "       train_regret_loser_15[slice37],\r\n",
        "       train_regret_loser_16[slice37],\r\n",
        "       train_regret_loser_17[slice37],\r\n",
        "       train_regret_loser_18[slice37],\r\n",
        "       train_regret_loser_19[slice37],\r\n",
        "       train_regret_loser_20[slice37]]\r\n",
        "\r\n",
        "winner37 = [train_regret_winner_1[slice37],\r\n",
        "       train_regret_winner_2[slice37],\r\n",
        "       train_regret_winner_3[slice37],\r\n",
        "       train_regret_winner_4[slice37],\r\n",
        "       train_regret_winner_5[slice37],\r\n",
        "       train_regret_winner_6[slice37],\r\n",
        "       train_regret_winner_7[slice37],\r\n",
        "       train_regret_winner_8[slice37],\r\n",
        "       train_regret_winner_9[slice37],\r\n",
        "       train_regret_winner_10[slice37],\r\n",
        "       train_regret_winner_11[slice37],\r\n",
        "       train_regret_winner_12[slice37],\r\n",
        "       train_regret_winner_13[slice37],\r\n",
        "       train_regret_winner_14[slice37],\r\n",
        "       train_regret_winner_15[slice37],\r\n",
        "       train_regret_winner_16[slice37],\r\n",
        "       train_regret_winner_17[slice37],\r\n",
        "       train_regret_winner_18[slice37],\r\n",
        "       train_regret_winner_19[slice37],\r\n",
        "       train_regret_winner_20[slice37]]\r\n",
        "\r\n",
        "loser37_results = pd.DataFrame(loser37).sort_values(by=[0], ascending=False)\r\n",
        "winner37_results = pd.DataFrame(winner37).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser37 = np.asarray(loser37_results[4:5][0])[0]\r\n",
        "median_loser37 = np.asarray(loser37_results[9:10][0])[0]\r\n",
        "upper_loser37 = np.asarray(loser37_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner37 = np.asarray(winner37_results[4:5][0])[0]\r\n",
        "median_winner37 = np.asarray(winner37_results[9:10][0])[0]\r\n",
        "upper_winner37 = np.asarray(winner37_results[14:15][0])[0]"
      ],
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVsY1buDxAzx"
      },
      "source": [
        "# Iteration47 :\r\n",
        "\r\n",
        "slice47 = 46\r\n",
        "\r\n",
        "loser47 = [train_regret_loser_1[slice47],\r\n",
        "       train_regret_loser_2[slice47],\r\n",
        "       train_regret_loser_3[slice47],\r\n",
        "       train_regret_loser_4[slice47],\r\n",
        "       train_regret_loser_5[slice47],\r\n",
        "       train_regret_loser_6[slice47],\r\n",
        "       train_regret_loser_7[slice47],\r\n",
        "       train_regret_loser_8[slice47],\r\n",
        "       train_regret_loser_9[slice47],\r\n",
        "       train_regret_loser_10[slice47],\r\n",
        "       train_regret_loser_11[slice47],\r\n",
        "       train_regret_loser_12[slice47],\r\n",
        "       train_regret_loser_13[slice47],\r\n",
        "       train_regret_loser_14[slice47],\r\n",
        "       train_regret_loser_15[slice47],\r\n",
        "       train_regret_loser_16[slice47],\r\n",
        "       train_regret_loser_17[slice47],\r\n",
        "       train_regret_loser_18[slice47],\r\n",
        "       train_regret_loser_19[slice47],\r\n",
        "       train_regret_loser_20[slice47]]\r\n",
        "\r\n",
        "winner47 = [train_regret_winner_1[slice47],\r\n",
        "       train_regret_winner_2[slice47],\r\n",
        "       train_regret_winner_3[slice47],\r\n",
        "       train_regret_winner_4[slice47],\r\n",
        "       train_regret_winner_5[slice47],\r\n",
        "       train_regret_winner_6[slice47],\r\n",
        "       train_regret_winner_7[slice47],\r\n",
        "       train_regret_winner_8[slice47],\r\n",
        "       train_regret_winner_9[slice47],\r\n",
        "       train_regret_winner_10[slice47],\r\n",
        "       train_regret_winner_11[slice47],\r\n",
        "       train_regret_winner_12[slice47],\r\n",
        "       train_regret_winner_13[slice47],\r\n",
        "       train_regret_winner_14[slice47],\r\n",
        "       train_regret_winner_15[slice47],\r\n",
        "       train_regret_winner_16[slice47],\r\n",
        "       train_regret_winner_17[slice47],\r\n",
        "       train_regret_winner_18[slice47],\r\n",
        "       train_regret_winner_19[slice47],\r\n",
        "       train_regret_winner_20[slice47]]\r\n",
        "\r\n",
        "loser47_results = pd.DataFrame(loser47).sort_values(by=[0], ascending=False)\r\n",
        "winner47_results = pd.DataFrame(winner47).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser47 = np.asarray(loser47_results[4:5][0])[0]\r\n",
        "median_loser47 = np.asarray(loser47_results[9:10][0])[0]\r\n",
        "upper_loser47 = np.asarray(loser47_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner47 = np.asarray(winner47_results[4:5][0])[0]\r\n",
        "median_winner47 = np.asarray(winner47_results[9:10][0])[0]\r\n",
        "upper_winner47 = np.asarray(winner47_results[14:15][0])[0]"
      ],
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fIy_EyTxA20"
      },
      "source": [
        "# Iteration57 :\r\n",
        "\r\n",
        "slice57 = 56\r\n",
        "\r\n",
        "loser57 = [train_regret_loser_1[slice57],\r\n",
        "       train_regret_loser_2[slice57],\r\n",
        "       train_regret_loser_3[slice57],\r\n",
        "       train_regret_loser_4[slice57],\r\n",
        "       train_regret_loser_5[slice57],\r\n",
        "       train_regret_loser_6[slice57],\r\n",
        "       train_regret_loser_7[slice57],\r\n",
        "       train_regret_loser_8[slice57],\r\n",
        "       train_regret_loser_9[slice57],\r\n",
        "       train_regret_loser_10[slice57],\r\n",
        "       train_regret_loser_11[slice57],\r\n",
        "       train_regret_loser_12[slice57],\r\n",
        "       train_regret_loser_13[slice57],\r\n",
        "       train_regret_loser_14[slice57],\r\n",
        "       train_regret_loser_15[slice57],\r\n",
        "       train_regret_loser_16[slice57],\r\n",
        "       train_regret_loser_17[slice57],\r\n",
        "       train_regret_loser_18[slice57],\r\n",
        "       train_regret_loser_19[slice57],\r\n",
        "       train_regret_loser_20[slice57]]\r\n",
        "\r\n",
        "winner57 = [train_regret_winner_1[slice57],\r\n",
        "       train_regret_winner_2[slice57],\r\n",
        "       train_regret_winner_3[slice57],\r\n",
        "       train_regret_winner_4[slice57],\r\n",
        "       train_regret_winner_5[slice57],\r\n",
        "       train_regret_winner_6[slice57],\r\n",
        "       train_regret_winner_7[slice57],\r\n",
        "       train_regret_winner_8[slice57],\r\n",
        "       train_regret_winner_9[slice57],\r\n",
        "       train_regret_winner_10[slice57],\r\n",
        "       train_regret_winner_11[slice57],\r\n",
        "       train_regret_winner_12[slice57],\r\n",
        "       train_regret_winner_13[slice57],\r\n",
        "       train_regret_winner_14[slice57],\r\n",
        "       train_regret_winner_15[slice57],\r\n",
        "       train_regret_winner_16[slice57],\r\n",
        "       train_regret_winner_17[slice57],\r\n",
        "       train_regret_winner_18[slice57],\r\n",
        "       train_regret_winner_19[slice57],\r\n",
        "       train_regret_winner_20[slice57]]\r\n",
        "\r\n",
        "loser57_results = pd.DataFrame(loser57).sort_values(by=[0], ascending=False)\r\n",
        "winner57_results = pd.DataFrame(winner57).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser57 = np.asarray(loser57_results[4:5][0])[0]\r\n",
        "median_loser57 = np.asarray(loser57_results[9:10][0])[0]\r\n",
        "upper_loser57 = np.asarray(loser57_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner57 = np.asarray(winner57_results[4:5][0])[0]\r\n",
        "median_winner57 = np.asarray(winner57_results[9:10][0])[0]\r\n",
        "upper_winner57 = np.asarray(winner57_results[14:15][0])[0]"
      ],
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umwdBl8XxA5r"
      },
      "source": [
        "# Iteration67 :\r\n",
        "\r\n",
        "slice67 = 66\r\n",
        "\r\n",
        "loser67 = [train_regret_loser_1[slice67],\r\n",
        "       train_regret_loser_2[slice67],\r\n",
        "       train_regret_loser_3[slice67],\r\n",
        "       train_regret_loser_4[slice67],\r\n",
        "       train_regret_loser_5[slice67],\r\n",
        "       train_regret_loser_6[slice67],\r\n",
        "       train_regret_loser_7[slice67],\r\n",
        "       train_regret_loser_8[slice67],\r\n",
        "       train_regret_loser_9[slice67],\r\n",
        "       train_regret_loser_10[slice67],\r\n",
        "       train_regret_loser_11[slice67],\r\n",
        "       train_regret_loser_12[slice67],\r\n",
        "       train_regret_loser_13[slice67],\r\n",
        "       train_regret_loser_14[slice67],\r\n",
        "       train_regret_loser_15[slice67],\r\n",
        "       train_regret_loser_16[slice67],\r\n",
        "       train_regret_loser_17[slice67],\r\n",
        "       train_regret_loser_18[slice67],\r\n",
        "       train_regret_loser_19[slice67],\r\n",
        "       train_regret_loser_20[slice67]]\r\n",
        "\r\n",
        "winner67 = [train_regret_winner_1[slice67],\r\n",
        "       train_regret_winner_2[slice67],\r\n",
        "       train_regret_winner_3[slice67],\r\n",
        "       train_regret_winner_4[slice67],\r\n",
        "       train_regret_winner_5[slice67],\r\n",
        "       train_regret_winner_6[slice67],\r\n",
        "       train_regret_winner_7[slice67],\r\n",
        "       train_regret_winner_8[slice67],\r\n",
        "       train_regret_winner_9[slice67],\r\n",
        "       train_regret_winner_10[slice67],\r\n",
        "       train_regret_winner_11[slice67],\r\n",
        "       train_regret_winner_12[slice67],\r\n",
        "       train_regret_winner_13[slice67],\r\n",
        "       train_regret_winner_14[slice67],\r\n",
        "       train_regret_winner_15[slice67],\r\n",
        "       train_regret_winner_16[slice67],\r\n",
        "       train_regret_winner_17[slice67],\r\n",
        "       train_regret_winner_18[slice67],\r\n",
        "       train_regret_winner_19[slice67],\r\n",
        "       train_regret_winner_20[slice67]]\r\n",
        "\r\n",
        "loser67_results = pd.DataFrame(loser67).sort_values(by=[0], ascending=False)\r\n",
        "winner67_results = pd.DataFrame(winner67).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser67 = np.asarray(loser67_results[4:5][0])[0]\r\n",
        "median_loser67 = np.asarray(loser67_results[9:10][0])[0]\r\n",
        "upper_loser67 = np.asarray(loser67_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner67 = np.asarray(winner67_results[4:5][0])[0]\r\n",
        "median_winner67 = np.asarray(winner67_results[9:10][0])[0]\r\n",
        "upper_winner67 = np.asarray(winner67_results[14:15][0])[0]"
      ],
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lF3KFX-ZxA8q"
      },
      "source": [
        "# Iteration77 :\r\n",
        "\r\n",
        "slice77 = 76\r\n",
        "\r\n",
        "loser77 = [train_regret_loser_1[slice77],\r\n",
        "       train_regret_loser_2[slice77],\r\n",
        "       train_regret_loser_3[slice77],\r\n",
        "       train_regret_loser_4[slice77],\r\n",
        "       train_regret_loser_5[slice77],\r\n",
        "       train_regret_loser_6[slice77],\r\n",
        "       train_regret_loser_7[slice77],\r\n",
        "       train_regret_loser_8[slice77],\r\n",
        "       train_regret_loser_9[slice77],\r\n",
        "       train_regret_loser_10[slice77],\r\n",
        "       train_regret_loser_11[slice77],\r\n",
        "       train_regret_loser_12[slice77],\r\n",
        "       train_regret_loser_13[slice77],\r\n",
        "       train_regret_loser_14[slice77],\r\n",
        "       train_regret_loser_15[slice77],\r\n",
        "       train_regret_loser_16[slice77],\r\n",
        "       train_regret_loser_17[slice77],\r\n",
        "       train_regret_loser_18[slice77],\r\n",
        "       train_regret_loser_19[slice77],\r\n",
        "       train_regret_loser_20[slice77]]\r\n",
        "\r\n",
        "winner77 = [train_regret_winner_1[slice77],\r\n",
        "       train_regret_winner_2[slice77],\r\n",
        "       train_regret_winner_3[slice77],\r\n",
        "       train_regret_winner_4[slice77],\r\n",
        "       train_regret_winner_5[slice77],\r\n",
        "       train_regret_winner_6[slice77],\r\n",
        "       train_regret_winner_7[slice77],\r\n",
        "       train_regret_winner_8[slice77],\r\n",
        "       train_regret_winner_9[slice77],\r\n",
        "       train_regret_winner_10[slice77],\r\n",
        "       train_regret_winner_11[slice77],\r\n",
        "       train_regret_winner_12[slice77],\r\n",
        "       train_regret_winner_13[slice77],\r\n",
        "       train_regret_winner_14[slice77],\r\n",
        "       train_regret_winner_15[slice77],\r\n",
        "       train_regret_winner_16[slice77],\r\n",
        "       train_regret_winner_17[slice77],\r\n",
        "       train_regret_winner_18[slice77],\r\n",
        "       train_regret_winner_19[slice77],\r\n",
        "       train_regret_winner_20[slice77]]\r\n",
        "\r\n",
        "loser77_results = pd.DataFrame(loser77).sort_values(by=[0], ascending=False)\r\n",
        "winner77_results = pd.DataFrame(winner77).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser77 = np.asarray(loser77_results[4:5][0])[0]\r\n",
        "median_loser77 = np.asarray(loser77_results[9:10][0])[0]\r\n",
        "upper_loser77 = np.asarray(loser77_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner77 = np.asarray(winner77_results[4:5][0])[0]\r\n",
        "median_winner77 = np.asarray(winner77_results[9:10][0])[0]\r\n",
        "upper_winner77 = np.asarray(winner77_results[14:15][0])[0]"
      ],
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7CVYft39xA_t"
      },
      "source": [
        "# Iteration87 :\r\n",
        "\r\n",
        "slice87 = 86\r\n",
        "\r\n",
        "loser87 = [train_regret_loser_1[slice87],\r\n",
        "       train_regret_loser_2[slice87],\r\n",
        "       train_regret_loser_3[slice87],\r\n",
        "       train_regret_loser_4[slice87],\r\n",
        "       train_regret_loser_5[slice87],\r\n",
        "       train_regret_loser_6[slice87],\r\n",
        "       train_regret_loser_7[slice87],\r\n",
        "       train_regret_loser_8[slice87],\r\n",
        "       train_regret_loser_9[slice87],\r\n",
        "       train_regret_loser_10[slice87],\r\n",
        "       train_regret_loser_11[slice87],\r\n",
        "       train_regret_loser_12[slice87],\r\n",
        "       train_regret_loser_13[slice87],\r\n",
        "       train_regret_loser_14[slice87],\r\n",
        "       train_regret_loser_15[slice87],\r\n",
        "       train_regret_loser_16[slice87],\r\n",
        "       train_regret_loser_17[slice87],\r\n",
        "       train_regret_loser_18[slice87],\r\n",
        "       train_regret_loser_19[slice87],\r\n",
        "       train_regret_loser_20[slice87]]\r\n",
        "\r\n",
        "winner87 = [train_regret_winner_1[slice87],\r\n",
        "       train_regret_winner_2[slice87],\r\n",
        "       train_regret_winner_3[slice87],\r\n",
        "       train_regret_winner_4[slice87],\r\n",
        "       train_regret_winner_5[slice87],\r\n",
        "       train_regret_winner_6[slice87],\r\n",
        "       train_regret_winner_7[slice87],\r\n",
        "       train_regret_winner_8[slice87],\r\n",
        "       train_regret_winner_9[slice87],\r\n",
        "       train_regret_winner_10[slice87],\r\n",
        "       train_regret_winner_11[slice87],\r\n",
        "       train_regret_winner_12[slice87],\r\n",
        "       train_regret_winner_13[slice87],\r\n",
        "       train_regret_winner_14[slice87],\r\n",
        "       train_regret_winner_15[slice87],\r\n",
        "       train_regret_winner_16[slice87],\r\n",
        "       train_regret_winner_17[slice87],\r\n",
        "       train_regret_winner_18[slice87],\r\n",
        "       train_regret_winner_19[slice87],\r\n",
        "       train_regret_winner_20[slice87]]\r\n",
        "\r\n",
        "loser87_results = pd.DataFrame(loser87).sort_values(by=[0], ascending=False)\r\n",
        "winner87_results = pd.DataFrame(winner87).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser87 = np.asarray(loser87_results[4:5][0])[0]\r\n",
        "median_loser87 = np.asarray(loser87_results[9:10][0])[0]\r\n",
        "upper_loser87 = np.asarray(loser87_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner87 = np.asarray(winner87_results[4:5][0])[0]\r\n",
        "median_winner87 = np.asarray(winner87_results[9:10][0])[0]\r\n",
        "upper_winner87 = np.asarray(winner87_results[14:15][0])[0]"
      ],
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0gPkLC0yxBCs"
      },
      "source": [
        "# Iteration97 :\r\n",
        "\r\n",
        "slice97 = 96\r\n",
        "\r\n",
        "loser97 = [train_regret_loser_1[slice97],\r\n",
        "       train_regret_loser_2[slice97],\r\n",
        "       train_regret_loser_3[slice97],\r\n",
        "       train_regret_loser_4[slice97],\r\n",
        "       train_regret_loser_5[slice97],\r\n",
        "       train_regret_loser_6[slice97],\r\n",
        "       train_regret_loser_7[slice97],\r\n",
        "       train_regret_loser_8[slice97],\r\n",
        "       train_regret_loser_9[slice97],\r\n",
        "       train_regret_loser_10[slice97],\r\n",
        "       train_regret_loser_11[slice97],\r\n",
        "       train_regret_loser_12[slice97],\r\n",
        "       train_regret_loser_13[slice97],\r\n",
        "       train_regret_loser_14[slice97],\r\n",
        "       train_regret_loser_15[slice97],\r\n",
        "       train_regret_loser_16[slice97],\r\n",
        "       train_regret_loser_17[slice97],\r\n",
        "       train_regret_loser_18[slice97],\r\n",
        "       train_regret_loser_19[slice97],\r\n",
        "       train_regret_loser_20[slice97]]\r\n",
        "\r\n",
        "winner97 = [train_regret_winner_1[slice97],\r\n",
        "       train_regret_winner_2[slice97],\r\n",
        "       train_regret_winner_3[slice97],\r\n",
        "       train_regret_winner_4[slice97],\r\n",
        "       train_regret_winner_5[slice97],\r\n",
        "       train_regret_winner_6[slice97],\r\n",
        "       train_regret_winner_7[slice97],\r\n",
        "       train_regret_winner_8[slice97],\r\n",
        "       train_regret_winner_9[slice97],\r\n",
        "       train_regret_winner_10[slice97],\r\n",
        "       train_regret_winner_11[slice97],\r\n",
        "       train_regret_winner_12[slice97],\r\n",
        "       train_regret_winner_13[slice97],\r\n",
        "       train_regret_winner_14[slice97],\r\n",
        "       train_regret_winner_15[slice97],\r\n",
        "       train_regret_winner_16[slice97],\r\n",
        "       train_regret_winner_17[slice97],\r\n",
        "       train_regret_winner_18[slice97],\r\n",
        "       train_regret_winner_19[slice97],\r\n",
        "       train_regret_winner_20[slice97]]\r\n",
        "\r\n",
        "loser97_results = pd.DataFrame(loser97).sort_values(by=[0], ascending=False)\r\n",
        "winner97_results = pd.DataFrame(winner97).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser97 = np.asarray(loser97_results[4:5][0])[0]\r\n",
        "median_loser97 = np.asarray(loser97_results[9:10][0])[0]\r\n",
        "upper_loser97 = np.asarray(loser97_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner97 = np.asarray(winner97_results[4:5][0])[0]\r\n",
        "median_winner97 = np.asarray(winner97_results[9:10][0])[0]\r\n",
        "upper_winner97 = np.asarray(winner97_results[14:15][0])[0]"
      ],
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BbgYEpYUxBFr"
      },
      "source": [
        "# Iteration8 :\r\n",
        "\r\n",
        "slice8 = 7\r\n",
        "\r\n",
        "loser8 = [train_regret_loser_1[slice8],\r\n",
        "       train_regret_loser_2[slice8],\r\n",
        "       train_regret_loser_3[slice8],\r\n",
        "       train_regret_loser_4[slice8],\r\n",
        "       train_regret_loser_5[slice8],\r\n",
        "       train_regret_loser_6[slice8],\r\n",
        "       train_regret_loser_7[slice8],\r\n",
        "       train_regret_loser_8[slice8],\r\n",
        "       train_regret_loser_9[slice8],\r\n",
        "       train_regret_loser_10[slice8],\r\n",
        "       train_regret_loser_11[slice8],\r\n",
        "       train_regret_loser_12[slice8],\r\n",
        "       train_regret_loser_13[slice8],\r\n",
        "       train_regret_loser_14[slice8],\r\n",
        "       train_regret_loser_15[slice8],\r\n",
        "       train_regret_loser_16[slice8],\r\n",
        "       train_regret_loser_17[slice8],\r\n",
        "       train_regret_loser_18[slice8],\r\n",
        "       train_regret_loser_19[slice8],\r\n",
        "       train_regret_loser_20[slice8]]\r\n",
        "\r\n",
        "winner8 = [train_regret_winner_1[slice8],\r\n",
        "       train_regret_winner_2[slice8],\r\n",
        "       train_regret_winner_3[slice8],\r\n",
        "       train_regret_winner_4[slice8],\r\n",
        "       train_regret_winner_5[slice8],\r\n",
        "       train_regret_winner_6[slice8],\r\n",
        "       train_regret_winner_7[slice8],\r\n",
        "       train_regret_winner_8[slice8],\r\n",
        "       train_regret_winner_9[slice8],\r\n",
        "       train_regret_winner_10[slice8],\r\n",
        "       train_regret_winner_11[slice8],\r\n",
        "       train_regret_winner_12[slice8],\r\n",
        "       train_regret_winner_13[slice8],\r\n",
        "       train_regret_winner_14[slice8],\r\n",
        "       train_regret_winner_15[slice8],\r\n",
        "       train_regret_winner_16[slice8],\r\n",
        "       train_regret_winner_17[slice8],\r\n",
        "       train_regret_winner_18[slice8],\r\n",
        "       train_regret_winner_19[slice8],\r\n",
        "       train_regret_winner_20[slice8]]\r\n",
        "\r\n",
        "loser8_results = pd.DataFrame(loser8).sort_values(by=[0], ascending=False)\r\n",
        "winner8_results = pd.DataFrame(winner8).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser8 = np.asarray(loser8_results[4:5][0])[0]\r\n",
        "median_loser8 = np.asarray(loser8_results[9:10][0])[0]\r\n",
        "upper_loser8 = np.asarray(loser8_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner8 = np.asarray(winner8_results[4:5][0])[0]\r\n",
        "median_winner8 = np.asarray(winner8_results[9:10][0])[0]\r\n",
        "upper_winner8 = np.asarray(winner8_results[14:15][0])[0]"
      ],
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "im9dHUqvxBH9"
      },
      "source": [
        "# Iteration18 :\r\n",
        "\r\n",
        "slice18 = 17\r\n",
        "\r\n",
        "loser18 = [train_regret_loser_1[slice18],\r\n",
        "       train_regret_loser_2[slice18],\r\n",
        "       train_regret_loser_3[slice18],\r\n",
        "       train_regret_loser_4[slice18],\r\n",
        "       train_regret_loser_5[slice18],\r\n",
        "       train_regret_loser_6[slice18],\r\n",
        "       train_regret_loser_7[slice18],\r\n",
        "       train_regret_loser_8[slice18],\r\n",
        "       train_regret_loser_9[slice18],\r\n",
        "       train_regret_loser_10[slice18],\r\n",
        "       train_regret_loser_11[slice18],\r\n",
        "       train_regret_loser_12[slice18],\r\n",
        "       train_regret_loser_13[slice18],\r\n",
        "       train_regret_loser_14[slice18],\r\n",
        "       train_regret_loser_15[slice18],\r\n",
        "       train_regret_loser_16[slice18],\r\n",
        "       train_regret_loser_17[slice18],\r\n",
        "       train_regret_loser_18[slice18],\r\n",
        "       train_regret_loser_19[slice18],\r\n",
        "       train_regret_loser_20[slice18]]\r\n",
        "\r\n",
        "winner18 = [train_regret_winner_1[slice18],\r\n",
        "       train_regret_winner_2[slice18],\r\n",
        "       train_regret_winner_3[slice18],\r\n",
        "       train_regret_winner_4[slice18],\r\n",
        "       train_regret_winner_5[slice18],\r\n",
        "       train_regret_winner_6[slice18],\r\n",
        "       train_regret_winner_7[slice18],\r\n",
        "       train_regret_winner_8[slice18],\r\n",
        "       train_regret_winner_9[slice18],\r\n",
        "       train_regret_winner_10[slice18],\r\n",
        "       train_regret_winner_11[slice18],\r\n",
        "       train_regret_winner_12[slice18],\r\n",
        "       train_regret_winner_13[slice18],\r\n",
        "       train_regret_winner_14[slice18],\r\n",
        "       train_regret_winner_15[slice18],\r\n",
        "       train_regret_winner_16[slice18],\r\n",
        "       train_regret_winner_17[slice18],\r\n",
        "       train_regret_winner_18[slice18],\r\n",
        "       train_regret_winner_19[slice18],\r\n",
        "       train_regret_winner_20[slice18]]\r\n",
        "\r\n",
        "loser18_results = pd.DataFrame(loser18).sort_values(by=[0], ascending=False)\r\n",
        "winner18_results = pd.DataFrame(winner18).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser18 = np.asarray(loser18_results[4:5][0])[0]\r\n",
        "median_loser18 = np.asarray(loser18_results[9:10][0])[0]\r\n",
        "upper_loser18 = np.asarray(loser18_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner18 = np.asarray(winner18_results[4:5][0])[0]\r\n",
        "median_winner18 = np.asarray(winner18_results[9:10][0])[0]\r\n",
        "upper_winner18 = np.asarray(winner18_results[14:15][0])[0]"
      ],
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IvqFjlBDxBK4"
      },
      "source": [
        "# Iteration28 :\r\n",
        "\r\n",
        "slice28 = 27\r\n",
        "\r\n",
        "loser28 = [train_regret_loser_1[slice28],\r\n",
        "       train_regret_loser_2[slice28],\r\n",
        "       train_regret_loser_3[slice28],\r\n",
        "       train_regret_loser_4[slice28],\r\n",
        "       train_regret_loser_5[slice28],\r\n",
        "       train_regret_loser_6[slice28],\r\n",
        "       train_regret_loser_7[slice28],\r\n",
        "       train_regret_loser_8[slice28],\r\n",
        "       train_regret_loser_9[slice28],\r\n",
        "       train_regret_loser_10[slice28],\r\n",
        "       train_regret_loser_11[slice28],\r\n",
        "       train_regret_loser_12[slice28],\r\n",
        "       train_regret_loser_13[slice28],\r\n",
        "       train_regret_loser_14[slice28],\r\n",
        "       train_regret_loser_15[slice28],\r\n",
        "       train_regret_loser_16[slice28],\r\n",
        "       train_regret_loser_17[slice28],\r\n",
        "       train_regret_loser_18[slice28],\r\n",
        "       train_regret_loser_19[slice28],\r\n",
        "       train_regret_loser_20[slice28]]\r\n",
        "\r\n",
        "winner28 = [train_regret_winner_1[slice28],\r\n",
        "       train_regret_winner_2[slice28],\r\n",
        "       train_regret_winner_3[slice28],\r\n",
        "       train_regret_winner_4[slice28],\r\n",
        "       train_regret_winner_5[slice28],\r\n",
        "       train_regret_winner_6[slice28],\r\n",
        "       train_regret_winner_7[slice28],\r\n",
        "       train_regret_winner_8[slice28],\r\n",
        "       train_regret_winner_9[slice28],\r\n",
        "       train_regret_winner_10[slice28],\r\n",
        "       train_regret_winner_11[slice28],\r\n",
        "       train_regret_winner_12[slice28],\r\n",
        "       train_regret_winner_13[slice28],\r\n",
        "       train_regret_winner_14[slice28],\r\n",
        "       train_regret_winner_15[slice28],\r\n",
        "       train_regret_winner_16[slice28],\r\n",
        "       train_regret_winner_17[slice28],\r\n",
        "       train_regret_winner_18[slice28],\r\n",
        "       train_regret_winner_19[slice28],\r\n",
        "       train_regret_winner_20[slice28]]\r\n",
        "\r\n",
        "loser28_results = pd.DataFrame(loser28).sort_values(by=[0], ascending=False)\r\n",
        "winner28_results = pd.DataFrame(winner28).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser28 = np.asarray(loser28_results[4:5][0])[0]\r\n",
        "median_loser28 = np.asarray(loser28_results[9:10][0])[0]\r\n",
        "upper_loser28 = np.asarray(loser28_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner28 = np.asarray(winner28_results[4:5][0])[0]\r\n",
        "median_winner28 = np.asarray(winner28_results[9:10][0])[0]\r\n",
        "upper_winner28 = np.asarray(winner28_results[14:15][0])[0]"
      ],
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "It5daJCEted3"
      },
      "source": [
        "# Iteration38 :\r\n",
        "\r\n",
        "slice38 = 37\r\n",
        "\r\n",
        "loser38 = [train_regret_loser_1[slice38],\r\n",
        "       train_regret_loser_2[slice38],\r\n",
        "       train_regret_loser_3[slice38],\r\n",
        "       train_regret_loser_4[slice38],\r\n",
        "       train_regret_loser_5[slice38],\r\n",
        "       train_regret_loser_6[slice38],\r\n",
        "       train_regret_loser_7[slice38],\r\n",
        "       train_regret_loser_8[slice38],\r\n",
        "       train_regret_loser_9[slice38],\r\n",
        "       train_regret_loser_10[slice38],\r\n",
        "       train_regret_loser_11[slice38],\r\n",
        "       train_regret_loser_12[slice38],\r\n",
        "       train_regret_loser_13[slice38],\r\n",
        "       train_regret_loser_14[slice38],\r\n",
        "       train_regret_loser_15[slice38],\r\n",
        "       train_regret_loser_16[slice38],\r\n",
        "       train_regret_loser_17[slice38],\r\n",
        "       train_regret_loser_18[slice38],\r\n",
        "       train_regret_loser_19[slice38],\r\n",
        "       train_regret_loser_20[slice38]]\r\n",
        "\r\n",
        "winner38 = [train_regret_winner_1[slice38],\r\n",
        "       train_regret_winner_2[slice38],\r\n",
        "       train_regret_winner_3[slice38],\r\n",
        "       train_regret_winner_4[slice38],\r\n",
        "       train_regret_winner_5[slice38],\r\n",
        "       train_regret_winner_6[slice38],\r\n",
        "       train_regret_winner_7[slice38],\r\n",
        "       train_regret_winner_8[slice38],\r\n",
        "       train_regret_winner_9[slice38],\r\n",
        "       train_regret_winner_10[slice38],\r\n",
        "       train_regret_winner_11[slice38],\r\n",
        "       train_regret_winner_12[slice38],\r\n",
        "       train_regret_winner_13[slice38],\r\n",
        "       train_regret_winner_14[slice38],\r\n",
        "       train_regret_winner_15[slice38],\r\n",
        "       train_regret_winner_16[slice38],\r\n",
        "       train_regret_winner_17[slice38],\r\n",
        "       train_regret_winner_18[slice38],\r\n",
        "       train_regret_winner_19[slice38],\r\n",
        "       train_regret_winner_20[slice38]]\r\n",
        "\r\n",
        "loser38_results = pd.DataFrame(loser38).sort_values(by=[0], ascending=False)\r\n",
        "winner38_results = pd.DataFrame(winner38).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser38 = np.asarray(loser38_results[4:5][0])[0]\r\n",
        "median_loser38 = np.asarray(loser38_results[9:10][0])[0]\r\n",
        "upper_loser38 = np.asarray(loser38_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner38 = np.asarray(winner38_results[4:5][0])[0]\r\n",
        "median_winner38 = np.asarray(winner38_results[9:10][0])[0]\r\n",
        "upper_winner38 = np.asarray(winner38_results[14:15][0])[0]"
      ],
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GekXOf_gtegw"
      },
      "source": [
        "# Iteration48 :\r\n",
        "\r\n",
        "slice48 = 47\r\n",
        "\r\n",
        "loser48 = [train_regret_loser_1[slice48],\r\n",
        "       train_regret_loser_2[slice48],\r\n",
        "       train_regret_loser_3[slice48],\r\n",
        "       train_regret_loser_4[slice48],\r\n",
        "       train_regret_loser_5[slice48],\r\n",
        "       train_regret_loser_6[slice48],\r\n",
        "       train_regret_loser_7[slice48],\r\n",
        "       train_regret_loser_8[slice48],\r\n",
        "       train_regret_loser_9[slice48],\r\n",
        "       train_regret_loser_10[slice48],\r\n",
        "       train_regret_loser_11[slice48],\r\n",
        "       train_regret_loser_12[slice48],\r\n",
        "       train_regret_loser_13[slice48],\r\n",
        "       train_regret_loser_14[slice48],\r\n",
        "       train_regret_loser_15[slice48],\r\n",
        "       train_regret_loser_16[slice48],\r\n",
        "       train_regret_loser_17[slice48],\r\n",
        "       train_regret_loser_18[slice48],\r\n",
        "       train_regret_loser_19[slice48],\r\n",
        "       train_regret_loser_20[slice48]]\r\n",
        "\r\n",
        "winner48 = [train_regret_winner_1[slice48],\r\n",
        "       train_regret_winner_2[slice48],\r\n",
        "       train_regret_winner_3[slice48],\r\n",
        "       train_regret_winner_4[slice48],\r\n",
        "       train_regret_winner_5[slice48],\r\n",
        "       train_regret_winner_6[slice48],\r\n",
        "       train_regret_winner_7[slice48],\r\n",
        "       train_regret_winner_8[slice48],\r\n",
        "       train_regret_winner_9[slice48],\r\n",
        "       train_regret_winner_10[slice48],\r\n",
        "       train_regret_winner_11[slice48],\r\n",
        "       train_regret_winner_12[slice48],\r\n",
        "       train_regret_winner_13[slice48],\r\n",
        "       train_regret_winner_14[slice48],\r\n",
        "       train_regret_winner_15[slice48],\r\n",
        "       train_regret_winner_16[slice48],\r\n",
        "       train_regret_winner_17[slice48],\r\n",
        "       train_regret_winner_18[slice48],\r\n",
        "       train_regret_winner_19[slice48],\r\n",
        "       train_regret_winner_20[slice48]]\r\n",
        "\r\n",
        "loser48_results = pd.DataFrame(loser48).sort_values(by=[0], ascending=False)\r\n",
        "winner48_results = pd.DataFrame(winner48).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser48 = np.asarray(loser48_results[4:5][0])[0]\r\n",
        "median_loser48 = np.asarray(loser48_results[9:10][0])[0]\r\n",
        "upper_loser48 = np.asarray(loser48_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner48 = np.asarray(winner48_results[4:5][0])[0]\r\n",
        "median_winner48 = np.asarray(winner48_results[9:10][0])[0]\r\n",
        "upper_winner48 = np.asarray(winner48_results[14:15][0])[0]"
      ],
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNpV6knOtejv"
      },
      "source": [
        "# Iteration58 :\r\n",
        "\r\n",
        "slice58 = 57\r\n",
        "\r\n",
        "loser58 = [train_regret_loser_1[slice58],\r\n",
        "       train_regret_loser_2[slice58],\r\n",
        "       train_regret_loser_3[slice58],\r\n",
        "       train_regret_loser_4[slice58],\r\n",
        "       train_regret_loser_5[slice58],\r\n",
        "       train_regret_loser_6[slice58],\r\n",
        "       train_regret_loser_7[slice58],\r\n",
        "       train_regret_loser_8[slice58],\r\n",
        "       train_regret_loser_9[slice58],\r\n",
        "       train_regret_loser_10[slice58],\r\n",
        "       train_regret_loser_11[slice58],\r\n",
        "       train_regret_loser_12[slice58],\r\n",
        "       train_regret_loser_13[slice58],\r\n",
        "       train_regret_loser_14[slice58],\r\n",
        "       train_regret_loser_15[slice58],\r\n",
        "       train_regret_loser_16[slice58],\r\n",
        "       train_regret_loser_17[slice58],\r\n",
        "       train_regret_loser_18[slice58],\r\n",
        "       train_regret_loser_19[slice58],\r\n",
        "       train_regret_loser_20[slice58]]\r\n",
        "\r\n",
        "winner58 = [train_regret_winner_1[slice58],\r\n",
        "       train_regret_winner_2[slice58],\r\n",
        "       train_regret_winner_3[slice58],\r\n",
        "       train_regret_winner_4[slice58],\r\n",
        "       train_regret_winner_5[slice58],\r\n",
        "       train_regret_winner_6[slice58],\r\n",
        "       train_regret_winner_7[slice58],\r\n",
        "       train_regret_winner_8[slice58],\r\n",
        "       train_regret_winner_9[slice58],\r\n",
        "       train_regret_winner_10[slice58],\r\n",
        "       train_regret_winner_11[slice58],\r\n",
        "       train_regret_winner_12[slice58],\r\n",
        "       train_regret_winner_13[slice58],\r\n",
        "       train_regret_winner_14[slice58],\r\n",
        "       train_regret_winner_15[slice58],\r\n",
        "       train_regret_winner_16[slice58],\r\n",
        "       train_regret_winner_17[slice58],\r\n",
        "       train_regret_winner_18[slice58],\r\n",
        "       train_regret_winner_19[slice58],\r\n",
        "       train_regret_winner_20[slice58]]\r\n",
        "\r\n",
        "loser58_results = pd.DataFrame(loser58).sort_values(by=[0], ascending=False)\r\n",
        "winner58_results = pd.DataFrame(winner58).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser58 = np.asarray(loser58_results[4:5][0])[0]\r\n",
        "median_loser58 = np.asarray(loser58_results[9:10][0])[0]\r\n",
        "upper_loser58 = np.asarray(loser58_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner58 = np.asarray(winner58_results[4:5][0])[0]\r\n",
        "median_winner58 = np.asarray(winner58_results[9:10][0])[0]\r\n",
        "upper_winner58 = np.asarray(winner58_results[14:15][0])[0]"
      ],
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VFsqaBqtemn"
      },
      "source": [
        "# Iteration68 :\r\n",
        "\r\n",
        "slice68 = 67\r\n",
        "\r\n",
        "loser68 = [train_regret_loser_1[slice68],\r\n",
        "       train_regret_loser_2[slice68],\r\n",
        "       train_regret_loser_3[slice68],\r\n",
        "       train_regret_loser_4[slice68],\r\n",
        "       train_regret_loser_5[slice68],\r\n",
        "       train_regret_loser_6[slice68],\r\n",
        "       train_regret_loser_7[slice68],\r\n",
        "       train_regret_loser_8[slice68],\r\n",
        "       train_regret_loser_9[slice68],\r\n",
        "       train_regret_loser_10[slice68],\r\n",
        "       train_regret_loser_11[slice68],\r\n",
        "       train_regret_loser_12[slice68],\r\n",
        "       train_regret_loser_13[slice68],\r\n",
        "       train_regret_loser_14[slice68],\r\n",
        "       train_regret_loser_15[slice68],\r\n",
        "       train_regret_loser_16[slice68],\r\n",
        "       train_regret_loser_17[slice68],\r\n",
        "       train_regret_loser_18[slice68],\r\n",
        "       train_regret_loser_19[slice68],\r\n",
        "       train_regret_loser_20[slice68]]\r\n",
        "\r\n",
        "winner68 = [train_regret_winner_1[slice68],\r\n",
        "       train_regret_winner_2[slice68],\r\n",
        "       train_regret_winner_3[slice68],\r\n",
        "       train_regret_winner_4[slice68],\r\n",
        "       train_regret_winner_5[slice68],\r\n",
        "       train_regret_winner_6[slice68],\r\n",
        "       train_regret_winner_7[slice68],\r\n",
        "       train_regret_winner_8[slice68],\r\n",
        "       train_regret_winner_9[slice68],\r\n",
        "       train_regret_winner_10[slice68],\r\n",
        "       train_regret_winner_11[slice68],\r\n",
        "       train_regret_winner_12[slice68],\r\n",
        "       train_regret_winner_13[slice68],\r\n",
        "       train_regret_winner_14[slice68],\r\n",
        "       train_regret_winner_15[slice68],\r\n",
        "       train_regret_winner_16[slice68],\r\n",
        "       train_regret_winner_17[slice68],\r\n",
        "       train_regret_winner_18[slice68],\r\n",
        "       train_regret_winner_19[slice68],\r\n",
        "       train_regret_winner_20[slice68]]\r\n",
        "\r\n",
        "loser68_results = pd.DataFrame(loser68).sort_values(by=[0], ascending=False)\r\n",
        "winner68_results = pd.DataFrame(winner68).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser68 = np.asarray(loser68_results[4:5][0])[0]\r\n",
        "median_loser68 = np.asarray(loser68_results[9:10][0])[0]\r\n",
        "upper_loser68 = np.asarray(loser68_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner68 = np.asarray(winner68_results[4:5][0])[0]\r\n",
        "median_winner68 = np.asarray(winner68_results[9:10][0])[0]\r\n",
        "upper_winner68 = np.asarray(winner68_results[14:15][0])[0]"
      ],
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XlFH0Wxtepo"
      },
      "source": [
        "# Iteration78 :\r\n",
        "\r\n",
        "slice78 = 77\r\n",
        "\r\n",
        "loser78 = [train_regret_loser_1[slice78],\r\n",
        "       train_regret_loser_2[slice78],\r\n",
        "       train_regret_loser_3[slice78],\r\n",
        "       train_regret_loser_4[slice78],\r\n",
        "       train_regret_loser_5[slice78],\r\n",
        "       train_regret_loser_6[slice78],\r\n",
        "       train_regret_loser_7[slice78],\r\n",
        "       train_regret_loser_8[slice78],\r\n",
        "       train_regret_loser_9[slice78],\r\n",
        "       train_regret_loser_10[slice78],\r\n",
        "       train_regret_loser_11[slice78],\r\n",
        "       train_regret_loser_12[slice78],\r\n",
        "       train_regret_loser_13[slice78],\r\n",
        "       train_regret_loser_14[slice78],\r\n",
        "       train_regret_loser_15[slice78],\r\n",
        "       train_regret_loser_16[slice78],\r\n",
        "       train_regret_loser_17[slice78],\r\n",
        "       train_regret_loser_18[slice78],\r\n",
        "       train_regret_loser_19[slice78],\r\n",
        "       train_regret_loser_20[slice78]]\r\n",
        "\r\n",
        "winner78 = [train_regret_winner_1[slice78],\r\n",
        "       train_regret_winner_2[slice78],\r\n",
        "       train_regret_winner_3[slice78],\r\n",
        "       train_regret_winner_4[slice78],\r\n",
        "       train_regret_winner_5[slice78],\r\n",
        "       train_regret_winner_6[slice78],\r\n",
        "       train_regret_winner_7[slice78],\r\n",
        "       train_regret_winner_8[slice78],\r\n",
        "       train_regret_winner_9[slice78],\r\n",
        "       train_regret_winner_10[slice78],\r\n",
        "       train_regret_winner_11[slice78],\r\n",
        "       train_regret_winner_12[slice78],\r\n",
        "       train_regret_winner_13[slice78],\r\n",
        "       train_regret_winner_14[slice78],\r\n",
        "       train_regret_winner_15[slice78],\r\n",
        "       train_regret_winner_16[slice78],\r\n",
        "       train_regret_winner_17[slice78],\r\n",
        "       train_regret_winner_18[slice78],\r\n",
        "       train_regret_winner_19[slice78],\r\n",
        "       train_regret_winner_20[slice78]]\r\n",
        "\r\n",
        "loser78_results = pd.DataFrame(loser78).sort_values(by=[0], ascending=False)\r\n",
        "winner78_results = pd.DataFrame(winner78).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser78 = np.asarray(loser78_results[4:5][0])[0]\r\n",
        "median_loser78 = np.asarray(loser78_results[9:10][0])[0]\r\n",
        "upper_loser78 = np.asarray(loser78_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner78 = np.asarray(winner78_results[4:5][0])[0]\r\n",
        "median_winner78 = np.asarray(winner78_results[9:10][0])[0]\r\n",
        "upper_winner78 = np.asarray(winner78_results[14:15][0])[0]"
      ],
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsL0PJVetesC"
      },
      "source": [
        "# Iteration88 :\r\n",
        "\r\n",
        "slice88 = 87\r\n",
        "\r\n",
        "loser88 = [train_regret_loser_1[slice88],\r\n",
        "       train_regret_loser_2[slice88],\r\n",
        "       train_regret_loser_3[slice88],\r\n",
        "       train_regret_loser_4[slice88],\r\n",
        "       train_regret_loser_5[slice88],\r\n",
        "       train_regret_loser_6[slice88],\r\n",
        "       train_regret_loser_7[slice88],\r\n",
        "       train_regret_loser_8[slice88],\r\n",
        "       train_regret_loser_9[slice88],\r\n",
        "       train_regret_loser_10[slice88],\r\n",
        "       train_regret_loser_11[slice88],\r\n",
        "       train_regret_loser_12[slice88],\r\n",
        "       train_regret_loser_13[slice88],\r\n",
        "       train_regret_loser_14[slice88],\r\n",
        "       train_regret_loser_15[slice88],\r\n",
        "       train_regret_loser_16[slice88],\r\n",
        "       train_regret_loser_17[slice88],\r\n",
        "       train_regret_loser_18[slice88],\r\n",
        "       train_regret_loser_19[slice88],\r\n",
        "       train_regret_loser_20[slice88]]\r\n",
        "\r\n",
        "winner88 = [train_regret_winner_1[slice88],\r\n",
        "       train_regret_winner_2[slice88],\r\n",
        "       train_regret_winner_3[slice88],\r\n",
        "       train_regret_winner_4[slice88],\r\n",
        "       train_regret_winner_5[slice88],\r\n",
        "       train_regret_winner_6[slice88],\r\n",
        "       train_regret_winner_7[slice88],\r\n",
        "       train_regret_winner_8[slice88],\r\n",
        "       train_regret_winner_9[slice88],\r\n",
        "       train_regret_winner_10[slice88],\r\n",
        "       train_regret_winner_11[slice88],\r\n",
        "       train_regret_winner_12[slice88],\r\n",
        "       train_regret_winner_13[slice88],\r\n",
        "       train_regret_winner_14[slice88],\r\n",
        "       train_regret_winner_15[slice88],\r\n",
        "       train_regret_winner_16[slice88],\r\n",
        "       train_regret_winner_17[slice88],\r\n",
        "       train_regret_winner_18[slice88],\r\n",
        "       train_regret_winner_19[slice88],\r\n",
        "       train_regret_winner_20[slice88]]\r\n",
        "\r\n",
        "loser88_results = pd.DataFrame(loser88).sort_values(by=[0], ascending=False)\r\n",
        "winner88_results = pd.DataFrame(winner88).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser88 = np.asarray(loser88_results[4:5][0])[0]\r\n",
        "median_loser88 = np.asarray(loser88_results[9:10][0])[0]\r\n",
        "upper_loser88 = np.asarray(loser88_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner88 = np.asarray(winner88_results[4:5][0])[0]\r\n",
        "median_winner88 = np.asarray(winner88_results[9:10][0])[0]\r\n",
        "upper_winner88 = np.asarray(winner88_results[14:15][0])[0]"
      ],
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYWEWGku1o_9"
      },
      "source": [
        "# Iteration98 :\r\n",
        "\r\n",
        "slice98 = 97\r\n",
        "\r\n",
        "loser98 = [train_regret_loser_1[slice98],\r\n",
        "       train_regret_loser_2[slice98],\r\n",
        "       train_regret_loser_3[slice98],\r\n",
        "       train_regret_loser_4[slice98],\r\n",
        "       train_regret_loser_5[slice98],\r\n",
        "       train_regret_loser_6[slice98],\r\n",
        "       train_regret_loser_7[slice98],\r\n",
        "       train_regret_loser_8[slice98],\r\n",
        "       train_regret_loser_9[slice98],\r\n",
        "       train_regret_loser_10[slice98],\r\n",
        "       train_regret_loser_11[slice98],\r\n",
        "       train_regret_loser_12[slice98],\r\n",
        "       train_regret_loser_13[slice98],\r\n",
        "       train_regret_loser_14[slice98],\r\n",
        "       train_regret_loser_15[slice98],\r\n",
        "       train_regret_loser_16[slice98],\r\n",
        "       train_regret_loser_17[slice98],\r\n",
        "       train_regret_loser_18[slice98],\r\n",
        "       train_regret_loser_19[slice98],\r\n",
        "       train_regret_loser_20[slice98]]\r\n",
        "\r\n",
        "winner98 = [train_regret_winner_1[slice98],\r\n",
        "       train_regret_winner_2[slice98],\r\n",
        "       train_regret_winner_3[slice98],\r\n",
        "       train_regret_winner_4[slice98],\r\n",
        "       train_regret_winner_5[slice98],\r\n",
        "       train_regret_winner_6[slice98],\r\n",
        "       train_regret_winner_7[slice98],\r\n",
        "       train_regret_winner_8[slice98],\r\n",
        "       train_regret_winner_9[slice98],\r\n",
        "       train_regret_winner_10[slice98],\r\n",
        "       train_regret_winner_11[slice98],\r\n",
        "       train_regret_winner_12[slice98],\r\n",
        "       train_regret_winner_13[slice98],\r\n",
        "       train_regret_winner_14[slice98],\r\n",
        "       train_regret_winner_15[slice98],\r\n",
        "       train_regret_winner_16[slice98],\r\n",
        "       train_regret_winner_17[slice98],\r\n",
        "       train_regret_winner_18[slice98],\r\n",
        "       train_regret_winner_19[slice98],\r\n",
        "       train_regret_winner_20[slice98]]\r\n",
        "\r\n",
        "loser98_results = pd.DataFrame(loser98).sort_values(by=[0], ascending=False)\r\n",
        "winner98_results = pd.DataFrame(winner98).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser98 = np.asarray(loser98_results[4:5][0])[0]\r\n",
        "median_loser98 = np.asarray(loser98_results[9:10][0])[0]\r\n",
        "upper_loser98 = np.asarray(loser98_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner98 = np.asarray(winner98_results[4:5][0])[0]\r\n",
        "median_winner98 = np.asarray(winner98_results[9:10][0])[0]\r\n",
        "upper_winner98 = np.asarray(winner98_results[14:15][0])[0]"
      ],
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "guLGgcX91pDK"
      },
      "source": [
        "# Iteration9 :\r\n",
        "\r\n",
        "slice9 = 8\r\n",
        "\r\n",
        "loser9 = [train_regret_loser_1[slice9],\r\n",
        "       train_regret_loser_2[slice9],\r\n",
        "       train_regret_loser_3[slice9],\r\n",
        "       train_regret_loser_4[slice9],\r\n",
        "       train_regret_loser_5[slice9],\r\n",
        "       train_regret_loser_6[slice9],\r\n",
        "       train_regret_loser_7[slice9],\r\n",
        "       train_regret_loser_8[slice9],\r\n",
        "       train_regret_loser_9[slice9],\r\n",
        "       train_regret_loser_10[slice9],\r\n",
        "       train_regret_loser_11[slice9],\r\n",
        "       train_regret_loser_12[slice9],\r\n",
        "       train_regret_loser_13[slice9],\r\n",
        "       train_regret_loser_14[slice9],\r\n",
        "       train_regret_loser_15[slice9],\r\n",
        "       train_regret_loser_16[slice9],\r\n",
        "       train_regret_loser_17[slice9],\r\n",
        "       train_regret_loser_18[slice9],\r\n",
        "       train_regret_loser_19[slice9],\r\n",
        "       train_regret_loser_20[slice9]]\r\n",
        "\r\n",
        "winner9 = [train_regret_winner_1[slice9],\r\n",
        "       train_regret_winner_2[slice9],\r\n",
        "       train_regret_winner_3[slice9],\r\n",
        "       train_regret_winner_4[slice9],\r\n",
        "       train_regret_winner_5[slice9],\r\n",
        "       train_regret_winner_6[slice9],\r\n",
        "       train_regret_winner_7[slice9],\r\n",
        "       train_regret_winner_8[slice9],\r\n",
        "       train_regret_winner_9[slice9],\r\n",
        "       train_regret_winner_10[slice9],\r\n",
        "       train_regret_winner_11[slice9],\r\n",
        "       train_regret_winner_12[slice9],\r\n",
        "       train_regret_winner_13[slice9],\r\n",
        "       train_regret_winner_14[slice9],\r\n",
        "       train_regret_winner_15[slice9],\r\n",
        "       train_regret_winner_16[slice9],\r\n",
        "       train_regret_winner_17[slice9],\r\n",
        "       train_regret_winner_18[slice9],\r\n",
        "       train_regret_winner_19[slice9],\r\n",
        "       train_regret_winner_20[slice9]]\r\n",
        "\r\n",
        "loser9_results = pd.DataFrame(loser9).sort_values(by=[0], ascending=False)\r\n",
        "winner9_results = pd.DataFrame(winner9).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser9 = np.asarray(loser9_results[4:5][0])[0]\r\n",
        "median_loser9 = np.asarray(loser9_results[9:10][0])[0]\r\n",
        "upper_loser9 = np.asarray(loser9_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner9 = np.asarray(winner9_results[4:5][0])[0]\r\n",
        "median_winner9 = np.asarray(winner9_results[9:10][0])[0]\r\n",
        "upper_winner9 = np.asarray(winner9_results[14:15][0])[0]"
      ],
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3_9-eoT1pGd"
      },
      "source": [
        "# Iteration19 :\r\n",
        "\r\n",
        "slice19 = 18\r\n",
        "\r\n",
        "loser19 = [train_regret_loser_1[slice19],\r\n",
        "       train_regret_loser_2[slice19],\r\n",
        "       train_regret_loser_3[slice19],\r\n",
        "       train_regret_loser_4[slice19],\r\n",
        "       train_regret_loser_5[slice19],\r\n",
        "       train_regret_loser_6[slice19],\r\n",
        "       train_regret_loser_7[slice19],\r\n",
        "       train_regret_loser_8[slice19],\r\n",
        "       train_regret_loser_9[slice19],\r\n",
        "       train_regret_loser_10[slice19],\r\n",
        "       train_regret_loser_11[slice19],\r\n",
        "       train_regret_loser_12[slice19],\r\n",
        "       train_regret_loser_13[slice19],\r\n",
        "       train_regret_loser_14[slice19],\r\n",
        "       train_regret_loser_15[slice19],\r\n",
        "       train_regret_loser_16[slice19],\r\n",
        "       train_regret_loser_17[slice19],\r\n",
        "       train_regret_loser_18[slice19],\r\n",
        "       train_regret_loser_19[slice19],\r\n",
        "       train_regret_loser_20[slice19]]\r\n",
        "\r\n",
        "winner19 = [train_regret_winner_1[slice19],\r\n",
        "       train_regret_winner_2[slice19],\r\n",
        "       train_regret_winner_3[slice19],\r\n",
        "       train_regret_winner_4[slice19],\r\n",
        "       train_regret_winner_5[slice19],\r\n",
        "       train_regret_winner_6[slice19],\r\n",
        "       train_regret_winner_7[slice19],\r\n",
        "       train_regret_winner_8[slice19],\r\n",
        "       train_regret_winner_9[slice19],\r\n",
        "       train_regret_winner_10[slice19],\r\n",
        "       train_regret_winner_11[slice19],\r\n",
        "       train_regret_winner_12[slice19],\r\n",
        "       train_regret_winner_13[slice19],\r\n",
        "       train_regret_winner_14[slice19],\r\n",
        "       train_regret_winner_15[slice19],\r\n",
        "       train_regret_winner_16[slice19],\r\n",
        "       train_regret_winner_17[slice19],\r\n",
        "       train_regret_winner_18[slice19],\r\n",
        "       train_regret_winner_19[slice19],\r\n",
        "       train_regret_winner_20[slice19]]\r\n",
        "\r\n",
        "loser19_results = pd.DataFrame(loser19).sort_values(by=[0], ascending=False)\r\n",
        "winner19_results = pd.DataFrame(winner19).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser19 = np.asarray(loser19_results[4:5][0])[0]\r\n",
        "median_loser19 = np.asarray(loser19_results[9:10][0])[0]\r\n",
        "upper_loser19 = np.asarray(loser19_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner19 = np.asarray(winner19_results[4:5][0])[0]\r\n",
        "median_winner19 = np.asarray(winner19_results[9:10][0])[0]\r\n",
        "upper_winner19 = np.asarray(winner19_results[14:15][0])[0]"
      ],
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ae5x13M1pRp"
      },
      "source": [
        "# Iteration29 :\r\n",
        "\r\n",
        "slice29 = 28\r\n",
        "\r\n",
        "loser29 = [train_regret_loser_1[slice29],\r\n",
        "       train_regret_loser_2[slice29],\r\n",
        "       train_regret_loser_3[slice29],\r\n",
        "       train_regret_loser_4[slice29],\r\n",
        "       train_regret_loser_5[slice29],\r\n",
        "       train_regret_loser_6[slice29],\r\n",
        "       train_regret_loser_7[slice29],\r\n",
        "       train_regret_loser_8[slice29],\r\n",
        "       train_regret_loser_9[slice29],\r\n",
        "       train_regret_loser_10[slice29],\r\n",
        "       train_regret_loser_11[slice29],\r\n",
        "       train_regret_loser_12[slice29],\r\n",
        "       train_regret_loser_13[slice29],\r\n",
        "       train_regret_loser_14[slice29],\r\n",
        "       train_regret_loser_15[slice29],\r\n",
        "       train_regret_loser_16[slice29],\r\n",
        "       train_regret_loser_17[slice29],\r\n",
        "       train_regret_loser_18[slice29],\r\n",
        "       train_regret_loser_19[slice29],\r\n",
        "       train_regret_loser_20[slice29]]\r\n",
        "\r\n",
        "winner29 = [train_regret_winner_1[slice29],\r\n",
        "       train_regret_winner_2[slice29],\r\n",
        "       train_regret_winner_3[slice29],\r\n",
        "       train_regret_winner_4[slice29],\r\n",
        "       train_regret_winner_5[slice29],\r\n",
        "       train_regret_winner_6[slice29],\r\n",
        "       train_regret_winner_7[slice29],\r\n",
        "       train_regret_winner_8[slice29],\r\n",
        "       train_regret_winner_9[slice29],\r\n",
        "       train_regret_winner_10[slice29],\r\n",
        "       train_regret_winner_11[slice29],\r\n",
        "       train_regret_winner_12[slice29],\r\n",
        "       train_regret_winner_13[slice29],\r\n",
        "       train_regret_winner_14[slice29],\r\n",
        "       train_regret_winner_15[slice29],\r\n",
        "       train_regret_winner_16[slice29],\r\n",
        "       train_regret_winner_17[slice29],\r\n",
        "       train_regret_winner_18[slice29],\r\n",
        "       train_regret_winner_19[slice29],\r\n",
        "       train_regret_winner_20[slice29]]\r\n",
        "\r\n",
        "loser29_results = pd.DataFrame(loser29).sort_values(by=[0], ascending=False)\r\n",
        "winner29_results = pd.DataFrame(winner29).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser29 = np.asarray(loser29_results[4:5][0])[0]\r\n",
        "median_loser29 = np.asarray(loser29_results[9:10][0])[0]\r\n",
        "upper_loser29 = np.asarray(loser29_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner29 = np.asarray(winner29_results[4:5][0])[0]\r\n",
        "median_winner29 = np.asarray(winner29_results[9:10][0])[0]\r\n",
        "upper_winner29 = np.asarray(winner29_results[14:15][0])[0]"
      ],
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XR9aPvBy1pUc"
      },
      "source": [
        "# Iteration39 :\r\n",
        "\r\n",
        "slice39 = 38\r\n",
        "\r\n",
        "loser39 = [train_regret_loser_1[slice39],\r\n",
        "       train_regret_loser_2[slice39],\r\n",
        "       train_regret_loser_3[slice39],\r\n",
        "       train_regret_loser_4[slice39],\r\n",
        "       train_regret_loser_5[slice39],\r\n",
        "       train_regret_loser_6[slice39],\r\n",
        "       train_regret_loser_7[slice39],\r\n",
        "       train_regret_loser_8[slice39],\r\n",
        "       train_regret_loser_9[slice39],\r\n",
        "       train_regret_loser_10[slice39],\r\n",
        "       train_regret_loser_11[slice39],\r\n",
        "       train_regret_loser_12[slice39],\r\n",
        "       train_regret_loser_13[slice39],\r\n",
        "       train_regret_loser_14[slice39],\r\n",
        "       train_regret_loser_15[slice39],\r\n",
        "       train_regret_loser_16[slice39],\r\n",
        "       train_regret_loser_17[slice39],\r\n",
        "       train_regret_loser_18[slice39],\r\n",
        "       train_regret_loser_19[slice39],\r\n",
        "       train_regret_loser_20[slice39]]\r\n",
        "\r\n",
        "winner39 = [train_regret_winner_1[slice39],\r\n",
        "       train_regret_winner_2[slice39],\r\n",
        "       train_regret_winner_3[slice39],\r\n",
        "       train_regret_winner_4[slice39],\r\n",
        "       train_regret_winner_5[slice39],\r\n",
        "       train_regret_winner_6[slice39],\r\n",
        "       train_regret_winner_7[slice39],\r\n",
        "       train_regret_winner_8[slice39],\r\n",
        "       train_regret_winner_9[slice39],\r\n",
        "       train_regret_winner_10[slice39],\r\n",
        "       train_regret_winner_11[slice39],\r\n",
        "       train_regret_winner_12[slice39],\r\n",
        "       train_regret_winner_13[slice39],\r\n",
        "       train_regret_winner_14[slice39],\r\n",
        "       train_regret_winner_15[slice39],\r\n",
        "       train_regret_winner_16[slice39],\r\n",
        "       train_regret_winner_17[slice39],\r\n",
        "       train_regret_winner_18[slice39],\r\n",
        "       train_regret_winner_19[slice39],\r\n",
        "       train_regret_winner_20[slice39]]\r\n",
        "\r\n",
        "loser39_results = pd.DataFrame(loser39).sort_values(by=[0], ascending=False)\r\n",
        "winner39_results = pd.DataFrame(winner39).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser39 = np.asarray(loser39_results[4:5][0])[0]\r\n",
        "median_loser39 = np.asarray(loser39_results[9:10][0])[0]\r\n",
        "upper_loser39 = np.asarray(loser39_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner39 = np.asarray(winner39_results[4:5][0])[0]\r\n",
        "median_winner39 = np.asarray(winner39_results[9:10][0])[0]\r\n",
        "upper_winner39 = np.asarray(winner39_results[14:15][0])[0]"
      ],
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vb7vh2K1pXp"
      },
      "source": [
        "# Iteration49 :\r\n",
        "\r\n",
        "slice49 = 48\r\n",
        "\r\n",
        "loser49 = [train_regret_loser_1[slice49],\r\n",
        "       train_regret_loser_2[slice49],\r\n",
        "       train_regret_loser_3[slice49],\r\n",
        "       train_regret_loser_4[slice49],\r\n",
        "       train_regret_loser_5[slice49],\r\n",
        "       train_regret_loser_6[slice49],\r\n",
        "       train_regret_loser_7[slice49],\r\n",
        "       train_regret_loser_8[slice49],\r\n",
        "       train_regret_loser_9[slice49],\r\n",
        "       train_regret_loser_10[slice49],\r\n",
        "       train_regret_loser_11[slice49],\r\n",
        "       train_regret_loser_12[slice49],\r\n",
        "       train_regret_loser_13[slice49],\r\n",
        "       train_regret_loser_14[slice49],\r\n",
        "       train_regret_loser_15[slice49],\r\n",
        "       train_regret_loser_16[slice49],\r\n",
        "       train_regret_loser_17[slice49],\r\n",
        "       train_regret_loser_18[slice49],\r\n",
        "       train_regret_loser_19[slice49],\r\n",
        "       train_regret_loser_20[slice49]]\r\n",
        "\r\n",
        "winner49 = [train_regret_winner_1[slice49],\r\n",
        "       train_regret_winner_2[slice49],\r\n",
        "       train_regret_winner_3[slice49],\r\n",
        "       train_regret_winner_4[slice49],\r\n",
        "       train_regret_winner_5[slice49],\r\n",
        "       train_regret_winner_6[slice49],\r\n",
        "       train_regret_winner_7[slice49],\r\n",
        "       train_regret_winner_8[slice49],\r\n",
        "       train_regret_winner_9[slice49],\r\n",
        "       train_regret_winner_10[slice49],\r\n",
        "       train_regret_winner_11[slice49],\r\n",
        "       train_regret_winner_12[slice49],\r\n",
        "       train_regret_winner_13[slice49],\r\n",
        "       train_regret_winner_14[slice49],\r\n",
        "       train_regret_winner_15[slice49],\r\n",
        "       train_regret_winner_16[slice49],\r\n",
        "       train_regret_winner_17[slice49],\r\n",
        "       train_regret_winner_18[slice49],\r\n",
        "       train_regret_winner_19[slice49],\r\n",
        "       train_regret_winner_20[slice49]]\r\n",
        "\r\n",
        "loser49_results = pd.DataFrame(loser49).sort_values(by=[0], ascending=False)\r\n",
        "winner49_results = pd.DataFrame(winner49).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser49 = np.asarray(loser49_results[4:5][0])[0]\r\n",
        "median_loser49 = np.asarray(loser49_results[9:10][0])[0]\r\n",
        "upper_loser49 = np.asarray(loser49_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner49 = np.asarray(winner49_results[4:5][0])[0]\r\n",
        "median_winner49 = np.asarray(winner49_results[9:10][0])[0]\r\n",
        "upper_winner49 = np.asarray(winner49_results[14:15][0])[0]"
      ],
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3wN7DWwt1paM"
      },
      "source": [
        "# Iteration59 :\r\n",
        "\r\n",
        "slice59 = 58\r\n",
        "\r\n",
        "loser59 = [train_regret_loser_1[slice59],\r\n",
        "       train_regret_loser_2[slice59],\r\n",
        "       train_regret_loser_3[slice59],\r\n",
        "       train_regret_loser_4[slice59],\r\n",
        "       train_regret_loser_5[slice59],\r\n",
        "       train_regret_loser_6[slice59],\r\n",
        "       train_regret_loser_7[slice59],\r\n",
        "       train_regret_loser_8[slice59],\r\n",
        "       train_regret_loser_9[slice59],\r\n",
        "       train_regret_loser_10[slice59],\r\n",
        "       train_regret_loser_11[slice59],\r\n",
        "       train_regret_loser_12[slice59],\r\n",
        "       train_regret_loser_13[slice59],\r\n",
        "       train_regret_loser_14[slice59],\r\n",
        "       train_regret_loser_15[slice59],\r\n",
        "       train_regret_loser_16[slice59],\r\n",
        "       train_regret_loser_17[slice59],\r\n",
        "       train_regret_loser_18[slice59],\r\n",
        "       train_regret_loser_19[slice59],\r\n",
        "       train_regret_loser_20[slice59]]\r\n",
        "\r\n",
        "winner59 = [train_regret_winner_1[slice59],\r\n",
        "       train_regret_winner_2[slice59],\r\n",
        "       train_regret_winner_3[slice59],\r\n",
        "       train_regret_winner_4[slice59],\r\n",
        "       train_regret_winner_5[slice59],\r\n",
        "       train_regret_winner_6[slice59],\r\n",
        "       train_regret_winner_7[slice59],\r\n",
        "       train_regret_winner_8[slice59],\r\n",
        "       train_regret_winner_9[slice59],\r\n",
        "       train_regret_winner_10[slice59],\r\n",
        "       train_regret_winner_11[slice59],\r\n",
        "       train_regret_winner_12[slice59],\r\n",
        "       train_regret_winner_13[slice59],\r\n",
        "       train_regret_winner_14[slice59],\r\n",
        "       train_regret_winner_15[slice59],\r\n",
        "       train_regret_winner_16[slice59],\r\n",
        "       train_regret_winner_17[slice59],\r\n",
        "       train_regret_winner_18[slice59],\r\n",
        "       train_regret_winner_19[slice59],\r\n",
        "       train_regret_winner_20[slice59]]\r\n",
        "\r\n",
        "loser59_results = pd.DataFrame(loser59).sort_values(by=[0], ascending=False)\r\n",
        "winner59_results = pd.DataFrame(winner59).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser59 = np.asarray(loser59_results[4:5][0])[0]\r\n",
        "median_loser59 = np.asarray(loser59_results[9:10][0])[0]\r\n",
        "upper_loser59 = np.asarray(loser59_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner59 = np.asarray(winner59_results[4:5][0])[0]\r\n",
        "median_winner59 = np.asarray(winner59_results[9:10][0])[0]\r\n",
        "upper_winner59 = np.asarray(winner59_results[14:15][0])[0]"
      ],
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqtgekml1pde"
      },
      "source": [
        "# Iteration69 :\r\n",
        "\r\n",
        "slice69 = 68\r\n",
        "\r\n",
        "loser69 = [train_regret_loser_1[slice69],\r\n",
        "       train_regret_loser_2[slice69],\r\n",
        "       train_regret_loser_3[slice69],\r\n",
        "       train_regret_loser_4[slice69],\r\n",
        "       train_regret_loser_5[slice69],\r\n",
        "       train_regret_loser_6[slice69],\r\n",
        "       train_regret_loser_7[slice69],\r\n",
        "       train_regret_loser_8[slice69],\r\n",
        "       train_regret_loser_9[slice69],\r\n",
        "       train_regret_loser_10[slice69],\r\n",
        "       train_regret_loser_11[slice69],\r\n",
        "       train_regret_loser_12[slice69],\r\n",
        "       train_regret_loser_13[slice69],\r\n",
        "       train_regret_loser_14[slice69],\r\n",
        "       train_regret_loser_15[slice69],\r\n",
        "       train_regret_loser_16[slice69],\r\n",
        "       train_regret_loser_17[slice69],\r\n",
        "       train_regret_loser_18[slice69],\r\n",
        "       train_regret_loser_19[slice69],\r\n",
        "       train_regret_loser_20[slice69]]\r\n",
        "\r\n",
        "winner69 = [train_regret_winner_1[slice69],\r\n",
        "       train_regret_winner_2[slice69],\r\n",
        "       train_regret_winner_3[slice69],\r\n",
        "       train_regret_winner_4[slice69],\r\n",
        "       train_regret_winner_5[slice69],\r\n",
        "       train_regret_winner_6[slice69],\r\n",
        "       train_regret_winner_7[slice69],\r\n",
        "       train_regret_winner_8[slice69],\r\n",
        "       train_regret_winner_9[slice69],\r\n",
        "       train_regret_winner_10[slice69],\r\n",
        "       train_regret_winner_11[slice69],\r\n",
        "       train_regret_winner_12[slice69],\r\n",
        "       train_regret_winner_13[slice69],\r\n",
        "       train_regret_winner_14[slice69],\r\n",
        "       train_regret_winner_15[slice69],\r\n",
        "       train_regret_winner_16[slice69],\r\n",
        "       train_regret_winner_17[slice69],\r\n",
        "       train_regret_winner_18[slice69],\r\n",
        "       train_regret_winner_19[slice69],\r\n",
        "       train_regret_winner_20[slice69]]\r\n",
        "\r\n",
        "loser69_results = pd.DataFrame(loser69).sort_values(by=[0], ascending=False)\r\n",
        "winner69_results = pd.DataFrame(winner69).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser69 = np.asarray(loser69_results[4:5][0])[0]\r\n",
        "median_loser69 = np.asarray(loser69_results[9:10][0])[0]\r\n",
        "upper_loser69 = np.asarray(loser69_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner69 = np.asarray(winner69_results[4:5][0])[0]\r\n",
        "median_winner69 = np.asarray(winner69_results[9:10][0])[0]\r\n",
        "upper_winner69 = np.asarray(winner69_results[14:15][0])[0]"
      ],
      "execution_count": 165,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPFTa1171pgF"
      },
      "source": [
        "# Iteration79 :\r\n",
        "\r\n",
        "slice79 = 78\r\n",
        "\r\n",
        "loser79 = [train_regret_loser_1[slice79],\r\n",
        "       train_regret_loser_2[slice79],\r\n",
        "       train_regret_loser_3[slice79],\r\n",
        "       train_regret_loser_4[slice79],\r\n",
        "       train_regret_loser_5[slice79],\r\n",
        "       train_regret_loser_6[slice79],\r\n",
        "       train_regret_loser_7[slice79],\r\n",
        "       train_regret_loser_8[slice79],\r\n",
        "       train_regret_loser_9[slice79],\r\n",
        "       train_regret_loser_10[slice79],\r\n",
        "       train_regret_loser_11[slice79],\r\n",
        "       train_regret_loser_12[slice79],\r\n",
        "       train_regret_loser_13[slice79],\r\n",
        "       train_regret_loser_14[slice79],\r\n",
        "       train_regret_loser_15[slice79],\r\n",
        "       train_regret_loser_16[slice79],\r\n",
        "       train_regret_loser_17[slice79],\r\n",
        "       train_regret_loser_18[slice79],\r\n",
        "       train_regret_loser_19[slice79],\r\n",
        "       train_regret_loser_20[slice79]]\r\n",
        "\r\n",
        "winner79 = [train_regret_winner_1[slice79],\r\n",
        "       train_regret_winner_2[slice79],\r\n",
        "       train_regret_winner_3[slice79],\r\n",
        "       train_regret_winner_4[slice79],\r\n",
        "       train_regret_winner_5[slice79],\r\n",
        "       train_regret_winner_6[slice79],\r\n",
        "       train_regret_winner_7[slice79],\r\n",
        "       train_regret_winner_8[slice79],\r\n",
        "       train_regret_winner_9[slice79],\r\n",
        "       train_regret_winner_10[slice79],\r\n",
        "       train_regret_winner_11[slice79],\r\n",
        "       train_regret_winner_12[slice79],\r\n",
        "       train_regret_winner_13[slice79],\r\n",
        "       train_regret_winner_14[slice79],\r\n",
        "       train_regret_winner_15[slice79],\r\n",
        "       train_regret_winner_16[slice79],\r\n",
        "       train_regret_winner_17[slice79],\r\n",
        "       train_regret_winner_18[slice79],\r\n",
        "       train_regret_winner_19[slice79],\r\n",
        "       train_regret_winner_20[slice79]]\r\n",
        "\r\n",
        "loser79_results = pd.DataFrame(loser79).sort_values(by=[0], ascending=False)\r\n",
        "winner79_results = pd.DataFrame(winner79).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser79 = np.asarray(loser79_results[4:5][0])[0]\r\n",
        "median_loser79 = np.asarray(loser79_results[9:10][0])[0]\r\n",
        "upper_loser79 = np.asarray(loser79_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner79 = np.asarray(winner79_results[4:5][0])[0]\r\n",
        "median_winner79 = np.asarray(winner79_results[9:10][0])[0]\r\n",
        "upper_winner79 = np.asarray(winner79_results[14:15][0])[0]"
      ],
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTnRF4lx1pio"
      },
      "source": [
        "# Iteration89 :\r\n",
        "\r\n",
        "slice89 = 88\r\n",
        "\r\n",
        "loser89 = [train_regret_loser_1[slice89],\r\n",
        "       train_regret_loser_2[slice89],\r\n",
        "       train_regret_loser_3[slice89],\r\n",
        "       train_regret_loser_4[slice89],\r\n",
        "       train_regret_loser_5[slice89],\r\n",
        "       train_regret_loser_6[slice89],\r\n",
        "       train_regret_loser_7[slice89],\r\n",
        "       train_regret_loser_8[slice89],\r\n",
        "       train_regret_loser_9[slice89],\r\n",
        "       train_regret_loser_10[slice89],\r\n",
        "       train_regret_loser_11[slice89],\r\n",
        "       train_regret_loser_12[slice89],\r\n",
        "       train_regret_loser_13[slice89],\r\n",
        "       train_regret_loser_14[slice89],\r\n",
        "       train_regret_loser_15[slice89],\r\n",
        "       train_regret_loser_16[slice89],\r\n",
        "       train_regret_loser_17[slice89],\r\n",
        "       train_regret_loser_18[slice89],\r\n",
        "       train_regret_loser_19[slice89],\r\n",
        "       train_regret_loser_20[slice89]]\r\n",
        "\r\n",
        "winner89 = [train_regret_winner_1[slice89],\r\n",
        "       train_regret_winner_2[slice89],\r\n",
        "       train_regret_winner_3[slice89],\r\n",
        "       train_regret_winner_4[slice89],\r\n",
        "       train_regret_winner_5[slice89],\r\n",
        "       train_regret_winner_6[slice89],\r\n",
        "       train_regret_winner_7[slice89],\r\n",
        "       train_regret_winner_8[slice89],\r\n",
        "       train_regret_winner_9[slice89],\r\n",
        "       train_regret_winner_10[slice89],\r\n",
        "       train_regret_winner_11[slice89],\r\n",
        "       train_regret_winner_12[slice89],\r\n",
        "       train_regret_winner_13[slice89],\r\n",
        "       train_regret_winner_14[slice89],\r\n",
        "       train_regret_winner_15[slice89],\r\n",
        "       train_regret_winner_16[slice89],\r\n",
        "       train_regret_winner_17[slice89],\r\n",
        "       train_regret_winner_18[slice89],\r\n",
        "       train_regret_winner_19[slice89],\r\n",
        "       train_regret_winner_20[slice89]]\r\n",
        "\r\n",
        "loser89_results = pd.DataFrame(loser89).sort_values(by=[0], ascending=False)\r\n",
        "winner89_results = pd.DataFrame(winner89).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser89 = np.asarray(loser89_results[4:5][0])[0]\r\n",
        "median_loser89 = np.asarray(loser89_results[9:10][0])[0]\r\n",
        "upper_loser89 = np.asarray(loser89_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner89 = np.asarray(winner89_results[4:5][0])[0]\r\n",
        "median_winner89 = np.asarray(winner89_results[9:10][0])[0]\r\n",
        "upper_winner89 = np.asarray(winner89_results[14:15][0])[0]"
      ],
      "execution_count": 167,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hiLAF38i1plO"
      },
      "source": [
        "# Iteration99 :\r\n",
        "\r\n",
        "slice99 = 98\r\n",
        "\r\n",
        "loser99 = [train_regret_loser_1[slice99],\r\n",
        "       train_regret_loser_2[slice99],\r\n",
        "       train_regret_loser_3[slice99],\r\n",
        "       train_regret_loser_4[slice99],\r\n",
        "       train_regret_loser_5[slice99],\r\n",
        "       train_regret_loser_6[slice99],\r\n",
        "       train_regret_loser_7[slice99],\r\n",
        "       train_regret_loser_8[slice99],\r\n",
        "       train_regret_loser_9[slice99],\r\n",
        "       train_regret_loser_10[slice99],\r\n",
        "       train_regret_loser_11[slice99],\r\n",
        "       train_regret_loser_12[slice99],\r\n",
        "       train_regret_loser_13[slice99],\r\n",
        "       train_regret_loser_14[slice99],\r\n",
        "       train_regret_loser_15[slice99],\r\n",
        "       train_regret_loser_16[slice99],\r\n",
        "       train_regret_loser_17[slice99],\r\n",
        "       train_regret_loser_18[slice99],\r\n",
        "       train_regret_loser_19[slice99],\r\n",
        "       train_regret_loser_20[slice99]]\r\n",
        "\r\n",
        "winner99 = [train_regret_winner_1[slice99],\r\n",
        "       train_regret_winner_2[slice99],\r\n",
        "       train_regret_winner_3[slice99],\r\n",
        "       train_regret_winner_4[slice99],\r\n",
        "       train_regret_winner_5[slice99],\r\n",
        "       train_regret_winner_6[slice99],\r\n",
        "       train_regret_winner_7[slice99],\r\n",
        "       train_regret_winner_8[slice99],\r\n",
        "       train_regret_winner_9[slice99],\r\n",
        "       train_regret_winner_10[slice99],\r\n",
        "       train_regret_winner_11[slice99],\r\n",
        "       train_regret_winner_12[slice99],\r\n",
        "       train_regret_winner_13[slice99],\r\n",
        "       train_regret_winner_14[slice99],\r\n",
        "       train_regret_winner_15[slice99],\r\n",
        "       train_regret_winner_16[slice99],\r\n",
        "       train_regret_winner_17[slice99],\r\n",
        "       train_regret_winner_18[slice99],\r\n",
        "       train_regret_winner_19[slice99],\r\n",
        "       train_regret_winner_20[slice99]]\r\n",
        "\r\n",
        "loser99_results = pd.DataFrame(loser99).sort_values(by=[0], ascending=False)\r\n",
        "winner99_results = pd.DataFrame(winner99).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser99 = np.asarray(loser99_results[4:5][0])[0]\r\n",
        "median_loser99 = np.asarray(loser99_results[9:10][0])[0]\r\n",
        "upper_loser99 = np.asarray(loser99_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner99 = np.asarray(winner99_results[4:5][0])[0]\r\n",
        "median_winner99 = np.asarray(winner99_results[9:10][0])[0]\r\n",
        "upper_winner99 = np.asarray(winner99_results[14:15][0])[0]"
      ],
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBd_N0F21pn0"
      },
      "source": [
        "# Iteration10 :\r\n",
        "\r\n",
        "slice10 = 9\r\n",
        "\r\n",
        "loser10 = [train_regret_loser_1[slice10],\r\n",
        "       train_regret_loser_2[slice10],\r\n",
        "       train_regret_loser_3[slice10],\r\n",
        "       train_regret_loser_4[slice10],\r\n",
        "       train_regret_loser_5[slice10],\r\n",
        "       train_regret_loser_6[slice10],\r\n",
        "       train_regret_loser_7[slice10],\r\n",
        "       train_regret_loser_8[slice10],\r\n",
        "       train_regret_loser_9[slice10],\r\n",
        "       train_regret_loser_10[slice10],\r\n",
        "       train_regret_loser_11[slice10],\r\n",
        "       train_regret_loser_12[slice10],\r\n",
        "       train_regret_loser_13[slice10],\r\n",
        "       train_regret_loser_14[slice10],\r\n",
        "       train_regret_loser_15[slice10],\r\n",
        "       train_regret_loser_16[slice10],\r\n",
        "       train_regret_loser_17[slice10],\r\n",
        "       train_regret_loser_18[slice10],\r\n",
        "       train_regret_loser_19[slice10],\r\n",
        "       train_regret_loser_20[slice10]]\r\n",
        "\r\n",
        "winner10 = [train_regret_winner_1[slice10],\r\n",
        "       train_regret_winner_2[slice10],\r\n",
        "       train_regret_winner_3[slice10],\r\n",
        "       train_regret_winner_4[slice10],\r\n",
        "       train_regret_winner_5[slice10],\r\n",
        "       train_regret_winner_6[slice10],\r\n",
        "       train_regret_winner_7[slice10],\r\n",
        "       train_regret_winner_8[slice10],\r\n",
        "       train_regret_winner_9[slice10],\r\n",
        "       train_regret_winner_10[slice10],\r\n",
        "       train_regret_winner_11[slice10],\r\n",
        "       train_regret_winner_12[slice10],\r\n",
        "       train_regret_winner_13[slice10],\r\n",
        "       train_regret_winner_14[slice10],\r\n",
        "       train_regret_winner_15[slice10],\r\n",
        "       train_regret_winner_16[slice10],\r\n",
        "       train_regret_winner_17[slice10],\r\n",
        "       train_regret_winner_18[slice10],\r\n",
        "       train_regret_winner_19[slice10],\r\n",
        "       train_regret_winner_20[slice10]]\r\n",
        "\r\n",
        "loser10_results = pd.DataFrame(loser10).sort_values(by=[0], ascending=False)\r\n",
        "winner10_results = pd.DataFrame(winner10).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser10 = np.asarray(loser10_results[4:5][0])[0]\r\n",
        "median_loser10 = np.asarray(loser10_results[9:10][0])[0]\r\n",
        "upper_loser10 = np.asarray(loser10_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner10 = np.asarray(winner10_results[4:5][0])[0]\r\n",
        "median_winner10 = np.asarray(winner10_results[9:10][0])[0]\r\n",
        "upper_winner10 = np.asarray(winner10_results[14:15][0])[0]"
      ],
      "execution_count": 169,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXjdqc4s1pqb"
      },
      "source": [
        "# Iteration20 :\r\n",
        "\r\n",
        "slice20 = 19\r\n",
        "\r\n",
        "loser20 = [train_regret_loser_1[slice20],\r\n",
        "       train_regret_loser_2[slice20],\r\n",
        "       train_regret_loser_3[slice20],\r\n",
        "       train_regret_loser_4[slice20],\r\n",
        "       train_regret_loser_5[slice20],\r\n",
        "       train_regret_loser_6[slice20],\r\n",
        "       train_regret_loser_7[slice20],\r\n",
        "       train_regret_loser_8[slice20],\r\n",
        "       train_regret_loser_9[slice20],\r\n",
        "       train_regret_loser_10[slice20],\r\n",
        "       train_regret_loser_11[slice20],\r\n",
        "       train_regret_loser_12[slice20],\r\n",
        "       train_regret_loser_13[slice20],\r\n",
        "       train_regret_loser_14[slice20],\r\n",
        "       train_regret_loser_15[slice20],\r\n",
        "       train_regret_loser_16[slice20],\r\n",
        "       train_regret_loser_17[slice20],\r\n",
        "       train_regret_loser_18[slice20],\r\n",
        "       train_regret_loser_19[slice20],\r\n",
        "       train_regret_loser_20[slice20]]\r\n",
        "\r\n",
        "winner20 = [train_regret_winner_1[slice20],\r\n",
        "       train_regret_winner_2[slice20],\r\n",
        "       train_regret_winner_3[slice20],\r\n",
        "       train_regret_winner_4[slice20],\r\n",
        "       train_regret_winner_5[slice20],\r\n",
        "       train_regret_winner_6[slice20],\r\n",
        "       train_regret_winner_7[slice20],\r\n",
        "       train_regret_winner_8[slice20],\r\n",
        "       train_regret_winner_9[slice20],\r\n",
        "       train_regret_winner_10[slice20],\r\n",
        "       train_regret_winner_11[slice20],\r\n",
        "       train_regret_winner_12[slice20],\r\n",
        "       train_regret_winner_13[slice20],\r\n",
        "       train_regret_winner_14[slice20],\r\n",
        "       train_regret_winner_15[slice20],\r\n",
        "       train_regret_winner_16[slice20],\r\n",
        "       train_regret_winner_17[slice20],\r\n",
        "       train_regret_winner_18[slice20],\r\n",
        "       train_regret_winner_19[slice20],\r\n",
        "       train_regret_winner_20[slice20]]\r\n",
        "\r\n",
        "loser20_results = pd.DataFrame(loser20).sort_values(by=[0], ascending=False)\r\n",
        "winner20_results = pd.DataFrame(winner20).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser20 = np.asarray(loser20_results[4:5][0])[0]\r\n",
        "median_loser20 = np.asarray(loser20_results[9:10][0])[0]\r\n",
        "upper_loser20 = np.asarray(loser20_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner20 = np.asarray(winner20_results[4:5][0])[0]\r\n",
        "median_winner20 = np.asarray(winner20_results[9:10][0])[0]\r\n",
        "upper_winner20 = np.asarray(winner20_results[14:15][0])[0]"
      ],
      "execution_count": 170,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQBVS6dx1ptB"
      },
      "source": [
        "# Iteration30 :\r\n",
        "\r\n",
        "slice30 = 29\r\n",
        "\r\n",
        "loser30 = [train_regret_loser_1[slice30],\r\n",
        "       train_regret_loser_2[slice30],\r\n",
        "       train_regret_loser_3[slice30],\r\n",
        "       train_regret_loser_4[slice30],\r\n",
        "       train_regret_loser_5[slice30],\r\n",
        "       train_regret_loser_6[slice30],\r\n",
        "       train_regret_loser_7[slice30],\r\n",
        "       train_regret_loser_8[slice30],\r\n",
        "       train_regret_loser_9[slice30],\r\n",
        "       train_regret_loser_10[slice30],\r\n",
        "       train_regret_loser_11[slice30],\r\n",
        "       train_regret_loser_12[slice30],\r\n",
        "       train_regret_loser_13[slice30],\r\n",
        "       train_regret_loser_14[slice30],\r\n",
        "       train_regret_loser_15[slice30],\r\n",
        "       train_regret_loser_16[slice30],\r\n",
        "       train_regret_loser_17[slice30],\r\n",
        "       train_regret_loser_18[slice30],\r\n",
        "       train_regret_loser_19[slice30],\r\n",
        "       train_regret_loser_20[slice30]]\r\n",
        "\r\n",
        "winner30 = [train_regret_winner_1[slice30],\r\n",
        "       train_regret_winner_2[slice30],\r\n",
        "       train_regret_winner_3[slice30],\r\n",
        "       train_regret_winner_4[slice30],\r\n",
        "       train_regret_winner_5[slice30],\r\n",
        "       train_regret_winner_6[slice30],\r\n",
        "       train_regret_winner_7[slice30],\r\n",
        "       train_regret_winner_8[slice30],\r\n",
        "       train_regret_winner_9[slice30],\r\n",
        "       train_regret_winner_10[slice30],\r\n",
        "       train_regret_winner_11[slice30],\r\n",
        "       train_regret_winner_12[slice30],\r\n",
        "       train_regret_winner_13[slice30],\r\n",
        "       train_regret_winner_14[slice30],\r\n",
        "       train_regret_winner_15[slice30],\r\n",
        "       train_regret_winner_16[slice30],\r\n",
        "       train_regret_winner_17[slice30],\r\n",
        "       train_regret_winner_18[slice30],\r\n",
        "       train_regret_winner_19[slice30],\r\n",
        "       train_regret_winner_20[slice30]]\r\n",
        "\r\n",
        "loser30_results = pd.DataFrame(loser30).sort_values(by=[0], ascending=False)\r\n",
        "winner30_results = pd.DataFrame(winner30).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser30 = np.asarray(loser30_results[4:5][0])[0]\r\n",
        "median_loser30 = np.asarray(loser30_results[9:10][0])[0]\r\n",
        "upper_loser30 = np.asarray(loser30_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner30 = np.asarray(winner30_results[4:5][0])[0]\r\n",
        "median_winner30 = np.asarray(winner30_results[9:10][0])[0]\r\n",
        "upper_winner30 = np.asarray(winner30_results[14:15][0])[0]"
      ],
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-iWfLkj1pvn"
      },
      "source": [
        "# Iteration40 :\r\n",
        "\r\n",
        "slice40 = 39\r\n",
        "\r\n",
        "loser40 = [train_regret_loser_1[slice40],\r\n",
        "       train_regret_loser_2[slice40],\r\n",
        "       train_regret_loser_3[slice40],\r\n",
        "       train_regret_loser_4[slice40],\r\n",
        "       train_regret_loser_5[slice40],\r\n",
        "       train_regret_loser_6[slice40],\r\n",
        "       train_regret_loser_7[slice40],\r\n",
        "       train_regret_loser_8[slice40],\r\n",
        "       train_regret_loser_9[slice40],\r\n",
        "       train_regret_loser_10[slice40],\r\n",
        "       train_regret_loser_11[slice40],\r\n",
        "       train_regret_loser_12[slice40],\r\n",
        "       train_regret_loser_13[slice40],\r\n",
        "       train_regret_loser_14[slice40],\r\n",
        "       train_regret_loser_15[slice40],\r\n",
        "       train_regret_loser_16[slice40],\r\n",
        "       train_regret_loser_17[slice40],\r\n",
        "       train_regret_loser_18[slice40],\r\n",
        "       train_regret_loser_19[slice40],\r\n",
        "       train_regret_loser_20[slice40]]\r\n",
        "\r\n",
        "winner40 = [train_regret_winner_1[slice40],\r\n",
        "       train_regret_winner_2[slice40],\r\n",
        "       train_regret_winner_3[slice40],\r\n",
        "       train_regret_winner_4[slice40],\r\n",
        "       train_regret_winner_5[slice40],\r\n",
        "       train_regret_winner_6[slice40],\r\n",
        "       train_regret_winner_7[slice40],\r\n",
        "       train_regret_winner_8[slice40],\r\n",
        "       train_regret_winner_9[slice40],\r\n",
        "       train_regret_winner_10[slice40],\r\n",
        "       train_regret_winner_11[slice40],\r\n",
        "       train_regret_winner_12[slice40],\r\n",
        "       train_regret_winner_13[slice40],\r\n",
        "       train_regret_winner_14[slice40],\r\n",
        "       train_regret_winner_15[slice40],\r\n",
        "       train_regret_winner_16[slice40],\r\n",
        "       train_regret_winner_17[slice40],\r\n",
        "       train_regret_winner_18[slice40],\r\n",
        "       train_regret_winner_19[slice40],\r\n",
        "       train_regret_winner_20[slice40]]\r\n",
        "\r\n",
        "loser40_results = pd.DataFrame(loser40).sort_values(by=[0], ascending=False)\r\n",
        "winner40_results = pd.DataFrame(winner40).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser40 = np.asarray(loser40_results[4:5][0])[0]\r\n",
        "median_loser40 = np.asarray(loser40_results[9:10][0])[0]\r\n",
        "upper_loser40 = np.asarray(loser40_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner40 = np.asarray(winner40_results[4:5][0])[0]\r\n",
        "median_winner40 = np.asarray(winner40_results[9:10][0])[0]\r\n",
        "upper_winner40 = np.asarray(winner40_results[14:15][0])[0]"
      ],
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-G5i-Cjw2U7K"
      },
      "source": [
        "# Iteration50 :\r\n",
        "\r\n",
        "slice50 = 49\r\n",
        "\r\n",
        "loser50 = [train_regret_loser_1[slice50],\r\n",
        "       train_regret_loser_2[slice50],\r\n",
        "       train_regret_loser_3[slice50],\r\n",
        "       train_regret_loser_4[slice50],\r\n",
        "       train_regret_loser_5[slice50],\r\n",
        "       train_regret_loser_6[slice50],\r\n",
        "       train_regret_loser_7[slice50],\r\n",
        "       train_regret_loser_8[slice50],\r\n",
        "       train_regret_loser_9[slice50],\r\n",
        "       train_regret_loser_10[slice50],\r\n",
        "       train_regret_loser_11[slice50],\r\n",
        "       train_regret_loser_12[slice50],\r\n",
        "       train_regret_loser_13[slice50],\r\n",
        "       train_regret_loser_14[slice50],\r\n",
        "       train_regret_loser_15[slice50],\r\n",
        "       train_regret_loser_16[slice50],\r\n",
        "       train_regret_loser_17[slice50],\r\n",
        "       train_regret_loser_18[slice50],\r\n",
        "       train_regret_loser_19[slice50],\r\n",
        "       train_regret_loser_20[slice50]]\r\n",
        "\r\n",
        "winner50 = [train_regret_winner_1[slice50],\r\n",
        "       train_regret_winner_2[slice50],\r\n",
        "       train_regret_winner_3[slice50],\r\n",
        "       train_regret_winner_4[slice50],\r\n",
        "       train_regret_winner_5[slice50],\r\n",
        "       train_regret_winner_6[slice50],\r\n",
        "       train_regret_winner_7[slice50],\r\n",
        "       train_regret_winner_8[slice50],\r\n",
        "       train_regret_winner_9[slice50],\r\n",
        "       train_regret_winner_10[slice50],\r\n",
        "       train_regret_winner_11[slice50],\r\n",
        "       train_regret_winner_12[slice50],\r\n",
        "       train_regret_winner_13[slice50],\r\n",
        "       train_regret_winner_14[slice50],\r\n",
        "       train_regret_winner_15[slice50],\r\n",
        "       train_regret_winner_16[slice50],\r\n",
        "       train_regret_winner_17[slice50],\r\n",
        "       train_regret_winner_18[slice50],\r\n",
        "       train_regret_winner_19[slice50],\r\n",
        "       train_regret_winner_20[slice50]]\r\n",
        "\r\n",
        "loser50_results = pd.DataFrame(loser50).sort_values(by=[0], ascending=False)\r\n",
        "winner50_results = pd.DataFrame(winner50).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser50 = np.asarray(loser50_results[4:5][0])[0]\r\n",
        "median_loser50 = np.asarray(loser50_results[9:10][0])[0]\r\n",
        "upper_loser50 = np.asarray(loser50_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner50 = np.asarray(winner50_results[4:5][0])[0]\r\n",
        "median_winner50 = np.asarray(winner50_results[9:10][0])[0]\r\n",
        "upper_winner50 = np.asarray(winner50_results[14:15][0])[0]"
      ],
      "execution_count": 173,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RY6qxZQA2U-r"
      },
      "source": [
        "# Iteration60 :\r\n",
        "\r\n",
        "slice60 = 59\r\n",
        "\r\n",
        "loser60 = [train_regret_loser_1[slice60],\r\n",
        "       train_regret_loser_2[slice60],\r\n",
        "       train_regret_loser_3[slice60],\r\n",
        "       train_regret_loser_4[slice60],\r\n",
        "       train_regret_loser_5[slice60],\r\n",
        "       train_regret_loser_6[slice60],\r\n",
        "       train_regret_loser_7[slice60],\r\n",
        "       train_regret_loser_8[slice60],\r\n",
        "       train_regret_loser_9[slice60],\r\n",
        "       train_regret_loser_10[slice60],\r\n",
        "       train_regret_loser_11[slice60],\r\n",
        "       train_regret_loser_12[slice60],\r\n",
        "       train_regret_loser_13[slice60],\r\n",
        "       train_regret_loser_14[slice60],\r\n",
        "       train_regret_loser_15[slice60],\r\n",
        "       train_regret_loser_16[slice60],\r\n",
        "       train_regret_loser_17[slice60],\r\n",
        "       train_regret_loser_18[slice60],\r\n",
        "       train_regret_loser_19[slice60],\r\n",
        "       train_regret_loser_20[slice60]]\r\n",
        "\r\n",
        "winner60 = [train_regret_winner_1[slice60],\r\n",
        "       train_regret_winner_2[slice60],\r\n",
        "       train_regret_winner_3[slice60],\r\n",
        "       train_regret_winner_4[slice60],\r\n",
        "       train_regret_winner_5[slice60],\r\n",
        "       train_regret_winner_6[slice60],\r\n",
        "       train_regret_winner_7[slice60],\r\n",
        "       train_regret_winner_8[slice60],\r\n",
        "       train_regret_winner_9[slice60],\r\n",
        "       train_regret_winner_10[slice60],\r\n",
        "       train_regret_winner_11[slice60],\r\n",
        "       train_regret_winner_12[slice60],\r\n",
        "       train_regret_winner_13[slice60],\r\n",
        "       train_regret_winner_14[slice60],\r\n",
        "       train_regret_winner_15[slice60],\r\n",
        "       train_regret_winner_16[slice60],\r\n",
        "       train_regret_winner_17[slice60],\r\n",
        "       train_regret_winner_18[slice60],\r\n",
        "       train_regret_winner_19[slice60],\r\n",
        "       train_regret_winner_20[slice60]]\r\n",
        "\r\n",
        "loser60_results = pd.DataFrame(loser60).sort_values(by=[0], ascending=False)\r\n",
        "winner60_results = pd.DataFrame(winner60).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser60 = np.asarray(loser60_results[4:5][0])[0]\r\n",
        "median_loser60 = np.asarray(loser60_results[9:10][0])[0]\r\n",
        "upper_loser60 = np.asarray(loser60_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner60 = np.asarray(winner60_results[4:5][0])[0]\r\n",
        "median_winner60 = np.asarray(winner60_results[9:10][0])[0]\r\n",
        "upper_winner60 = np.asarray(winner60_results[14:15][0])[0]"
      ],
      "execution_count": 174,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBqgVxVH2VCU"
      },
      "source": [
        "# Iteration70 :\r\n",
        "\r\n",
        "slice70 = 69\r\n",
        "\r\n",
        "loser70 = [train_regret_loser_1[slice70],\r\n",
        "       train_regret_loser_2[slice70],\r\n",
        "       train_regret_loser_3[slice70],\r\n",
        "       train_regret_loser_4[slice70],\r\n",
        "       train_regret_loser_5[slice70],\r\n",
        "       train_regret_loser_6[slice70],\r\n",
        "       train_regret_loser_7[slice70],\r\n",
        "       train_regret_loser_8[slice70],\r\n",
        "       train_regret_loser_9[slice70],\r\n",
        "       train_regret_loser_10[slice70],\r\n",
        "       train_regret_loser_11[slice70],\r\n",
        "       train_regret_loser_12[slice70],\r\n",
        "       train_regret_loser_13[slice70],\r\n",
        "       train_regret_loser_14[slice70],\r\n",
        "       train_regret_loser_15[slice70],\r\n",
        "       train_regret_loser_16[slice70],\r\n",
        "       train_regret_loser_17[slice70],\r\n",
        "       train_regret_loser_18[slice70],\r\n",
        "       train_regret_loser_19[slice70],\r\n",
        "       train_regret_loser_20[slice70]]\r\n",
        "\r\n",
        "winner70 = [train_regret_winner_1[slice70],\r\n",
        "       train_regret_winner_2[slice70],\r\n",
        "       train_regret_winner_3[slice70],\r\n",
        "       train_regret_winner_4[slice70],\r\n",
        "       train_regret_winner_5[slice70],\r\n",
        "       train_regret_winner_6[slice70],\r\n",
        "       train_regret_winner_7[slice70],\r\n",
        "       train_regret_winner_8[slice70],\r\n",
        "       train_regret_winner_9[slice70],\r\n",
        "       train_regret_winner_10[slice70],\r\n",
        "       train_regret_winner_11[slice70],\r\n",
        "       train_regret_winner_12[slice70],\r\n",
        "       train_regret_winner_13[slice70],\r\n",
        "       train_regret_winner_14[slice70],\r\n",
        "       train_regret_winner_15[slice70],\r\n",
        "       train_regret_winner_16[slice70],\r\n",
        "       train_regret_winner_17[slice70],\r\n",
        "       train_regret_winner_18[slice70],\r\n",
        "       train_regret_winner_19[slice70],\r\n",
        "       train_regret_winner_20[slice70]]\r\n",
        "\r\n",
        "loser70_results = pd.DataFrame(loser70).sort_values(by=[0], ascending=False)\r\n",
        "winner70_results = pd.DataFrame(winner70).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser70 = np.asarray(loser70_results[4:5][0])[0]\r\n",
        "median_loser70 = np.asarray(loser70_results[9:10][0])[0]\r\n",
        "upper_loser70 = np.asarray(loser70_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner70 = np.asarray(winner70_results[4:5][0])[0]\r\n",
        "median_winner70 = np.asarray(winner70_results[9:10][0])[0]\r\n",
        "upper_winner70 = np.asarray(winner70_results[14:15][0])[0]"
      ],
      "execution_count": 175,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YF7mlDZL1pyN"
      },
      "source": [
        "# Iteration80 :\r\n",
        "\r\n",
        "slice80 = 79\r\n",
        "\r\n",
        "loser80 = [train_regret_loser_1[slice80],\r\n",
        "       train_regret_loser_2[slice80],\r\n",
        "       train_regret_loser_3[slice80],\r\n",
        "       train_regret_loser_4[slice80],\r\n",
        "       train_regret_loser_5[slice80],\r\n",
        "       train_regret_loser_6[slice80],\r\n",
        "       train_regret_loser_7[slice80],\r\n",
        "       train_regret_loser_8[slice80],\r\n",
        "       train_regret_loser_9[slice80],\r\n",
        "       train_regret_loser_10[slice80],\r\n",
        "       train_regret_loser_11[slice80],\r\n",
        "       train_regret_loser_12[slice80],\r\n",
        "       train_regret_loser_13[slice80],\r\n",
        "       train_regret_loser_14[slice80],\r\n",
        "       train_regret_loser_15[slice80],\r\n",
        "       train_regret_loser_16[slice80],\r\n",
        "       train_regret_loser_17[slice80],\r\n",
        "       train_regret_loser_18[slice80],\r\n",
        "       train_regret_loser_19[slice80],\r\n",
        "       train_regret_loser_20[slice80]]\r\n",
        "\r\n",
        "winner80 = [train_regret_winner_1[slice80],\r\n",
        "       train_regret_winner_2[slice80],\r\n",
        "       train_regret_winner_3[slice80],\r\n",
        "       train_regret_winner_4[slice80],\r\n",
        "       train_regret_winner_5[slice80],\r\n",
        "       train_regret_winner_6[slice80],\r\n",
        "       train_regret_winner_7[slice80],\r\n",
        "       train_regret_winner_8[slice80],\r\n",
        "       train_regret_winner_9[slice80],\r\n",
        "       train_regret_winner_10[slice80],\r\n",
        "       train_regret_winner_11[slice80],\r\n",
        "       train_regret_winner_12[slice80],\r\n",
        "       train_regret_winner_13[slice80],\r\n",
        "       train_regret_winner_14[slice80],\r\n",
        "       train_regret_winner_15[slice80],\r\n",
        "       train_regret_winner_16[slice80],\r\n",
        "       train_regret_winner_17[slice80],\r\n",
        "       train_regret_winner_18[slice80],\r\n",
        "       train_regret_winner_19[slice80],\r\n",
        "       train_regret_winner_20[slice80]]\r\n",
        "\r\n",
        "loser80_results = pd.DataFrame(loser80).sort_values(by=[0], ascending=False)\r\n",
        "winner80_results = pd.DataFrame(winner80).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser80 = np.asarray(loser80_results[4:5][0])[0]\r\n",
        "median_loser80 = np.asarray(loser80_results[9:10][0])[0]\r\n",
        "upper_loser80 = np.asarray(loser80_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner80 = np.asarray(winner80_results[4:5][0])[0]\r\n",
        "median_winner80 = np.asarray(winner80_results[9:10][0])[0]\r\n",
        "upper_winner80 = np.asarray(winner80_results[14:15][0])[0]"
      ],
      "execution_count": 176,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1Rw8IkD1p0z"
      },
      "source": [
        "# Iteration90 :\r\n",
        "\r\n",
        "slice90 = 89\r\n",
        "\r\n",
        "loser90 = [train_regret_loser_1[slice90],\r\n",
        "       train_regret_loser_2[slice90],\r\n",
        "       train_regret_loser_3[slice90],\r\n",
        "       train_regret_loser_4[slice90],\r\n",
        "       train_regret_loser_5[slice90],\r\n",
        "       train_regret_loser_6[slice90],\r\n",
        "       train_regret_loser_7[slice90],\r\n",
        "       train_regret_loser_8[slice90],\r\n",
        "       train_regret_loser_9[slice90],\r\n",
        "       train_regret_loser_10[slice90],\r\n",
        "       train_regret_loser_11[slice90],\r\n",
        "       train_regret_loser_12[slice90],\r\n",
        "       train_regret_loser_13[slice90],\r\n",
        "       train_regret_loser_14[slice90],\r\n",
        "       train_regret_loser_15[slice90],\r\n",
        "       train_regret_loser_16[slice90],\r\n",
        "       train_regret_loser_17[slice90],\r\n",
        "       train_regret_loser_18[slice90],\r\n",
        "       train_regret_loser_19[slice90],\r\n",
        "       train_regret_loser_20[slice90]]\r\n",
        "\r\n",
        "winner90 = [train_regret_winner_1[slice90],\r\n",
        "       train_regret_winner_2[slice90],\r\n",
        "       train_regret_winner_3[slice90],\r\n",
        "       train_regret_winner_4[slice90],\r\n",
        "       train_regret_winner_5[slice90],\r\n",
        "       train_regret_winner_6[slice90],\r\n",
        "       train_regret_winner_7[slice90],\r\n",
        "       train_regret_winner_8[slice90],\r\n",
        "       train_regret_winner_9[slice90],\r\n",
        "       train_regret_winner_10[slice90],\r\n",
        "       train_regret_winner_11[slice90],\r\n",
        "       train_regret_winner_12[slice90],\r\n",
        "       train_regret_winner_13[slice90],\r\n",
        "       train_regret_winner_14[slice90],\r\n",
        "       train_regret_winner_15[slice90],\r\n",
        "       train_regret_winner_16[slice90],\r\n",
        "       train_regret_winner_17[slice90],\r\n",
        "       train_regret_winner_18[slice90],\r\n",
        "       train_regret_winner_19[slice90],\r\n",
        "       train_regret_winner_20[slice90]]\r\n",
        "\r\n",
        "loser90_results = pd.DataFrame(loser90).sort_values(by=[0], ascending=False)\r\n",
        "winner90_results = pd.DataFrame(winner90).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser90 = np.asarray(loser90_results[4:5][0])[0]\r\n",
        "median_loser90 = np.asarray(loser90_results[9:10][0])[0]\r\n",
        "upper_loser90 = np.asarray(loser90_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner90 = np.asarray(winner90_results[4:5][0])[0]\r\n",
        "median_winner90 = np.asarray(winner90_results[9:10][0])[0]\r\n",
        "upper_winner90 = np.asarray(winner90_results[14:15][0])[0]"
      ],
      "execution_count": 177,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iiljCrq21p3a"
      },
      "source": [
        "# Iteration100 :\r\n",
        "\r\n",
        "slice100 = 99\r\n",
        "\r\n",
        "loser100 = [train_regret_loser_1[slice100],\r\n",
        "       train_regret_loser_2[slice100],\r\n",
        "       train_regret_loser_3[slice100],\r\n",
        "       train_regret_loser_4[slice100],\r\n",
        "       train_regret_loser_5[slice100],\r\n",
        "       train_regret_loser_6[slice100],\r\n",
        "       train_regret_loser_7[slice100],\r\n",
        "       train_regret_loser_8[slice100],\r\n",
        "       train_regret_loser_9[slice100],\r\n",
        "       train_regret_loser_10[slice100],\r\n",
        "       train_regret_loser_11[slice100],\r\n",
        "       train_regret_loser_12[slice100],\r\n",
        "       train_regret_loser_13[slice100],\r\n",
        "       train_regret_loser_14[slice100],\r\n",
        "       train_regret_loser_15[slice100],\r\n",
        "       train_regret_loser_16[slice100],\r\n",
        "       train_regret_loser_17[slice100],\r\n",
        "       train_regret_loser_18[slice100],\r\n",
        "       train_regret_loser_19[slice100],\r\n",
        "       train_regret_loser_20[slice100]]\r\n",
        "\r\n",
        "winner100 = [train_regret_winner_1[slice100],\r\n",
        "       train_regret_winner_2[slice100],\r\n",
        "       train_regret_winner_3[slice100],\r\n",
        "       train_regret_winner_4[slice100],\r\n",
        "       train_regret_winner_5[slice100],\r\n",
        "       train_regret_winner_6[slice100],\r\n",
        "       train_regret_winner_7[slice100],\r\n",
        "       train_regret_winner_8[slice100],\r\n",
        "       train_regret_winner_9[slice100],\r\n",
        "       train_regret_winner_10[slice100],\r\n",
        "       train_regret_winner_11[slice100],\r\n",
        "       train_regret_winner_12[slice100],\r\n",
        "       train_regret_winner_13[slice100],\r\n",
        "       train_regret_winner_14[slice100],\r\n",
        "       train_regret_winner_15[slice100],\r\n",
        "       train_regret_winner_16[slice100],\r\n",
        "       train_regret_winner_17[slice100],\r\n",
        "       train_regret_winner_18[slice100],\r\n",
        "       train_regret_winner_19[slice100],\r\n",
        "       train_regret_winner_20[slice100]]\r\n",
        "\r\n",
        "loser100_results = pd.DataFrame(loser100).sort_values(by=[0], ascending=False)\r\n",
        "winner100_results = pd.DataFrame(winner100).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser100 = np.asarray(loser100_results[4:5][0])[0]\r\n",
        "median_loser100 = np.asarray(loser100_results[9:10][0])[0]\r\n",
        "upper_loser100 = np.asarray(loser100_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner100 = np.asarray(winner100_results[4:5][0])[0]\r\n",
        "median_winner100 = np.asarray(winner100_results[9:10][0])[0]\r\n",
        "upper_winner100 = np.asarray(winner100_results[14:15][0])[0]"
      ],
      "execution_count": 178,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OItJgqUT2oNb"
      },
      "source": [
        "### Summarize arrays: 'Loser'\r\n",
        "\r\n",
        "lower_loser = [lower_loser1,\r\n",
        "            lower_loser2,\r\n",
        "            lower_loser3,\r\n",
        "            lower_loser4,\r\n",
        "            lower_loser5,\r\n",
        "            lower_loser6,\r\n",
        "            lower_loser7,\r\n",
        "            lower_loser8,\r\n",
        "            lower_loser9,\r\n",
        "            lower_loser10,\r\n",
        "            lower_loser11,\r\n",
        "            lower_loser12,\r\n",
        "            lower_loser13,\r\n",
        "            lower_loser14,\r\n",
        "            lower_loser15,\r\n",
        "            lower_loser16,\r\n",
        "            lower_loser17,\r\n",
        "            lower_loser18,\r\n",
        "            lower_loser19,\r\n",
        "            lower_loser20,\r\n",
        "            lower_loser21,\r\n",
        "            lower_loser22,\r\n",
        "            lower_loser23,\r\n",
        "            lower_loser24,\r\n",
        "            lower_loser25,\r\n",
        "            lower_loser26,\r\n",
        "            lower_loser27,\r\n",
        "            lower_loser28,\r\n",
        "            lower_loser29,\r\n",
        "            lower_loser30,\r\n",
        "            lower_loser31,\r\n",
        "            lower_loser32,\r\n",
        "            lower_loser33,\r\n",
        "            lower_loser34,\r\n",
        "            lower_loser35,\r\n",
        "            lower_loser36,\r\n",
        "            lower_loser37,\r\n",
        "            lower_loser38,\r\n",
        "            lower_loser39,\r\n",
        "            lower_loser40,\r\n",
        "            lower_loser41,\r\n",
        "            lower_loser42,\r\n",
        "            lower_loser43,\r\n",
        "            lower_loser44,\r\n",
        "            lower_loser45,\r\n",
        "            lower_loser46,\r\n",
        "            lower_loser47,\r\n",
        "            lower_loser48,\r\n",
        "            lower_loser49,\r\n",
        "            lower_loser50,\r\n",
        "            lower_loser51,\r\n",
        "            lower_loser52,\r\n",
        "            lower_loser53,\r\n",
        "            lower_loser54,\r\n",
        "            lower_loser55,\r\n",
        "            lower_loser56,\r\n",
        "            lower_loser57,\r\n",
        "            lower_loser58,\r\n",
        "            lower_loser59,\r\n",
        "            lower_loser60,\r\n",
        "            lower_loser61,\r\n",
        "            lower_loser62,\r\n",
        "            lower_loser63,\r\n",
        "            lower_loser64,\r\n",
        "            lower_loser65,\r\n",
        "            lower_loser66,\r\n",
        "            lower_loser67,\r\n",
        "            lower_loser68,\r\n",
        "            lower_loser69,\r\n",
        "            lower_loser70,\r\n",
        "            lower_loser71,\r\n",
        "            lower_loser72,\r\n",
        "            lower_loser73,\r\n",
        "            lower_loser74,\r\n",
        "            lower_loser75,\r\n",
        "            lower_loser76,\r\n",
        "            lower_loser77,\r\n",
        "            lower_loser78,\r\n",
        "            lower_loser79,\r\n",
        "            lower_loser80,\r\n",
        "            lower_loser81,\r\n",
        "            lower_loser82,\r\n",
        "            lower_loser83,\r\n",
        "            lower_loser84,\r\n",
        "            lower_loser85,\r\n",
        "            lower_loser86,\r\n",
        "            lower_loser87,\r\n",
        "            lower_loser88,\r\n",
        "            lower_loser89,\r\n",
        "            lower_loser90,\r\n",
        "            lower_loser91,\r\n",
        "            lower_loser92,\r\n",
        "            lower_loser93,\r\n",
        "            lower_loser94,\r\n",
        "            lower_loser95,\r\n",
        "            lower_loser96,\r\n",
        "            lower_loser97,\r\n",
        "            lower_loser98,\r\n",
        "            lower_loser99,\r\n",
        "            lower_loser100,\r\n",
        "            lower_loser101]\r\n",
        "\r\n",
        "median_loser = [median_loser1,\r\n",
        "            median_loser2,\r\n",
        "            median_loser3,\r\n",
        "            median_loser4,\r\n",
        "            median_loser5,\r\n",
        "            median_loser6,\r\n",
        "            median_loser7,\r\n",
        "            median_loser8,\r\n",
        "            median_loser9,\r\n",
        "            median_loser10,\r\n",
        "            median_loser11,\r\n",
        "            median_loser12,\r\n",
        "            median_loser13,\r\n",
        "            median_loser14,\r\n",
        "            median_loser15,\r\n",
        "            median_loser16,\r\n",
        "            median_loser17,\r\n",
        "            median_loser18,\r\n",
        "            median_loser19,\r\n",
        "            median_loser20,\r\n",
        "            median_loser21,\r\n",
        "            median_loser22,\r\n",
        "            median_loser23,\r\n",
        "            median_loser24,\r\n",
        "            median_loser25,\r\n",
        "            median_loser26,\r\n",
        "            median_loser27,\r\n",
        "            median_loser28,\r\n",
        "            median_loser29,\r\n",
        "            median_loser30,\r\n",
        "            median_loser31,\r\n",
        "            median_loser32,\r\n",
        "            median_loser33,\r\n",
        "            median_loser34,\r\n",
        "            median_loser35,\r\n",
        "            median_loser36,\r\n",
        "            median_loser37,\r\n",
        "            median_loser38,\r\n",
        "            median_loser39,\r\n",
        "            median_loser40,\r\n",
        "            median_loser41,\r\n",
        "            median_loser42,\r\n",
        "            median_loser43,\r\n",
        "            median_loser44,\r\n",
        "            median_loser45,\r\n",
        "            median_loser46,\r\n",
        "            median_loser47,\r\n",
        "            median_loser48,\r\n",
        "            median_loser49,\r\n",
        "            median_loser50,\r\n",
        "            median_loser51,\r\n",
        "            median_loser52,\r\n",
        "            median_loser53,\r\n",
        "            median_loser54,\r\n",
        "            median_loser55,\r\n",
        "            median_loser56,\r\n",
        "            median_loser57,\r\n",
        "            median_loser58,\r\n",
        "            median_loser59,\r\n",
        "            median_loser60,\r\n",
        "            median_loser61,\r\n",
        "            median_loser62,\r\n",
        "            median_loser63,\r\n",
        "            median_loser64,\r\n",
        "            median_loser65,\r\n",
        "            median_loser66,\r\n",
        "            median_loser67,\r\n",
        "            median_loser68,\r\n",
        "            median_loser69,\r\n",
        "            median_loser70,\r\n",
        "            median_loser71,\r\n",
        "            median_loser72,\r\n",
        "            median_loser73,\r\n",
        "            median_loser74,\r\n",
        "            median_loser75,\r\n",
        "            median_loser76,\r\n",
        "            median_loser77,\r\n",
        "            median_loser78,\r\n",
        "            median_loser79,\r\n",
        "            median_loser80,\r\n",
        "            median_loser81,\r\n",
        "            median_loser82,\r\n",
        "            median_loser83,\r\n",
        "            median_loser84,\r\n",
        "            median_loser85,\r\n",
        "            median_loser86,\r\n",
        "            median_loser87,\r\n",
        "            median_loser88,\r\n",
        "            median_loser89,\r\n",
        "            median_loser90,\r\n",
        "            median_loser91,\r\n",
        "            median_loser92,\r\n",
        "            median_loser93,\r\n",
        "            median_loser94,\r\n",
        "            median_loser95,\r\n",
        "            median_loser96,\r\n",
        "            median_loser97,\r\n",
        "            median_loser98,\r\n",
        "            median_loser99,\r\n",
        "            median_loser100,\r\n",
        "            median_loser101]\r\n",
        "\r\n",
        "upper_loser = [upper_loser1,\r\n",
        "            upper_loser2,\r\n",
        "            upper_loser3,\r\n",
        "            upper_loser4,\r\n",
        "            upper_loser5,\r\n",
        "            upper_loser6,\r\n",
        "            upper_loser7,\r\n",
        "            upper_loser8,\r\n",
        "            upper_loser9,\r\n",
        "            upper_loser10,\r\n",
        "            upper_loser11,\r\n",
        "            upper_loser12,\r\n",
        "            upper_loser13,\r\n",
        "            upper_loser14,\r\n",
        "            upper_loser15,\r\n",
        "            upper_loser16,\r\n",
        "            upper_loser17,\r\n",
        "            upper_loser18,\r\n",
        "            upper_loser19,\r\n",
        "            upper_loser20,\r\n",
        "            upper_loser21,\r\n",
        "            upper_loser22,\r\n",
        "            upper_loser23,\r\n",
        "            upper_loser24,\r\n",
        "            upper_loser25,\r\n",
        "            upper_loser26,\r\n",
        "            upper_loser27,\r\n",
        "            upper_loser28,\r\n",
        "            upper_loser29,\r\n",
        "            upper_loser30,\r\n",
        "            upper_loser31,\r\n",
        "            upper_loser32,\r\n",
        "            upper_loser33,\r\n",
        "            upper_loser34,\r\n",
        "            upper_loser35,\r\n",
        "            upper_loser36,\r\n",
        "            upper_loser37,\r\n",
        "            upper_loser38,\r\n",
        "            upper_loser39,\r\n",
        "            upper_loser40,\r\n",
        "            upper_loser41,\r\n",
        "            upper_loser42,\r\n",
        "            upper_loser43,\r\n",
        "            upper_loser44,\r\n",
        "            upper_loser45,\r\n",
        "            upper_loser46,\r\n",
        "            upper_loser47,\r\n",
        "            upper_loser48,\r\n",
        "            upper_loser49,\r\n",
        "            upper_loser50,\r\n",
        "            upper_loser51,\r\n",
        "            upper_loser52,\r\n",
        "            upper_loser53,\r\n",
        "            upper_loser54,\r\n",
        "            upper_loser55,\r\n",
        "            upper_loser56,\r\n",
        "            upper_loser57,\r\n",
        "            upper_loser58,\r\n",
        "            upper_loser59,\r\n",
        "            upper_loser60,\r\n",
        "            upper_loser61,\r\n",
        "            upper_loser62,\r\n",
        "            upper_loser63,\r\n",
        "            upper_loser64,\r\n",
        "            upper_loser65,\r\n",
        "            upper_loser66,\r\n",
        "            upper_loser67,\r\n",
        "            upper_loser68,\r\n",
        "            upper_loser69,\r\n",
        "            upper_loser70,\r\n",
        "            upper_loser71,\r\n",
        "            upper_loser72,\r\n",
        "            upper_loser73,\r\n",
        "            upper_loser74,\r\n",
        "            upper_loser75,\r\n",
        "            upper_loser76,\r\n",
        "            upper_loser77,\r\n",
        "            upper_loser78,\r\n",
        "            upper_loser79,\r\n",
        "            upper_loser80,\r\n",
        "            upper_loser81,\r\n",
        "            upper_loser82,\r\n",
        "            upper_loser83,\r\n",
        "            upper_loser84,\r\n",
        "            upper_loser85,\r\n",
        "            upper_loser86,\r\n",
        "            upper_loser87,\r\n",
        "            upper_loser88,\r\n",
        "            upper_loser89,\r\n",
        "            upper_loser90,\r\n",
        "            upper_loser91,\r\n",
        "            upper_loser92,\r\n",
        "            upper_loser93,\r\n",
        "            upper_loser94,\r\n",
        "            upper_loser95,\r\n",
        "            upper_loser96,\r\n",
        "            upper_loser97,\r\n",
        "            upper_loser98,\r\n",
        "            upper_loser99,\r\n",
        "            upper_loser100,\r\n",
        "            upper_loser101]"
      ],
      "execution_count": 179,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u19FndLT2oU3"
      },
      "source": [
        "### Summarize arrays: 'Winner'\r\n",
        "\r\n",
        "lower_winner = [lower_winner1,\r\n",
        "            lower_winner2,\r\n",
        "            lower_winner3,\r\n",
        "            lower_winner4,\r\n",
        "            lower_winner5,\r\n",
        "            lower_winner6,\r\n",
        "            lower_winner7,\r\n",
        "            lower_winner8,\r\n",
        "            lower_winner9,\r\n",
        "            lower_winner10,\r\n",
        "            lower_winner11,\r\n",
        "            lower_winner12,\r\n",
        "            lower_winner13,\r\n",
        "            lower_winner14,\r\n",
        "            lower_winner15,\r\n",
        "            lower_winner16,\r\n",
        "            lower_winner17,\r\n",
        "            lower_winner18,\r\n",
        "            lower_winner19,\r\n",
        "            lower_winner20,\r\n",
        "            lower_winner21,\r\n",
        "            lower_winner22,\r\n",
        "            lower_winner23,\r\n",
        "            lower_winner24,\r\n",
        "            lower_winner25,\r\n",
        "            lower_winner26,\r\n",
        "            lower_winner27,\r\n",
        "            lower_winner28,\r\n",
        "            lower_winner29,\r\n",
        "            lower_winner30,\r\n",
        "            lower_winner31,\r\n",
        "            lower_winner32,\r\n",
        "            lower_winner33,\r\n",
        "            lower_winner34,\r\n",
        "            lower_winner35,\r\n",
        "            lower_winner36,\r\n",
        "            lower_winner37,\r\n",
        "            lower_winner38,\r\n",
        "            lower_winner39,\r\n",
        "            lower_winner40,\r\n",
        "            lower_winner41,\r\n",
        "            lower_winner42,\r\n",
        "            lower_winner43,\r\n",
        "            lower_winner44,\r\n",
        "            lower_winner45,\r\n",
        "            lower_winner46,\r\n",
        "            lower_winner47,\r\n",
        "            lower_winner48,\r\n",
        "            lower_winner49,\r\n",
        "            lower_winner50,\r\n",
        "            lower_winner51,\r\n",
        "            lower_winner52,\r\n",
        "            lower_winner53,\r\n",
        "            lower_winner54,\r\n",
        "            lower_winner55,\r\n",
        "            lower_winner56,\r\n",
        "            lower_winner57,\r\n",
        "            lower_winner58,\r\n",
        "            lower_winner59,\r\n",
        "            lower_winner60,\r\n",
        "            lower_winner61,\r\n",
        "            lower_winner62,\r\n",
        "            lower_winner63,\r\n",
        "            lower_winner64,\r\n",
        "            lower_winner65,\r\n",
        "            lower_winner66,\r\n",
        "            lower_winner67,\r\n",
        "            lower_winner68,\r\n",
        "            lower_winner69,\r\n",
        "            lower_winner70,\r\n",
        "            lower_winner71,\r\n",
        "            lower_winner72,\r\n",
        "            lower_winner73,\r\n",
        "            lower_winner74,\r\n",
        "            lower_winner75,\r\n",
        "            lower_winner76,\r\n",
        "            lower_winner77,\r\n",
        "            lower_winner78,\r\n",
        "            lower_winner79,\r\n",
        "            lower_winner80,\r\n",
        "            lower_winner81,\r\n",
        "            lower_winner82,\r\n",
        "            lower_winner83,\r\n",
        "            lower_winner84,\r\n",
        "            lower_winner85,\r\n",
        "            lower_winner86,\r\n",
        "            lower_winner87,\r\n",
        "            lower_winner88,\r\n",
        "            lower_winner89,\r\n",
        "            lower_winner90,\r\n",
        "            lower_winner91,\r\n",
        "            lower_winner92,\r\n",
        "            lower_winner93,\r\n",
        "            lower_winner94,\r\n",
        "            lower_winner95,\r\n",
        "            lower_winner96,\r\n",
        "            lower_winner97,\r\n",
        "            lower_winner98,\r\n",
        "            lower_winner99,\r\n",
        "            lower_winner100,\r\n",
        "            lower_winner101]\r\n",
        "\r\n",
        "median_winner = [median_winner1,\r\n",
        "            median_winner2,\r\n",
        "            median_winner3,\r\n",
        "            median_winner4,\r\n",
        "            median_winner5,\r\n",
        "            median_winner6,\r\n",
        "            median_winner7,\r\n",
        "            median_winner8,\r\n",
        "            median_winner9,\r\n",
        "            median_winner10,\r\n",
        "            median_winner11,\r\n",
        "            median_winner12,\r\n",
        "            median_winner13,\r\n",
        "            median_winner14,\r\n",
        "            median_winner15,\r\n",
        "            median_winner16,\r\n",
        "            median_winner17,\r\n",
        "            median_winner18,\r\n",
        "            median_winner19,\r\n",
        "            median_winner20,\r\n",
        "            median_winner21,\r\n",
        "            median_winner22,\r\n",
        "            median_winner23,\r\n",
        "            median_winner24,\r\n",
        "            median_winner25,\r\n",
        "            median_winner26,\r\n",
        "            median_winner27,\r\n",
        "            median_winner28,\r\n",
        "            median_winner29,\r\n",
        "            median_winner30,\r\n",
        "            median_winner31,\r\n",
        "            median_winner32,\r\n",
        "            median_winner33,\r\n",
        "            median_winner34,\r\n",
        "            median_winner35,\r\n",
        "            median_winner36,\r\n",
        "            median_winner37,\r\n",
        "            median_winner38,\r\n",
        "            median_winner39,\r\n",
        "            median_winner40,\r\n",
        "            median_winner41,\r\n",
        "            median_winner42,\r\n",
        "            median_winner43,\r\n",
        "            median_winner44,\r\n",
        "            median_winner45,\r\n",
        "            median_winner46,\r\n",
        "            median_winner47,\r\n",
        "            median_winner48,\r\n",
        "            median_winner49,\r\n",
        "            median_winner50,\r\n",
        "            median_winner51,\r\n",
        "            median_winner52,\r\n",
        "            median_winner53,\r\n",
        "            median_winner54,\r\n",
        "            median_winner55,\r\n",
        "            median_winner56,\r\n",
        "            median_winner57,\r\n",
        "            median_winner58,\r\n",
        "            median_winner59,\r\n",
        "            median_winner60,\r\n",
        "            median_winner61,\r\n",
        "            median_winner62,\r\n",
        "            median_winner63,\r\n",
        "            median_winner64,\r\n",
        "            median_winner65,\r\n",
        "            median_winner66,\r\n",
        "            median_winner67,\r\n",
        "            median_winner68,\r\n",
        "            median_winner69,\r\n",
        "            median_winner70,\r\n",
        "            median_winner71,\r\n",
        "            median_winner72,\r\n",
        "            median_winner73,\r\n",
        "            median_winner74,\r\n",
        "            median_winner75,\r\n",
        "            median_winner76,\r\n",
        "            median_winner77,\r\n",
        "            median_winner78,\r\n",
        "            median_winner79,\r\n",
        "            median_winner80,\r\n",
        "            median_winner81,\r\n",
        "            median_winner82,\r\n",
        "            median_winner83,\r\n",
        "            median_winner84,\r\n",
        "            median_winner85,\r\n",
        "            median_winner86,\r\n",
        "            median_winner87,\r\n",
        "            median_winner88,\r\n",
        "            median_winner89,\r\n",
        "            median_winner90,\r\n",
        "            median_winner91,\r\n",
        "            median_winner92,\r\n",
        "            median_winner93,\r\n",
        "            median_winner94,\r\n",
        "            median_winner95,\r\n",
        "            median_winner96,\r\n",
        "            median_winner97,\r\n",
        "            median_winner98,\r\n",
        "            median_winner99,\r\n",
        "            median_winner100,\r\n",
        "            median_winner101]\r\n",
        "\r\n",
        "upper_winner = [upper_winner1,\r\n",
        "            upper_winner2,\r\n",
        "            upper_winner3,\r\n",
        "            upper_winner4,\r\n",
        "            upper_winner5,\r\n",
        "            upper_winner6,\r\n",
        "            upper_winner7,\r\n",
        "            upper_winner8,\r\n",
        "            upper_winner9,\r\n",
        "            upper_winner10,\r\n",
        "            upper_winner11,\r\n",
        "            upper_winner12,\r\n",
        "            upper_winner13,\r\n",
        "            upper_winner14,\r\n",
        "            upper_winner15,\r\n",
        "            upper_winner16,\r\n",
        "            upper_winner17,\r\n",
        "            upper_winner18,\r\n",
        "            upper_winner19,\r\n",
        "            upper_winner20,\r\n",
        "            upper_winner21,\r\n",
        "            upper_winner22,\r\n",
        "            upper_winner23,\r\n",
        "            upper_winner24,\r\n",
        "            upper_winner25,\r\n",
        "            upper_winner26,\r\n",
        "            upper_winner27,\r\n",
        "            upper_winner28,\r\n",
        "            upper_winner29,\r\n",
        "            upper_winner30,\r\n",
        "            upper_winner31,\r\n",
        "            upper_winner32,\r\n",
        "            upper_winner33,\r\n",
        "            upper_winner34,\r\n",
        "            upper_winner35,\r\n",
        "            upper_winner36,\r\n",
        "            upper_winner37,\r\n",
        "            upper_winner38,\r\n",
        "            upper_winner39,\r\n",
        "            upper_winner40,\r\n",
        "            upper_winner41,\r\n",
        "            upper_winner42,\r\n",
        "            upper_winner43,\r\n",
        "            upper_winner44,\r\n",
        "            upper_winner45,\r\n",
        "            upper_winner46,\r\n",
        "            upper_winner47,\r\n",
        "            upper_winner48,\r\n",
        "            upper_winner49,\r\n",
        "            upper_winner50,\r\n",
        "            upper_winner51,\r\n",
        "            upper_winner52,\r\n",
        "            upper_winner53,\r\n",
        "            upper_winner54,\r\n",
        "            upper_winner55,\r\n",
        "            upper_winner56,\r\n",
        "            upper_winner57,\r\n",
        "            upper_winner58,\r\n",
        "            upper_winner59,\r\n",
        "            upper_winner60,\r\n",
        "            upper_winner61,\r\n",
        "            upper_winner62,\r\n",
        "            upper_winner63,\r\n",
        "            upper_winner64,\r\n",
        "            upper_winner65,\r\n",
        "            upper_winner66,\r\n",
        "            upper_winner67,\r\n",
        "            upper_winner68,\r\n",
        "            upper_winner69,\r\n",
        "            upper_winner70,\r\n",
        "            upper_winner71,\r\n",
        "            upper_winner72,\r\n",
        "            upper_winner73,\r\n",
        "            upper_winner74,\r\n",
        "            upper_winner75,\r\n",
        "            upper_winner76,\r\n",
        "            upper_winner77,\r\n",
        "            upper_winner78,\r\n",
        "            upper_winner79,\r\n",
        "            upper_winner80,\r\n",
        "            upper_winner81,\r\n",
        "            upper_winner82,\r\n",
        "            upper_winner83,\r\n",
        "            upper_winner84,\r\n",
        "            upper_winner85,\r\n",
        "            upper_winner86,\r\n",
        "            upper_winner87,\r\n",
        "            upper_winner88,\r\n",
        "            upper_winner89,\r\n",
        "            upper_winner90,\r\n",
        "            upper_winner91,\r\n",
        "            upper_winner92,\r\n",
        "            upper_winner93,\r\n",
        "            upper_winner94,\r\n",
        "            upper_winner95,\r\n",
        "            upper_winner96,\r\n",
        "            upper_winner97,\r\n",
        "            upper_winner98,\r\n",
        "            upper_winner99,\r\n",
        "            upper_winner100,\r\n",
        "            upper_winner101]"
      ],
      "execution_count": 180,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84XIzmWD2oba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "outputId": "e3b9a85d-04b3-4548-828a-7b2e06418aa8"
      },
      "source": [
        "### Visualize!\r\n",
        "\r\n",
        "title = obj_func\r\n",
        "\r\n",
        "plt.figure()\r\n",
        "\r\n",
        "plt.plot(median_loser, color = 'Green')\r\n",
        "plt.plot(median_winner, color = 'Purple')\r\n",
        "\r\n",
        "xstar = np.arange(0, max_iter+1, step=1)\r\n",
        "plt.fill_between(xstar, lower_loser, upper_loser, facecolor = 'Green', alpha=0.4, label='GP EI Regret IQR: dEI')\r\n",
        "plt.fill_between(xstar, lower_winner, upper_winner, facecolor = 'Purple', alpha=0.4, label='STP CBM Regret IQR: dCBM')\r\n",
        "\r\n",
        "plt.title(title, weight = 'bold', family = 'Arial')\r\n",
        "plt.xlabel('(Post-initialization) iteration $\\it{k}$', weight = 'bold', family = 'Arial') \r\n",
        "plt.ylabel('log(Regret)', weight = 'bold', family = 'Arial') \r\n",
        "plt.legend(loc=1) # add plot legend\r\n",
        "\r\n",
        "plt.show()"
      ],
      "execution_count": 183,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5dkR2Id2oiu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1f001ad-2f60-45b8-ce63-2888fce1874a"
      },
      "source": [
        "time_lose, time_win"
      ],
      "execution_count": 182,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3508.6495599746704, 13830.415667295456)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 182
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNLBF57p2opi"
      },
      "source": [
        ""
      ],
      "execution_count": 182,
      "outputs": []
    }
  ]
}